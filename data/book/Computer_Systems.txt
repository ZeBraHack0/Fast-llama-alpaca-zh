COMPUTER SYSTEMS 
A Programmer¡¯s Perspective 
Bryant O¡¯Hallaron 
Computer Systems 
A Programmer¡¯s Perspective 
This page intentionally left blank 



Computer Systems 
A Programmer¡¯s Perspective 
Randal E. Bryant 
Carnegie Mellon University 

David R. O¡¯Hallaron 
Carnegie Mellon University and Intel Labs 
Prentice Hall 
Boston Columbus Indianapolis New York San Francisco Upper Saddle River Amsterdam Cape Town Dubai London Madrid Milan Munich Paris Montreal Toronto Delhi Mexico City Sao Paulo Sydney Hong Kong Seoul Singapore Taipei Tokyo 
Editorial Director: Marcia Horton Editor-in-Chief: Michael Hirsch Acquisitions Editor: Matt Goldstein Editorial Assistant: Chelsea Bell Director of Marketing: Margaret Waples Marketing Coordinator: Kathryn Ferranti Managing Editor: Jeff Holcomb Senior Manufacturing Buyer: Carol Melville Art Director: Linda Knowles Cover Designer: Elena Sidorova Image Interior Permission Coordinator: Richard Rodrigues Cover Art: . Randal E. Bryant and David R. O¡¯Hallaron Media Producer: Katelyn Boller Project Management and Interior Design: Paul C. Anagnostopoulos, Windfall Software Composition: Joe Snowden, Coventry Composition Printer/Binder: Edwards Brothers Cover Printer: Lehigh-Phoenix Color/Hagerstown 
Copyright . 2011, 2003 by Randal E. Bryant and David R. O¡¯Hallaron. All rights reserved. Manufactured in the United States of America. This publication is protected by Copyright, and permission should be obtained from the publisher prior to any prohibited reproduction, storage in a retrieval system, or transmission in any form or by any means, electronic, mechanical, photocopying, recording, or likewise. To obtain permission(s) to use material from this work, please submit a written request to Pearson Education, Inc., Permissions Department, 501 Boylston Street, Suite 900, Boston, Massachusetts 02116. 
Many of the designations by manufacturers and seller to distinguish their products are claimed as trademarks. Where those designations appear in this book, and the publisher was aware of a trademark claim, the designations have been printed in initial caps or all caps. 
Library of Congress Cataloging-in-Publication Data 
Bryant, Randal. Computer systems : a programmer¡¯s perspective / Randal E. Bryant, David R. 
O¡¯Hallaron.¡ª2nd ed.  
p. cm. Includes bibliographical references and index. ISBN-13: 978-0-13-610804-7 (alk. paper) ISBN-10: 0-13-610804-0 (alk. paper) 1. Computer systems. 2. Computers. 3. Telecomm(Computer systems) I. O¡¯Hallaron, David Richard. QA76.5.B795 2010 004¡ªdc22  unication. 4II. Title.  . User interfaces  
2009053083  
1098765432 1¡ªEB¡ª14 13 12 11 10  



To the students and instructors of the 15-213 course at Carnegie Mellon University, for inspiring us to develop and re.ne the material for this book. 
This page intentionally left blank 




Contents 
Preface 
xix 
About 
the 
Authors 
xxxiii 

1 

A 
Tour 
of 
Computer 
Systems 
1 

1.1 
Information 
Is 
Bits 
+ 
Context 
3 

1.2 
Programs 
Are 
Translated 
by 
Other 
Programs 
into 
Different 
Forms 
4 

1.3 
It 
Pays 
to 
Understand 
How 
Compilation 
Systems 
Work 
6 

1.4 
Processors 
Read 
and 
Interpret 
Instructions 
Stored 
in 
Memory 
7 

1.4.1 
Hardware 
Organization 
of 
a 
System 
7 

1.4.2 
Running 
the 
hello 
Program 
10 

1.5 
Caches 
Matter 
12 

1.6 
Storage 
Devices 
Form 
a 
Hierarchy 
13 

1.7 
The 
Operating 
System 
Manages 
the 
Hardware 
14 
1.7.1 
Processes 
16 
1.7.2 
Threads 
17 

1.7.3 
Virtual 
Memory 
17 
1.7.4 
Files 
19 



1.8 
Systems 
Communicate 
with 
Other 
Systems 
Using 
Networks 
20 

1.9 
Important 
Themes 
21 

1.9.1 
Concurrency 
and 
Parallelism 
21 

1.9.2 
The 
Importance 
of 
Abstractions 
in 
Computer 
Systems 
24 
1.10 
Summary 
25 
Bibliographic 
Notes 
26 

Part 
I 
Program 
Structure 
and 
Execution 

2 





Representing 
and 
Manipulating 
Information 
29 

2.1 
Information 
Storage 
33 

2.1.1 Hexadecimal Notation 34 2.1.2 Words 38 
2.1.3 Data Sizes 38 
2.1.4 Addressing and Byte Ordering 39 
2.1.5 Representing Strings 46 
2.1.6 Representing Code 47 
2.1.7 Introduction to Boolean Algebra 48 
2.1.8 Bit-Level Operations in C 51 
2.1.9 Logical Operations in C 54 
2.1.10 Shift Operations in C 54 



2.2 
Integer 
Representations 
56 

2.2.1 Integral Data Types 57 
2.2.2 Unsigned Encodings 58 
2.2.3 Two¡¯s-Complement Encodings 60 
2.2.4 Conversions Between Signed and Unsigned 65 
2.2.5 Signed vs. Unsigned in C 69 
2.2.6 Expanding the Bit Representation of a Number 71 
2.2.7 Truncating Numbers 75 
2.2.8 Advice on Signed vs. Unsigned 76 

2.3 
Integer 
Arithmetic 
79 

2.3.1 Unsigned Addition 79 
2.3.2 Two¡¯s-Complement Addition 83 
2.3.3 Two¡¯s-Complement Negation 87 
2.3.4 Unsigned Multiplication 88 
2.3.5 Two¡¯s-Complement Multiplication 89 
2.3.6 Multiplying by Constants 92 
2.3.7 Dividing by Powers of Two 95 
2.3.8 Final Thoughts on Integer Arithmetic 98 

2.4 
Floating 
Point 
99 

2.4.1 Fractional Binary Numbers 100 
2.4.2 IEEE Floating-Point Representation 103 
2.4.3 Example Numbers 105 2.4.4 Rounding 110 
2.4.5 Floating-Point Operations 113 
2.4.6 Floating Point in C 114 2.5 
Summary 
118 
Bibliographic 
Notes 
119 
Homework 
Problems 
119 
Solutions 
to 
Practice 
Problems 
134 

3 



Machine-Level 
Representation 
of 
Programs 
153 


3.1 
A 
Historical 
Perspective 
156 

3.2 
Program 
Encodings 
159 


3.2.1 Machine-Level Code 160 
3.2.2 Code Examples 162 
3.2.3 Notes on Formatting 165 

3.3 
Data 
Formats 
167 

3.4 
Accessing 
Information 
168 

3.4.1 Operand Speci.ers 169 
3.4.2 Data Movement Instructions 171 
3.4.3 Data Movement Example 174 

3.5 
Arithmetic 
and 
Logical 
Operations 
177 

3.5.1 Load Effective Address 177 
3.5.2 Unary and Binary Operations 178 
3.5.3 Shift Operations 179 3.5.4 Discussion 180 
3.5.5 Special Arithmetic Operations 182 3.6 
Control 
185 

3.6.1 Condition Codes 185 
3.6.2 Accessing the Condition Codes 187 
3.6.3 Jump Instructions and Their Encodings 189 
3.6.4 Translating Conditional Branches 193 3.6.5 Loops 197 
3.6.6 Conditional Move Instructions 206 

3.6.7 Switch Statements 213 3.7 
Procedures 
219 

3.7.1 Stack Frame Structure 219 
3.7.2 Transferring Control 221 
3.7.3 Register Usage Conventions 223 
3.7.4 Procedure Example 224 
3.7.5 Recursive Procedures 229 


3.8 
Array 
Allocation 
and 
Access 
232 

3.8.1 Basic Principles 232 
3.8.2 Pointer Arithmetic 233 
3.8.3 Nested Arrays 235 
3.8.4 Fixed-Size Arrays 237 
3.8.5 Variable-Size Arrays 238 
3.9 
Heterogeneous 
Data 
Structures 
241 
3.9.1 Structures 241 3.9.2 Unions 244 

3.9.3 Data Alignment 248 


3.10 
Putting 
It 
Together: 
Understanding 
Pointers 
252 

3.11 
Life 
in 
the 
Real 
World: 
Using 
the 
gdb 
Debugger 
254 

3.12 
Out-of-Bounds 
Memory 
References 
and 
Buffer 
Over.ow 
256 

3.12.1 Thwarting Buffer Over.ow Attacks 261 
Contents 



3.13 
x86-64: 
Extending 
IA32 
to 
64 
Bits 
267 

3.13.1 History and Motivation for x86-64 268 
3.13.2 An Overview of x86-64 270 
3.13.3 Accessing Information 273 3.13.4 Control 279 
3.13.5 Data Structures 290 
3.13.6 Concluding Observations about x86-64 291 
3.14 
Machine-Level 
Representations 
of 
Floating-Point 
Programs 
292 
3.15 
Summary 
293 
Bibliographic 
Notes 
294 
Homework 
Problems 
294 
Solutions 
to 
Practice 
Problems 
308 

4 


Processor 
Architecture 
333 

4.1 
The 
Y86 
Instruction 
Set 
Architecture 
336 

4.1.1 Programmer-Visible State 336 4.1.2 Y86 Instructions 337 
4.1.3 Instruction Encoding 339 4.1.4 Y86 Exceptions 344 4.1.5 Y86 Programs 345 
4.1.6 Some Y86 Instruction Details 350 

4.2 
Logic 
Design 
and 
the 
Hardware 
Control 
Language 
HCL 
352 

4.2.1 Logic Gates 353 
4.2.2 Combinational Circuits and HCL Boolean Expressions 354 
4.2.3 Word-Level Combinational Circuits and HCL Integer Expressions 355 
4.2.4 Set Membership 360 
4.2.5 Memory and Clocking 361 4.3 
Sequential 
Y86 
Implementations 
364 

4.3.1 Organizing Processing into Stages 364 
4.3.2 SEQ Hardware Structure 375 
4.3.3 SEQ Timing 379 
4.3.4 SEQ Stage Implementations 383 



4.4 
General 
Principles 
of 
Pipelining 
391 

4.4.1 Computational Pipelines 392 
4.4.2 A Detailed Look at Pipeline Operation 393 
4.4.3 Limitations of Pipelining 394 

4.4.4 Pipelining a System with Feedback 398 4.5 
Pipelined 
Y86 
Implementations 
400 

4.5.1 SEQ+: Rearranging the Computation Stages 400 

4.5.2 Inserting Pipeline Registers 401 
4.5.3 Rearranging and Relabeling Signals 405 
4.5.4 Next PC Prediction 406 
4.5.5 Pipeline Hazards 408 
4.5.6 Avoiding Data Hazards by Stalling 413 
4.5.7 Avoiding Data Hazards by Forwarding 415 
4.5.8 Load/Use Data Hazards 418 
4.5.9 Exception Handling 420 
4.5.10 PIPE Stage Implementations 423 
4.5.11 Pipeline Control Logic 431 
4.5.12 Performance Analysis 444 
4.5.13 Un.nished Business 446 4.6 
Summary 
449 
4.6.1 Y86 Simulators 450 Bibliographic 
Notes 
451 
Homework 
Problems 
451 
Solutions 
to 
Practice 
Problems 
457 

5 




Optimizing 
Program 
Performance 
473 

5.1 
Capabilities 
and 
Limitations 
of 
Optimizing 
Compilers 
476 

5.2 
Expressing 
Program 
Performance 
480 

5.3 
Program 
Example 
482 

5.4 
Eliminating 
Loop 
Inef.ciencies 
486 

5.5 
Reducing 
Procedure 
Calls 
490 

5.6 
Eliminating 
Unneeded 
Memory 
References 
491 

5.7 
Understanding 
Modern 
Processors 
496 

5.7.1 Overall Operation 497 
5.7.2 Functional Unit Performance 500 
5.7.3 An Abstract Model of Processor Operation 502 
5.8 
Loop 
Unrolling 
509 

5.9 
Enhancing 
Parallelism 
513 

5.9.1 Multiple Accumulators 514 
5.9.2 Reassociation Transformation 518 
5.10 
Summary 
of 
Results 
for 
Optimizing 
Combining 
Code 
524 

5.11 
Some 
Limiting 
Factors 
525 

5.11.1 Register Spilling 525 
5.11.2 Branch Prediction and Misprediction Penalties 526 
5.12 
Understanding 
Memory 
Performance 
531 

5.12.1 Load Performance 531 
5.12.2 Store Performance 532 
5.13 
Life 
in 
the 
Real 
World: 
Performance 
Improvement 
Techniques 
539 

5.14 
Identifying 
and 
Eliminating 
Performance 
Bottlenecks 
540 

5.14.1 Program Pro.ling 540 
5.14.2 Using a Pro.ler to Guide Optimization 542 
5.14.3 Amdahl¡¯s Law 545 5.15 
Summary 
547 
Bibliographic 
Notes 
548 
Homework 
Problems 
549 
Solutions 
to 
Practice 
Problems 
552 

6 



The 
Memory 
Hierarchy 
559 

6.1 
Storage 
Technologies 
561 

6.1.1 Random-Access Memory 561 
6.1.2 Disk Storage 570 
6.1.3 Solid State Disks 581 
6.1.4 Storage Technology Trends 583 6.2 
Locality 
586 

6.2.1 Locality of References to Program Data 587 
6.2.2 Locality of Instruction Fetches 588 
6.2.3 Summary of Locality 589 


6.3 
The 
Memory 
Hierarchy 
591 

6.3.1 Caching in the Memory Hierarchy 592 
6.3.2 Summary of Memory Hierarchy Concepts 595 

6.4 
Cache 
Memories 
596 

6.4.1 Generic Cache Memory Organization 597 
6.4.2 Direct-Mapped Caches 599 
6.4.3 Set Associative Caches 606 
6.4.4 Fully Associative Caches 608 
6.4.5 Issues with Writes 611 
6.4.6 Anatomy of a Real Cache Hierarchy 612 
6.4.7 Performance Impact of Cache Parameters 614 


6.5 
Writing 
Cache-friendly 
Code 
615 

6.6 
Putting 
It 
Together: 
The 
Impact 
of 
Caches 
on 
Program 
Performance 
620 

6.6.1 The Memory Mountain 621 
6.6.2 Rearranging Loops to Increase Spatial Locality 625 
6.6.3 Exploiting Locality in Your Programs 629 6.7 
Summary 
629 
Bibliographic 
Notes 
630 
Homework 
Problems 
631 
Solutions 
to 
Practice 
Problems 
642 


Part 
II 
Running 
Programs 
on 
a 
System 

7 
Linking 
653 


7.1 
Compiler 
Drivers 
655 

7.2 
Static 
Linking 
657 

7.3 
Object 
Files 
657 

7.4 
Relocatable 
Object 
Files 
658 

7.5 
Symbols 
and 
Symbol 
Tables 
660 

7.6 
Symbol 
Resolution 
663 

7.6.1 How Linkers Resolve Multiply De.ned Global Symbols 664 
7.6.2 Linking with Static Libraries 667 
7.6.3 How Linkers Use Static Libraries to Resolve References 670 7.7 
Relocation 
672 

7.7.1 Relocation Entries 672 
7.7.2 Relocating Symbol References 673 
7.8 
Executable 
Object 
Files 
678 

7.9 
Loading 
Executable 
Object 
Files 
679 

7.10 
Dynamic 
Linking 
with 
Shared 
Libraries 
681 

7.11 
Loading 
and 
Linking 
Shared 
Libraries 
from 
Applications 
683 

7.12 
Position-Independent 
Code 
(PIC) 
687 

7.13 
Tools 
for 
Manipulating 
Object 
Files 
690 
7.14 
Summary 
691 
Bibliographic 
Notes 
691 
Homework 
Problems 
692 
Solutions 
to 
Practice 
Problems 
698 

8 

Exceptional 
Control 
Flow 
701 

8.1 
Exceptions 
703 

8.1.1 Exception Handling 704 
8.1.2 Classes of Exceptions 706 
8.1.3 Exceptions in Linux/IA32 Systems 708 8.2 
Processes 
712 

8.2.1 Logical Control Flow 712 
8.2.2 Concurrent Flows 713 
8.2.3 Private Address Space 714 
8.2.4 User and Kernel Modes 714 
8.2.5 Context Switches 716 8.3 
System 
Call 
Error 
Handling 
717 

8.4 
Process 
Control 
718 

8.4.1 Obtaining Process IDs 719 
8.4.2 Creating and Terminating Processes 719 
8.4.3 Reaping Child Processes 723 
8.4.4 Putting Processes to Sleep 729 
8.4.5 Loading and Running Programs 730 
8.4.6 Using fork and execve to Run Programs 733 8.5 
Signals 
736 

8.5.1 Signal Terminology 738 
8.5.2 Sending Signals 739 
8.5.3 Receiving Signals 742 
8.5.4 Signal Handling Issues 745 
8.5.5 Portable Signal Handling 752 
8.5.6 Explicitly Blocking and Unblocking Signals 753 
8.5.7 Synchronizing Flows to Avoid Nasty Concurrency Bugs 755 
8.6 
Nonlocal 
Jumps 
759 

8.7 
Tools 
for 
Manipulating 
Processes 
762 
8.8 
Summary 
763 
Bibliographic 
Notes 
763 
Homework 
Problems 
764 
Solutions 
to 
Practice 
Problems 
771 

9 
Virtual 
Memory 
775 

9.1 
Physical 
and 
Virtual 
Addressing 
777 

9.2 
Address 
Spaces 
778 

9.3 
VM 
as 
a 
Tool 
for 
Caching 
779 

9.3.1 DRAM Cache Organization 780 
9.3.2 Page Tables 780 
9.3.3 Page Hits 782 
9.3.4 Page Faults 782 
9.3.5 Allocating Pages 783 
9.3.6 Locality to the Rescue Again 784 
9.4 
VM 
as 
a 
Tool 
for 
Memory 
Management 
785 

9.5 
VM 
as 
a 
Tool 
for 
Memory 
Protection 
786 

9.6 
Address 
Translation 
787 

9.6.1 Integrating Caches and VM 791 
9.6.2 Speeding up Address Translation with a TLB 791 
9.6.3 Multi-Level Page Tables 792 
9.6.4 Putting It Together: End-to-end Address Translation 794 
9.7 
Case 
Study: 
The 
Intel 
Core 
i7/Linux 
Memory 
System 
799 


9.7.1 Core i7 Address Translation 800 
9.7.2 Linux Virtual Memory System 803 
9.8 
Memory 
Mapping 
807 

9.8.1 Shared Objects Revisited 807 
9.8.2 The fork Function Revisited 809 
9.8.3 The execve Function Revisited 810 
9.8.4 User-level Memory Mapping with the mmap Function 810 
9.9 
Dynamic 
Memory 
Allocation 
812 

9.9.1 The malloc and free Functions 814 
9.9.2 Why Dynamic Memory Allocation? 816 
9.9.3 Allocator Requirements and Goals 817 9.9.4 Fragmentation 819 
9.9.5 Implementation Issues 820 
9.9.6 Implicit Free Lists 820 
9.9.7 Placing Allocated Blocks 822 
9.9.8 Splitting Free Blocks 823 
9.9.9 Getting Additional Heap Memory 823 
9.9.10 Coalescing Free Blocks 824 
9.9.11 Coalescing with Boundary Tags 824 
9.9.12 Putting It Together: Implementing a Simple Allocator 827 
9.9.13 Explicit Free Lists 835 
9.9.14 Segregated Free Lists 836 
9.10 
Garbage 
Collection 
838 

9.10.1 Garbage Collector Basics 839 
9.10.2 Mark&Sweep Garbage Collectors 840 
9.10.3 Conservative Mark&Sweep for C Programs 842 
9.11 
Common 
Memory-Related 
Bugs 
in 
C 
Programs 
843 

9.11.1 Dereferencing Bad Pointers 843 
9.11.2 Reading Uninitialized Memory 843 
9.11.3 Allowing Stack Buffer Over.ows 844 
9.11.4 Assuming that Pointers and the Objects They Point to Are the Same Size 844 
9.11.5 Making Off-by-One Errors 845 
9.11.6 Referencing a Pointer Instead of the Object It Points to 845 
9.11.7 Misunderstanding Pointer Arithmetic 846 
9.11.8 Referencing Nonexistent Variables 846 
9.11.9 Referencing Data in Free Heap Blocks 847 
9.11.10 Introducing Memory Leaks 847 9.12 
Summary 
848 
Bibliographic 
Notes 
848 
Homework 
Problems 
849 
Solutions 
to 
Practice 
Problems 
853 

Part 
III 
Interaction 
and 
Communication 
Between 
Programs 

10 
System-Level 
I/O 
861 

10.1 
Unix 
I/O 
862 

10.2 
Opening 
and 
Closing 
Files 
863 

10.3 
Reading 
and 
Writing 
Files 
865 

10.4 
Robust 
Reading 
and 
Writing 
with 
the 
Rio 
Package 
867 

10.4.1 Rio Unbuffered Input and Output Functions 867 
10.4.2 Rio Buffered Input Functions 868 
10.5 
Reading 
File 
Metadata 
873 

10.6 
Sharing 
Files 
875 

10.7 
I/O 
Redirection 
877 

10.8 
Standard 
I/O 
879 

10.9 
Putting 
It 
Together: 
Which 
I/O 
Functions 
Should 
I 
Use? 
880 
10.10 
Summary 
881 
Bibliographic 
Notes 
882 
Homework 
Problems 
882 
Solutions 
to 
Practice 
Problems 
883 

11 
Network 
Programming 
885 

11.1 
The 
Client-Server 
Programming 
Model 
886 
11.2 
Networks 
887 

11.3 
The 
Global 
IP 
Internet 
891 

11.3.1 IP Addresses 893 
11.3.2 Internet Domain Names 895 
11.3.3 Internet Connections 899 
11.4 
The 
Sockets 
Interface 
900 

11.4.1 Socket Address Structures 901 
11.4.2 The socket Function 902 
11.4.3 The connect Function 903 
11.4.4 The open_clientfd Function 903 
11.4.5 The bind Function 904 
11.4.6 The listen Function 905 
11.4.7 The open_listenfd Function 905 
11.4.8 The accept Function 907 
11.4.9 Example Echo Client and Server 908 

11.5 
Web 
Servers 
911 

11.5.1 Web Basics 911 
11.5.2 Web Content 912 
11.5.3 HTTP Transactions 914 
11.5.4 Serving Dynamic Content 916 
11.6 
Putting 
It 
Together: 
The 
Tiny 
Web 
Server 
919 
11.7 
Summary 
927 
Bibliographic 
Notes 
928 
Homework 
Problems 
928 
Solutions 
to 
Practice 
Problems 
929 

12 

Concurrent 
Programming 
933 

12.1 
Concurrent 
Programming 
with 
Processes 
935 

12.1.1 A Concurrent Server Based on Processes 936 
12.1.2 Pros and Cons of Processes 937 
12.2 
Concurrent 
Programming 
with 
I/O 
Multiplexing 
939 

12.2.1 A Concurrent Event-Driven Server Based on I/O Multiplexing 942 
12.2.2 Pros and Cons of I/O Multiplexing 946 
12.3 
Concurrent 
Programming 
with 
Threads 
947 

12.3.1 Thread Execution Model 948 
12.3.2 Posix Threads 948 
12.3.3 Creating Threads 950 
12.3.4 Terminating Threads 950 
12.3.5 Reaping Terminated Threads 951 
12.3.6 Detaching Threads 951 
12.3.7 Initializing Threads 952 
12.3.8 A Concurrent Server Based on Threads 952 
12.4 
Shared 
Variables 
in 
Threaded 
Programs 
954 

12.4.1 Threads Memory Model 955 
12.4.2 Mapping Variables to Memory 956 
12.4.3 Shared Variables 956 
12.5 
Synchronizing 
Threads 
with 
Semaphores 
957 

12.5.1 Progress Graphs 960 12.5.2 Semaphores 963 
12.5.3 Using Semaphores for Mutual Exclusion 964 
12.5.4 Using Semaphores to Schedule Shared Resources 966 
12.5.5 Putting It Together: A Concurrent Server Based on Prethreading 970 
12.6 
Using 
Threads 
for 
Parallelism 
974 
12.7 
Other 
Concurrency 
Issues 
979 

12.7.1 Thread Safety 979 12.7.2 Reentrancy 980 
12.7.3 Using Existing Library Functions in Threaded Programs 982 12.7.4 Races 983 12.7.5 Deadlocks 985 12.8 
Summary 
988 
Bibliographic 
Notes 
989 
Homework 
Problems 
989 
Solutions 
to 
Practice 
Problems 
994 

A 
Error 
Handling 
999 



A.1 
Error 
Handling 
in 
Unix 
Systems 
1000 

A.2 
Error-Handling 
Wrappers 
1001 
References 
1005 
Index 
1011 

Preface 


This book (CS:APP) is for computer scientists, computer engineers, and others who want to be able to write better programs by learning what is going on ¡°under the hood¡± of a computer system. 
Our aim is to explain the enduring concepts underlying all computer systems, and to show you the concrete ways that these ideas affect the correctness, perfor-mance, and utility of your application programs. Other systems books are written from a builder¡¯s perspective, describing how to implement the hardware or the sys-tems software, including the operating system, compiler, and network interface. This book is written from a programmer¡¯s perspective, describing how application programmers can use their knowledge of a system to write better programs. Of course, learning what a system is supposed to do provides a good .rst step in learn-ing how to build one, and so this book also serves as a valuable introduction to those who go on to implement systems hardware and software. 
If you study and learn the concepts in this book, you will be on your way to becoming the rare ¡°power programmer¡± who knows how things work and how to .x them when they break. Our aim is to present the fundamental concepts in ways that you will .nd useful right away. You will also be prepared to delve deeper, studying such topics as compilers, computer architecture, operating systems, em-bedded systems, and networking. 
Assumptions about the Reader¡¯s Background 
The presentation of machine code in the book is based on two related formats supported by Intel and its competitors, colloquially known as ¡°x86.¡± IA32 is the machine code that has become the de facto standard for a wide range of systems. x86-64 is an extension of IA32 to enable programs to operate on larger data and to reference a wider range of memory addresses. Since x86-64 systems are able to run IA32 code, both of these forms of machine code will see widespread use for the foreseeable future. We consider how these machines execute C programs on Unix or Unix-like (such as Linux) operating systems. (To simplify our presentation, we will use the term ¡°Unix¡± as an umbrella term for systems having Unix as their heritage, including Solaris, Mac OS, and Linux.) The text contains numerous programming examples that have been compiled and run on Linux systems. We assume that you have access to such a machine and are able to log in and do simple things such as changing directories. 
If your computer runs Microsoft Windows, you have two choices. First, you can get a copy 
of 
Linux 
(www.ubuntu.com) 
and 
install 
it 
as a ¡°dual boot¡± option, so that your machine can run either operating system. Alternatively, by installing a 
copy 
of 
the 
Cygwin 
tools 
(www.cygwin.com), you can run a Unix-like shell under Windows and have an environment very close to that provided by Linux. Not all features of Linux are available under Cygwin, however. 
We also assume that you have some familiarity with C or C++. If your only prior experience is with Java, the transition will require more effort on your part, but we will help you. Java and C share similar syntax and control statements. However, there are aspects of C, particularly pointers, explicit dynamic memory allocation, and formatted I/O, that do not exist in Java. Fortunately, C is a small language, and it is clearly and beautifully described in the classic ¡°K&R¡± text by Brian Kernighan and Dennis Ritchie [58]. Regardless of your programming background, consider K&R an essential part of your personal systems library. 
Several of the early chapters in the book explore the interactions between C programs and their machine-language counterparts. The machine-language examples were all generated by the GNU gcc compiler running on IA32 and x86-64 processors. We do not assume any prior experience with hardware, machine language, or assembly-language programming. 

New to C? Advice on the C programming language 
To help readers whose background in C programming is weak (or nonexistent), we have also included these special notes to highlight features that are especially important in C. We assume you are familiar with C++ or Java. 
How to Read the Book 
Learning how computer systems work from a programmer¡¯s perspective is great fun, mainly because you can do it actively. Whenever you learn something new, you can try it out right away and see the result .rst hand. In fact, we believe that the only way to learn systems is to do systems, either working concrete problems or writing and running programs on real systems. 
This theme pervades the entire book. When a new concept is introduced, it is followed in the text by one or more practice problems that you should work immediately to test your understanding. Solutions to the practice problems are at the end of each chapter. As you read, try to solve each problem on your own, and then check the solution to make sure you are on the right track. Each chapter is followed by a set of homework problems of varying dif.culty. Your instructor has the solutions to the homework problems in an Instructor¡¯s Manual. For each homework problem, we show a rating of the amount of effort we feel it will require: 

¡ô Should require just a few minutes. Little or no programming required. ¡ô¡ô Might require up to 20 minutes. Often involves writing and testing some code. Many of these are derived from problems we have given on exams. ¡ô¡ô¡ô Requires a signi.cant effort, perhaps 1¨C2 hours. Generally involves writing 
and testing a signi.cant amount of code. ¡ô¡ô¡ô¡ô A lab assignment, requiring up to 10 hours of effort. 
code/intro/hello.c 

1 #include <stdio.h> 2 3 int main() 4 { 5 printf("hello, world\n"); 6 return 0; 
7 } 
code/intro/hello.c 

Figure 1 A typical code example. 
Each code example in the text was formatted directly, without any manual intervention, from a C program compiled with gcc and tested on a Linux system. Of course, your system may have a different version of gcc, or a different compiler altogether, and so your compiler might generate different machine code, but the overall behavior should be the same. All of the source code is available from the CS:APP Web page at csapp.cs.cmu.edu. In the text, the .le names of the source programs are documented in horizontal bars that surround the formatted code. For example, the program in Figure 1 can be found in the .le hello.c in directory code/intro/. We encourage you to try running the example programs on your system as you encounter them. 
To avoid having a book that is overwhelming, both in bulk and in content, we have created a number of Web asides containing material that supplements the main presentation of the book. These asides are referenced within the book with a notation of the form CHAP:TOP, where CHAP is a short encoding of the chapter subject, and TOP is short code for the topic that is covered. For example, Web Aside data:bool contains supplementary material on Boolean algebra for the presentation on data representations in Chapter 2, while Web Aside arch:vlog contains material describing processor designs using the Verilog hardware descrip-tion language, supplementing the presentation of processor design in Chapter 4. All of these Web asides are available from the CS:APP Web page. 
Aside What is an aside? 
You will encounter asides of this form throughout the text. Asides are parenthetical remarks that give you some additional insight into the current topic. Asides serve a number of purposes. Some are little history lessons. For example, where did C, Linux, and the Internet come from? Other asides are meant to clarify ideas that students often .nd confusing. For example, what is the difference between a cache line, set, and block? Other asides give real-world examples. For example, how a .oating-point error crashed a French rocket, or what the geometry of an actual Seagate disk drive looks like. Finally, some asides are just fun stuff. For example, what is a ¡°hoinky¡±? 
Book Overview 
The CS:APP book consists of 12 chapters designed to capture the core ideas in computer systems: 
. Chapter 1: A Tour of Computer Systems. This chapter introduces the major ideas and themes in computer systems by tracing the life cycle of a simple ¡°hello, world¡± program. 
. Chapter 2: Representing and Manipulating Information. We cover computer arithmetic, emphasizing the properties of unsigned and two¡¯s-complement number representations that affect programmers. We consider how numbers are represented and therefore what range of values can be encoded for a given word size. We consider the effect of casting between signed and unsigned num-bers. We cover the mathematical properties of arithmetic operations. Novice programmers are often surprised to learn that the (two¡¯s-complement) sum or product of two positive numbers can be negative. On the other hand, two¡¯s-complement arithmetic satis.es the algebraic properties of a ring, and hence a compiler can safely transform multiplication by a constant into a sequence of shifts and adds. We use the bit-level operations of C to demonstrate the prin-ciples and applications of Boolean algebra. We cover the IEEE .oating-point format in terms of how it represents values and the mathematical properties of .oating-point operations. 
Having a solid understanding of computer arithmetic is critical to writing reliable programs. For example, programmers and compilers cannot replace the expression (x<y) with (x-y < 0), due to the possibility of over.ow. They cannot even replace it with the expression (-y < -x), due to the asymmetric range of negative and positive numbers in the two¡¯s-complement represen-tation. Arithmetic over.ow is a common source of programming errors and security vulnerabilities, yet few other books cover the properties of computer arithmetic from a programmer¡¯s perspective. 
. Chapter 3: Machine-Level Representation of Programs. We teach you how to read the IA32 and x86-64 assembly language generated by a C compiler. We cover the basic instruction patterns generated for different control constructs, such as conditionals, loops, and switch statements. We cover the implemen-tation of procedures, including stack allocation, register usage conventions, and parameter passing. We cover the way different data structures such as structures, unions, and arrays are allocated and accessed. We also use the machine-level view of programs as a way to understand common code se-curity vulnerabilities, such as buffer over.ow, and steps that the programmer, the compiler, and the operating system can take to mitigate these threats. Learning the concepts in this chapter helps you become a better programmer, because you will understand how programs are represented on a machine. One certain bene.t is that you will develop a thorough and concrete under-standing of pointers. 
. Chapter 4: Processor Architecture. This chapter covers basic combinational and sequential logic elements, and then shows how these elements can be 
Preface xxiii 

combined in a datapath that executes a simpli.ed subset of the IA32 instruc-tion set called ¡°Y86.¡± We begin with the design of a single-cycle datapath. This design is conceptually very simple, but it would not be very fast. We then intro-duce pipelining, where the different steps required to process an instruction are implemented as separate stages. At any given time, each stage can work on a different instruction. Our .ve-stage processor pipeline is much more re-alistic. The control logic for the processor designs is described using a simple hardware description language called HCL. Hardware designs written in HCL can be compiled and linked into simulators provided with the textbook, and they can be used to generate Verilog descriptions suitable for synthesis into working hardware. 
. Chapter 5: Optimizing Program Performance. This chapter introduces a num-ber of techniques for improving code performance, with the idea being that programmers learn to write their C code in such a way that a compiler can then generate ef.cient machine code. We start with transformations that re-duce the work to be done by a program and hence should be standard practice when writing any program for any machine. We then progress to transforma-tions that enhance the degree of instruction-level parallelism in the generated machine code, thereby improving their performance on modern ¡°superscalar¡± processors. To motivate these transformations, we introduce a simple opera-tional model of how modern out-of-order processors work, and show how to measure the potential performance of a program in terms of the critical paths through a graphical representation of a program. You will be surprised how much you can speed up a program by simple transformations of the C code. 
. Chapter 6: The Memory Hierarchy.The memory system is one of the most visi-ble parts of a computer system to application programmers. To this point, you have relied on a conceptual model of the memory system as a linear array with uniform access times. In practice, a memory system is a hierarchy of storage devices with different capacities, costs, and access times. We cover the differ-ent types of RAM and ROM memories and the geometry and organization of magnetic-disk and solid-state drives. We describe how these storage devices are arranged in a hierarchy. We show how this hierarchy is made possible by locality of reference. We make these ideas concrete by introducing a unique view of a memory system as a ¡°memory mountain¡± with ridges of temporal locality and slopes of spatial locality. Finally, we show you how to improve the performance of application programs by improving their temporal and spatial locality. 
. Chapter 7: Linking. This chapter covers both static and dynamic linking, in-cluding the ideas of relocatable and executable object .les, symbol resolution, relocation, static libraries, shared object libraries, and position-independent code. Linking is not covered in most systems texts, but we cover it for sev-eral reasons. First, some of the most confusing errors that programmers can encounter are related to glitches during linking, especially for large software packages. Second, the object .les produced by linkers are tied to concepts such as loading, virtual memory, and memory mapping. 
. 
. 
. 
. 
Chapter 8: Exceptional Control Flow. In this part of the presentation, we step beyond the single-program model by introducing the general concept of exceptional control .ow (i.e., changes in control .ow that are outside the normal branches and procedure calls). We cover examples of exceptional control .ow that exist at all levels of the system, from low-level hardware exceptions and interrupts, to context switches between concurrent processes, to abrupt changes in control .ow caused by the delivery of Unix signals, to the nonlocal jumps in C that break the stack discipline. 
This is the part of the book where we introduce the fundamental idea of a process, an abstraction of an executing program. You will learn how pro-cesses work and how they can be created and manipulated from application programs. We show how application programmers can make use of multiple processes via Unix system calls. When you .nish this chapter, you will be able to write a Unix shell with job control. It is also your .rst introduction to the nondeterministic behavior that arises with concurrent program execution. 
Chapter 9: Virtual Memory. Our presentation of the virtual memory system seeks to give some understanding of how it works and its characteristics. We want you to know how it is that the different simultaneous processes can each use an identical range of addresses, sharing some pages but having individual copies of others. We also cover issues involved in managing and manipulating virtual memory. In particular, we cover the operation of storage allocators such as the Unix malloc and free operations. Covering this material serves several purposes. It reinforces the concept that the virtual memory space is just an array of bytes that the program can subdivide into different storage units. It helps you understand the effects of programs containing memory ref-erencing errors such as storage leaks and invalid pointer references. Finally, many application programmers write their own storage allocators optimized toward the needs and characteristics of the application. This chapter, more than any other, demonstrates the bene.t of covering both the hardware and the software aspects of computer systems in a uni.ed way. Traditional com-puter architecture and operating systems texts present only part of the virtual memory story. 
Chapter 10: System-Level I/O. We cover the basic concepts of Unix I/O such as .les and descriptors. We describe how .les are shared, how I/O redirection works, and how to access .le metadata. We also develop a robust buffered I/O package that deals correctly with a curious behavior known as short counts, where the library function reads only part of the input data. We cover the C standard I/O library and its relationship to Unix I/O, focusing on limitations of standard I/O that make it unsuitable for network programming. In general, the topics covered in this chapter are building blocks for the next two chapters on network and concurrent programming. 
Chapter 11: Network Programming. Networks are interesting I/O devices to program, tying together many of the ideas that we have studied earlier in the text, such as processes, signals, byte ordering, memory mapping, and dynamic storage allocation. Network programs also provide a compelling context for concurrency, which is the topic of the next chapter. This chapter is a thin slice through network programming that gets you to the point where you can write a Web server. We cover the client-server model that underlies all network applications. We present a programmer¡¯s view of the Internet, and show how to write Internet clients and servers using the sockets interface. Finally, we introduce HTTP and develop a simple iterative Web server. 

. Chapter 12: Concurrent Programming. This chapter introduces concurrent programming using Internet server design as the running motivational ex-ample. We compare and contrast the three basic mechanisms for writing con-current programs¡ªprocesses, I/O multiplexing, and threads¡ªand show how to use them to build concurrent Internet servers. We cover basic principles of synchronization using P and V semaphore operations, thread safety and reen-trancy, race conditions, and deadlocks. Writing concurrent code is essential for most server applications. We also describe the use of thread-level pro-gramming to express parallelism in an application program, enabling faster execution on multi-core processors. Getting all of the cores working on a sin-gle computational problem requires a careful coordination of the concurrent threads, both for correctness and to achieve high performance. 
New to this Edition 
The .rst edition of this book was published with a copyright of 2003. Consider-
ing the rapid evolution of computer technology, the book content has held up 
surprisingly well. Intel x86 machines running Unix-like operating systems and 
programmed in C proved to be a combination that continues to encompass many 
systems today. Changes in hardware technology and compilers and the experience 
of many instructors teaching the material have prompted a substantial revision. Here are some of the more signi.cant changes: 
. Chapter 2: Representing and Manipulating Information.We have tried to make this material more accessible, with more careful explanations of concepts and with many more practice and homework problems. We moved some of the more theoretical aspects to Web asides. We also describe some of the security vulnerabilities that arise due to the over.ow properties of computer arithmetic. 
. Chapter 3: Machine-Level Representation of Programs.We have extended our coverage to include x86-64, the extension of x86 processors to a 64-bit word size. We also use the code generated by a more recent version of gcc. We have enhanced our coverage of buffer over.ow vulnerabilities. We have created Web asides on two different classes of instructions for .oating point, and also a view of the more exotic transformations made when compilers attempt higher degrees of optimization. Another Web aside describes how to embed x86 assembly code within a C program. 
. Chapter 4: Processor Architecture. We include a more careful exposition of exception detection and handling in our processor design. We have also cre-ated a Web aside showing a mapping of our processor designs into Verilog, enabling synthesis into working hardware. 
. Chapter 5: Optimizing Program Performance. We have greatly changed our description of how an out-of-order processor operates, and we have created a simple technique for analyzing program performance based on the paths in a data-.ow graph representation of a program. A Web aside describes how C programmers can write programs that make use of the SIMD (single-instruction, multiple-data) instructions found in more recent versions of x86 processors. 
. Chapter 6: The Memory Hierarchy. We have added material on solid-state disks, and we have updated our presentation to be based on the memory hierarchy of an Intel Core i7 processor. 
. Chapter 7: Linking. This chapter has changed only slightly. . Chapter 8: Exceptional Control Flow. We have enhanced our discussion of how the process model introduces some fundamental concepts of concurrency, such as nondeterminism. . Chapter 9: Virtual Memory.We have updated our memory system case study to describe the 64-bit Intel Core i7 processor. We have also updated our sample 
implementation of malloc to work for both 32-bit and 64-bit execution. . Chapter 10: System-Level I/O. This chapter has changed only slightly. . Chapter 11: Network Programming. This chapter has changed only slightly. . Chapter 12: Concurrent Programming.We have increased our coverage of the 
general principles of concurrency, and we also describe how programmers can use thread-level parallelism to make programs run faster on multi-core machines. 
In addition, we have added and revised a number of practice and homework problems. 
Origins of the Book 
The book stems from an introductory course that we developed at Carnegie Mel-lon University in the Fall of 1998, called 15-213: Introduction to Computer Systems (ICS) [14]. The ICS course has been taught every semester since then, each time to about 150¨C250 students, ranging from sophomores to masters degree students and with a wide variety of majors. It is a required course for all undergraduates in the CS and ECE departments at Carnegie Mellon, and it has become a prerequisite for most upper-level systems courses. 
The idea with ICS was to introduce students to computers in a different way. Few of our students would have the opportunity to build a computer system. On the other hand, most students, including all computer scientists and computer engineers, will be required to use and program computers on a daily basis. So we decided to teach about systems from the point of view of the programmer, using the following .lter: we would cover a topic only if it affected the performance, correctness, or utility of user-level C programs. 

For example, topics such as hardware adder and bus designs were out. Topics such as machine language were in, but instead of focusing on how to write assem-bly language by hand, we would look at how a C compiler translates C constructs into machine code, including pointers, loops, procedure calls, and switch state-ments. Further, we would take a broader and more holistic view of the system as both hardware and systems software, covering such topics as linking, loading, processes, signals, performance optimization, virtual memory, I/O, and network and concurrent programming. 
This approach allowed us to teach the ICS course in a way that is practical, concrete, hands-on, and exciting for the students. The response from our students and faculty colleagues was immediate and overwhelmingly positive, and we real-ized that others outside of CMU might bene.t from using our approach. Hence this book, which we developed from the ICS lecture notes, and which we have now revised to re.ect changes in technology and how computer systems are im-plemented. 
For Instructors: Courses Based on the Book 
Instructors can use the CS:APP book to teach .ve different kinds of systems courses (Figure 2). The particular course depends on curriculum requirements, personal taste, and the backgrounds and abilities of the students. From left to right in the .gure, the courses are characterized by an increasing emphasis on the programmer¡¯s perspective of a system. Here is a brief description: 
. ORG: A computer organization course with traditional topics covered in an untraditional style. Traditional topics such as logic design, processor architec-ture, assembly language, and memory systems are covered. However, there is more emphasis on the impact for the programmer. For example, data repre-sentations are related back to the data types and operations of C programs, and the presentation on assembly code is based on machine code generated by a C compiler rather than hand-written assembly code. 
. ORG+: The ORG course with additional emphasis on the impact of hardware on the performance of application programs. Compared to ORG, students learn more about code optimization and about improving the memory per-formance of their C programs. 
. ICS: The baseline ICS course, designed to produce enlightened programmers who understand the impact of the hardware, operating system, and compila-tion system on the performance and correctness of their application programs. A signi.cant difference from ORG+ is that low-level processor architecture is not covered. Instead, programmers work with a higher-level model of a mod-ern out-of-order processor. The ICS course .ts nicely into a 10-week quarter, and can also be stretched to a 15-week semester if covered at a more leisurely pace. 
Course  
Chapter  Topic  ORG  ORG+  ICS  ICS+  SP  
1 2 3 4  Tour of systems Data representation Machine language Processor architecture  . . . .  . . . .  . . .  . . .  .  (d) .  
5 6 7 8 9 10 11 12  Code optimization Memory hierarchy Linking Exceptional control .ow Virtual memory System-level I/O Network programming Concurrent programming   (a)  (b)  . . .  . .  (c) . .  . .  (c) . . . . .   (a) . . . . . .  

Figure 2 Five systems courses based on the CS:APP book. Notes: (a) Hardware only, 
(b) No dynamic storage allocation, (c) No dynamic linking, (d) No .oating point. ICS+ is the 15-213 course from Carnegie Mellon. 
. ICS+: The baseline ICS course with additional coverage of systems program-ming topics such as system-level I/O, network programming, and concurrent programming. This is the semester-long Carnegie Mellon course, which covers every chapter in CS:APP except low-level processor architecture. 
. SP: A systems programming course. Similar to the ICS+ course, but drops .oating point and performance optimization, and places more emphasis on systems programming, including process control, dynamic linking, system-level I/O, network programming, and concurrent programming. Instructors might want to supplement from other sources for advanced topics such as daemons, terminal control, and Unix IPC. 
The main message of Figure 2 is that the CS:APP book gives a lot of options to students and instructors. If you want your students to be exposed to lower-level processor architecture, then that option is available via the ORG and ORG+ courses. On the other hand, if you want to switch from your current computer organization course to an ICS or ICS+ course, but are wary are making such a drastic change all at once, then you can move toward ICS incrementally. You can start with ORG, which teaches the traditional topics in a nontraditional way. Once you are comfortable with that material, then you can move to ORG+, and eventually to ICS. If students have no experience in C (for example they have only programmed in Java), you could spend several weeks on C and then cover the material of ORG or ICS. 

Finally, we note that the ORG+ and SP courses would make a nice two-term 
(either quarters or semesters) sequence. Or you might consider offering ICS+ as 
one term of ICS and one term of SP. 
Classroom-Tested Laboratory Exercises 
The ICS+ course at Carnegie Mellon receives very high evaluations from students. Median scores of 5.0/5.0 and means of 4.6/5.0 are typical for the student course evaluations. Students cite the fun, exciting, and relevant laboratory exercises as the primary reason. The labs are available from the CS:APP Web page. Here are examples of the labs that are provided with the book: 
. Data Lab. This lab requires students to implement simple logical and arith-metic functions, but using a highly restricted subset of C. For example, they must compute the absolute value of a number using only bit-level operations. This lab helps students understand the bit-level representations of C data types and the bit-level behavior of the operations on data. 
. Binary Bomb Lab. A binary bomb is a program provided to students as an object-code .le. When run, it prompts the user to type in six different strings. If any of these is incorrect, the bomb ¡°explodes,¡± printing an error message and logging the event on a grading server. Students must ¡°defuse¡± their own unique bombs by disassembling and reverse engineering the programs to determine what the six strings should be. The lab teaches students to understand assembly language, and also forces them to learn how to use a debugger. 
. Buffer Over.ow Lab. Students are required to modify the run-time behavior of a binary executable by exploiting a buffer over.ow vulnerability. This lab teaches the students about the stack discipline, and teaches them about the danger of writing code that is vulnerable to buffer over.ow attacks. 
. Architecture Lab. Several of the homework problems of Chapter 4 can be combined into a lab assignment, where students modify the HCL description of a processor to add new instructions, change the branch prediction policy, or add or remove bypassing paths and register ports. The resulting processors can be simulated and run through automated tests that will detect most of the possible bugs. This lab lets students experience the exciting parts of processor design without requiring a complete background in logic design and hardware description languages. 
. Performance Lab. Students must optimize the performance of an application kernel function such as convolution or matrix transposition. This lab provides a very clear demonstration of the properties of cache memories, and gives students experience with low-level program optimization. 
. Shell Lab.Students implement their own Unix shell program with job control, including the ctrl-c and ctrl-z keystrokes, fg, bg, and jobs commands. This is the student¡¯s .rst introduction to concurrency, and gives them a clear idea of Unix process control, signals, and signal handling. 
. Malloc Lab. Students implement their own versions of malloc, free, and (optionally) realloc. This lab gives students a clear understanding of data layout and organization, and requires them to evaluate different trade-offs between space and time ef.ciency. 
. Proxy Lab. Students implement a concurrent Web proxy that sits between their browsers and the rest of the World Wide Web. This lab exposes the students to such topics as Web clients and servers, and ties together many of the concepts from the course, such as byte ordering, .le I/O, process control, signals, signal handling, memory mapping, sockets, and concurrency. Students like being able to see their programs in action with real Web browsers and Web servers. 
The CS:APP Instructor¡¯s Manual has a detailed discussion of the labs, as well as directions for downloading the support software. 
Acknowledgments for the Second Edition 
We are deeply grateful to the many people who have helped us produce this second edition of the CS:APP text. 
First and foremost, we would to recognize our colleagues who have taught the ICS course at Carnegie Mellon for their insightful feedback and encouragement: Guy Blelloch, Roger Dannenberg, David Eckhardt, Greg Ganger, Seth Goldstein, Greg Kesden, Bruce Maggs, Todd Mowry, Andreas Nowatzyk, Frank Pfenning, and Markus Pueschel. 
Thanks also to our sharp-eyed readers who contributed reports to the errata page for the .rst edition: Daniel Amelang, Rui Baptista, Quarup Barreirinhas, Michael Bombyk, J ¡§
org Brauer, Jordan Brough, Yixin Cao, James Caroll, Rui Car-valho, Hyoung-Kee Choi, Al Davis, Grant Davis, Christian Dufour, Mao Fan, Tim Freeman, Inge Frick, Max Gebhardt, Jeff Goldblat, Thomas Gross, Anita Gupta, John Hampton, Hiep Hong, Greg Israelsen, Ronald Jones, Haudy Kazemi, Brian Kell, Constantine Kousoulis, Sacha Krakowiak, Arun Krishnaswamy, Mar-tin Kulas, Michael Li, Zeyang Li, Ricky Liu, Mario Lo Conte, Dirk Maas, Devon Macey, Carl Marcinik, Will Marrero, Simone Martins, Tao Men, Mark Morris-sey, Venkata Naidu, Bhas Nalabothula, Thomas Niemann, Eric Peskin, David Po, Anne Rogers, John Ross, Michael Scott, Seiki, Ray Shih, Darren Shultz, Erik Silkensen, Suryanto, Emil Tarazi, Nawanan Theera-Ampornpunt, Joe Trdinich, Michael Trigoboff, James Troup, Martin Vopatek, Alan West, Betsy Wolff, Tim Wong, James Woodruff, Scott Wright, Jackie Xiao, Guanpeng Xu, Qing Xu, Caren Yang, Yin Yongsheng, Wang Yuanxuan, Steven Zhang, and Day Zhong. Special thanks to Inge Frick, who identi.ed a subtle deep copy bug in our lock-and-copy example, and to Ricky Liu, for his amazing proofreading skills. 
Our Intel Labs colleagues Andrew Chien and Limor Fix were exceptionally supportive throughout the writing of the text. Steve Schlosser graciously provided some disk drive characterizations. Casey Helfrich and Michael Ryan installed and maintained our new Core i7 box. Michael Kozuch, Babu Pillai, and Jason Campbell provided valuable insight on memory system performance, multi-core 
Preface xxxi 

systems, and the power wall. Phil Gibbons and Shimin Chen shared their consid-erable expertise on solid-state disk designs. 
We have been able to call on the talents of many, including Wen-Mei Hwu, Markus Pueschel, and Jiri Simsa, to provide both detailed comments and high-level advice. James Hoe helped us create a Verilog version of the Y86 processor and did all of the work needed to synthesize working hardware. 
Many thanks to our colleagues who provided reviews of the draft manu-script: James Archibald (Brigham Young University), Richard Carver (George Mason University), Mirela Damian (Villanova University), Peter Dinda (North-western University), John Fiore (Temple University), Jason Fritts (St. Louis Uni-versity), John Greiner (Rice University), Brian Harvey (University of California, Berkeley), Don Heller (Penn State University), Wei Chung Hsu (University of Minnesota), Michelle Hugue (University of Maryland), Jeremy Johnson (Drexel University), Geoff Kuenning (Harvey Mudd College), Ricky Liu, Sam Mad-den (MIT), Fred Martin (University of Massachusetts, Lowell), Abraham Matta (Boston University), Markus Pueschel (Carnegie Mellon University), Norman Ramsey (Tufts University), Glenn Reinmann (UCLA), Michela Taufer (Univer-sity of Delaware), and Craig Zilles (UIUC). 
Paul Anagnostopoulos of Windfall Software did an outstanding job of type-setting the book and leading the production team. Many thanks to Paul and his superb team: Rick Camp (copyeditor), Joe Snowden (compositor), MaryEllen N. Oliver (proofreader), Laurel Muller (artist), and Ted Laux (indexer). 
Finally, we would like to thank our friends at Prentice Hall. Marcia Horton has always been there for us. Our editor Matt Goldstein provided stellar leadership from beginning to end. We are profoundly grateful for their help, encouragement, and insights. 
Acknowledgments from the First Edition 
We are deeply indebted to many friends and colleagues for their thoughtful crit-icisms and encouragement. A special thanks to our 15-213 students, whose infec-tious energy and enthusiasm spurred us on. Nick Carter and Vinny Furia gener-ously provided their malloc package. 
Guy Blelloch, Greg Kesden, Bruce Maggs, and Todd Mowry taught the course over multiple semesters, gave us encouragement, and helped improve the course material. Herb Derby provided early spiritual guidance and encouragement. Al-lan Fisher, Garth Gibson, Thomas Gross, Satya, Peter Steenkiste, and Hui Zhang encouraged us to develop the course from the start. A suggestion from Garth early on got the whole ball rolling, and this was picked up and re.ned with the help of a group led by Allan Fisher. Mark Stehlik and Peter Lee have been very supportive about building this material into the undergraduate curriculum. Greg Kesden provided helpful feedback on the impact of ICS on the OS course. Greg Ganger and Jiri Schindler graciously provided some disk drive characterizations and answered our questions on modern disks. Tom Stricker showed us the mem-ory mountain. James Hoe provided useful ideas and feedback on how to present processor architecture. 
A special group of students¡ªKhalil Amiri, Angela Demke Brown, Chris Colohan, Jason Crawford, Peter Dinda, Julio Lopez, Bruce Lowekamp, Jeff Pierce, Sanjay Rao, Balaji Sarpeshkar, Blake Scholl, Sanjit Seshia, Greg Stef-fan, Tiankai Tu, Kip Walker, and Yinglian Xie¡ªwere instrumental in helping us develop the content of the course. In particular, Chris Colohan established a fun (and funny) tone that persists to this day, and invented the legendary ¡°binary bomb¡± that has proven to be a great tool for teaching machine code and debugging concepts. 
Chris Bauer, Alan Cox, Peter Dinda, Sandhya Dwarkadas, John Greiner, Bruce Jacob, Barry Johnson, Don Heller, Bruce Lowekamp, Greg Morrisett, Brian Noble, Bobbie Othmer, Bill Pugh, Michael Scott, Mark Smotherman, Greg Steffan, and Bob Wier took time that they did not have to read and advise us on early drafts of the book. A very special thanks to Al Davis (University of Utah), Peter Dinda (Northwestern University), John Greiner (Rice University), Wei Hsu (University of Minnesota), Bruce Lowekamp (College of William & Mary), Bobbie Othmer (University of Minnesota), Michael Scott (University of Rochester), and Bob Wier (Rocky Mountain College) for class testing the Beta version. A special thanks to their students as well! 
We would also like to thank our colleagues at Prentice Hall. Marcia Horton, Eric Frank, and Harold Stone have been un.agging in their support and vision. Harold also helped us present an accurate historical perspective on RISC and CISC processor architectures. Jerry Ralya provided sharp insights and taught us a lot about good writing. 
Finally, we would like to acknowledge the great technical writers Brian Kernighan and the late W. Richard Stevens, for showing us that technical books can be beautiful. 
Thank you all. 
Randy Bryant Dave O¡¯Hallaron 
Pittsburgh, Pennsylvania 

About 
the 
Authors 


Randal E. Bryant received his Bachelor¡¯s degree from the University of Michigan in 1973 and then attended graduate school at the Massachusetts Institute of Technology, receiving a Ph.D. degree in computer sci-ence in 1981. He spent three years as an Assistant Professor at the California Institute of Technology, and has been on the faculty at Carnegie Mellon since 1984. He is currently a University Professor of Com-puter Science and Dean of the School of Computer Science. He also holds a courtesy appointment with 

the Department of Electrical and Computer Engineering. 
He has taught courses in computer systems at both the undergraduate and graduate level for over 30 years. Over many years of teaching computer archi-tecture courses, he began shifting the focus from how computers are designed to one of how programmers can write more ef.cient and reliable programs if they understand the system better. Together with Professor O¡¯Hallaron, he developed the course 15-213 ¡°Introduction to Computer Systems¡± at Carnegie Mellon that is the basis for this book. He has also taught courses in algorithms, programming, computer networking, and VLSI design. 
Most of Professor Bryant¡¯s research concerns the design of software tools to help software and hardware designers verify the correctness of their systems. These include several types of simulators, as well as formal veri.cation tools that prove the correctness of a design using mathematical methods. He has published over 150 technical papers. His research results are used by major computer manu-facturers, including Intel, FreeScale, IBM, and Fujitsu. He has won several major awards for his research. These include two inventor recognition awards and a technical achievement award from the Semiconductor Research Corporation, the Kanellakis Theory and Practice Award from the Association for Computer Ma-chinery (ACM), and the W. R. G. Baker Award, the Emmanuel Piore Award, and the Phil Kaufman Award from the Institute of Electrical and Electronics Engi-neers (IEEE). He is a Fellow of both the ACM and the IEEE and a member of the U.S. National Academy of Engineering. 

David R. O¡¯Hallaron is the Director of Intel Labs Pittsburgh and an Associate Professor in Computer Science and Electrical and Computer Engineering at Carnegie Mellon University. He received his Ph.D. from the University of Virginia. 
He has taught computer systems courses at the undergraduate and graduate levels on such topics as computer architecture, introductory computer sys-tems, parallel processor design, and Internet services. Together with Professor Bryant, he developed the 
course at Carnegie Mellon that led to this book. In 2004, he was awarded the Herbert Simon Award for Teaching Excellence by the CMU School of Computer Science, an award for which the winner is chosen based on a poll of the students. 
Professor O¡¯Hallaron works in the area of computer systems, with speci.c interests in software systems for scienti.c computing, data-intensive computing, and virtualization. The best known example of his work is the Quake project, a group of computer scientists, civil engineers, and seismologists who have devel-oped the ability to predict the motion of the ground during strong earthquakes. In 2003, Professor O¡¯Hallaron and the other members of the Quake team won the Gordon Bell Prize, the top international prize in high-performance computing. 

CHAPTER 
1 
A 
Tour 
of 
Computer 
Systems 
1.1 
Information 
Is 
Bits 
+ 
Context 
3 
1.2 
Programs 
Are 
Translated 
by 
Other 
Programs 
into 
Different 
Forms 
4 
1.3 
It 
Pays 
to 
Understand 
How 
Compilation 
Systems 
Work 
6 
1.4 
Processors 
Read 
and 
Interpret 
Instructions 
Stored 
in 
Memory 
7 
1.5 
Caches 
Matter 
12 
1.6 
Storage 
Devices 
Form 
a 
Hierarchy 
13 
1.7 
The 
Operating 
System 
Manages 
the 
Hardware 
14 
1.8 
Systems 
Communicate 
with 
Other 
Systems 
Using 
Networks 
20 
1.9 
Important 
Themes 
21 
1.10 
Summary 
25 
Bibliographic 
Notes 
26 
Chapter 1 A Tour of Computer Systems 
A computer system consists of hardware and systems software that work together to run application programs. Speci.c implementations of systems change over time, but the underlying concepts do not. All computer systems have similar hardware and software components that perform similar functions. This book is written for programmers who want to get better at their craft by understanding how these components work and how they affect the correctness and performance of their programs. 
You are poised for an exciting journey. If you dedicate yourself to learning the concepts in this book, then you will be on your way to becoming a rare ¡°power pro-grammer,¡± enlightened by an understanding of the underlying computer system and its impact on your application programs. 
You are going to learn practical skills such as how to avoid strange numerical errors caused by the way that computers represent numbers. You will learn how to optimize your C code by using clever tricks that exploit the designs of modern processors and memory systems. You will learn how the compiler implements procedure calls and how to use this knowledge to avoid the security holes from buffer over.ow vulnerabilities that plague network and Internet software. You will learn how to recognize and avoid the nasty errors during linking that confound the average programmer. You will learn how to write your own Unix shell, your own dynamic storage allocation package, and even your own Web server. You will learn the promises and pitfalls of concurrency, a topic of increasing importance as multiple processor cores are integrated onto single chips. 
In their classic text on the C programming language [58], Kernighan and Ritchie introduce readers to C using the hello program shown in Figure 1.1. Although hello is a very simple program, every major part of the system must work in concert in order for it to run to completion. In a sense, the goal of this book is to help you understand what happens and why, when you run hello on your system. 
We begin our study of systems by tracing the lifetime of the hello program, from the time it is created by a programmer, until it runs on a system, prints its simple message, and terminates. As we follow the lifetime of the program, we will brie.y introduce the key concepts, terminology, and components that come into play. Later chapters will expand on these ideas. 
code/intro/hello.c 
1 #include <stdio.h> 2 3 int main() 4 { 5 printf("hello, world\n"); 
6 } 
code/intro/hello.c 
Figure 1.1 The hello program. 

Section 1.1 Information Is Bits + Context 

1.1 
Information 
Is 
Bits 
+ 
Context 
 
Our hello program begins life as a source program (or source .le) that the  
programmer creates with an editor and saves in a text .le called hello.c.The  
source program is a sequence of bits, each with a value of 0 or 1, organized  
in 8-bit chunks called bytes. Each byte represents some text character in the  
program.  
Most modern systems represent text characters using the ASCII standard that  
represents each character with a unique byte-sized integer value. For example,  
Figure 1.2 shows the ASCII representation of the hello.c program.  
The hello.c program is stored in a .le as a sequence of bytes. Each byte has  
an integer value that corresponds to some character. For example, the .rst byte  
has the integer value 35, which corresponds to the character ¡®#¡¯. The second byte  
has the integer value 105, which corresponds to the character ¡®i¡¯, and so on. Notice  
that each text line is terminated by the invisible newline character ¡®\n¡¯, which is  
represented by the integer value 10. Files such as hello.c that consist exclusively  
of ASCII characters are known as text .les. All other .les are known as binary  
.les.  
The representation of hello.c illustrates a fundamental idea: All information  
in a system¡ªincluding disk .les, programs stored in memory, user data stored in  
memory, and data transferred across a network¡ªis represented as a bunch of bits.  
The only thing that distinguishes different data objects is the context in which  
we view them. For example, in different contexts, the same sequence of bytes  
might represent an integer, .oating-point number, character string, or machine  
instruction.  
As programmers, we need to understand machine representations of numbers  
because they are not the same as integers and real numbers. They are .nite  
approximations that can behave in unexpected ways. This fundamental idea is  
explored in detail in Chapter 2.  
#  i  n  c  l  u  d  e  <sp>  <  s  t  d  i  o  .  
35  105  110  99  108  117  100  101  32  60  115  116  100  105  111  46  
h  >  \n  \n  i  n  t  <sp>  m  a  i  n  (  )  \n  {  
104  62  10  10  105  110  116  32  109  97  105  110  40  41  10  123  
\n  <sp>  <sp>  <sp>  <sp>  p  r  i  n  t  f  (  "  h  e  l  
10  32  32  32  32  112  114  105  110  116  102  40  34  104  101  108  

l  o  ,  <sp>  w  o  r  l  d  \  n  "  )  ;  \n  }  
108  111  44  32  119  111  114  108  100  92  110  34  41  59  10  125  
Figure 1.2  The ASCII text representation of hello.c.  

Chapter 1 A Tour of Computer Systems 
Aside Origins of the C programming language 
C was developed from 1969 to 1973 by Dennis Ritchie of Bell Laboratories. The American National Standards Institute (ANSI) rati.ed the ANSI C standard in 1989, and this standardization later became the responsibility of the International Standards Organization (ISO). The standards de.ne the C language and a set of library functions known as the C standard library. Kernighan and Ritchie describe ANSI C in their classic book, which is known affectionately as ¡°K&R¡± [58]. In Ritchie¡¯s words [88], C is ¡°quirky, .awed, and an enormous success.¡± So why the success? 
. C was closely tied with the Unix operating system. C was developed from the beginning as the system programming language for Unix. Most of the Unix kernel, and all of its supporting tools and libraries, were written in C. As Unix became popular in universities in the late 1970s and early 1980s, many people were exposed to C and found that they liked it. Since Unix was written almost entirely in C, it could be easily ported to new machines, which created an even wider audience for both C and Unix. 
. C is a small, simple language.The design was controlled by a single person, rather than a committee, and the result was a clean, consistent design with little baggage. The K&R book describes the complete language and standard library, with numerous examples and exercises, in only 261 pages. The simplicity of C made it relatively easy to learn and to port to different computers. 
. C was designed for a practical purpose. C was designed to implement the Unix operating system. Later, other people found that they could write the programs they wanted, without the language getting in the way. 
C is the language of choice for system-level programming, and there is a huge installed base of application-level programs as well. However, it is not perfect for all programmers and all situations. C pointers are a common source of confusion and programming errors. C also lacks explicit support for useful abstractions such as classes, objects, and exceptions. Newer languages such as C++ and Java address these issues for application-level programs. 
1.2 
Programs 
Are 
Translated 
by 
Other 
Programs 
into 
Different 
Forms 

The hello program begins life as a high-level C program because it can be read and understood by human beings in that form. However, in order to run hello.c on the system, the individual C statements must be translated by other programs into a sequence of low-level machine-language instructions. These instructions are then packaged in a form called an executable object program and stored as a binary disk .le. Object programs are also referred to as executable object .les. 
On a Unix system, the translation from source .le to object .le is performed by a compiler driver: 
unix> gcc -o hello hello.c 


Section 1.2 Programs Are Translated by Other Programs into Different Forms 

(text) program (text) programs  program  
(text) (binary)  (binary)  
Figure 1.3 The compilation system.  
Here, the gcc compiler driver reads the source .le hello.c and translates it into  
an executable object .le hello. The translation is performed in the sequence  
of four phases shown in Figure 1.3. The programs that perform the four phases  
(preprocessor, compiler, assembler, and linker) are known collectively as the  
compilation system.  
. Preprocessing phase. The preprocessor (cpp) modi.es the original C program  
according to directives that begin with the # character. For example, the  
#include <stdio.h> command in line 1 of hello.c tells the preprocessor  
to read the contents of the system header .le stdio.h and insert it directly  
into the program text. The result is another C program, typically with the .i  
suf.x.  
. Compilation phase. The compiler (cc1) translates the text .le hello.i into  
the text .le hello.s, which contains an assembly-language program. Each  
statement in an assembly-language program exactly describes one low-level  
machine-language instruction in a standard text form. Assembly language is  
useful because it provides a common output language for different compilers  
for different high-level languages. For example, C compilers and Fortran  
compilers both generate output .les in the same assembly language.  
. Assembly phase. Next, the assembler (as) translates hello.s into machine- 
language instructions, packages them in a form known as a relocatable object  
program, and stores the result in the object .le hello.o.The hello.o .le is  
a binary .le whose bytes encode machine language instructions rather than  
characters. If we were to view hello.o with a text editor, it would appear to  
be gibberish.  
. Linking phase.Notice that our hello program calls the printf function, which  
is part of the standard C library provided by every C compiler. The printf  
function resides in a separate precompiled object .le called printf.o, which  
must somehow be merged with our hello.o program. The linker (ld) handles  
this merging. The result is the hello .le, which is an executable object .le (or  
simply executable) that is ready to be loaded into memory and executed by  
the system.  

Chapter 1 A Tour of Computer Systems 
Aside The GNU project 
GCC is one of many useful tools developed by the GNU (short for GNU¡¯s Not Unix) project. The GNU project is a tax-exempt charity started by Richard Stallman in 1984, with the ambitious goal of developing a complete Unix-like system whose source code is unencumbered by restrictions on how it can be modi.ed or distributed. The GNU project has developed an environment with all the major components of a Unix operating system, except for the kernel, which was developed separately by the Linux project. The GNU environment includes the emacs editor, gcc compiler, gdb debugger, assembler, linker, utilities for manipulating binaries, and other components. The gcc compiler has grown to support many different languages, with the ability to generate code for many different machines. Supported languages include C, C++, Fortran, Java, Pascal, Objective-C, and Ada. 
The GNU project is a remarkable achievement, and yet it is often overlooked. The modern open-source movement (commonly associated with Linux) owes its intellectual origins to the GNU project¡¯s notion of free software (¡°free¡± as in ¡°free speech,¡± not ¡°free beer¡±). Further, Linux owes much of its popularity to the GNU tools, which provide the environment for the Linux kernel. 
1.3 
It 
Pays 
to 
Understand 
How 
Compilation 
Systems 
Work 

For simple programs such as hello.c, we can rely on the compilation system to produce correct and ef.cient machine code. However, there are some important reasons why programmers need to understand how compilation systems work: 
. Optimizing program performance. Modern compilers are sophisticated tools that usually produce good code. As programmers, we do not need to know the inner workings of the compiler in order to write ef.cient code. However, in order to make good coding decisions in our C programs, we do need a basic understanding of machine-level code and how the compiler translates different C statements into machine code. For example, is a switch statement always more ef.cient than a sequence of if-else statements? How much overhead is incurred by a function call? Is a while loop more ef.cient than a for loop? Are pointer references more ef.cient than array indexes? Why does our loop run so much faster if we sum into a local variable instead of an argument that is passed by reference? How can a function run faster when we simply rearrange the parentheses in an arithmetic expression? 
In Chapter 3, we will introduce two related machine languages: IA32, the 32-bit code that has become ubiquitous on machines running Linux, Windows, and more recently the Macintosh operating systems, and x86-64, a 64-bit extension found in more recent microprocessors. We describe how compilers translate different C constructs into these languages. In Chapter 5, you will learn how to tune the performance of your C programs by making simple transformations to the C code that help the compiler do its job better. In Chapter 6, you will learn about the hierarchical nature of the memory system, how C compilers store data arrays in memory, and how your C programs can exploit this knowledge to run more ef.ciently. 

. Understanding link-time errors. In our experience, some of the most perplex-ing programming errors are related to the operation of the linker, especially when you are trying to build large software systems. For example, what does it mean when the linker reports that it cannot resolve a reference? What is the difference between a static variable and a global variable? What happens if you de.ne two global variables in different C .les with the same name? What is the difference between a static library and a dynamic library? Why does it matter what order we list libraries on the command line? And scariest of all, why do some linker-related errors not appear until run time? You will learn the answers to these kinds of questions in Chapter 7. 
. Avoiding security holes. For many years, buffer over.ow vulnerabilities have accounted for the majority of security holes in network and Internet servers. These vulnerabilities exist because too few programmers understand the need to carefully restrict the quantity and forms of data they accept from untrusted sources. A .rst step in learning secure programming is to understand the con-sequences of the way data and control information are stored on the program stack. We cover the stack discipline and buffer over.ow vulnerabilities in Chapter 3 as part of our study of assembly language. We will also learn about methods that can be used by the programmer, compiler, and operating system to reduce the threat of attack. 
1.4 
Processors 
Read 
and 
Interpret 
Instructions 
Stored 
in 
Memory 

At this point, our hello.c source program has been translated by the compilation system into an executable object .le called hello that is stored on disk. To run the executable .le on a Unix system, we type its name to an application program known as a shell: 
unix> ./hello hello, world unix> 

The shell is a command-line interpreter that prints a prompt, waits for you to type a command line, and then performs the command. If the .rst word of the command line does not correspond to a built-in shell command, then the shell assumes that it is the name of an executable .le that it should load and run. So in this case, the shell loads and runs the hello program and then waits for it to terminate. The hello program prints its message to the screen and then terminates. The shell then prints a prompt and waits for the next input command line. 
1.4.1 
Hardware 
Organization 
of 
a 
System 

To understand what happens to our hello program when we run it, we need to understand the hardware organization of a typical system, which is shown in Figure 1.4. This particular picture is modeled after the family of Intel Pentium 
Chapter 1 A Tour of Computer Systems 
Hardware organization of a typical system. CPU: Central Processing Unit, ALU: Arithmetic/Logic Unit, PC: Program counter, USB: Universal Serial Bus. 

Mouse Keyboard Display 
hello executable stored on disk 
systems, but all systems have a similar look and feel. Don¡¯t worry about the complexity of this .gure just now. We will get to its various details in stages throughout the course of the book. 
Buses 
Running throughout the system is a collection of electrical conduits called buses that carry bytes of information back and forth between the components. Buses are typically designed to transfer .xed-sized chunks of bytes known as words.The number of bytes in a word (the word size) is a fundamental system parameter that varies across systems. Most machines today have word sizes of either 4 bytes (32 bits) or 8 bytes (64 bits). For the sake of our discussion here, we will assume a word size of 4 bytes, and we will assume that buses transfer only one word at a time. 
I/O Devices 
Input/output (I/O) devices are the system¡¯s connection to the external world. Our example system has four I/O devices: a keyboard and mouse for user input, a display for user output, and a disk drive (or simply disk) for long-term storage of data and programs. Initially, the executable hello program resides on the disk. 
Each I/O device is connected to the I/O bus by either a controller or an adapter. The distinction between the two is mainly one of packaging. Controllers are chip sets in the device itself or on the system¡¯s main printed circuit board (often called the motherboard). An adapter is a card that plugs into a slot on the motherboard. Regardless, the purpose of each is to transfer information back and forth between the I/O bus and an I/O device. 

Chapter 6 has more to say about how I/O devices such as disks work. In Chapter 10, you will learn how to use the Unix I/O interface to access devices from your application programs. We focus on the especially interesting class of devices known as networks, but the techniques generalize to other kinds of devices as well. 
Main Memory 

The main memory is a temporary storage device that holds both a program and the data it manipulates while the processor is executing the program. Physically, main memory consists of a collection of dynamic random access memory (DRAM) chips. Logically, memory is organized as a linear array of bytes, each with its own unique address (array index) starting at zero. In general, each of the machine instructions that constitute a program can consist of a variable number of bytes. The sizes of data items that correspond to C program variables vary according to type. For example, on an IA32 machine running Linux, data of type short requires two bytes, types int, float, and long four bytes, and type double eight bytes. 
Chapter 6 has more to say about how memory technologies such as DRAM chips work, and how they are combined to form main memory. 
Processor 

The central processing unit (CPU), or simply processor, is the engine that inter-prets (or executes) instructions stored in main memory. At its core is a word-sized storage device (or register) called the program counter (PC). At any point in time, the PC points at (contains the address of) some machine-language instruction in main memory.1 
From the time that power is applied to the system, until the time that the power is shut off, a processor repeatedly executes the instruction pointed at by the program counter and updates the program counter to point to the next instruction. A processor appears to operate according to a very simple instruction execution model, de.ned by its instruction set architecture. In this model, instructions execute in strict sequence, and executing a single instruction involves performing a series of steps. The processor reads the instruction from memory pointed at by the program counter (PC), interprets the bits in the instruction, performs some simple operation dictated by the instruction, and then updates the PC to point to the next instruction, which may or may not be contiguous in memory to the instruction that was just executed. 
There are only a few of these simple operations, and they revolve around main memory, the register .le, and the arithmetic/logic unit (ALU). The register .le is a small storage device that consists of a collection of word-sized registers, each with its own unique name. The ALU computes new data and address values. Here are some examples of the simple operations that the CPU might carry out at the request of an instruction: 
1. PC is also a commonly used acronym for ¡°personal computer.¡± However, the distinction between the two should be clear from the context. 
. Load: Copy a byte or a word from main memory into a register, overwriting 
the previous contents of the register. 
. Store: Copy a byte or a word from a register to a location in main memory, 
overwriting the previous contents of that location. 
. Operate: Copy the contents of two registers to the ALU, perform an arithmetic operation on the two words, and store the result in a register, overwriting the previous contents of that register. 
. Jump: Extract a word from the instruction itself and copy that word into the program counter (PC), overwriting the previous value of the PC. 
We say that a processor appears to be a simple implementation of its in-struction set architecture, but in fact modern processors use far more complex mechanisms to speed up program execution. Thus, we can distinguish the pro-cessor¡¯s instruction set architecture, describing the effect of each machine-code instruction, from its microarchitecture, describing how the processor is actually implemented. When we study machine code in Chapter 3, we will consider the abstraction provided by the machine¡¯s instruction set architecture. Chapter 4 has more to say about how processors are actually implemented. 
1.4.2 
Running 
the 
hello 
Program 

Given this simple view of a system¡¯s hardware organization and operation, we can begin to understand what happens when we run our example program. We must omit a lot of details here that will be .lled in later, but for now we will be content with the big picture. 
Initially, the shell program is executing its instructions, waiting for us to type a command. As we type the characters ¡°./hello¡± at the keyboard, the shell program reads each one into a register, and then stores it in memory, as shown in Figure 1.5. 
When we hit the enter key on the keyboard, the shell knows that we have .nished typing the command. The shell then loads the executable hello .le by executing a sequence of instructions that copies the code and data in the hello object .le from disk to main memory. The data include the string of characters ¡°hello, world\n¡± that will eventually be printed out. 
Using a technique known as direct memory access (DMA, discussed in Chap-ter 6), the data travels directly from disk to main memory, without passing through the processor. This step is shown in Figure 1.6. 
Once the code and data in the hello object .le are loaded into memory, the processor begins executing the machine-language instructions in the hello pro-gram¡¯s main routine. These instructions copy the bytes in the ¡°hello, world\n¡± string from memory to the register .le, and from there to the display device, where they are displayed on the screen. This step is shown in Figure 1.7. 


¡°hello¡± 
CPU CPU


Figure 1.7 

Writing the output string from memory to the display. 

1.5 
Caches 
Matter 

An important lesson from this simple example is that a system spends a lot of time moving information from one place to another. The machine instructions in the hello program are originally stored on disk. When the program is loaded, they are copied to main memory. As the processor runs the program, instruc-tions are copied from main memory into the processor. Similarly, the data string ¡°hello,world\n¡±, originally on disk, is copied to main memory, and then copied from main memory to the display device. From a programmer¡¯s perspective, much of this copying is overhead that slows down the ¡°real work¡± of the program. Thus, a major goal for system designers is to make these copy operations run as fast as possible. 
Because of physical laws, larger storage devices are slower than smaller stor-age devices. And faster devices are more expensive to build than their slower counterparts. For example, the disk drive on a typical system might be 1000 times larger than the main memory, but it might take the processor 10,000,000 times longer to read a word from disk than from memory. 
Similarly, a typical register .le stores only a few hundred bytes of information, as opposed to billions of bytes in the main memory. However, the processor can read data from the register .le almost 100 times faster than from memory. Even more troublesome, as semiconductor technology progresses over the years, this processor-memory gap continues to increase. It is easier and cheaper to make processors run faster than it is to make main memory run faster. 
To deal with the processor-memory gap, system designers include smaller faster storage devices called cache memories (or simply caches) that serve as temporary staging areas for information that the processor is likely to need in the near future. Figure 1.8 shows the cache memories in a typical system. An L1 cache on the processor chip holds tens of thousands of bytes and can be accessed nearly as fast as the register .le. A larger L2 cache with hundreds of thousands to millions of bytes is connected to the processor by a special bus. It might take 5 times longer for the process to access the L2 cache than the L1 cache, but this is still 5 to 10 times faster than accessing the main memory. The L1 and L2 caches are implemented with a hardware technology known as static random access memory (SRAM). Newer and more powerful systems even have three levels of cache: L1, L2, and L3. The idea behind caching is that a system can get the effect of both a very large memory and a very fast one by exploiting locality, the tendency for programs to access data and code in localized regions. By setting up caches to hold data that is likely to be accessed often, we can perform most memory operations using the fast caches. 


One of the most important lessons in this book is that application program-mers who are aware of cache memories can exploit them to improve the perfor-mance of their programs by an order of magnitude. You will learn more about these important devices and how to exploit them in Chapter 6. 
1.6 
Storage 
Devices 
Form 
a 
Hierarchy 

This notion of inserting a smaller, faster storage device (e.g., cache memory) between the processor and a larger slower device (e.g., main memory) turns out to be a general idea. In fact, the storage devices in every computer system are organized as a memory hierarchy similar to Figure 1.9. As we move from the top of the hierarchy to the bottom, the devices become slower, larger, and less costly per byte. The register .le occupies the top level in the hierarchy, which is known as level 0, or L0. We show three levels of caching L1 to L3, occupying memory hierarchy levels 1 to 3. Main memory occupies level 4, and so on. 
The main idea of a memory hierarchy is that storage at one level serves as a cache for storage at the next lower level. Thus, the register .le is a cache for the L1 cache. Caches L1 and L2 are caches for L2 and L3, respectively. The L3 cache is a cache for the main memory, which is a cache for the disk. On some networked systems with distributed .le systems, the local disk serves as a cache for data stored on the disks of other systems. 

Smaller, faster, and 

L1: costlier (per byte) storage 
L2: devices 
L3: 
Larger, slower, and cheaper (per byte) storage devices 



CPU registers hold words retrieved from cache memory. L1 cache holds cache lines retrieved from L2 cache. 


L2 cache holds cache lines retrieved from L3 cache. 
L3 cache holds cache lines retrieved from memory. 
Main memory holds disk blocks retrieved from local disks. 

Local disks hold files retrieved from disks on remote network server. 
Just as programmers can exploit knowledge of the different caches to improve 
performance, programmers can exploit their understanding of the entire memory 
hierarchy. Chapter 6 will have much more to say about this. 
1.7 
The 
Operating 
System 
Manages 
the 
Hardware 

Back to our hello example. When the shell loaded and ran the hello program, and when the hello program printed its message, neither program accessed the keyboard, display, disk, or main memory directly. Rather, they relied on the services provided by the operating system. We can think of the operating system as a layer of software interposed between the application program and the hardware, as shown in Figure 1.10. All attempts by an application program to manipulate the hardware must go through the operating system. 
The operating system has two primary purposes: (1) to protect the hardware from misuse by runaway applications, and (2) to provide applications with simple and uniform mechanisms for manipulating complicated and often wildly different low-level hardware devices. The operating system achieves both goals via the 
Figure 1.10 
Application programs  
Operating system  
Processor  Main memory  I/O devices  

Software
Layered view of a 
computer system. 

Hardware 
Figure 1.11 

Abstractions provided by an operating system. 


fundamental abstractions shown in Figure 1.11: processes, virtual memory, and .les. As this .gure suggests, .les are abstractions for I/O devices, virtual memory is an abstraction for both the main memory and disk I/O devices, and processes are abstractions for the processor, main memory, and I/O devices. We will discuss each in turn. 
Aside Unix and Posix 
The 1960s was an era of huge, complex operating systems, such as IBM¡¯s OS/360 and Honeywell¡¯s Multics systems. While OS/360 was one of the most successful software projects in history, Multics dragged on for years and never achieved wide-scale use. Bell Laboratories was an original partner in the Multics project, but dropped out in 1969 because of concern over the complexity of the project and the lack of progress. In reaction to their unpleasant Multics experience, a group of Bell Labs researchers¡ª Ken Thompson, Dennis Ritchie, Doug McIlroy, and Joe Ossanna¡ªbegan work in 1969 on a simpler operating system for a DEC PDP-7 computer, written entirely in machine language. Many of the ideas in the new system, such as the hierarchical .le system and the notion of a shell as a user-level process, were borrowed from Multics but implemented in a smaller, simpler package. In 1970, Brian Kernighan dubbed the new system ¡°Unix¡± as a pun on the complexity of ¡°Multics.¡± The kernel was rewritten in C in 1973, and Unix was announced to the outside world in 1974 [89]. 
Because Bell Labs made the source code available to schools with generous terms, Unix developed a large following at universities. The most in.uential work was done at the University of California at Berkeley in the late 1970s and early 1980s, with Berkeley researchers adding virtual memory and the Internet protocols in a series of releases called Unix 4.xBSD (Berkeley Software Distribution). Concurrently, Bell Labs was releasing their own versions, which became known as System V Unix. Versions from other vendors, such as the Sun Microsystems Solaris system, were derived from these original BSD and System V versions. 
Trouble arose in the mid 1980s as Unix vendors tried to differentiate themselves by adding new and often incompatible features. To combat this trend, IEEE (Institute for Electrical and Electronics Engineers) sponsored an effort to standardize Unix, later dubbed ¡°Posix¡± by Richard Stallman. The result was a family of standards, known as the Posix standards, that cover such issues as the C language interface for Unix system calls, shell programs and utilities, threads, and network programming. As more systems comply more fully with the Posix standards, the differences between Unix versions are gradually disappearing. 
1.7.1 
Processes 

When a program such as hello runs on a modern system, the operating system provides the illusion that the program is the only one running on the system. The program appears to have exclusive use of both the processor, main memory, and I/O devices. The processor appears to execute the instructions in the program, one after the other, without interruption. And the code and data of the program appear to be the only objects in the system¡¯s memory. These illusions are provided by the notion of a process, one of the most important and successful ideas in computer science. 
A process is the operating system¡¯s abstraction for a running program. Multi-ple processes can run concurrently on the same system, and each process appears to have exclusive use of the hardware. By concurrently, we mean that the instruc-tions of one process are interleaved with the instructions of another process. In most systems, there are more processes to run than there are CPUs to run them. Traditional systems could only execute one program at a time, while newer multi-core processors can execute several programs simultaneously. In either case, a single CPU can appear to execute multiple processes concurrently by having the processor switch among them. The operating system performs this interleaving with a mechanism known as context switching. To simplify the rest of this discus-sion, we consider only a uniprocessor system containing a single CPU. We will return to the discussion of multiprocessor systems in Section 1.9.1. 
The operating system keeps track of all the state information that the process needs in order to run. This state, which is known as the context, includes infor-mation such as the current values of the PC, the register .le, and the contents of main memory. At any point in time, a uniprocessor system can only execute the code for a single process. When the operating system decides to transfer con-trol from the current process to some new process, it performs a context switch by saving the context of the current process, restoring the context of the new process, and then passing control to the new process. The new process picks up exactly where it left off. Figure 1.12 shows the basic idea for our example hello scenario. 
There are two concurrent processes in our example scenario: the shell process 
and the hello process. Initially, the shell process is running alone, waiting for input 
on the command line. When we ask it to run the hello program, the shell carries 
Figure 1.12 
Process context switching. 


out our request by invoking a special function known as a system call that passes control to the operating system. The operating system saves the shell¡¯s context, creates a new hello process and its context, and then passes control to the new hello process. After hello terminates, the operating system restores the context of the shell process and passes control back to it, where it waits for the next command line input. 
Implementing the process abstraction requires close cooperation between both the low-level hardware and the operating system software. We will explore how this works, and how applications can create and control their own processes, in Chapter 8. 
1.7.2 
Threads 


Although we normally think of a process as having a single control .ow, in modern systems a process can actually consist of multiple execution units, called threads, each running in the context of the process and sharing the same code and global data. Threads are an increasingly important programming model because of the requirement for concurrency in network servers, because it is easier to share data between multiple threads than between multiple processes, and because threads are typically more ef.cient than processes. Multi-threading is also one way to make programs run faster when multiple processors are available, as we will discuss in Section 1.9.1. You will learn the basic concepts of concurrency, including how to write threaded programs, in Chapter 12. 
1.7.3 
Virtual 
Memory 

Virtual memory is an abstraction that provides each process with the illusion that it has exclusive use of the main memory. Each process has the same uniform view of memory, which is known as its virtual address space. The virtual address space for Linux processes is shown in Figure 1.13. (Other Unix systems use a similar layout.) In Linux, the topmost region of the address space is reserved for code and data in the operating system that is common to all processes. The lower region of the address space holds the code and data de.ned by the user¡¯s process. Note that addresses in the .gure increase from the bottom to the top. 
The virtual address space seen by each process consists of a number of well-de.ned areas, each with a speci.c purpose. You will learn more about these areas later in the book, but it will be helpful to look brie.y at each, starting with the lowest addresses and working our way up: 
. Program code and data.Code begins at the same .xed address for all processes, followed by data locations that correspond to global C variables. The code and data areas are initialized directly from the contents of an executable object .le, in our case the hello executable. You will learn more about this part of the address space when we study linking and loading in Chapter 7. 

Memory 

invisible to
Figure 1.13 

Process virtual address 
user code 
space. 
printf function 

Loaded from the hello executable file 
0x08048000 (32) 
0x00400000 (64) 0 
. Heap.The code and data areas are followed immediately by the run-time heap. Unlike the code and data areas, which are .xed in size once the process begins running, the heap expands and contracts dynamically at run time as a result of calls to C standard library routines such as malloc and free. We will study heaps in detail when we learn about managing virtual memory in Chapter 9. 
. Shared libraries.Near the middle of the address space is an area that holds the code and data for shared libraries such as the C standard library and the math library. The notion of a shared library is a powerful but somewhat dif.cult concept. You will learn how they work when we study dynamic linking in Chapter 7. 
. Stack. At the top of the user¡¯s virtual address space is the user stack that the compiler uses to implement function calls. Like the heap, the user stack expands and contracts dynamically during the execution of the program. In particular, each time we call a function, the stack grows. Each time we return from a function, it contracts. You will learn how the compiler uses the stack in Chapter 3. 
. Kernel virtual memory. The kernel is the part of the operating system that is always resident in memory. The top region of the address space is reserved for the kernel. Application programs are not allowed to read or write the contents of this area or to directly call functions de.ned in the kernel code. 
For virtual memory to work, a sophisticated interaction is required between 
the hardware and the operating system software, including a hardware translation 
of every address generated by the processor. The basic idea is to store the contents of a process¡¯s virtual memory on disk, and then use the main memory as a cache for the disk. Chapter 9 explains how this works and why it is so important to the operation of modern systems. 

1.7.4 
Files 


A .le is a sequence of bytes, nothing more and nothing less. Every I/O device, including disks, keyboards, displays, and even networks, is modeled as a .le. All input and output in the system is performed by reading and writing .les, using a small set of system calls known as Unix I/O. 
This simple and elegant notion of a .le is nonetheless very powerful because it provides applications with a uniform view of all of the varied I/O devices that might be contained in the system. For example, application programmers who manipulate the contents of a disk .le are blissfully unaware of the speci.c disk technology. Further, the same program will run on different systems that use different disk technologies. You will learn about Unix I/O in Chapter 10. 
Aside The Linux project 
In August 1991, a Finnish graduate student named Linus Torvalds modestly announced a new Unix-like operating system kernel: 
From: torvalds@klaava.Helsinki.FI (Linus Benedict Torvalds) Newsgroups: comp.os.minix Subject: What would you like to see most in minix? Summary: small poll for my new operating system Date: 25 Aug 91 20:57:08 GMT 
Hello everybody out there using minix -I¡¯m doing a (free) operating system (just a hobby, won¡¯t be big and professional like gnu) for 386(486) AT clones. This has been brewing since April, and is starting to get ready. I¡¯d like any feedback on things people like/dislike in minix, as my OS resembles it somewhat (same physical layout of the file-system (due to practical reasons) among other things). 
I¡¯ve currently ported bash(1.08) and gcc(1.40), and things seem to work. This implies that I¡¯ll get something practical within a few months, and I¡¯d like to know what features most people would want. Any suggestions are welcome, but I won¡¯t promise I¡¯ll implement them :-) 
Linus (torvalds@kruuna.helsinki.fi) 
The rest, as they say, is history. Linux has evolved into a technical and cultural phenomenon. By combining forces with the GNU project, the Linux project has developed a complete, Posix-compliant version of the Unix operating system, including the kernel and all of the supporting infrastructure. Linux is available on a wide array of computers, from hand-held devices to mainframe computers. A group at IBM has even ported Linux to a wristwatch! 
1.8 
Systems 
Communicate 
with 
Other 
Systems 
Using 
Networks 

Up to this point in our tour of systems, we have treated a system as an isolated collection of hardware and software. In practice, modern systems are often linked to other systems by networks. From the point of view of an individual system, the network can be viewed as just another I/O device, as shown in Figure 1.14. When the system copies a sequence of bytes from main memory to the network adapter, the data .ows across the network to another machine, instead of, say, to a local disk drive. Similarly, the system can read data sent from other machines and copy this data to its main memory. 
With the advent of global networks such as the Internet, copying information from one machine to another has become one of the most important uses of computer systems. For example, applications such as email, instant messaging, the World Wide Web, FTP, and telnet are all based on the ability to copy information over a network. 
Returning to our hello example, we could use the familiar telnet application to run hello on a remote machine. Suppose we use a telnet client running on our 



¡°hello, world\n¡± 
to client
string on display 

Figure 1.15 Using telnet to run hello remotely over a network. 
local machine to connect to a telnet server on a remote machine. After we log in to the remote machine and run a shell, the remote shell is waiting to receive an input command. From this point, running the hello program remotely involves the .ve basic steps shown in Figure 1.15. 
After we type the ¡°hello¡± string to the telnet client and hit the enter key, the client sends the string to the telnet server. After the telnet server receives the string from the network, it passes it along to the remote shell program. Next, the remote shell runs the hello program, and passes the output line back to the telnet server. Finally, the telnet server forwards the output string across the network to the telnet client, which prints the output string on our local terminal. 
This type of exchange between clients and servers is typical of all network applications. In Chapter 11, you will learn how to build network applications, and apply this knowledge to build a simple Web server. 
1.9 
Important 
Themes 

This concludes our initial whirlwind tour of systems. An important idea to take away from this discussion is that a system is more than just hardware. It is a collection of intertwined hardware and systems software that must cooperate in order to achieve the ultimate goal of running application programs. The rest of this book will .ll in some details about the hardware and the software, and it will show how, by knowing these details, you can write programs that are faster, more reliable, and more secure. 
To close out this chapter, we highlight several important concepts that cut across all aspects of computer systems. We will discuss the importance of these concepts at multiple places within the book. 
1.9.1 
Concurrency 
and 
Parallelism 

Throughout the history of digital computers, two demands have been constant forces driving improvements: we want them to do more, and we want them to run faster. Both of these factors improve when the processor does more things at once. We use the term concurrency to refer to the general concept of a system with multiple, simultaneous activities, and the term parallelism to refer to the use of concurrency to make a system run faster. Parallelism can be exploited at multiple levels of abstraction in a computer system. We highlight three levels here, working from the highest to the lowest level in the system hierarchy. 
Thread-Level Concurrency 
Building on the process abstraction, we are able to devise systems where multiple programs execute at the same time, leading to concurrency. With threads, we can even have multiple control .ows executing within a single process. Support for concurrent execution has been found in computer systems since the advent of time-sharing in the early 1960s. Traditionally, this concurrent execution was only simulated, by having a single computer rapidly switch among its executing processes, much as a juggler keeps multiple balls .ying through the air. This form of concurrency allows multiple users to interact with a system at the same time, such as when many people want to get pages from a single Web server. It also allows a single user to engage in multiple tasks concurrently, such as having a Web browser in one window, a word processor in another, and streaming music playing at the same time. Until recently, most actual computing was done by a single processor, even if that processor had to switch among multiple tasks. This con.guration is known as a uniprocessor system. 
When we construct a system consisting of multiple processors all under the control of a single operating system kernel, we have a multiprocessor system. Such systems have been available for large-scale computing since the 1980s, but they have more recently become commonplace with the advent of multi-core processors and hyperthreading. Figure 1.16 shows a taxonomy of these different processor types. 
Multi-core processors have several CPUs (referred to as ¡°cores¡±) integrated onto a single integrated-circuit chip. Figure 1.17 illustrates the organization of an Intel Core i7 processor, where the microprocessor chip has four CPU cores, each with its own L1 and L2 caches but sharing the higher levels of cache as well as the interface to main memory. Industry experts predict that they will be able to have dozens, and ultimately hundreds, of cores on a single chip. 
Hyperthreading, sometimes called simultaneous multi-threading, is a tech-nique that allows a single CPU to execute multiple .ows of control. It involves having multiple copies of some of the CPU hardware, such as program counters and register .les, while having only single copies of other parts of the hardware, such as the units that perform .oating-point arithmetic. Whereas a conventional 

Categorizing different processor con.gurations. 
Multiprocessors are becoming prevalent with the advent of multi-core processors and hyperthreading. 
Figure 1.16 All processors 

Uniprocessors  Multiprocessors Multi-Hyper- 
core threaded  

Section 1.9 Important Themes 23 


processor requires around 20,000 clock cycles to shift between different threads, a hyperthreaded processor decides which of its threads to execute on a cycle-by-cycle basis. It enables the CPU to make better advantage of its processing resources. For example, if one thread must wait for some data to be loaded into a cache, the CPU can proceed with the execution of a different thread. As an ex-ample, the Intel Core i7 processor can have each core executing two threads, and so a four-core system can actually execute eight threads in parallel. 
The use of multiprocessing can improve system performance in two ways. First, it reduces the need to simulate concurrency when performing multiple tasks. As mentioned, even a personal computer being used by a single person is expected to perform many activities concurrently. Second, it can run a single application program faster, but only if that program is expressed in terms of multiple threads that can effectively execute in parallel. Thus, although the principles of concur-rency have been formulated and studied for over 50 years, the advent of multi-core and hyperthreaded systems has greatly increased the desire to .nd ways to write application programs that can exploit the thread-level parallelism available with the hardware. Chapter 12 will look much more deeply into concurrency and its use to provide a sharing of processing resources and to enable more parallelism in program execution. 
Instruction-Level Parallelism 
At a much lower level of abstraction, modern processors can execute multiple instructions at one time, a property known as instruction-level parallelism.For 
example, early microprocessors, such as the 1978-vintage Intel 8086 required multiple (typically, 3¨C10) clock cycles to execute a single instruction. More recent processors can sustain execution rates of 2¨C4 instructions per clock cycle. Any given instruction requires much longer from start to .nish, perhaps 20 cycles or more, but the processor uses a number of clever tricks to process as many as 100 instructions at a time. In Chapter 4, we will explore the use of pipelining, where the actions required to execute an instruction are partitioned into different steps and the processor hardware is organized as a series of stages, each performing one of these steps. The stages can operate in parallel, working on different parts of different instructions. We will see that a fairly simple hardware design can sustain an execution rate close to one instruction per clock cycle. 
Processors that can sustain execution rates faster than one instruction per cycle are known as superscalar processors. Most modern processors support super-scalar operation. In Chapter 5, we will describe a high-level model of such proces-sors. We will see that application programmers can use this model to understand the performance of their programs. They can then write programs such that the generated code achieves higher degrees of instruction-level parallelism and there-fore runs faster. 
Single-Instruction, Multiple-Data (SIMD) Parallelism 
At the lowest level, many modern processors have special hardware that allows a single instruction to cause multiple operations to be performed in parallel, a mode known as single-instruction, multiple-data, or ¡°SIMD¡± parallelism. For example, recent generations of Intel and AMD processors have instructions that can add four pairs of single-precision .oating-point numbers (C data type float) in parallel. 
These SIMD instructions are provided mostly to speed up applications that process image, sound, and video data. Although some compilers attempt to auto-matically extract SIMD parallelism from C programs, a more reliable method is to write programs using special vector data types supported in compilers such as gcc. We describe this style of programming in Web Aside opt:simd, as a supplement to the more general presentation on program optimization found in Chapter 5. 
1.9.2 
The 
Importance 
of 
Abstractions 
in 
Computer 
Systems 

The use of abstractions is one of the most important concepts in computer science. For example, one aspect of good programming practice is to formulate a simple application-program interface (API) for a set of functions that allow programmers to use the code without having to delve into its inner workings. Different program-ming languages provide different forms and levels of support for abstraction, such as Java class declarations and C function prototypes. 
We have already been introduced to several of the abstractions seen in com-puter systems, as indicated in Figure 1.18. On the processor side, the instruction set architecture provides an abstraction of the actual processor hardware. With this abstraction, a machine-code program behaves as if it were executed on a proces-
Figure 1.18 

Some abstractions pro-vided by a computer system. A major theme in computer systems is to provide abstract represen-tations at different levels to hide the complexity of the actual implementations. 


sor that performs just one instruction at a time. The underlying hardware is far more elaborate, executing multiple instructions in parallel, but always in a way that is consistent with the simple, sequential model. By keeping the same execu-tion model, different processor implementations can execute the same machine code, while offering a range of cost and performance. 
On the operating system side, we have introduced three abstractions: .les as an abstraction of I/O, virtual memory as an abstraction of program memory, and processes as an abstraction of a running program. To these abstractions we add a new one: the virtual machine, providing an abstraction of the entire computer, including the operating system, the processor, and the programs. The idea of a virtual machine was introduced by IBM in the 1960s, but it has become more prominent recently as a way to manage computers that must be able to run programs designed for multiple operating systems (such as Microsoft Windows, MacOS, and Linux) or different versions of the same operating system. 
We will return to these abstractions in subsequent sections of the book. 
1.10 
Summary 

A computer system consists of hardware and systems software that cooperate to run application programs. Information inside the computer is represented as groups of bits that are interpreted in different ways, depending on the context. Programs are translated by other programs into different forms, beginning as ASCII text and then translated by compilers and linkers into binary executable .les. 
Processors read and interpret binary instructions that are stored in main memory. Since computers spend most of their time copying data between memory, I/O devices, and the CPU registers, the storage devices in a system are arranged in a hierarchy, with the CPU registers at the top, followed by multiple levels of hardware cache memories, DRAM main memory, and disk storage. Storage devices that are higher in the hierarchy are faster and more costly per bit than those lower in the hierarchy. Storage devices that are higher in the hierarchy serve as caches for devices that are lower in the hierarchy. Programmers can optimize the performance of their C programs by understanding and exploiting the memory hierarchy. 
The operating system kernel serves as an intermediary between the applica-tion and the hardware. It provides three fundamental abstractions: (1) Files are abstractions for I/O devices. (2) Virtual memory is an abstraction for both main memory and disks. (3) Processes are abstractions for the processor, main memory, and I/O devices. 
Finally, networks provide ways for computer systems to communicate with 
one another. From the viewpoint of a particular system, the network is just another 
I/O device. 
Bibliographic 
Notes 

Ritchie has written interesting .rst hand accounts of the early days of C and Unix [87, 88]. Ritchie and Thompson presented the .rst published account of Unix [89]. Silberschatz, Gavin, and Gagne [98] provide a comprehensive history of the different .avors of Unix. The GNU (www.gnu.org) 
and Linux (www.linux.org) 
Web 
pages have loads of current and historical information. The Posix standards are available online at (www.unix.org). 


Part 
I 
Program 
Structure 
and 
Execution 
O
ur exploration of computer systems starts by studying the com-puter itself, comprising a processor and a memory subsystem. At the core, we require ways to represent basic data types, such as approximations to integer and real arithmetic. From there we can con-sider how machine-level instructions manipulate data and how a com-piler translates C programs into these instructions. Next, we study several methods of implementing a processor to gain a better understanding of how hardware resources are used to execute instructions. Once we under-stand compilers and machine-level code, we can examine how to maxi-mize program performance by writing C programs that, when compiled, achieve the maximum possible performance. We conclude with the de-sign of the memory subsystem, one of the most complex components of a modern computer system. 
This part of the book will give you a deep understanding of how application programs are represented and executed. You will gain skills that help you write programs that are secure, reliable, and make the best use of the computing resources. 
This page intentionally left blank 
CHAPTER 
2 

Representing 
and 
Manipulating 
Information 

2.1 Information Storage 33 
2.2 Integer Representations 56 
2.3 Integer Arithmetic 79 
2.4 Floating Point 99 2.5 Summary 118 Bibliographic Notes 119 Homework Problems 119 Solutions to Practice Problems 134 
Modern computers store and process information represented as 2-valued signals. These lowly binary digits, or bits, form the basis of the digital revolution. The familiar decimal, or base-10, representation has been in use for over 1000 years, having been developed in India, improved by Arab mathematicians in the 12th century, and brought to the West in the 13th century by the Italian mathematician Leonardo Pisano (c. 1170 ¨C c. 1250), better known as Fibonacci. Using decimal notation is natural for ten-.ngered humans, but binary values work better when building machines that store and process information. Two-valued signals can readily be represented, stored, and transmitted, for example, as the presence or absence of a hole in a punched card, as a high or low voltage on a wire, or as a magnetic domain oriented clockwise or counterclockwise. The electronic circuitry for storing and performing computations on 2-valued signals is very simple and reliable, enabling manufacturers to integrate millions, or even billions, of such circuits on a single silicon chip. 
In isolation, a single bit is not very useful. When we group bits together and apply some interpretation that gives meaning to the different possible bit patterns, however, we can represent the elements of any .nite set. For example, using a binary number system, we can use groups of bits to encode nonnegative numbers. By using a standard character code, we can encode the letters and symbols in a document. We cover both of these encodings in this chapter, as well as encodings to represent negative numbers and to approximate real numbers. 
We consider the three most important representations of numbers. Unsigned encodings are based on traditional binary notation, representing numbers greater than or equal to 0. Two¡¯s-complement encodings are the most common way to represent signed integers, that is, numbers that may be either positive or neg-ative. Floating-point encodings are a base-two version of scienti.c notation for representing real numbers. Computers implement arithmetic operations, such as addition and multiplication, with these different representations, similar to the corresponding operations on integers and real numbers. 
Computer representations use a limited number of bits to encode a number, and hence some operations can over.ow when the results are too large to be rep-resented. This can lead to some surprising results. For example, on most of today¡¯s computers (those using a 32-bit representation of data type int), computing the expression 
200* 300*400 * 500 
yields .884,901,888. This runs counter to the properties of integer arithmetic¡ª computing the product of a set of positive numbers has yielded a negative result. 
On the other hand, integer computer arithmetic satis.es many of the familiar properties of true integer arithmetic. For example, multiplication is associative and commutative, so that computing any of the following C expressions yields .884,901,888: 
(500 * 400) * (300 * 200) ((500 * 400) * 300) * 200 ((200 * 500) * 300) * 400 400 * (200 * (300 * 500)) 

The computer might not generate the expected result, but at least it is consistent! 
Floating-point arithmetic has altogether different mathematical properties. The product of a set of positive numbers will always be positive, although over-.ow will yield the special value +¡Þ. Floating-point arithmetic is not associative, due to the .nite precision of the representation. For example, the C expression (3.14+1e20)-1e20 will evaluate to 0.0 on most machines, while 3.14+(1e20-1e20) will evaluate to 3.14. The different mathematical properties of integer vs. .oating-point arithmetic stem from the difference in how they handle the .nite-ness of their representations¡ªinteger representations can encode a comparatively small range of values, but do so precisely, while .oating-point representations can encode a wide range of values, but only approximately. 
By studying the actual number representations, we can understand the ranges of values that can be represented and the properties of the different arithmetic operations. This understanding is critical to writing programs that work correctly over the full range of numeric values and that are portable across different combi-nations of machine, operating system, and compiler. As we will describe, a number of computer security vulnerabilities have arisen due to some of the subtleties of computer arithmetic. Whereas in an earlier era program bugs would only incon-venience people when they happened to be triggered, there are now legions of hackers who try to exploit any bug they can .nd to obtain unauthorized access to other people¡¯s systems. This puts a higher level of obligation on programmers to understand how their programs work and how they can be made to behave in undesirable ways. 
Computers use several different binary representations to encode numeric values. You will need to be familiar with these representations as you progress into machine-level programming in Chapter 3. We describe these encodings in this chapter and show you how to reason about number representations. 
We derive several ways to perform arithmetic operations by directly manip-ulating the bit-level representations of numbers. Understanding these techniques will be important for understanding the machine-level code generated by compil-ers in their attempt to optimize the performance of arithmetic expression eval-uation. 
Our treatment of this material is based on a core set of mathematical prin-ciples. We start with the basic de.nitions of the encodings and then derive such properties as the range of representable numbers, their bit-level representations, and the properties of the arithmetic operations. We believe it is important for you to examine the material from this abstract viewpoint, because programmers need to have a clear understanding of how computer arithmetic relates to the more familiar integer and real arithmetic. 
Aside How to read this chapter 
If you .nd equations and formulas daunting, do not let that stop you from getting the most out of this chapter! We provide full derivations of mathematical ideas for completeness, but the best way to read this material is often to skip over the derivation on your initial reading. Instead, study the examples provided, and be sure to work all of the practice problems. The examples will give you an intuition behind the ideas, and the practice problems engage you in active learning, helping you put thoughts into action. With these as background, you will .nd it much easier to go back and follow the derivations. Be assured, as well, that the mathematical skills required to understand this material are within reach of someone with good grasp of high school algebra. 
The C++ programming language is built upon C, using the exact same numeric representations and operations. Everything said in this chapter about C also holds for C++. The Java language de.nition, on the other hand, created a new set of standards for numeric representations and operations. Whereas the C standards are designed to allow a wide range of implementations, the Java standard is quite speci.c on the formats and encodings of data. We highlight the representations and operations supported by Java at several places in the chapter. 

Aside The evolution of the C programming language 
As was described in an aside in Section 1.2, the C programming language was .rst developed by Dennis Ritchie of Bell Laboratories for use with the Unix operating system (also developed at Bell Labs). At the time, most system programs, such as operating systems, had to be written largely in assembly code, in order to have access to the low-level representations of different data types. For example, it was not feasible to write a memory allocator, such as is provided by the malloc library function, in other high-level languages of that era. 
The original Bell Labs version of C was documented in the .rst edition of the book by Brian Kernighan and Dennis Ritchie [57]. Over time, C has evolved through the efforts of several standard-ization groups. The .rst major revision of the original Bell Labs C led to the ANSI C standard in 1989, by a group working under the auspices of the American National Standards Institute. ANSI C was a major departure from Bell Labs C, especially in the way functions are declared. ANSI C is described in the second edition of Kernighan and Ritchie¡¯s book [58], which is still considered one of the best references on C. 
The International Standards Organization took over responsibility for standardizing the C lan-guage, adopting a version that was substantially the same as ANSI C in 1990 and hence is referred to as ¡°ISO C90.¡± This same organization sponsored an updating of the language in 1999, yielding ¡°ISO C99.¡± Among other things, this version introduced some new data types and provided support for text strings requiring characters not found in the English language. 
The GNU Compiler Collection (gcc) can compile programs according to the conventions of several 
different versions of the C language, based on different command line options, as shown in Figure 2.1. 
For example, to compile program prog.c according to ISO C99, we could give the command line 
unix> gcc -std=c99 prog.c 
The options -ansi and -std=c89 have the same effect¡ªthe code is compiled according to the ANSI or ISO C90 standard. (C90 is sometimes referred to as ¡°C89,¡± since its standardization effort began in 1989.) The option -std=c99 causes the compiler to follow the ISO C99 convention. 
C version gcc command line option GNU 89 none, -std=gnu89 ANSI, ISO C90 -ansi, -std=c89 ISO C99 -std=c99 GNU 99 -std=gnu99 
Figure 2.1 Specifying different versions of C to gcc. 
As of the writing of this book, when no option is speci.ed, the program will be compiled according to a version of C based on ISO C90, but including some features of C99, some of C++, and others speci.c to gcc. This version can be speci.ed explicitly using the option -std=gnu89. The GNU project is developing a version that combines ISO C99, plus other features, that can be speci.ed with command line option -std=gnu99. (Currently, this implementation is incomplete.) This will become the default version. 
2.1 
Information 
Storage 

Rather than accessing individual bits in memory, most computers use blocks of eight bits, or bytes, as the smallest addressable unit of memory. A machine-level program views memory as a very large array of bytes, referred to as virtual memory. Every byte of memory is identi.ed by a unique number, known as its address, and the set of all possible addresses is known as the virtual address space. As indicated by its name, this virtual address space is just a conceptual image presented to the machine-level program. The actual implementation (presented in Chapter 9) uses a combination of random-access memory (RAM), disk storage, special hardware, and operating system software to provide the program with what appears to be a monolithic byte array. 
In subsequent chapters, we will cover how the compiler and run-time system partitions this memory space into more manageable units to store the different program objects, that is, program data, instructions, and control information. Various mechanisms are used to allocate and manage the storage for different parts of the program. This management is all performed within the virtual address space. For example, the value of a pointer in C¡ªwhether it points to an integer, a structure, or some other program object¡ªis the virtual address of the .rst byte of some block of storage. The C compiler also associates type information with each pointer, so that it can generate different machine-level code to access the value stored at the location designated by the pointer depending on the type of that value. Although the C compiler maintains this type information, the actual machine-level program it generates has no information about data types. It simply treats each program object as a block of bytes, and the program itself as a sequence of bytes. 
New to C? The role of pointers in C 
Pointers are a central feature of C. They provide the mechanism for referencing elements of data structures, including arrays. Just like a variable, a pointer has two aspects: its value and its type.The value indicates the location of some object, while its type indicates what kind of object (e.g., integer or .oating-point number) is stored at that location. 
2.1.1 Hexadecimal Notation 
A single byte consists of 8 bits. In binary notation, its value ranges from 000000002 to 111111112. When viewed as a decimal integer, its value ranges from 010 to 25510. Neither notation is very convenient for describing bit patterns. Binary notation is too verbose, while with decimal notation, it is tedious to convert to and from bit patterns. Instead, we write bit patterns as base-16, or hexadecimal numbers. Hexadecimal (or simply ¡°hex¡±) uses digits ¡®0¡¯ through ¡®9¡¯ along with characters ¡®A¡¯ through ¡®F¡¯ to represent 16 possible values. Figure 2.2 shows the decimal and binary values associated with the 16 hexadecimal digits. Written in hexadecimal, the value of a single byte can range from 0016 to FF16. 
In C, numeric constants starting with 0x or 0X are interpreted as being in hexadecimal. The characters ¡®A¡¯ through ¡®F¡¯ may be written in either upper or lower case. For example, we could write the number FA1D37B16 as 0xFA1D37B, as 0xfa1d37b, or even mixing upper and lower case, e.g., 0xFa1D37b. We will use the C notation for representing hexadecimal values in this book. 
A common task in working with machine-level programs is to manually con-vert between decimal, binary, and hexadecimal representations of bit patterns. Converting between binary and hexadecimal is straightforward, since it can be performed one hexadecimal digit at a time. Digits can be converted by referring to a chart such as that shown in Figure 2.2. One simple trick for doing the conver-sion in your head is to memorize the decimal equivalents of hex digits A, C, and F. The hex values B, D, and E can be translated to decimal by computing their values relative to the .rst three. 
For example, suppose you are given the number 0x173A4C. You can convert this to binary format by expanding each hexadecimal digit, as follows: 
Hex digit  0  1  2  3  4  5  6  7  
Decimal value  0  1  2  3  4  5  6  7  
Binary value  0000  0001  0010  0011  0100  0101  0110  0111  
Hex digit  8  9  A  B  C  D  E  F  
Decimal value  8  9  10  11  12  13  14  15  
Binary value  1000  1001  1010  1011  1100  1101  1110  1111  
Figure 2.2 Hexadecimal notation. Each Hex digit encodes one of 16 values. 



Hexadecimal 173 A 4 C Binary 0001 0111 0011 1010 0100 1100 
This gives the binary representation 000101110011101001001100. 
Conversely, given a binary number 1111001010110110110011, you convert it to hexadecimal by .rst splitting it into groups of 4 bits each. Note, however, that if the total number of bits is not a multiple of 4, you should make the leftmost group be the one with fewer than 4 bits, effectively padding the number with leading zeros. Then you translate each group of 4 bits into the corresponding hexadecimal digit: 
Binary 11 1100 1010 1101 1011 0011 Hexadecimal 3 C ADB3 
Practice Problem 2.1 
Perform the following number conversions: 
A. 0x39A7F8 to binary 
B. Binary 1100100101111011 to hexadecimal 
C. 0xD5E4C to binary 
D. Binary 1001101110011110110101 to hexadecimal 
When a value x is a power of two, that is, x = 2n for some nonnegative integer n, we can readily write x in hexadecimal form by remembering that the binary representation of x is simply 1 followed by n zeros. The hexadecimal digit 0 represents four binary zeros. So, for n written in the form i + 4j , where 0 ¡Ü i ¡Ü 3, we can write x with a leading hex digit of 1 (i = 0), 2 (i = 1), 4 (i = 2), or 8 (i = 3), followed by j hexadecimal 0s. As an example, for x = 2048 = 211, we have n = 11 = 3 + 4 . 2, giving hexadecimal representation 0x800. 
Practice Problem 2.2 
Fill in the blank entries in the following table, giving the decimal and hexadecimal representations of different powers of 2: 
n 2n (Decimal) 2n (Hexadecimal) 
9 512 0x200 19 
16,384 
0x10000 

17 32 
0x80 

Converting between decimal and hexadecimal representations requires using multiplication or division to handle the general case. To convert a decimal num-ber x to hexadecimal, we can repeatedly divide x by 16, giving a quotient q and a remainder r, such that x = q. 16 + r. We then use the hexadecimal digit represent-ing r as the least signi.cant digit and generate the remaining digits by repeating the process on q. As an example, consider the conversion of decimal 314156: 
314156 = 19634 . 16 + 12 (C) 19634 = 1227 . 16 + 2 (2) 1227 = 76 . 16 + 11 (B) 76 = 4 . 16 + 12 (C) 4 = 0 . 16 + 4 (4) 
From this we can read off the hexadecimal representation as 0x4CB2C. 
Conversely, to convert a hexadecimal number to decimal, we can multiply 
each of the hexadecimal digits by the appropriate power of 16. For example, given 
the number 0x7AF, we compute its decimal equivalent as 7 . 162 + 10 . 16 + 15 = 
7 . 256 + 10 . 16 + 15 = 1792 + 160 + 15 = 1967. 
Practice Problem 2.3 
A single byte can be represented by two hexadecimal digits. Fill in the missing entries in the following table, giving the decimal, binary, and hexadecimal values of different byte patterns: 
Decimal  Binary  Hexadecimal  
0  0000 0000  0x00  
167  
62  
188  
0011 0111  
1000 1000  
1111 0011  
0x52  
0xAC  
0xE7  


Aside Converting between decimal and hexadecimal 
For converting larger values between decimal and hexadecimal, it is best to let a computer or calculator do the work. For example, the following script in the Perl language converts a list of numbers (given on the command line) from decimal to hexadecimal: 
bin/d2h 

1 #!/usr/local/bin/perl 2 # Convert list of decimal numbers into hex 3 4 for ($i = 0; $i < @ARGV; $i++) { 5 printf("%d\t= 0x%x\n", $ARGV[$i], $ARGV[$i]); 
6 } 
bin/d2h 

Once this .le has been set to be executable, the command 
unix> ./d2h 100 500 751 
yields output 
100 = 0x64 500 = 0x1f4 751 = 0x2ef 

Similarly, the following script converts from hexadecimal to decimal: 
bin/h2d 

1 #!/usr/local/bin/perl 2 # Convert list of hex numbers into decimal 3 4 for ($i = 0; $i < @ARGV; $i++) { 5 $val = hex($ARGV[$i]); 6 printf("0x%x = %d\n", $val, $val); 
7 } 
bin/h2d 

Practice Problem 2.4 
Without converting the numbers to decimal or binary, try to solve the follow-ing arithmetic problems, giving the answers in hexadecimal. Hint: Just modify the methods you use for performing decimal addition and subtraction to use base 16. 
A. 0x503c + 0x8 = 
B. 0x503c . 0x40 = 
C. 0x503c + 64 = 
D. 0x50ea . 0x503c = 
2.1.2 Words 
Every computer has a word size, indicating the nominal size of integer and pointer data. Since a virtual address is encoded by such a word, the most important system parameter determined by the word size is the maximum size of the virtual address space. That is, for a machine with a w-bit word size, the virtual addresses can range from0 to 2w . 1, giving the program access to at most 2w bytes. 
Most personal computers today have a 32-bit word size. This limits the virtual address space to 4 gigabytes (written 4 GB), that is, just over 4 ¡Á 109 bytes. Al-though this is ample space for most applications, we have reached the point where many large-scale scienti.c and database applications require larger amounts of storage. Consequently, high-end machines with 64-bit word sizes are becoming in-creasingly common as storage costs decrease. As hardware costs drop over time, even desktop and laptop machines will switch to 64-bit word sizes, and so we will consider the general case of a w-bit word size, as well as the special cases of w = 32 and w = 64. 
2.1.3 Data Sizes 
Computers and compilers support multiple data formats using different ways to encode data, such as integers and .oating point, as well as different lengths. For example, many machines have instructions for manipulating single bytes, as well as integers represented as 2-, 4-, and 8-byte quantities. They also support .oating-point numbers represented as 4-and 8-byte quantities. 
The C language supports multiple data formats for both integer and .oating-point data. The C data type char represents a single byte. Although the name ¡°char¡± derives from the fact that it is used to store a single character in a text string, it can also be used to store integer values. The C data type int can also be pre.xed by the quali.ers short, long, and recently long long, providing integer representations of various sizes. Figure 2.3 shows the number of bytes allocated for different C data types. The exact number depends on both the machine and the compiler. We show typical sizes for 32-bit and 64-bit machines. Observe that ¡°short¡± integers have 2-byte allocations, while an unquali.ed int is 4 bytes. A ¡°long¡± integer uses the full word size of the machine. The ¡°long long¡± integer data type, introduced in ISO C99, allows the full range of 64-bit integers. For 32-bit machines, the compiler must compile operations for this data type by generating code that performs sequences of 32-bit operations. 
C declaration  32-bit  64-bit  
char  1  1  
short int  2  2  
int  4  4  
long int  4  8  
long long int  8  8  
char *  4  8  
float  4  4  
double  8  8  
Figure 2.3 Sizes (in bytes) of C numeric data types. The number of bytes allocated varies with machine and compiler. This chart shows the values typical of 32-bit and 64-bit machines. 



Figure 2.3 also shows that a pointer (e.g., a variable declared as being of type ¡°char *¡±) uses the full word size of the machine. Most machines also support two different .oating-point formats: single precision, declared in C as float, and double precision, declared in C as double. These formats use 4 and 8 bytes, respectively. 
New to C? Declaring pointers 
For any data type T , the declaration 
T *p; 

indicates that p is a pointer variable, pointing to an object of type T . For example, 
char *p; 

is the declaration of a pointer to an object of type char. 
Programmers should strive to make their programs portable across different machines and compilers. One aspect of portability is to make the program insensi-tive to the exact sizes of the different data types. The C standards set lower bounds on the numeric ranges of the different data types, as will be covered later, but there are no upper bounds. Since 32-bit machines have been the standard since around 1980, many programs have been written assuming the allocations listed for this word size in Figure 2.3. Given the increasing availability of 64-bit machines, many hidden word size dependencies will show up as bugs in migrating these programs to new machines. For example, many programmers assume that a program object declared as type int can be used to store a pointer. This works .ne for most 32-bit machines but leads to problems on a 64-bit machine. 
2.1.4 Addressing and Byte Ordering 
For program objects that span multiple bytes, we must establish two conventions: what the address of the object will be, and how we will order the bytes in memory. In virtually all machines, a multi-byte object is stored as a contiguous sequence of bytes, with the address of the object given by the smallest address of the bytes used. For example, suppose a variable x of type int has address 0x100, that is, the value of the address expression &x is 0x100. Then the 4 bytes of x would be stored in memory locations 0x100, 0x101, 0x102, and 0x103. 
For ordering the bytes representing an object, there are two common conven-tions. Consider a w-bit integer having a bit representation [x .1,x .2,...,x1,x0],
ww 
where x .1 is the most signi.cant bit and x0 is the least. Assuming wis a multiple 
w 
of 8, these bits can be grouped as bytes, with the most signi.cant byte having bits [x .1,x .2,...,x .8], the least signi.cant byte having bits [x7,x6,...,x0], and
ww w 
the other bytes having bits from the middle. Some machines choose to store the ob-ject in memory ordered from least signi.cant byte to most, while other machines store them from most to least. The former convention¡ªwhere the least signi.-cant byte comes .rst¡ªis referred to as little endian. This convention is followed by most Intel-compatible machines. The latter convention¡ªwhere the most sig-ni.cant byte comes .rst¡ªis referred to as big endian. This convention is followed by most machines from IBM and Sun Microsystems. Note that we said ¡°most.¡± The conventions do not split precisely along corporate boundaries. For example, both IBM and Sun manufacture machines that use Intel-compatible processors and hence are little endian. Many recent microprocessors are bi-endian, meaning that they can be con.gured to operate as either little-or big-endian machines. 
Continuing our earlier example, suppose the variable x of type int and at 
address 0x100 has a hexadecimal value of 0x01234567. The ordering of the bytes 
within the address range 0x100 through 0x103 depends on the type of machine: 
Big endian 
0x100 0x101 0x102 0x103 
. . .  01  23  45  67  . . .  

Little endian 
0x100 0x101 0x102 0x103 
. . .  67  45  23  01  . . .  

Note that in the word 0x01234567 the high-order byte has hexadecimal value 0x01, while the low-order byte has value 0x67. 
People get surprisingly emotional about which byte ordering is the proper one. In fact, the terms ¡°little endian¡± and ¡°big endian¡± come from the book Gulliver¡¯s Travels by Jonathan Swift, where two warring factions could not agree as to how a soft-boiled egg should be opened¡ªby the little end or by the big. Just like the egg issue, there is no technological reason to choose one byte ordering convention over the other, and hence the arguments degenerate into bickering about socio-political issues. As long as one of the conventions is selected and adhered to consistently, the choice is arbitrary. 

Aside Origin of ¡°endian¡± 
Here is how Jonathan Swift, writing in 1726, described the history of the controversy between big and little endians: 
. . . Lilliput and Blefuscu... have, as I was going to tell you, been engaged in a most obstinate war for six-and-thirty moons past. It began upon the following occasion. It is allowed on all hands, that the primitive way of breaking eggs, before we eat them, was upon the larger end; but his present majesty¡¯s grandfather, while he was a boy, going to eat an egg, and breaking it according to the ancient practice, happened to cut one of his .ngers. Whereupon the emperor his father published an edict, commanding all his subjects, upon great penalties, to break the smaller end of their eggs. The people so highly resented this law, that our histories tell us, there have been six rebellions raised on that account; wherein one emperor lost his life, and another his crown. These civil commotions were constantly fomented by the monarchs of Blefuscu; and when they were quelled, the exiles always .ed for refuge to that empire. It is computed that eleven thousand persons have at several times suffered death, rather than submit to break their eggs at the smaller end. Many hundred large volumes have been published upon this controversy: but the books of the Big-endians have been long forbidden, and the whole party rendered incapable by law of holding employments. 
In his day, Swift was satirizing the continued con.icts between England (Lilliput) and France (Blefuscu). Danny Cohen, an early pioneer in networking protocols, .rst applied these terms to refer to byte ordering [25], and the terminology has been widely adopted. 
For most application programmers, the byte orderings used by their machines are totally invisible; programs compiled for either class of machine give identical results. At times, however, byte ordering becomes an issue. The .rst is when binary data are communicated over a network between different machines. A common problem is for data produced by a little-endian machine to be sent to a big-endian machine, or vice versa, leading to the bytes within the words being in reverse order for the receiving program. To avoid such problems, code written for networking applications must follow established conventions for byte ordering to make sure the sending machine converts its internal representation to the network standard, while the receiving machine converts the network standard to its internal representation. We will see examples of these conversions in Chapter 11. 
A second case where byte ordering becomes important is when looking at the byte sequences representing integer data. This occurs often when inspecting machine-level programs. As an example, the following line occurs in a .le that gives a text representation of the machine-level code for an Intel IA32 processor: 
80483bd: 01 05 64 94 04 08 add %eax,0x8049464 
This line was generated by a disassembler, a tool that determines the instruction sequence represented by an executable program .le. We will learn more about disassemblers and how to interpret lines such as this in Chapter 3. For now, we simply note that this line states that the hexadecimal byte sequence 01 05 64 94 04 08 is the byte-level representation of an instruction that adds a word of data to the value stored at address 0x8049464. If we take the .nal 4 bytes of the sequence, 6494 04 08, and write them in reverse order, we have 08 04 94 
64. Dropping the leading 0, we have the value 0x8049464, the numeric value written on the right. Having bytes appear in reverse order is a common occurrence when reading machine-level program representations generated for little-endian 
1 #include <stdio.h> 2 3 typedef unsigned char *byte_pointer; 4 5 void show_bytes(byte_pointer start, int len) { 6 int i; 7 for (i= 0; i< len; i++) 8 printf(" %.2x", start[i]); 9 printf("\n"); 
10 
} 11 12 void show_int(int x) { 13 show_bytes((byte_pointer) &x, sizeof(int)); 

14 
} 15 16 void show_float(float x) { 17 show_bytes((byte_pointer) &x, sizeof(float)); 

18 
} 19 20 void show_pointer(void *x) { 21 show_bytes((byte_pointer) &x, sizeof(void *)); 


22 } 
Figure 2.4 Code to print the byte representation of program objects. This code uses casting to circumvent the type system. Similar functions are easily de.ned for other data types. 
machines such as this one. The natural way to write a byte sequence is to have the lowest-numbered byte on the left and the highest on the right, but this is contrary to the normal way of writing numbers with the most signi.cant digit on the left and the least on the right. 
A third case where byte ordering becomes visible is when programs are written that circumvent the normal type system. In the C language, this can be done using a cast to allow an object to be referenced according to a different data type from which it was created. Such coding tricks are strongly discouraged for most application programming, but they can be quite useful and even necessary for system-level programming. 
Figure 2.4 shows C code that uses casting to access and print the byte rep-resentations of different program objects. We use typedef to de.ne data type byte_pointer as a pointer to an object of type ¡°unsigned char.¡± Such a byte pointer references a sequence of bytes where each byte is considered to be a non-negative integer. The .rst routine show_bytes is given the address of a sequence of bytes, indicated by a byte pointer, and a byte count. It prints the individual bytes in hexadecimal. The C formatting directive ¡°%.2x¡± indicates that an integer should be printed in hexadecimal with at least two digits. 

New to C? Naming data types with typedef 
The typedef declaration in C provides a way of giving a name to a data type. This can be a great help in improving code readability, since deeply nested type declarations can be dif.cult to decipher. 
The syntax for typedef is exactly like that of declaring a variable, except that it uses a type name rather than a variable name. Thus, the declaration of byte_pointer in Figure 2.4 has the same form as the declaration of a variable of type ¡°unsigned char *.¡± 
For example, the declaration 
typedef int *int_pointer; int_pointer ip; 
de.nes type ¡°int_pointer¡± to be a pointer to an int, and declares a variable ip of this type. Alterna-tively, we could declare this variable directly as 
int *ip; 

New to C? Formatted printing with printf 
The printf function (along with its cousins fprintf and sprintf) provides a way to print information with considerable control over the formatting details. The .rst argument is a format string, while any remaining arguments are values to be printed. Within the format string, each character sequence starting with ¡®%¡¯ indicates how to format the next argument. Typical examples include ¡®%d¡¯ to print a decimal integer, ¡®%f¡¯ to print a .oating-point number, and ¡®%c¡¯ to print a character having the character code given by the argument. 
New to C? Pointers and arrays 
In function show_bytes (Figure 2.4), we see the close connection between pointers and arrays, as will be discussed in detail in Section 3.8. We see that this function has an argument start of type byte_ pointer (which has been de.ned to be a pointer to unsigned char), but we see the array reference start[i] on line 8. In C, we can dereference a pointer with array notation, and we can reference array elements with pointer notation. In this example, the reference start[i] indicates that we want to read the byte that is i positions beyond the location pointed to by start. 
Procedures show_int, show_float, and show_pointer demonstrate how to use procedure show_bytes to print the byte representations of C program objects of type int, float, and void *, respectively. Observe that they simply pass show_ bytes a pointer &x to their argument x, casting the pointer to be of type ¡°unsigned char *.¡± This cast indicates to the compiler that the program should consider the pointer to be to a sequence of bytes rather than to an object of the original data type. This pointer will then be to the lowest byte address occupied by the object. 
New to C? Pointer creation and dereferencing 
In lines 13, 17, and 21 of Figure 2.4, we see uses of two operations that give C (and therefore C++) its distinctive character. The C ¡°address of¡± operator & creates a pointer. On all three lines, the expression &x creates a pointer to the location holding the object indicated by variable x. The type of this pointer depends on the type of x, and hence these three pointers are of type int *, float *, and void **, respectively. (Data type void * is a special kind of pointer with no associated type information.) 
The cast operator converts from one data type to another. Thus, the cast (byte_pointer) &x indicates that whatever type the pointer &x had before, the program will now reference a pointer to data of type unsigned char. The casts shown here do not change the actual pointer; they simply direct the compiler to refer to the data being pointed to according to the new data type. 
These procedures use the C sizeof operator to determine the number of bytes used by the object. In general, the expression sizeof(T ) returns the number of bytes required to store an object of type T . Using sizeof rather than a .xed value is one step toward writing code that is portable across different machine types. 
We ran the code shown in Figure 2.5 on several different machines, giving the results shown in Figure 2.6. The following machines were used: 
Linux 32: Intel IA32 processor running Linux Windows: Intel IA32 processor running Windows Sun: Sun Microsystems SPARC processor running Solaris Linux 64: Intel x86-64 processor running Linux 
Our argument 12,345 has hexadecimal representation 0x00003039. For the int data, we get identical results for all machines, except for the byte ordering. In particular, we can see that the least signi.cant byte value of 0x39 is printed .rst for Linux 32, Windows, and Linux 64, indicating little-endian machines, and last for Sun, indicating a big-endian machine. Similarly, the bytes of the float data are identical, except for the byte ordering. On the other hand, the pointer values are completely different. The different machine/operating system con.gurations 

code/data/show-bytes.c  
1  void test_show_bytes(int  val) {  
2  int ival = val;  
3  float fval = (float)  ival;  
4  int *pval = &ival;  
5  show_int(ival);  
6  show_float(fval);  
7  show_pointer(pval);  
8  }  
code/data/show-bytes.c  
Figure 2.5 Byte representation examples. This code prints the byte representations of sample data objects. 


Machine Value Type Bytes (hex) 
Linux 32 12,345 int 39300000 Windows 12,345 int 39300000 Sun 12,345 int 00003039 Linux 64 12,345 int 39300000 
Linux 32 12,345.0 float 00 e4 40 46 Windows 12,345.0 float 00 e4 40 46 Sun 12,345.0 float 46 40 e4 00 Linux 64 12,345.0 float 00 e4 40 46 
Linux 32 &ival int * e4 f9 ff bf Windows &ival int * b4 cc 22 00 Sun &ival int * ef ff fa 0c Linux 64 &ival int* b8 11 e5 ff ff 7f 00 00 
Figure 2.6 Byte representations of different data values. Results for int and float are identical, except for byte ordering. Pointer values are machine dependent. 
use different conventions for storage allocation. One feature to note is that the Linux 32, Windows, and Sun machines use 4-byte addresses, while the Linux 64 machine uses 8-byte addresses. 
Observe that although the .oating-point and the integer data both encode the numeric value 12,345, they have very different byte patterns: 0x00003039 for the integer, and 0x4640E400 for .oating point. In general, these two formats use different encoding schemes. If we expand these hexadecimal patterns into binary form and shift them appropriately, we .nd a sequence of 13 matching bits, indicated by a sequence of asterisks, as follows: 
00003039 00000000000000000011000000111001 ************* 
4640E400 
01000110010000001110010000000000 
This is not coincidental. We will return to this example when we study .oating-point formats. 
Practice Problem 2.5 
Consider the following three calls to show_bytes: 
int val = 0x87654321; byte_pointer valp = (byte_pointer) &val; show_bytes(valp, 1); /* A. */ show_bytes(valp, 2); /* B. */ show_bytes(valp, 3); /* C. */ 
Indicate which of the following values will be printed by each call on a little-endian machine and on a big-endian machine: 
A. Little endian: Big endian: 
B. Little endian: Big endian: 
C. Little endian: Big endian: 
Practice Problem 2.6 
Using show_int and show_float, we determine that the integer 3510593 has hexa-decimal representation 0x00359141, while the .oating-point number 3510593.0 has hexadecimal representation 0x4A564504. 
A. Write the binary representations of these two hexadecimal values. 
B. Shift these two strings relative to one another to maximize the number of matching bits. How many bits match? 
C. What parts of the strings do not match? 
2.1.5 Representing Strings 
A string in C is encoded by an array of characters terminated by the null (having value 0) character. Each character is represented by some standard encoding, with the most common being the ASCII character code. Thus, if we run our routine show_bytes with arguments "12345" and 6 (to include the terminating character), we get the result 31 32 33 34 35 00. Observe that the ASCII code for decimal digit x happens to be 0x3x, and that the terminating byte has the hex representation 0x00. This same result would be obtained on any system using ASCII as its character code, independent of the byte ordering and word size conventions. As a consequence, text data is more platform-independent than binary data. 

Aside Generating an ASCII table 
You can display a table showing the ASCII character code by executing the command man ascii. 
Practice Problem 2.7 
What would be printed as a result of the following call to show_bytes? 
const char *s = "abcdef"; show_bytes((byte_pointer) s, strlen(s)); 
Note that letters ¡®a¡¯ through ¡®z¡¯ have ASCII codes 0x61 through 0x7A. 

Aside The Unicode standard for text encoding 
The ASCII character set is suitable for encoding English-language documents, but it does not have much in the way of special characters, such as the French ¡®.c.¡¯ It is wholly unsuited for encoding documents in languages such as Greek, Russian, and Chinese. Over the years, a variety of methods have been developed to encode text for different languages. The Unicode Consortium has devised the most comprehensive and widely accepted standard for encoding text. The current Unicode standard (version 5.0) has a repertoire of nearly 100,000 characters supporting languages ranging from Albanian to Xamtanga (a language spoken by the Xamir people of Ethiopia). 
The base encoding, known as the ¡°Universal Character Set¡± of Unicode, uses a 32-bit representa-tion of characters. This would seem to require every string of text to consist of 4 bytes per character. However, alternative codings are possible where common characters require just 1 or 2 bytes, while less common ones require more. In particular, the UTF-8 representation encodes each character as a sequence of bytes, such that the standard ASCII characters use the same single-byte encodings as they have in ASCII, implying that all ASCII byte sequences have the same meaning in UTF-8 as they do in ASCII. 
The Java programming language uses Unicode in its representations of strings. Program libraries are also available for C to support Unicode. 
2.1.6 Representing Code 
Consider the following C function: 
1  int sum(int  x,  int y) {  
2  returnx+y;  
3  }  

When compiled on our sample machines, we generate machine code having the following byte representations: 
Linux 32: 55 89 e5 8b 45 0c 03 45 08 c9 c3 Windows: 55 89 e5 8b 45 0c 03 45 08 5d c3 Sun: 81 c3 e0 08 90 02 00 09 Linux 64: 55 48 89 e5 89 7d fc 89 75 f8 03 45 fc c9 c3 
Here we .nd that the instruction codings are different. Different machine types use different and incompatible instructions and encodings. Even identical proces-sors running different operating systems have differences in their coding conven-tions and hence are not binary compatible. Binary code is seldom portable across different combinations of machine and operating system. 
A fundamental concept of computer systems is that a program, from the perspective of the machine, is simply a sequence of bytes. The machine has no information about the original source program, except perhaps some auxiliary tables maintained to aid in debugging. We will see this more clearly when we study machine-level programming in Chapter 3. 
~  &  01  |  01  ^  01  
0  1  0  0  0  0  0  1  0  0  1  
1  0  1  0  1  1  1  1  1  1  0  

Figure 2.7 Operations of Boolean algebra. Binary values 1 and 0 encode logic values True and False, while operations ~, &, |, and ^ encode logical operations Not, And, Or, and Exclusive-Or, respectively. 
2.1.7 Introduction to Boolean Algebra 
Since binary values are at the core of how computers encode, store, and manipu-late information, a rich body of mathematical knowledge has evolved around the study of the values 0 and 1. This started with the work of George Boole (1815¨C 1864) around 1850 and thus is known as Boolean algebra. Boole observed that by encoding logic values True and False as binary values 1 and 0, he could formulate an algebra that captures the basic principles of logical reasoning. 
The simplest Boolean algebra is de.ned over the 2-element set {0,1}. Fig-ure 2.7 de.nes several operations in this algebra. Our symbols for representing these operations are chosen to match those used by the C bit-level operations, as will be discussed later. The Boolean operation ~ corresponds to the logical op-eration Not, denoted by the symbol .. That is, we say that .P is true when P is not true, and vice versa. Correspondingly, ~p equals 1 when p equals 0, and vice versa. Boolean operation & corresponds to the logical operation And, de-noted by the symbol ¡Ä. We say that P ¡Ä Q holds when both P is true and Qis true. Correspondingly, p& q equals 1 only when p= 1 and q= 1. Boolean opera-tion | corresponds to the logical operation Or, denoted by the symbol ¡Å.Wesay that P ¡Å Qholds when either P is true or Qis true. Correspondingly, p| qequals 1 when either p= 1or q= 1. Boolean operation ^ corresponds to the logical op-eration Exclusive-Or, denoted by the symbol ¨. We say that P ¨ Qholds when either P is true or Qis true, but not both. Correspondingly, p^ q equals 1 when either p= 1 and q= 0, or p= 0 and q= 1. 
Claude Shannon (1916¨C2001), who later founded the .eld of information theory, .rst made the connection between Boolean algebra and digital logic. In his 1937 master¡¯s thesis, he showed that Boolean algebra could be applied to the design and analysis of networks of electromechanical relays. Although computer technology has advanced considerably since, Boolean algebra still plays a central role in the design and analysis of digital systems. 
We can extend the four Boolean operations to also operate on bit vectors, strings of zeros and ones of some .xed length w. We de.ne the operations over bit vectors according their applications to the matching elements of the arguments. Let aand bdenote the bit vectors [aw.1,aw.2,...,a0]and [bw.1,bw.2,...,b0], respectively. We de.ne a& b to also be a bit vector of length w, where the ith element equals ai & bi, for 0 ¡Ü i<w. The operations |, ^, and ~ are extended to bit vectors in a similar fashion. 

As examples, consider the case where w =4, and with arguments a =[0110] and b =[1100]. Then the four operations a & b, a | b, a ^ b, and ~b yield 
0110 0110 0110 & 1100 | 1100 ^ 1100 ~ 1100 0100 1110 1010 0011 
Practice Problem 2.8 
Fill in the following table showing the results of evaluating Boolean operations on 
bit vectors.  
Operation a b  Result [01101001] [01010101]  
~a ~b  
a & b a | b a ^ b  

Web Aside DATA:BOOL More on Boolean algebra and Boolean rings 
The Boolean operations |, &, and ~ operating on bit vectors of length w form a Boolean algebra, for any integer w> 0. The simplest is the case where w =1, and there are just two elements, but for the more general case there are 2w bit vectors of length w. Boolean algebra has many of the same properties as arithmetic over integers. For example, just as multiplication distributes over addition, written a .(b +c) =(a .b) +(a .c), Boolean operation & distributes over |, written a & (b | c) =(a & b) | (a & c). In addition, however, Boolean operation | distributes over &, and so we can write a | (b & c) = (a | b) & (a | c), whereas we cannot say that a +(b .c) =(a +b) . (a +c) holds for all integers. 
When we consider operations ^, &, and ~ operating on bit vectors of length w, we get a different mathematical form, known as a Boolean ring. Boolean rings have many properties in common with integer arithmetic. For example, one property of integer arithmetic is that every value x has an additive inverse .x, such that x +.x =0. A similar property holds for Boolean rings, where ^ is the ¡°addition¡± operation, but in this case each element is its own additive inverse. That is, a ^ a =0 for any value a, where we use 0 here to represent a bit vector of all zeros. We can see this holds for single bits, since 0 ^ 0 =1 ^ 1 =0, and it extends to bit vectors as well. This property holds even when we rearrange terms and combine them in a different order, and so (a ^ b) ^ a =b. This property leads to some interesting results and clever tricks, as we will explore in Problem 2.10. 
One useful application of bit vectors is to represent .nite sets. We can encode 
any subset A .{0, 1,...,w .1}with a bit vector [aw.1,...,a1,a0], where ai =1if 
and only if i ¡ÊA. For example, recalling that we write aw.1 on the left and a0 on the 
..
right, bit vector a=[01101001]encodes the set A={0,3,5,6}, while bit vector b= [01010101]encodes the set B={0,2,4,6}. With this way of encoding sets, Boolean operations | and & correspond to set union and intersection, respectively, and ~ corresponds to set complement. Continuing our earlier example, the operation a& byields bit vector [01000001], while A¡ÉB={0,6}. 
We will see the encoding of sets by bit vectors in a number of practical applications. For example, in Chapter 8, we will see that there are a number of different signals that can interrupt the execution of a program. We can selectively enable or disable different signals by specifying a bit-vector mask, wherea1in bit position iindicates that signal iis enabled, and a 0 indicates that it is disabled. Thus, the mask represents the set of enabled signals. 
Practice Problem 2.9 
Computers generate color pictures on a video screen or liquid crystal display by mixing three different colors of light: red, green, and blue. Imagine a simple scheme, with three different lights, each of which can be turned on or off, project-ing onto a glass screen: 
Light sources Glass screen 
Red 
Green 
Blue 

We can then create eight different colors based on the absence (0) or presence (1) of light sources R, G, and B: 
RGB  Color  
0  0  0  Black  
0  0  1  Blue  
0  1  0  Green  
0  1  1  Cyan  
1  0  0  Red  
1  0  1  Magenta  
1  1  0  Yellow  
1  1  1  White  

Each of these colors can be represented as a bit vector of length 3, and we can apply Boolean operations to them. 

A. The complement of a color is formed by turning off the lights that are on and turning on the lights that are off. What would be the complement of each of the eight colors listed above? 
B. Describe the effect of applying Boolean operations on the following colors: 
Blue | Green = 
Yellow & Cyan = Red ^ Magenta = 
2.1.8 Bit-Level Operations in C 
One useful feature of C is that it supports bit-wise Boolean operations. In fact, the symbols we have used for the Boolean operations are exactly those used by C: | for Or, & for And, ~ for Not, and ^ for Exclusive-Or. These can be applied to any ¡°integral¡± data type, that is, one declared as type char or int, with or without quali.ers such as short, long, long long,or unsigned. Here are some examples of expression evaluation for data type char: 
C expression Binary expression Binary result Hexadecimal result 
~0x41  ~[0100 0001]  [1011 1110]  0xBE  
~0x00  ~[0000 0000]  [1111 1111]  0xFF  
0x69 & 0x55  [0110 1001] & [0101 0101]  [0100 0001]  0x41  
0x69 | 0x55  [0110 1001] | [0101 0101]  [0111 1101]  0x7D  

As our examples show, the best way to determine the effect of a bit-level ex-pression is to expand the hexadecimal arguments to their binary representations, perform the operations in binary, and then convert back to hexadecimal. 
Practice Problem 2.10 
As an application of the property that a ^ a = 0 for any bit vector a, consider the following program: 
1 void inplace_swap(int *x, int *y) { 
2 *y=*x^*y; /*Step1*/ 
3 *x=*x^*y; /*Step2*/ 
4 *y=*x^*y; /*Step3*/ 
5 } 

As the name implies, we claim that the effect of this procedure is to swap the values stored at the locations denoted by pointer variables x and y. Note that unlike the usual technique for swapping two values, we do not need a third location to temporarily store one value while we are moving the other. There is no performance advantage to this way of swapping; it is merely an intellectual amusement. 
Starting with values a and b in the locations pointed to by x and y, respectively, .ll in the table that follows, giving the values stored at the two locations after each step of the procedure. Use the properties of ^ to show that the desired effect is achieved. Recall that every element is its own additive inverse (that is, a ^ a = 0). 
Step *x *y Initially ab Step 1 
Step 2 Step 3 
Practice Problem 2.11 
Armed with the function inplace_swap from Problem 2.10, you decide to write code that will reverse the elements of an array by swapping elements from opposite ends of the array, working toward the middle. 
You arrive at the following function: 
1 void reverse_array(int a[], int cnt) { 2 int first, last; 3 for (first = 0, last = cnt-1; 4 first <= last; 5 first++,last--) 6 inplace_swap(&a[first], &a[last]); 
7 } 
When you apply your function to an array containing elements 1, 2, 3, and 4, you .nd the array now has, as expected, elements 4, 3, 2, and 1. When you try it on an array with elements 1, 2, 3, 4, and 5, however, you are surprised to see that the array now has elements 5, 4, 0, 2, and 1. In fact, you discover that the code always works correctly on arrays of even length, but it sets the middle element to 0 whenever the array has odd length. 
A. For an array of odd length cnt = 2k + 1, what are the values of variables first and last in the .nal iteration of function reverse_array? 
B. Why does this call to function xor_swap set the array element to 0? 
C. What simple modi.cation to the code for reverse_array would eliminate this problem? 
One common use of bit-level operations is to implement masking operations, where a mask is a bit pattern that indicates a selected set of bits within a word. As an example, the mask 0xFF (having ones for the least signi.cant 8 bits) indicates the low-order byte of a word. The bit-level operation x & 0xFF yields a value consisting of the least signi.cant byte of x, but with all other bytes set to 0. For example, with x = 0x89ABCDEF, the expression would yield 0x000000EF. The expression ~0 will yield a mask of all ones, regardless of the word size of the machine. Although the same mask can be written 0xFFFFFFFF for a 32-bit machine, such code is not as portable. 

Practice Problem 2.12 
Write C expressions, in terms of variable x, for the following values. Your code should work for any word size w ¡Ý 8. For reference, we show the result of evalu-ating the expressions for x = 0x87654321, with w = 32. 
A. The least signi.cant byte of x, with all other bits set to 0. [0x00000021]. 
B. All but the least signi.cant byte of x complemented, with the least signi.cant byte left unchanged. [0x789ABC21]. 
C. The least signi.cant byte set to all 1s, and all other bytes of x left unchanged. [0x876543FF]. 
Practice Problem 2.13 
The Digital Equipment VAX computer was a very popular machine from the late 1970s until the late 1980s. Rather than instructions for Boolean operations And and Or, it had instructions bis (bit set) and bic (bit clear). Both instructions take a data word x and a mask word m. They generate a result z consisting of the bits of x modi.ed according to the bits of m. With bis, the modi.cation involves setting z to 1 at each bit position where m is 1. With bic, the modi.cation involves setting z to 0 at each bit position where m is 1. 
To see how these operations relate to the C bit-level operations, assume we have functions bis and bic implementing the bit set and bit clear operations, and that we want to use these to implement functions computing bit-wise operations | and ^, without using any other C operations. Fill in the missing code below. 
Hint: Write C expressions for the operations bis and bic.  
/* Declarations of int bis(int x, int int bic(int x, int  functions m); m);  implem enting  operati ons  bis  and  bic  */  
/* Compute x|y using only int bool_or(int x, int y) int result = ; return result; }  calls {  to  functions  bis  and  bic  */  
/* Compute x^y using only calls int bool_xor(int x, int y) { int result = ; return result; }  to  functions  bis  and  bic  */  

2.1.9 Logical Operations in C 
C also provides a set of logical operators ||, &&, and !, which correspond to the Or, And, and Not operations of logic. These can easily be confused with the bit-level operations, but their function is quite different. The logical operations treat any nonzero argument as representing True and argument 0 as representing False. They return either 1 or 0, indicating a result of either True or False, respectively. Here are some examples of expression evaluation: 
Expression Result 
!0x41 0x00 !0x00 0x01 !!0x41 0x01 0x69 && 0x55 0x01 0x69 || 0x55 0x01 
Observe that a bit-wise operation will have behavior matching that of its logical counterpart only in the special case in which the arguments are restricted to 0 or 1. 
A second important distinction between the logical operators && and || ver-sus their bit-level counterparts & and | is that the logical operators do not evaluate their second argument if the result of the expression can be determined by evaluat-ing the .rst argument. Thus, for example, the expression a&&5/a will never cause a division by zero, and the expression p && *p++ will never cause the dereferencing of a null pointer. 
Practice Problem 2.14 
Suppose that x and y have byte values 0x66 and 0x39, respectively. Fill in the following table indicating the byte values of the different C expressions: 
Expression Value Expression Value 
x&y x&&y x|y x||y ~x|~y !x||!y x&!y x&&~y 
Practice Problem 2.15 
Using only bit-level and logical operations, write a C expression that is equivalent to x==y. In other words, it will return 1 when x and y are equal, and 0 otherwise. 
2.1.10 Shift Operations in C 
C also provides a set of shift operations for shifting bit patterns to the left and to the right. For an operand x having bit representation [x .1,x .2,...,x0],
nn 
the C expression x<<k yields a value with bit representation [xn.k.1,xn.k.2, ...,x0,0,...0]. That is, x is shifted k bits to the left, dropping off the k most signi.cant bits and .lling the right end with kzeros. The shift amount should be a value between 0 and n. 1. Shift operations associate from left to right, so x<<j << k is equivalent to (x << j) << k. 

There is a corresponding right shift operation x>>k, but it has a slightly subtle behavior. Generally, machines support two forms of right shift: logical and arithmetic. A logical right shift .lls the left end with k zeros, giving a result [0,...,0,x .1,x .2,...x ]. An arithmetic right shift .lls the left end with krepe-
nn k 
titions of the most signi.cant bit, giving a result [x .1,...,x .1,x .1,x .2,...x ].
n nnn k 

This convention might seem peculiar, but as we will see it is useful for operating on signed integer data. 
As examples, the following table shows the effect of applying the different shift operations to some sample 8-bit data: 
Operation  Values  
Argument x  [01100011]  [10010101]  
x<<4  [00110000]  [01010000]  
x>>4 (logical)  [00000110]  [00001001]  
x>>4 (arithmetic)  [00000110]  [11111001]  

The italicized digits indicate the values that .ll the right (left shift) or left (right shift) ends. Observe that all but one entry involves .lling with zeros. The exception is the case of shifting [10010101] right arithmetically. Since its most signi.cant bit is 1, this will be used as the .ll value. 
The C standards do not precisely de.ne which type of right shift should be used. For unsigned data (i.e., integral objects declared with the quali.er unsigned), right shifts must be logical. For signed data (the default), either arithmetic or logical shifts may be used. This unfortunately means that any code assuming one form or the other will potentially encounter portability problems. In practice, however, almost all compiler/machine combinations use arithmetic right shifts for signed data, and many programmers assume this to be the case. 
Java, on the other hand, has a precise de.nition of how right shifts should be performed. The expression x>>k shifts x arithmetically by k positions, while x >>> k shifts it logically. 
Aside Shifting by k, for large values of k 
For a data type consisting of w bits, what should be the effect of shifting by some value k¡Ý w?For example, what should be the effect of computing the following expressions on a 32-bit machine: 
int lval = 0xFEDCBA98 << 32; int aval = 0xFEDCBA98 >> 36; unsigned uval = 0xFEDCBA98u >> 40; 
The C standards carefully avoid stating what should be done in such a case. On many machines, the shift instructions consider only the lower log2 w bits of the shift amount when shifting a w-bit value, and so the shift amount is effectively computed as k mod w. For example, on a 32-bit machine following this convention, the above three shifts are computed as if they were by amounts 0, 4, and 8, respectively, giving results 
lval  0xFEDCBA98  
aval  0xFFEDCBA9  
uval  0x00FEDCBA  

This behavior is not guaranteed for C programs, however, and so shift amounts should be kept less than the word size. 
Java, on the other hand, speci.cally requires that shift amounts should be computed in the modular fashion we have shown. 
Aside Operator precedence issues with shift operations 
It might be tempting to write the expression 1<<2 + 3<<4, intending it to mean (1<<2) + (3<<4). But, in C, the former expression is equivalent to 1 << (2+3) << 4, since addition (and subtraction) have higher precedence than shifts. The left-to-right associativity rule then causes this to be parenthesized as (1 << (2+3)) << 4, giving value 512, rather than the intended 52. 
Getting the precedence wrong in C expressions is a common source of program errors, and often these are dif.cult to spot by inspection. When in doubt, put in parentheses! 
Practice Problem 2.16 
Fill in the table below showing the effects of the different shift operations on single-byte quantities. The best way to think about shift operations is to work with binary representations. Convert the initial values to binary, perform the shifts, and then convert back to hexadecimal. Each of the answers should be 8 binary digits or 2 hexadecimal digits. 
(Logical) (Arithmetic) 
x x<<3 x>>2 x>>2 
Hex Binary Binary Hex Binary Hex Binary Hex 
0xC3 
0x75 0x87 0x66 

2.2 
Integer 
Representations 

In this section, we describe two different ways bits can be used to encode integers¡ª one that can only represent nonnegative numbers, and one that can represent 

C data type Minimum Maximum 
char .128 127 unsigned char 0 255 short [int] .32,768 32,767 unsigned short [int] 0 65,535 int .2,147,483,648 2,147,483,647 unsigned [int] 0 4,294,967,295 long [int] .2,147,483,648 2,147,483,647 unsigned long [int] 0 4,294,967,295 long long [int] .9,223,372,036,854,775,808 9,223,372,036,854,775,807 unsigned long long [int] 0 18,446,744,073,709,551,615 
Figure 2.8 Typical ranges for C integral data types on a 32-bit machine. Text in square brackets is optional. 
C data type Minimum Maximum 
char .128 127 unsigned char 0 255 short [int] .32,768 32,767 unsigned short [int] 0 65,535 int .2,147,483,648 2,147,483,647 unsigned [int] 0 4,294,967,295 long [int] .9,223,372,036,854,775,808 9,223,372,036,854,775,807 unsigned long [int] 0 18,446,744,073,709,551,615 long long [int] .9,223,372,036,854,775,808 9,223,372,036,854,775,807 unsigned long long [int] 0 18,446,744,073,709,551,615 
Figure 2.9 Typical ranges for C integral data types on a 64-bit machine. Text in square brackets is optional. 
negative, zero, and positive numbers. We will see later that they are strongly related both in their mathematical properties and their machine-level implemen-tations. We also investigate the effect of expanding or shrinking an encoded integer to .t a representation with a different length. 
2.2.1 Integral Data Types 
C supports a variety of integral data types¡ªones that represent .nite ranges of integers. These are shown in Figures 2.8 and 2.9, along with the ranges of values they can have for ¡°typical¡± 32-and 64-bit machines. Each type can specify a size with keyword char, short, long,or long long, as well as an indication of whether the represented numbers are all nonnegative (declared as unsigned), or possibly 
C data type Minimum Maximum 
char .127 127 unsigned char 0 255 short [int] .32,767 32,767 unsigned short [int] 0 65,535 int .32,767 32,767 unsigned [int] 0 65,535 long [int] .2,147,483,647 2,147,483,647 unsigned long [int] 0 4,294,967,295 long long [int] .9,223,372,036,854,775,807 9,223,372,036,854,775,807 unsigned long long [int] 0 18,446,744,073,709,551,615 
Figure 2.10 Guaranteed ranges for C integral data types. Text in square brackets is optional. The C standards require that the data types have at least these ranges of values. 
negative (the default). As we saw in Figure 2.3, the number of bytes allocated for the different sizes vary according to machine¡¯s word size and the compiler. Based on the byte allocations, the different sizes allow different ranges of values to be represented. The only machine-dependent range indicated is for size designator long. Most 64-bit machines use an 8-byte representation, giving a much wider range of values than the 4-byte representation used on 32-bit machines. 
One important feature to note in Figures 2.8 and 2.9 is that the ranges are not symmetric¡ªthe range of negative numbers extends one further than the range of positive numbers. We will see why this happens when we consider how negative numbers are represented. 
The C standards de.ne minimum ranges of values that each data type must be able to represent. As shown in Figure 2.10, their ranges are the same or smaller than the typical implementations shown in Figures 2.8 and 2.9. In particular, we see that they require only a symmetric range of positive and negative numbers. We also see that data type int could be implemented with 2-byte numbers, although this is mostly a throwback to the days of 16-bit machines. We also see that size long could be implemented with 4-byte numbers, as is often the case. Data type long long was introduced with ISO C99, and it requires at least an 8-byte representation. 

New to C? Signed and unsigned numbers in C, C++, and Java 
Both C and C++ support signed (the default) and unsigned numbers. Java supports only signed numbers. 
2.2.2 Unsigned Encodings 
Assume we have an integer data type of wbits. We write a bit vector as either x,to denote the entire vector, or as [x .1,x .2,...,x0], to denote the individual bits 
ww 
within the vector. Treating xas a number written in binary notation, we obtain the 
Figure 2.11 
23 = 8 
Unsigned number 
22 = 4

examples for w= 4. When bit i in the binary 21 = 2 representation has value 
20 = 1

1, it contributes 2i to the value. 
[0001] [0101] [1011] [1111] 


unsigned interpretation of x. We express this interpretation as a function B2U
w (for ¡°binary to unsigned,¡± length w): 
w.1 
 
B2Uw(x)= . xi2i (2.1) i=0 

In this equation, the notation ¡°= .¡± means that the left-hand side is de.ned to be equal to the right-hand side. The function B2U maps strings of zeros and ones
w of length wto nonnegative integers. As examples, Figure 2.11 shows the mapping, given by B2U, from bit vectors to integers for the following cases: 
B2U4([0001]) = 0 .23 + 0 .22 + 0 .21 + 1 .20 = 0 + 0 + 0 + 1 = 1 B2U4([0101]) = 0 .23 + 1 .22 + 0 .21 + 1 .20 = 0 + 4 + 0 + 1 = 5 B2U4([1011]) = 1 .23 + 0 .22 + 1 .21 + 1 .20 = 8 + 0 + 2 + 1 = 11 B2U4([1111]) = 1 .23 + 1 .22 + 1 .21 + 1 .20 = 8 + 4 + 2 + 1 = 15 
(2.2) 

In the .gure, we represent each bit position iby a rightward-pointing blue bar of length 2i . The numeric value associated with a bit vector then equals the combined length of the bars for which the corresponding bit values are 1. 
Let us consider the range of values that can be represented using wbits. The least value is given by bit vector [00 ...0] having integer value 0, and the greatest 
 
.w.1

value is given by bit vector [11 ...1] having integer value UMax = 2i = 
wi=0 2w . 1. Using the 4-bit case as an example, we have UMax4 = B2U4([1111])= 
24 . 1 = 15. Thus, the function B2U can be de.ned as a mapping B2U : {0,1}w ¡ú
ww {0,...,2w . 1}. The unsigned binary representation has the important property that every number between 0 and 2w . 1has a unique encoding as a w-bit value. For example, there is only one representation of decimal value 11 as an unsigned, 4-bit number, namely [1011]. This property is captured in mathematical terms by stating that function B2U is a bijection¡ªit associates a unique value to each bit vector of
w 

length w; conversely, each integer between 0 and 2w .1 has a unique binary representation as a bit vector of length w. 
2.2.3 Two¡¯s-Complement Encodings 
For many applications, we wish to represent negative values as well. The most com-mon computer representation of signed numbers is known as two¡¯s-complement form. This is de.ned by interpreting the most signi.cant bit of the word to have negative weight. We express this interpretation as a function B2T (for ¡°binary 
w 
to two¡¯s-complement¡± length w): 
w.2 
 
.
B2Tw(x) =.xw.12w.1 + xi2i (2.3) i=0 
The most signi.cant bit xw.1 is also called the sign bit. Its ¡°weight¡± is .2w.1, the negation of its weight in an unsigned representation. When the sign bit is set to 1, the represented value is negative, and when set to 0 the value is nonnegative. As examples, Figure 2.12 shows the mapping, given by B2T, from bit vectors to integers for the following cases: 
B2T4([0001]) =.0 . 23 +0 . 22 +0 . 21 +1 . 20 = 0 +0 +0 +1 = 1 B2T4([0101]) =.0 . 23 +1 . 22 +0 . 21 +1 . 20 = 0 +4 +0 +1 = 5 B2T4([1011]) =.1 . 23 +0 . 22 +1 . 21 +1 . 20 =.8 +0 +2 +1 =.5 B2T4([1111]) =.1 . 23 +1 . 22 +1 . 21 +1 . 20 =.8 +4 +2 +1 =.1 
(2.4) 
In the .gure, we indicate that the sign bit has negative weight by showing it as a leftward-pointing gray bar. The numeric value associated with a bit vector is then given by the combination of the possible leftward-pointing gray bar and the rightward-pointing blue bars. 
Figure 2.12 
Two¡¯s-complement 
22 = 4

number examples for w =4. Bit 3 serves as a 21 = 2 sign bit, and so, when 
20 = 1 

set to 1, it contributes 

¨C8¨C7¨C6¨C5¨C4¨C3¨C2¨C1 0 1 2 3 4 5 6 7 8

.23 =.8 to the value. This weighting is shown as a 
[0001]

leftward-pointing gray bar. 
[0101] 
[1011] 
[1111] 



We see that the bit patterns are identical for Figures 2.11 and 2.12 (as well as for Equations 2.2 and 2.4), but the values differ when the most signi.cant bit is 1, since in one case it has weight +8, and in the other case it has weight .8. 
Let us consider the range of values that can be represented as a w-bit two¡¯s-complement number. The least representable value is given by bit vector [10 ...0] (set the bit with negative weight, but clear all others), having integer value 
.

TMin =.2w.1. The greatest value is given by bit vector [01 ...1] (clear the bit 
w 
 
.w.2

with negative weight, but set all others), having integer value TMax = 2i = 
wi=0 2w.1 .1. Using the 4-bit case as an example, we have TMin4 =B2T4([1000])= .23 =.8, and TMax4 =B2T4([0111])=22 +21 +20 =4 +2 +1 =7. We can see that B2T is a mapping of bit patterns of length wto numbers be-
w 

tween TMin and TMax , written as B2T : {0,1}w ¡ú{.2w.1 ,...,2w.1 .1}.As 
ww w 
we saw with the unsigned representation, every number within the representable range has a unique encoding as a w-bit two¡¯s-complement number. In mathemat-ical terms, we say that the function B2T is a bijection¡ªit associates a unique
w 

value to each bit vector of length w; conversely, each integer between .2w.1 and 2w.1 .1 has a unique binary representation as a bit vector of length w. 
Practice Problem 2.17 
Assuming w=4, we can assign a numeric value to each possible hexadecimal digit, assuming either an unsigned or a two¡¯s-complement interpretation. Fill in the following table according to these interpretations by writing out the nonzero powers of two in the summations shown in Equations 2.1 and 2.3: 
x 

Hexadecimal Binary B2U4(x) B2T4(x) 
0xE [1110] 23 +22 +21 =14 .23 +22 +21 =.2 0x0 
0x5 0x8 0xD 0xF 

Figure 2.13 shows the bit patterns and numeric values for several important numbers for different word sizes. The .rst three give the ranges of representable integers in terms of the values of UMax , TMin , and TMax . We will refer
ww w 

to these three special values often in the ensuing discussion. We will drop the subscript wand refer to the values UMax, TMin, and TMax when wcan be inferred from context or is not central to the discussion. 
A few points are worth highlighting about these numbers. First, as observed in Figures 2.8 and 2.9, the two¡¯s-complement range is asymmetric: |TMin|=|TMax|+1, that is, there is no positive counterpart to TMin. As we shall see, this leads to some peculiar properties of two¡¯s-complement arithmetic and can be the source of subtle program bugs. This asymmetry arises, because half the bit pat-terns (those with the sign bit set to 1) represent negative numbers, while half (those with the sign bit set to 0) represent nonnegative numbers. Since 0 is nonnegative, this means that it can represent one less positive number than negative. Second, the maximum unsigned value is just over twice the maximum two¡¯s-complement value: UMax = 2TMax + 1. All of the bit patterns that denote negative numbers in two¡¯s-complement notation become positive values in an unsigned representa-tion. Figure 2.13 also shows the representations of constants .1 and 0. Note that .1 has the same bit representation as UMax¡ªa string of all ones. Numeric value 0 is represented as a string of all zeros in both representations. 
Word size w  
Value  8  16  32  64  
UMaxw TMinw TMaxw .1  0xFF 255 0x80 .128 0x7F 127 0xFF  0xFFFF 65,535 0x8000 .32,768 0x7FFF 32,767 0xFFFF  0xFFFFFFFF 4,294,967,295 0x80000000 .2,147,483,648 0x7FFFFFFF 2,147,483,647 0xFFFFFFFF  0xFFFFFFFFFFFFFFFF 18,446,744,073,709,551,615 0x8000000000000000 .9,223,372,036,854,775,808 0x7FFFFFFFFFFFFFFF 9,223,372,036,854,775,807 0xFFFFFFFFFFFFFFFF  
0  0x00  0x0000  0x00000000  0x0000000000000000  
Figure 2.13 Important numbers. Both numeric values and hexadecimal representations are shown. 


The C standards do not require signed integers to be represented in two¡¯s-complement form, but nearly all machines do so. Programmers who are con-cerned with maximizing portability across all possible machines should not assume any particular range of representable values, beyond the ranges indicated in Fig-ure 2.10, nor should they assume any particular representation of signed numbers. On the other hand, many programs are written assuming a two¡¯s-complement representation of signed numbers, and the ¡°typical¡± ranges shown in Figures 2.8 and 2.9, and these programs are portable across a broad range of machines and compilers. The .le <limits.h> in the C library de.nes a set of constants delim-iting the ranges of the different integer data types for the particular machine on which the compiler is running. For example, it de.nes constants INT_MAX, INT_ MIN, and UINT_MAX describing the ranges of signed and unsigned integers. For a two¡¯s-complement machine in which data type int has w bits, these constants correspond to the values of TMax , TMin , and UMax .
ww w 

Aside Exact-size integer types 
For some programs, it is essential that data types be encoded using representations with speci.c sizes. For example, when writing programs to enable a machine to communicate over the Internet according to a standard protocol, it is important to have data types compatible with those speci.ed by the protocol. 
We have seen that some C data types, especially long, have different ranges on different machines, and in fact the C standards only specify the minimum ranges for any data type, and not the exact ranges. Although we can choose data types that will be compatible with standard representations on most machines, there is not guarantee of portability. 
The ISO C99 standard introduces another class of integer types in the .le stdint.h. This .le de.nes a set of data types with declarations of the form intN_t and uintN_t, specifying N-bit signed and unsigned integers, for different values of N. The exact values of Nare implementation dependent, but most compilers allow values of 8, 16, 32, and 64. Thus, we can unambiguously declare an unsigned, 16-bit variable by giving it type uint16_t, and a signed variable of 32 bits as int32_t. 
Along with these data types are a set of macros de.ning the minimum and maximum values for each value of N. These have names of the form INTN_MIN, INTN_MAX, and UINTN_MAX. 
The Java standard is quite speci.c about integer data type ranges and repre-sentations. It requires a two¡¯s-complement representation with the exact ranges shown for the 64-bit case (Figure 2.9). In Java, the single-byte data type is called byte instead of char, and there is no long long data type. These detailed require-ments are intended to enable Java programs to behave identically regardless of the machines running them. 
Aside Alternative representations of signed numbers 
There are two other standard representations for signed numbers: 
Ones¡¯ Complement: This is the same as two¡¯s complement, except that the most signi.cant bit has weight .(2w.1 . 1)rather than . 2w.1: 
w.2 
 
B2O (x)= . .x .1(2w.1 . 1)+ x2i i=0 
ww i 

Sign-Magnitude: The most signi.cant bit is a sign bit that determines whether the remaining bits should be given negative or positive weight: 
 
w.2 
 
x
B2S (x)= .(.1)w.1 .x2i 
wi i=0 

Both of these representations have the curious property that there are two different encodings of the number 0. For both representations, [00 ...0] is interpreted as +0. The value .0 can be represented in sign-magnitude form as [10 ...0] and in ones¡¯-complement as [11 ...1]. Although machines based on ones¡¯-complement representations were built in the past, almost all modern machines use two¡¯s complement. We will see that sign-magnitude encoding is used with .oating-point numbers. 
Note the different position of apostrophes: Two¡¯s complement versus Ones¡¯ complement. The term ¡°two¡¯s complement¡± arises from the fact that for nonnegative x we compute a w-bit representation of .x as 2w . x (a single two). The term ¡°ones¡¯ complement¡± comes from the property that we can compute .xin this notation as [111 ...1] . x(multiple ones). 
64  Chapter 2  Representing and Manipulating Information  
Weight 1 2 4 8 16 32 64 128 256 512 1,024 2,048 4,096 8,192 16,384 ¡À32,768 Total  12,345 Bit Value 1 1 0 0 0 0 1 8 1 16 1 32 0 0 0 0 0 0 0 0 0 0 0 0 1 4,096 1 8,192 0 0 0 0 12,345  .12,345 Bit Value 1 1 1 2 1 4 0 0 0 0 0 0 1 64 1 128 1 256 1 512 1 1,024 1 2,048 0 0 0 0 1 16,384 1 .32,768 .12,345  53,191 Bit Value 1 1 1 2 1 4 0 0 0 0 0 0 1 64 1 128 1 256 1 512 1 1,024 1 2,048 0 0 0 0 1 16,384 1 32,768 53,191  
Figure 2.14 Two¡¯s-complement representations of 12,345 and .12,345, and unsigned representation of 53,191. Note that the latter two have identical bit representations. 


As an example, consider the following code: 
1 short x = 12345; 
2 short mx = -x; 
3 
4 show_bytes((byte_pointer) &x, sizeof(short)); 
5 show_bytes((byte_pointer) &mx, sizeof(short)); 
When run on a big-endian machine, this code prints 30 39 and cf c7, indi-cating that x has hexadecimal representation 0x3039, while mx has hexadeci-mal representation 0xCFC7. Expanding these into binary, we get bit patterns [0011000000111001] for x and [1100111111000111] for mx. As Figure 2.14 shows, Equation 2.3 yields values 12,345 and .12,345 for these two bit patterns. 
Practice Problem 2.18 
In Chapter 3, we will look at listings generated by a disassembler, a program that converts an executable program .le back to a more readable ASCII form. These .les contain many hexadecimal numbers, typically representing values in two¡¯s-complement form. Being able to recognize these numbers and understand their signi.cance (for example, whether they are negative or positive) is an important skill. 

For the lines labeled A¨CJ (on the right) in the following listing, convert the hexadecimal values (in 32-bit two¡¯s-complement form) shown to the right of the instruction names (sub, mov, and add) into their decimal equivalents: 
8048337: 81 ec b8 01 00 00 sub $0x1b8,%esp A. 804833d: 8b 55 08 mov 0x8(%ebp),%edx 8048340: 83 c2 14 add $0x14,%edx B. 8048343: 8b 85 58 fe ff ff mov 0xfffffe58(%ebp),%eax C. 8048349: 03 02 add (%edx),%eax 804834b: 89 85 74 fe ff ff mov %eax,0xfffffe74(%ebp) D. 8048351: 8b 55 08 mov 0x8(%ebp),%edx 8048354: 83 c2 44 add $0x44,%edx E. 8048357: 8b 85 c8 fe ff ff mov 0xfffffec8(%ebp),%eax F. 804835d: 89 02 mov %eax,(%edx) 804835f: 8b 45 10 mov 0x10(%ebp),%eax G. 8048362: 03 45 0c add 0xc(%ebp),%eax H. 8048365: 89 85 ec fe ff ff mov %eax,0xfffffeec(%ebp) I. 804836b: 8b 45 08 mov 0x8(%ebp),%eax 804836e: 83 c0 20 add $0x20,%eax J. 8048371: 8b 00 mov (%eax),%eax 
2.2.4 Conversions Between Signed and Unsigned 
C allows casting between different numeric data types. For example, suppose variable x is declared as int and u as unsigned. The expression (unsigned) x converts the value of x to an unsigned value, and (int) u converts the value of u to a signed integer. What should be the effect of casting signed value to unsigned, or vice versa? From a mathematical perspective, one can imagine several different conventions. Clearly, we want to preserve any value that can be represented in both forms. On the other hand, converting a negative value to unsigned might yield zero. Converting an unsigned value that is too large to be represented in two¡¯s-complement form might yield TMax. For most implementations of C, however, the answer to this question is based on a bit-level perspective, rather than on a numeric one. 
For example, consider the following code: 
1 short int v = -12345; 
2 unsigned short uv = (unsigned short) v; 
3 printf("v = %d, uv = %u\n", v, uv); 
When run on a two¡¯s-complement machine, it generates the following output: 
v = -12345, uv = 53191 
What we see here is that the effect of casting is to keep the bit values identical but change how these bits are interpreted. We saw in Figure 2.14 that the 16-bit two¡¯s-complement representation of .12,345 is identical to the 16-bit unsigned representation of 53,191. Casting from short int to unsigned short changed the numeric value, but not the bit representation. 
Similarly, consider the following code: 
1 unsigned u = 4294967295u; /* UMax_32 */ 2 int tu = (int) u; 3 printf("u = %u, tu = %d\n", u, tu); 
When run on a two¡¯s-complement machine, it generates the following output: 
u = 4294967295, tu = -1 
We can see from Figure 2.13 that, for a 32-bit word size, the bit patterns represent-ing 4,294,967,295 (UMax32) in unsigned form and .1 in two¡¯s-complement form are identical. In casting from unsigned int to int, the underlying bit representa-tion stays the same. 
This is a general rule for how most C implementations handle conversions between signed and unsigned numbers with the same word size¡ªthe numeric values might change, but the bit patterns do not. Let us capture this principle in a more mathematical form. Since both B2U and B2T are bijections, they
ww 
have well-de.ned inverses. De.ne U2B to be B2U.1, and T2B to be B2T.1.
ww
ww 
These functions give the unsigned or two¡¯s-complement bit patterns for a numeric value. That is, given an integer x in the range 0 ¡Üx< 2w , the function U2B (x) 
w 
gives the unique w-bit unsigned representation of x. Similarly, when x is in the range .2w.1 ¡Üx< 2w.1, the function T2B (x) gives the unique w-bit two¡¯s-
w 
complement representation of x. Observe that for values in the range 0 ¡Üx< 2w.1, both of these functions will yield the same bit representation¡ªthe most signi.cant bit will be 0, and hence it does not matter whether this bit has positive or negative weight. 
.
Now de.ne the function U2T as U2T (x) =B2T (U2B (x)). This function 
ww ww 
takes a number between 0 and 2w .1 and yields a number between .2w.1 and 2w.1 .1, where the two numbers have identical bit representations, except that the argument is unsigned, while the result has a two¡¯s-complement representa-tion. Similarly, for x between .2w.1 and 2w.1 .1, the function T2U , de.ned as 
w 
.
T2U (x) =B2U (T2B (x)), yields the number having the same unsigned repre-
w ww 
sentation as the two¡¯s-complement representation of x. 
Pursuing our earlier examples, we see from Figure 2.14 that T2U16(.12,345) =53,191, and U2T16(53,191) =.12,345. That is, the 16-bit pattern written in hexadecimal as 0xCFC7 is both the two¡¯s-complement representation of .12,345 and the unsigned representation of 53,191. Similarly, from Figure 2.13, we see that T2U32(.1) =4,294,967,295, and U2T32(4,294,967,295) =.1. That is, UMax has the same bit representation in unsigned form as does .1 in two¡¯s-complement form. 
We see, then, that function U2T describes the conversion of an unsigned number to its 2-complement counterpart, while T2U converts in the opposite direction. These describe the effect of casting between these data types in most C implementations. 

Practice Problem 2.19 
Using the table you .lled in when solving Problem 2.17, .ll in the following table describing the function T2U4: 
x T2U4(x) 
.8 
.3 .2 .1 0 5 

To get a better understanding of the relation between a signed number x and its unsigned counterpart T2U (x), we can use the fact that they have identical bit 
w 

representations to derive a numerical relationship. Comparing Equations 2.1 and 2.3, we can see that for bit pattern x, if we compute the difference B2U (x) .
w 

B2T (x), the weighted sums for bits from 0 to w .2 will cancel each other, 
w ww
leaving a value: B2U (x) .B2T (x) =xw.1(2w.1 ..2w.1) =xw.12w. This gives a relationship B2U (x) =xw.12w +B2T (x).Ifwelet x =T2B (x), we then have 
ww w 
B2U (T2B (x)) =T2U (x) =xw.12w +x (2.5)
ww w 

This relationship is useful for proving relationships between unsigned and two¡¯s-complement arithmetic. In the two¡¯s-complement representation of x, bit xw.1 determines whether or not x is negative, giving 
 
x +2w,x< 0 
T2U (x) = (2.6)
w 
x, x ¡Ý0 

As examples, Figure 2.15 compares how functions B2U and B2T assign values to bit patterns for w =4. For the two¡¯s-complement case, the most signi.cant bit serves as the sign bit, which we diagram as a gray, leftward-pointing bar. For the unsigned case, this bit has positive weight, which we show as a black, rightward-pointing bar. In going from two¡¯s complement to unsigned, the most signi.cant bit changes its weight from .8to +8. As a consequence, the values that are negative in a two¡¯s-complement representation increase by 24 =16 with an unsigned representation. Thus, .5 becomes +11, and .1 becomes +15. 
Figure 2.16 illustrates the general behavior of function T2U. As it shows, when mapping a signed number to its unsigned counterpart, negative numbers are con-verted to large positive numbers, while nonnegative numbers remain unchanged. 

23 = 8 22 = 4 21 = 2 20 = 1 

¨C8¨C7¨C6¨C5¨C4¨C3¨C2¨C1 0 1 2 3 4 5 6 7 8 9 10111213141516 

Figure 2.16 

Conversion from two¡¯s complement to unsigned. 
Function T2U converts negative numbers to large positive numbers. 

2w 
2w
1 
1 Unsigned
2w 
Two¡¯s 
0
complement 0 
1
2w 
Practice Problem 2.20 
Explain how Equation 2.6 applies to the entries in the table you generated when solving Problem 2.19. 
Going in the other direction, we wish to derive the relationship between an unsigned number u and its signed counterpart U2T (u), both having bit repre-
w 
sentations u =U2B (u). We have
w 
B2T (U2B (u)) =U2T (u) =.uw.12w +u (2.7)
ww w 
In the unsigned representation of u, bit uw.1 determines whether or not u is greater than or equal to 2w.1, giving 
 
u, u< 2w.1 
U2T (u) = (2.8)
w 
u .2w,u ¡Ý2w.1 

Section 2.2 Integer Representations 69 

Figure 2.17  2w  
Conversion from un- 
signed to two¡¯s com- 
plement. Function U2T converts numbers greater  Unsigned  2w  1  2w  1  
than 2w.1 .1 to negative  
values.  0  0  Two¡¯s complement  


1
2w 

This behavior is illustrated in Figure 2.17. For small (< 2w.1) numbers, the conver-sion from unsigned to signed preserves the numeric value. Large (¡Ý2w.1) numbers are converted to negative values. 
To summarize, we considered the effects of converting in both directions be-tween unsigned and two¡¯s-complement representations. For values x in the range 0 ¡Üx< 2w.1, we have T2U (x) =x and U2T (x) =x. That is, numbers in this 
ww 

range have identical unsigned and two¡¯s-complement representations. For val-ues outside of this range, the conversions either add or subtract 2w . For exam-ple, we have T2U (.1) =.1 +2w =UMax ¡ªthe negative number closest to
ww 
0 maps to the largest unsigned number. At the other extreme, one can see that T2U (TMin ) =.2w.1 +2w =2w.1 =TMax +1¡ªthe most negative number 
ww w 
maps to an unsigned number just outside the range of positive, two¡¯s-complement numbers. Using the example of Figure 2.14, we can see that T2U16(.12,345) = 65,536 +.12,345 =53,191. 
2.2.5 Signed vs. Unsigned in C 
As indicated in Figures 2.8 and 2.9, C supports both signed and unsigned arithmetic for all of its integer data types. Although the C standard does not specify a particu-lar representation of signed numbers, almost all machines use two¡¯s complement. Generally, most numbers are signed by default. For example, when declaring a constant such as 12345 or 0x1A2B, the value is considered signed. Adding charac-ter ¡®U¡¯or¡®u¡¯ as a suf.x creates an unsigned constant, e.g., 12345U or 0x1A2Bu. 
C allows conversion between unsigned and signed. The rule is that the under-lying bit representation is not changed. Thus, on a two¡¯s-complement machine, the effect is to apply the function U2T when converting from unsigned to signed, and 
w 

T2U when converting from signed to unsigned, where w is the number of bits 
w 

for the data type. Conversions can happen due to explicit casting, such as in the following code: 
1 int tx, ty; 
2 unsigned ux, uy; 
3 

4 tx = (int) ux; 
5 uy = (unsigned) ty; 
Alternatively, they can happen implicitly when an expression of one type is as-signed to a variable of another, as in the following code: 
1 int tx, ty; 2 unsigned ux, uy; 3 4 tx = ux; /* Cast to signed */ 5 uy = ty; /* Cast to unsigned */ 
When printing numeric values with printf, the directives %d, %u, and %x are used to print a number as a signed decimal, an unsigned decimal, and in hexadecimal format, respectively. Note that printf does not make use of any type information, and so it is possible to print a value of type int with directive %u and a value of type unsigned with directive %d. For example, consider the following code: 
1 int x =-1; 2 unsigned u = 2147483648; /* 2 to the 31st */ 3 4 printf("x = %u = %d\n", x, x); 5 printf("u = %u = %d\n", u, u); 
When run on a 32-bit machine, it prints the following: 
x = 4294967295 = -1 u = 2147483648 = -2147483648 
In both cases, printf prints the word .rst as if it represented an unsigned number, and second as if it represented a signed number. We can see the conversion routines in action: T2U32(.1) = UMax32 = 232 . 1 and U2T32(231) = 231 . 232 = 
. 231 
= TMin32. 
Some of the peculiar behavior arises due to C¡¯s handling of expressions con-taining combinations of signed and unsigned quantities. When an operation is performed where one operand is signed and the other is unsigned, C implicitly casts the signed argument to unsigned and performs the operations assuming the numbers are nonnegative. As we will see, this convention makes little dif-ference for standard arithmetic operations, but it leads to nonintuitive results for relational operators such as < and >. Figure 2.18 shows some sample rela-tional expressions and their resulting evaluations, assuming a 32-bit machine us-ing two¡¯s-complement representation. Consider the comparison -1<0U. Since the second operand is unsigned, the .rst one is implicitly cast to unsigned, and hence the expression is equivalent to the comparison 4294967295U < 0U (recall that T2U (.1) = UMax ), which of course is false. The other cases can be under-
ww 
stood by similar analyses. 
Practice Problem 2.21 
Assuming the expressions are evaluated on a 32-bit machine that uses two¡¯s-complement arithmetic, .ll in the following table describing the effect of casting and relational operations, in the style of Figure 2.18: 

Expression Type Evaluation 
-2147483647-1 == 2147483648U 
-2147483647-1 < 2147483647 -2147483647-1U < 2147483647 -2147483647-1 < -2147483647 -2147483647-1U < -2147483647 
Web Aside DATA:TMIN Writing TMin in C 
In Figure 2.18 and in Problem 2.21, we carefully wrote the value of TMin32 as -2147483647-1.Why not simply write it as either -2147483648 or 0x80000000? Looking at the C header .le limits.h,we see that they use a similar method as we have to write TMin32 and TMax32: 
/* Minimum and maximum values a ¡®signed int¡¯ can hold. */ 
#define INT_MAX 2147483647 #define INT_MIN (-INT_MAX -1) 
Unfortunately, a curious interaction between the asymmetry of the two¡¯s-complement representation and the conversion rules of C force us to write TMin32 in this unusual way. Although understanding this issue requires us to delve into one of the murkier corners of the C language standards, it will help us appreciate some of the subtleties of integer data types and representations. 
2.2.6 Expanding the Bit Representation of a Number 
One common operation is to convert between integers having different word sizes while retaining the same numeric value. Of course, this may not be possible when the destination data type is too small to represent the desired value. Converting from a smaller to a larger data type, however, should always be possible. To convert an unsigned number to a larger data type, we can simply add leading zeros to the representation; this operation is known as zero extension. For converting a two¡¯s-complement number to a larger data type, the rule is to perform a sign extension, adding copies of the most signi.cant bit to the representation. Thus, if our original value has bit representation [xw.1,xw.2,...,x0], the expanded representation is [xw.1,...,xw.1,xw.1,xw.2,...,x0]. (We show the sign bit xw.1 in blue to highlight its role in sign extension.) 
Expression  Type  Evaluation  
0==0U  unsigned  1  
-1<0  signed  1  
-1<0U  unsigned  0 *  
2147483647 > -2147483647-1  signed  1  
2147483647U > -2147483647-1  unsigned  0 *  
2147483647 > (int) 2147483648U  signed  1 *  
-1>-2  signed  1  
(unsigned) -1 > -2  unsigned  1  
Figure 2.18 Effects of C promotion rules. Nonintuitive cases marked by ¡®*¡¯. When either operand of a comparison is unsigned, the other operand is implicitly cast to unsigned. See Web Aside data:tmin for why we write TMin32 as -2147483647-1. 


As an example, consider the following code: 
1 short sx = -12345; /* -12345 */ 2 unsigned short usx = sx; /* 53191 */ 3 int x = sx; /* -12345 */ 4 unsigned ux = usx; /* 53191 */ 5 6 printf("sx = %d:\t", sx); 7 show_bytes((byte_pointer) &sx, sizeof(short)); 8 printf("usx = %u:\t", usx); 9 show_bytes((byte_pointer) &usx, sizeof(unsigned short)); 
10 printf("x = %d:\t", x); 11 show_bytes((byte_pointer) &x, sizeof(int)); 12 printf("ux = %u:\t", ux); 13 show_bytes((byte_pointer) &ux, sizeof(unsigned)); 
When run on a 32-bit big-endian machine using a two¡¯s-complement representa-tion, this code prints the output 
sx = -12345: cf c7 usx = 53191: cf c7 x = -12345: ff ff cf c7 ux = 53191: 00 00 cf c7 
We see that although the two¡¯s-complement representation of .12,345 and the unsigned representation of 53,191 are identical for a 16-bit word size, they dif-fer for a 32-bit word size. In particular, .12,345 has hexadecimal representation 0xFFFFCFC7, while 53,191 has hexadecimal representation 0x0000CFC7. The for-mer has been sign extended¡ª16 copies of the most signi.cant bit 1, having hexa-decimal representation 0xFFFF, have been added as leading bits. The latter has been extended with 16 leading zeros, having hexadecimal representation 0x0000. 
As an illustration, Figure 2.19 shows the result of applying expanding from word size w=3to w=4 by sign extension. Bit vector [101] represents the value .4 +1 =.3. Applying sign extension gives bit vector [1101] representing the value .8 +4 +1 =.3. We can see that, for w=4, the combined value of the two most signi.cant bits is .8 +4 =.4, matching the value of the sign bit for w=3. Similarly, bit vectors [111] and [1111] both represent the value .1. 
Can we justify that sign extension works? What we want to prove is that 
B2Tw+k([xw.1,...,xw.1,xw.1,xw.2,...,x0])=B2Tw([xw.1,xw.2,...,x0]) 
ktimes 
Figure 2.19 

Examples of sign exten-sion from w=3 to w=4. 


For w=4, the combined 22 = 4 weight of the upper 2 bits 
21 = 2 

is .8 +4 =.4, matching 
20 = 1

that of the sign bit for 


w=3. ¨C8¨C7¨C6¨C5¨C4¨C3¨C2¨C1 0 1 2 3 4 5 6 7 8 


where, in the expression on the left-hand side, we have made kadditional copies of bit x .1. The proof follows by induction on k. That is, if we can prove that sign 
w 

extending by 1 bit preserves the numeric value, then this property will hold when sign extending by an arbitrary number of bits. Thus, the task reduces to proving that 
B2T +1([xw.1,xw.1,xw.2,...,x0])=B2T ([xw.1,x .2,...,x0])
w ww 
Expanding the left-hand expression with Equation 2.3 gives the following: 
w.1 
 

B2Tw+1([xw.1,xw.1,xw.2,...,x0])=.xw.12w + xi2i 
i=0 
w.2 
 
=.xw.12w +xw.12w.1 + x2i 
i i=0 
w.2 
 
=.x 2w . 2w.1 + x2i 
w.1 i i=0 
w.2 
 
=.xw.12w.1 + x2i 
i i=0 
=B2T ([xw.1,xw.2,...,x0])
w 

The key property we exploit is that 2w . 2w.1 =2w.1. Thus, the combined effect of adding a bit of weight .2w and of converting the bit having weight .2w.1 to be one with weight 2w.1 is to preserve the original numeric value. 
Practice Problem 2.22 
Show that each of the following bit vectors is a two¡¯s-complement representation of .5 by applying Equation 2.3: 
A. [1011] 
B. [11011] 
C. [111011] Observe that the second and third bit vectors can be derived from the .rst by sign extension. 
One point worth making is that the relative order of conversion from one data size to another and between unsigned and signed can affect the behavior of a program. Consider the following code: 
1 short sx = -12345; /* -12345 */ 2 unsigned uy = sx; /* Mystery! */ 3 4 printf("uy = %u:\t", uy); 5 show_bytes((byte_pointer) &uy, sizeof(unsigned)); 
When run on a big-endian machine, this code causes the following output to be printed: 
uy = 4294954951: ff ff cf c7 
This shows that when converting from short to unsigned, we .rst change the size and then from signed to unsigned. That is, (unsigned) sx is equivalent to (unsigned) (int) sx, evaluating to 4,294,954,951, not (unsigned) (unsigned short) sx, which evaluates to 53,191. Indeed this convention is required by the C standards. 
Practice Problem 2.23 
Consider the following C functions: 
int fun1(unsigned word) { 
return (int) ((word << 24) >> 24); } 
int fun2(unsigned word) { 
return ((int) word << 24) >> 24; } 
Assume these are executed on a machine with a 32-bit word size that uses two¡¯s-complement arithmetic. Assume also that right shifts of signed values are per-formed arithmetically, while right shifts of unsigned values are performed logically. 

A. Fill in the following table showing the effect of these functions for several example arguments. You will .nd it more convenient to work with a hexa-decimal representation. Just remember that hex digits 8 through F have their most signi.cant bits equal to 1. 
w fun1(w) fun2(w) 0x00000076 
0x87654321 0x000000C9 0xEDCBA987 

B. Describe in words the useful computation each of these functions performs. 
2.2.7 Truncating Numbers 
Suppose that, rather than extending a value with extra bits, we reduce the number of bits representing a number. This occurs, for example, in the code: 
1 int x = 53191; 2 short sx = (short) x; /* -12345 */ 3 int y = sx; /* -12345 */ 
On a typical 32-bit machine, when we cast x to be short, we truncate the 32-bit int to be a 16-bit short int. As we saw before, this 16-bit pattern is the two¡¯s-complement representation of .12,345. When we cast this back to int, sign extension will set the high-order 16 bits to ones, yielding the 32-bit two¡¯s-complement representation of .12,345. 
When truncating a w-bit number x= [x .1,xw.2,...,x0]to a k-bit number, 
w we drop the high-order w. k bits, giving a bit vector x = [xk.1,xk.2,...,x0]. Truncating a number can alter its value¡ªa form of over.ow. We now investigate what numeric value will result. For an unsigned number x, the result of truncating it to kbits is equivalent to computing xmod 2k . This can be seen by applying the modulus operation to Equation 2.1: 
w.1 
 

B2U ([xw.1,xw.2,...,x0])mod 2k = x2i mod 2k i=0 
wi 
k.1 
 
2i
= x mod 2k 
i i=0 
k.1 
 
2i
= x
i i=0 
= B2Uk([xk.1,xk.2,...,x0]) 

In this derivation, we make use of the property that 2i mod 2k = 0 for any i¡Ý k, k.1 k.1
and that 2i ¡Ü= 2k . 1 <2k .
i=0 xi i=02i For a two¡¯s-complement number x, a similar argument shows that 
B2T ([xw.1,x .2,...,x0])mod 2k = B2Uk([xk.1,xk.2,...,x0]). That is, xmod
ww 2k can be represented by an unsigned number having bit-level representation [xk.1,xk.2,...,x0]. In general, however, we treat the truncated number as being signed. This will have numeric value U2Tk(xmod 2k). Summarizing, the effect of truncation for unsigned numbers is 
B2Uk([xk.1,xk.2,...,x0])= B2U ([xw.1,xw.2,...,x0])mod 2k, (2.9)
w 
while the effect for two¡¯s-complement numbers is 
B2Tk([xk.1,xk.2,...,x0])= U2Tk(B2U ([xw.1,xw.2,...,x0])mod 2k) (2.10)
w 
Practice Problem 2.24 
Suppose we truncate a 4-bit value (represented by hex digits 0 through F)toa3-bit value (represented as hex digits 0 through 7). Fill in the table below showing the effect of this truncation for some cases, in terms of the unsigned and two¡¯s-complement interpretations of those bit patterns. 

Explain how Equations 2.9 and 2.10 apply to these cases. 
2.2.8 Advice on Signed vs. Unsigned 
As we have seen, the implicit casting of signed to unsigned leads to some non-intuitive behavior. Nonintuitive features often lead to program bugs, and ones involving the nuances of implicit casting can be especially dif.cult to see. Since the casting takes place without any clear indication in the code, programmers often overlook its effects. 
The following two practice problems illustrate some of the subtle errors that can arise due to implicit casting and the unsigned data type. 

Practice Problem 2.25 
Consider the following code that attempts to sum the elements of an array a, where the number of elements is given by parameter length: 
1 /* WARNING: This is buggy code */ 2 float sum_elements(float a[], unsigned length) { 3 int i; 4 float result = 0; 5 6 for (i = 0; i <= length-1; i++) 7 result += a[i]; 8 return result; 
9 } 

When run with argument length equal to 0, this code should return 0.0. Instead it encounters a memory error. Explain why this happens. Show how this code can be corrected. 
Practice Problem 2.26 
You are given the assignment of writing a function that determines whether one string is longer than another. You decide to make use of the string library function strlen having the following declaration: 
/* Prototype for library function strlen */ 
size_t strlen(const char *s); 
Here is your .rst attempt at the function: 
/* Determine whether string s is longer than string t */ /* WARNING: This function is buggy */ 
int strlonger(char *s, char *t) { 
return strlen(s) -strlen(t) > 0; } 
When you test this on some sample data, things do not seem to work quite right. You investigate further and determine that data type size_t is de.ned (via typedef) in header .le stdio.h to be unsigned int. 
A. For what cases will this function produce an incorrect result? 
B. Explain how this incorrect result comes about. 
C. Show how to .x the code so that it will work reliably. 
Aside Security vulnerability in getpeername 
In 2002, programmers involved in the FreeBSD open source operating systems project realized that their implementation of the getpeername library function had a security vulnerability. A simpli.ed version of their code went something like this: 
1 /* 2 * Illustration of code vulnerability similar to that found in 3 * FreeBSD¡¯s implementation of getpeername() 4 */ 5 6 /* Declaration of library function memcpy */ 7 void *memcpy(void *dest, void *src, size_t n); 8 9 /* Kernel memory region holding user-accessible data */ 
10 #define KSIZE 1024 11 char kbuf[KSIZE]; 12 13 /* Copy at most maxlen bytes from kernel region to user buffer */ 14 int copy_from_kernel(void *user_dest, int maxlen) { 15 /* Byte count len is minimum of buffer size and maxlen */ 16 int len = KSIZE < maxlen ? KSIZE : maxlen; 17 memcpy(user_dest, kbuf, len); 18 return len; 
19 } 

In this code, we show the prototype for library function memcpy on line 7, which is designed to copy a speci.ed number of bytes n from one region of memory to another. 
The function copy_from_kernel, starting at line 14, is designed to copy some of the data main-tained by the operating system kernel to a designated region of memory accessible to the user. Most of the data structures maintained by the kernel should not be readable by a user, since they may con-tain sensitive information about other users and about other jobs running on the system, but the region shown as kbuf was intended to be one that the user could read. The parameter maxlen is intended to be the length of the buffer allocated by the user and indicated by argument user_dest. The computation at line 16 then makes sure that no more bytes are copied than are available in either the source or the destination buffer. 
Suppose, however, that some malicious programmer writes code that calls copy_from_kernel with a negative value of maxlen. Then the minimum computation on line 16 will compute this value for len, which will then be passed as the parameter n to memcpy. Note, however, that parameter n is declared as having data type size_t. This data type is declared (via typedef) in the library .le stdio.h. Typically it is de.ned to be unsigned int on 32-bit machines. Since argument n is unsigned, memcpy will treat it as a very large, positive number and attempt to copy that many bytes from the kernel region to the user¡¯s buffer. Copying that many bytes (at least 231) will not actually work, because the program will encounter invalid addresses in the process, but the program could read regions of the kernel memory for which it is not authorized. 
We can see that this problem arises due to the mismatch between data types: in one place the length parameter is signed; in another place it is unsigned. Such mismatches can be a source of bugs and, as this example shows, can even lead to security vulnerabilities. Fortunately, there were no reported cases where a programmer had exploited the vulnerability in FreeBSD. They issued a security advisory, ¡°FreeBSD-SA-02:38.signed-error,¡± advising system administrators on how to apply a patch that would remove the vulnerability. The bug can be .xed by declaring parameter maxlen to copy_from_kernel to be of type size_t, to be consistent with parameter n of memcpy. We should also declare local variable len and the return value to be of type size_t. 
We have seen multiple ways in which the subtle features of unsigned arith-metic, and especially the implicit conversion of signed to unsigned, can lead to errors or vulnerabilities. One way to avoid such bugs is to never use unsigned numbers. In fact, few languages other than C support unsigned integers. Appar-ently these other language designers viewed them as more trouble than they are worth. For example, Java supports only signed integers, and it requires that they be implemented with two¡¯s-complement arithmetic. The normal right shift oper-ator >> is guaranteed to perform an arithmetic shift. The special operator >>> is de.ned to perform a logical right shift. 
Unsigned values are very useful when we want to think of words as just col-lections of bits with no numeric interpretation. This occurs, for example, when packing a word with .ags describing various Boolean conditions. Addresses are naturally unsigned, so systems programmers .nd unsigned types to be helpful. Unsigned values are also useful when implementing mathematical packages for modular arithmetic and for multiprecision arithmetic, in which numbers are rep-resented by arrays of words. 
2.3 
Integer 
Arithmetic 

Many beginning programmers are surprised to .nd that adding two positive num-bers can yield a negative result, and that the comparison x<y can yield a different result than the comparison x-y<0. These properties are artifacts of the .nite na-ture of computer arithmetic. Understanding the nuances of computer arithmetic can help programmers write more reliable code. 
2.3.1 Unsigned Addition 
Consider two nonnegative integers x and y, such that 0 ¡Ü x, y ¡Ü 2w . 1. Each of these numbers can be represented by w-bit unsigned numbers. If we compute their sum, however, we have a possible range 0 ¡Ü x + y ¡Ü 2w+1 . 2. Representing this sum could require w + 1 bits. For example, Figure 2.20 shows a plot of the function x + y when x and y have 4-bit representations. The arguments (shown on the hor-izontal axes) range from 0 to 15, but the sum ranges from 0 to 30. The shape of the function is a sloping plane (the function is linear in both dimensions). If we were 
Integer addition 
32 
28 
24 
20 
16 
14 12
12 
10 
8 
8 46 4
0 0 
2
2 
4 
6 
0
8 
10 
12 
14 
Figure 2.20 Integer addition. With a 4-bit word size, the sum could require 5 bits. 
to maintain the sum as a w+1-bit number and add it to another value, we may re-quire w+ 2 bits, and so on. This continued ¡°word size in.ation¡± means we cannot place any bound on the word size required to fully represent the results of arith-metic operations. Some programming languages, such as Lisp, actually support in.nite precision arithmetic to allow arbitrary (within the memory limits of the machine, of course) integer arithmetic. More commonly, programming languages support .xed-precision arithmetic, and hence operations such as ¡°addition¡± and ¡°multiplication¡± differ from their counterpart operations over integers. 
Unsigned arithmetic can be viewed as a form of modular arithmetic. Unsigned addition is equivalent to computing the sum modulo 2w . This value can be com-puted by simply discarding the high-order bit in the w+1-bit representation of x+ y. For example, consider a 4-bit number representation with x= 9 and y= 12, having bit representations [1001] and [1100], respectively. Their sum is 21, having a 5-bit representation [10101]. But if we discard the high-order bit, we get [0101], that is, decimal value 5. This matches the value 21 mod 16 = 5. 
In general, we can see that if x+ y<2w , the leading bit in the w+1-bit representation of the sum will equal 0, and hence discarding it will not change the numeric value. On the other hand, if 2w¡Ü x+ y<2w+1, the leading bit in 

Relation between integer 2w1 addition and unsigned addition. When x+ y is 
2w

greater than 2w. 1, the sum over.ows. 
0 

the w+1-bit representation of the sum will equal 1, and hence discarding it is equivalent to subtracting 2wfrom the sum. These two cases are illustrated in Figure 
2.21. This will give us a value in the range 0 ¡Ü x+ y. 2w<2w+1 . 2w= 2w, which is precisely the modulo 2w sum of x and y. Let us de.ne the operation +u for 
w 

arguments xand ysuch that 0 ¡Ü x,y<2w as 
 
x+ y, x+ y<2w 

x+u y= (2.11)
wx+ y. 2w, 2w¡Ü x+ y<2w+1 

This is precisely the result we get in C when performing addition on two w-bit unsigned values. 
An arithmetic operation is said to over.ow when the full integer result cannot .t within the word size limits of the data type. As Equation 2.11 indicates, over.ow occurs when the two operands sum to 2w or more. Figure 2.22 shows a plot of the unsigned addition function for word size w= 4. The sum is computed modulo 
u

24 = 16. When x+ y<16, there is no over.ow, and x+4 yis simply x+ y. This is shown as the region forming a sloping plane labeled ¡°Normal.¡± When x+ y¡Ý 16, the addition over.ows, having the effect of decrementing the sum by 16. This is shown as the region forming a sloping plane labeled ¡°Over.ow.¡± 
When executing C programs, over.ows are not signaled as errors. At times, however, we might wish to determine whether over.ow has occurred. For exam-
.

ple, suppose we compute s= x+u y, and we wish to determine whether sequals
w 

x+ y. We claim that over.ow has occurred if and only if s<x(or equivalently, s<y). To see this, observe that x+ y¡Ý x, and hence if sdid not over.ow, we will surely have s¡Ý x. On the other hand, if sdid over.ow, we have s= x+ y. 2w . Given that y<2w , we have y. 2w<0, and hence s= x+ (y. 2w)<x.Inour earlier example, we saw that 9 +4u 12 = 5. We can see that over.ow occurred, since 5 <9. 

Practice Problem 2.27 
Write a function with the following prototype: 
/* Determine whether arguments can be added without overflow */ 
int uadd_ok(unsigned x, unsigned y); 
This function should return 1 if arguments x and y can be added without causing over.ow. 
Unsigned addition (4¨Cbit word) 
Overflow 
16  
14  Normal  
12  
10  
8  14  
6  12  
10  
4  8  
2  6  
0  4  
0  2  4  6  8  10  12  0  2  
Figure 2.22 Unsigned addition. With a 4-bit word size, addition is performed modulo 16. 


14 
Modular addition forms a mathematical structure known as an abelian group, named after the Danish mathematician Niels Henrik Abel (1802¨C1829). That is, it is commutative (that¡¯s where the ¡°abelian¡± part comes in) and associative; it has an identity element 0, and every element has an additive inverse. Let us consider the set of w-bit unsigned numbers with addition operation +u . For every value x,
w 
there must be some value -u x such that -u x +u x = 0. When x = 0, the additive 
w ww 
inverse is clearly 0. For x> 0, consider the value 2w . x. Observe that this number is in the range 0 < 2w . x< 2w, and (x + 2w . x) mod 2w = 2w mod 2w = 0. Hence, it is the inverse of x under +u . These two cases lead to the following equation for 
w 
0 ¡Ü x< 2w: 
 
u x, x = 0 
-x = (2.12)
w 
2w . x, x> 0 
Practice Problem 2.28 
We can represent a bit pattern of length w = 4 with a single hex digit. For an unsigned interpretation of these digits, use Equation 2.12 to .ll in the following table giving the values and the bit representations (in hex) of the unsigned additive inverses of the digits shown. 

x -4u x 
Hex Decimal Decimal Hex 
0 
5 8 D F 

2.3.2 Two¡¯s-Complement Addition 
With two¡¯s-complement addition, we must decide what to do when the result is either too large (positive) or too small (negative) to represent. Given integer values x and y in the range .2w.1 ¡Ü x, y ¡Ü 2w.1 . 1, their sum is in the range . 2w ¡Ü x + y ¡Ü 2w . 2, potentially requiring w + 1 bits to represent exactly. As before, we avoid ever-expanding data sizes by truncating the representation to w bits. The result is not as familiar mathematically as modular addition, however. 
The w-bit two¡¯s-complement sum of two numbers has the exact same bit-level representation as the unsigned sum. In fact, most computers use the same machine instruction to perform either unsigned or signed addition. Thus, we can de.ne two¡¯s-complement addition for word size w, denoted as +t , on operands x and y
w such that . 2w.1 ¡Ü x,y < 2w.1 as 
t . u 
x + y = U2T (T2U (x) + T2U (y)) (2.13)
ww w
ww 

By Equation 2.5, we can write T2U (x) as xw.12w + x, and T2U (y) as yw.12w +
ww 

y. Using the property that +u is simply addition modulo 2w , along with the prop-
w 

erties of modular addition, we then have 
x +t y = U2T (T2U (x) +u T2U (y)) 
ww w

ww 
= U2T [(xw.12w + x + yw.12w + y) mod 2w]
w 
= U2T [(x + y) mod 2w]
w 

The terms xw.12w and yw.12w drop out since they equal 0 modulo 2w . 
.

To better understand this quantity, let us de.ne z as the integer sum z = x + y, 
.. 
z as z = z mod 2w , and z as z = U2Tw(z ). The value z is equal to x +t y.We 
w 

can divide the analysis into four cases, as illustrated in Figure 2.23: 
1. . 2w ¡Ü z< . 2w.1. Then we will have z = z + 2w . This gives 0 ¡Ü z< . 2w.1 + 2w = 2w.1. Examining Equation 2.8, we see that z is in the range such that z = z . This case is referred to as negative over.ow. We have added two negative numbers x and y (that¡¯s the only way we can have z< . 2w.1) and obtained a nonnegative result z = x + y + 2w . 
Figure 2.23  x y  
Relation between integer  +2w  
and two¡¯s-complement addition. When x+ y is  Case 4  
.1less than . 2w, there is  +2w  1  
a negative over.ow. When it is greater than 2w.1 + 1,  Case 3  
 



there is a positive over.ow. 
0 
Case 2 1
¨C2w 
Case 1 
¨C2w 
2. . 2w.1 ¡Ü z<0. Then we will again have z = z+ 2w , giving . 2w.1 + 2w = 2w.1 ¡Ü z<2w . Examining Equation 2.8, we see that z is in such a range that z = z . 2w, and therefore z = z . 2w = z+ 2w . 2w = z. That is, our two¡¯s-complement sum z equals the integer sum x+ y. 
.1 .1
3. 
0 ¡Ü z<2w . Then we will have z = z, giving 0 ¡Ü z<2w , and hence z = z = z. Again, the two¡¯s-complement sum z equals the integer sum x+ y. 

4. 
2w.1 ¡Ü z<2w . We will again have z = z, giving 2w.1 ¡Ü z<2w . But in this range we have z = z . 2w, giving z = x+ y. 2w . This case is referred to as positive over.ow. We have added two positive numbers xand y(that¡¯s the only way we can have z¡Ý 2w.1) and obtained a negative result z = x+ y. 2w . 


By the preceding analysis, we have shown that when operation +t is applied 
w 
to values xand yin the range .2w.1 ¡Ü x,y¡Ü 2w.1 . 1, we have 
. .. .. 
x+ y. 2w, 2w.1 ¡Ü x+ y Positive over.ow 
x+t y= x+ y, .2w.1 ¡Ü x+ y<2w.1 Normal (2.14)
w 
x+ y+ 2w,x+ y<. 2w.1 Negative over.ow 
As an illustration, Figure 2.24 shows some examples of 4-bit two¡¯s-complement addition. Each example is labeled by the case to which it corresponds in the derivation of Equation 2.14. Note that 24 = 16, and hence negative over.ow yields a result 16 more than the integer sum, and positive over.ow yields a result 16 less. We include bit-level representations of the operands and the result. Observe that the result can be obtained by performing binary addition of the operands and truncating the result to four bits. 
Figure 2.25 illustrates two¡¯s-complement addition for word size w= 4. The operands range between .8 and 7. When x+ y<.8, two¡¯s-complement addition has a negative under.ow, causing the sum to be incremented by 16. When .8 ¡Ü x+ y<8, the addition yields x+ y. When x+ y¡Ý 8, the addition has a negative over.ow, causing the sum to be decremented by 16. Each of these three ranges forms a sloping plane in the .gure. 

Section 2.3 Integer Arithmetic 85 

x  y  x+ y  t x+4 y  Case  
.8  .5  .13  3  1  
[1000]  [1011]  [10011]  [0011]  
.8  .8  .16  0  1  
[1000]  [1000]  [10000]  [0000]  
.8  5  .3  .3  2  
[1000]  [0101]  [11101]  [1101]  
2  5  7  7  3  
[0010]  [0101]  [00111]  [0111]  
5  5  10  .6  4  
[0101]  [0101]  [01010]  [1010]  

Figure 2.24 Two¡¯s-complement addition examples. The bit-level representation of the 4-bit two¡¯s-complement sum can be obtained by performing binary addition of the operands and truncating the result to 4 bits. 
Two¡¯s-complement addition (4-bit word) 
Normal 
Negative Positiveoverflow overflow
8 6 4 2 0 

6 2 
4 2
4 

0 6 
2 8 
4 8 
6 
6 
4 2 0 82 4 6 

Figure 2.25 Two¡¯s-complement addition. With a 4-bit word size, addition can have a negative over.ow when x+ y<.8 and a positive over.ow when x+ y¡Ý 8. 
Equation 2.14 also lets us identify the cases where over.ow has occurred. When both xand yare negative but x+t y¡Ý 0, we have negative over.ow. When 
w 
both xand yare positive but x+t y<0, we have positive over.ow. 
w 
Practice Problem 2.29 
Fill in the following table in the style of Figure 2.24. Give the integer values of the 5-bit arguments, the values of both their integer and two¡¯s-complement sums, the bit-level representation of the two¡¯s-complement sum, and the case from the derivation of Equation 2.14. 
xy x+ yx+5t y Case 
[10100]  [10001]  
[11000]  [11000]  
[10111]  [01000]  
[00010]  [00101]  
[01100]  [00100]  

Practice Problem 2.30 
Write a function with the following prototype: 
/* Determine whether arguments can be added without overflow */ 
int tadd_ok(int x, int y); 
This function should return 1 if arguments x and y can be added without causing over.ow. 
Practice Problem 2.31 
Your coworker gets impatient with your analysis of the over.ow conditions for two¡¯s-complement addition and presents you with the following implementation of tadd_ok: 
/* Determine whether arguments can be added without overflow */ /* WARNING: This code is buggy. */ 
int tadd_ok(int x, int y) { 
int sum = x+y; 
return (sum-x == y) && (sum-y == x); } 
You look at the code and laugh. Explain why. 

Practice Problem 2.32 
You are assigned the task of writing code for a function tsub_ok, with arguments x and y, that will return 1 if computing x-y does not cause over.ow. Having just written the code for Problem 2.30, you write the following: 
/* Determine whether arguments can be subtracted without overflow */ /* WARNING: This code is buggy. */ 
int tsub_ok(int x, int y) { 
return tadd_ok(x, -y); } 
For what values of x and y will this function give incorrect results? Writing a correct version of this function is left as an exercise (Problem 2.74). 
2.3.3 Two¡¯s-Complement Negation 
We can see that every number xin the range .2w.1 ¡Üx<2w.1 has an additive in-
t

verse under + as follows. First, for x=.2w.1, we can see that its additive inverse 
w 
.1t

is simply .x. That is, we have .2w<.x<2w.1 and .x+ x=.x+x=0. For 
w 
.1 

x=.2w =TMin , on the other hand, .x=2w.1 cannot be represented as a w-
w 

bit number. We claim that this special value has itself as the additive inverse under 
t .1t
+ . The value of .2w + .2w.1 is given by the third case of Equation 2.14, since 
ww 
.1t

.2w.1 +.2w.1 =.2w . This gives .2w + .2w.1 =.2w+2w=0. From this 
w 

analysis, we can de.ne the two¡¯s-complement negation operation -t for xin the 
w 

range .2w.1 ¡Üx<2w.1 as 
 
.1 .1 
t .2w,x=.2w 
-x= (2.15)
w .x, x>.2w.1 

Practice Problem 2.33 
We can represent a bit pattern of length w=4 with a single hex digit. For a two¡¯s-complement interpretation of these digits, .ll in the following table to determine the additive inverses of the digits shown: 
x -4t x 
Hex Decimal Decimal Hex 
0 
5 8 D F 

What do you observe about the bit patterns generated by two¡¯s-complement and unsigned (Problem 2.28) negation? 
Web Aside DATA:TNEG Bit-level representation of two¡¯s-complement negation 
There are several clever ways to determine the two¡¯s-complement negation of a value represented at the bit level. These techniques are both useful, such as when one encounters the value 0xfffffffa when debugging a program, and they lend insight into the nature of the two¡¯s-complement representation. 
One technique for performing two¡¯s-complement negation at the bit level is to complement the bits and then increment the result. In C, we can state that for any integer value x, computing the expressions -x and ~x+1 will give identical results. 
Here are some examples with a 4-bit word size: 
x  ~x  incr(~x)  
[0101]  5  [1010]  .6  [1011]  .5  
[0111]  7  [1000]  .8  [1001]  .7  
[1100]  .4  [0011]  3  [0100]  4  
[0000]  0  [1111]  .1  [0000]  0  
[1000]  .8  [0111]  7  [1000]  .8  

For our earlier example, we know that the complement of 0xf is 0x0, and the complement of 0xa is 0x5, and so 0xfffffffa is the two¡¯s-complement representation of .6. 
A second way to perform two¡¯s-complement negation of a number x is based on splitting the bit vector into two parts. Let kbe the position of the rightmost 1, so the bit-level representation of xhas the form [x .1,x .2,...,xk+1,1,0,...0]. (This is possible as long as x= 0.) The negation is then written 
ww in binary form as [~xw.1,~xw.2,...~ xk+1,1,0,...,0]. That is, we complement each bit to the left of bit position k. We illustrate this idea with some 4-bit numbers, where we highlight the rightmost pattern 1,0,...,0 in italics: 
x .x [1100] .4 [0100]4 [1000] .8[1000] .8 [0101] 5 [1011] .5 [0111] 7 [1001] .7 
2.3.4 Unsigned Multiplication 
Integers x and y in the range 0 ¡Ü x,y¡Ü 2w . 1 can be represented as w-bit un-signed numbers, but their product x.y can range between 0 and (2w . 1)2 = 22w . 2w+1 + 1. This could require as many as 2wbits to represent. Instead, un-signed multiplication in C is de.ned to yield the w-bit value given by the low-order wbits of the 2w-bit integer product. By Equation 2.9, this can be seen to be equiv-alent to computing the product modulo 2w . Thus, the effect of the w-bit unsigned multiplication operation *u is 
w 
x*u y= (x.y)mod 2w (2.16)
w 

2.3.5 Two¡¯s-Complement Multiplication 
Integers x and y in the range .2w.1 ¡Üx, y ¡Ü2w.1 .1 can be represented as w-bit two¡¯s-complement numbers, but their product x.y can range between .2w.1 . (2w.1 .1) =.22w.2 +2w.1 and .2w.1 . .2w.1 =22w.2. This could require as many as 2w bits to represent in two¡¯s-complement form¡ªmost cases would .t into 2w .1 bits, but the special case of 22w.2 requires the full 2w bits (to include a sign bit of 0). Instead, signed multiplication in C generally is performed by truncating the 2w-bit product to w bits. By Equation 2.10, the effect of the w-bit two¡¯s-complement multiplication operation *t is 
w 
x *t y =U2T ((x .y) mod 2w) (2.17)
w
w 

We claim that the bit-level representation of the product operation is identical for both unsigned and two¡¯s-complement multiplication. That is, given bit vectors x and y of length w, the bit-level representation of the unsigned product B2U (x) *u 
w w 

B2U (y) is identical to the bit-level representation of the two¡¯s-complement 
w 

product B2T (x) *t B2T (y). This implies that the machine can use a single type 
ww
w 
of multiply instruction to multiply both signed and unsigned integers. 
As illustrations, Figure 2.26 shows the results of multiplying different 3-bit numbers. For each pair of bit-level operands, we perform both unsigned and two¡¯s-complement multiplication, yielding 6-bit products, and then truncate these to 3 bits. The unsigned truncated product always equals x.y mod 8. The bit-level representations of both truncated products are identical for both unsigned and two¡¯s-complement multiplication, even though the full 6-bit representations differ. 
To show that the low-order bits of the two products (unsigned and two¡¯s complement) are identical, let x =B2T (x) and y =B2T (y) be the two¡¯s-
ww 

complement values denoted by these bit patterns, and let x =B2U (x) and y = 
w 

B2Uw(y) be the unsigned values. From Equation 2.5, we have x =x +xw.12w , and y = y + yw.12w . Computing the product of these values modulo 2w gives the following: 
Mode  x  y  x . y  Truncated x . y  
Unsigned  5  [101]  3  [011]  15  [001111]  7  [111]  
Two¡¯s comp.  .3  [101]  3  [011]  .9  [110111]  .1  [111]  
Unsigned  4  [100]  7  [111]  28  [011100]  4  [100]  
Two¡¯s comp.  .4  [100]  .1  [111]  4  [000100]  .4  [100]  
Unsigned  3  [011]  3  [011]  9  [001001]  1  [001]  
Two¡¯s comp.  3  [011]  3  [011]  9  [001001]  1  [001]  
Figure 2.26 Three-bit unsigned and two¡¯s-complement multiplication examples. Although the bit-level representations of the full products may differ, those of the truncated products are identical. 


(x .y) mod 2w = [(x + xw.12w) . (y + yw.12w)] mod 2w (2.18) 
= [x.y + (xw.1y + yw.1x)2w + xw.1yw.122w] mod 2w = (x .y) mod 2w All of the terms with weight 2w drop out due to the modulus operator, and so we have shown that the low-order w bits of x.y and x .y are identical. 
Practice Problem 2.34 
Fill in the following table showing the results of multiplying different 3-bit num-bers, in the style of Figure 2.26: 
Mode  x  y  x . y  Truncated x . y  
Unsigned  [100]  [101]  
Two¡¯s comp.  [100]  [101]  
Unsigned  [010]  [111]  
Two¡¯s comp.  [010]  [111]  
Unsigned  [110]  [110]  
Two¡¯s comp.  [110]  [110]  

We can see that unsigned arithmetic and two¡¯s-complement arithmetic over w-bit numbers are isomorphic¡ªthe operations +u, -u , and *u have the exact same 
ww w 
effect at the bit level as do +t, -t , and *t. 
ww w 
Practice Problem 2.35 
You are given the assignment to develop code for a function tmult_ok that will determine whether two arguments can be multiplied without causing over.ow. Here is your solution: 
/* Determine whether arguments can be multiplied without overflow */ int tmult_ok(int x, int y) { 
int p=x*y; 
/* Either x is zero, or dividing p by x gives y */ 
return !x || p/x == y; } 
You test this code for a number of values of x and y, and it seems to work properly. Your coworker challenges you, saying, ¡°If I can¡¯t use subtraction to test whether addition has over.owed (see Problem 2.31), then how can you use division to test whether multiplication has over.owed?¡± 
Devise a mathematical justi.cation of your approach, along the following lines. First, argue that the case x = 0 is handled correctly. Otherwise, consider w-bit numbers x (x = 0), y, p, and q, where p is the result of performing two¡¯s-complement multiplication on x and y, and q is the result of dividing p by x. 

1. Show that x.y, the integer product of x and y, can be written in the form 
x.y = p + t2w , where t = 0 if and only if the computation of p over.ows. 
2. 
Show that p can be written in the form p = x.q + r, where |r| < |x|. 

3. 
Show that q = y if and only if r = t = 0. 


Practice Problem 2.36 
For the case where data type int has 32 bits, devise a version of tmult_ok (Prob-lem 2.35) that uses the 64-bit precision of data type long long, without using division. 
Aside Security vulnerability in the XDR library 
In 2002, it was discovered that code supplied by Sun Microsystems to implement the XDR library, a 
widely used facility for sharing data structures between programs, had a security vulnerability arising 
from the fact that multiplication can over.ow without any notice being given to the program. Code similar to that containing the vulnerability is shown below: 
1 /* 2 * Illustration of code vulnerability similar to that found in 3 * Sun¡¯s XDR library. 4 */ 5 void* copy_elements(void *ele_src[], int ele_cnt, size_t ele_size) { 6 /* 7 * Allocate buffer for ele_cnt objects, each of ele_size bytes 8 * and copy from locations designated by ele_src 9 */ 
10 void *result = malloc(ele_cnt * ele_size); 
11 if (result == NULL) 
12 /* malloc failed */ 
13 return NULL; 
14 void *next = result; 
15 int i; 

16 for (i = 0; i < ele_cnt; i++) { 
17 /* Copy object i to destination */ 
18 memcpy(next, ele_src[i], ele_size); 
19 /* Move pointer to next memory region */ 
20 next += ele_size; 
21 } 22 return result; 
23 } 

The function copy_elements is designed to copy ele_cnt data structures, each consisting of ele_ 
size bytes into a buffer allocated by the function on line 10. The number of bytes required is computed 
as ele_cnt * ele_size. 
Imagine, however, that a malicious programmer calls this function with ele_cnt being 1,048,577 (220 + 1) and ele_size being 4,096 (212). Then the multiplication on line 10 will over.ow, causing only 4096 bytes to be allocated, rather than the 4,294,971,392 bytes required to hold that much data. The loop starting at line 16 will attempt to copy all of those bytes, overrunning the end of the allocated buffer, and therefore corrupting other data structures. This could cause the program to crash or otherwise misbehave. 
The Sun code was used by almost every operating system, and in such widely used programs as Internet Explorer and the Kerberos authentication system. The Computer Emergency Response Team (CERT), an organization run by the Carnegie Mellon Software Engineering Institute to track security vulnerabilities and breaches, issued advisory ¡°CA-2002-25,¡± and many companies rushed to patch their code. Fortunately, there were no reported security breaches caused by this vulnerability. 
A similar vulnerability existed in many implementations of the library function calloc. These have since been patched. 
Practice Problem 2.37 
You are given the task of patching the vulnerability in the XDR code shown above. You decide to eliminate the possibility of the multiplication over.owing (on a 32-bit machine, at least) by computing the number of bytes to allocate using data type long long unsigned. You replace the original call to malloc (line 10) as follows: 
long long unsigned asize = ele_cnt * (long long unsigned) ele_size; void *result = malloc(asize); 
A. Does your code provide any improvement over the original? 
B. How would you change the code to eliminate the vulnerability, assuming data type size_t is the same as unsigned int, and these are 32 bits long? 
2.3.6 Multiplying by Constants 
On most machines, the integer multiply instruction is fairly slow, requiring 10 or more clock cycles, whereas other integer operations¡ªsuch as addition, subtrac-tion, bit-level operations, and shifting¡ªrequire only 1 clock cycle. As a conse-quence, one important optimization used by compilers is to attempt to replace multiplications by constant factors with combinations of shift and addition oper-ations. We will .rst consider the case of multiplying by a power of 2, and then generalize this to arbitrary constants. 
Let xbe the unsigned integer represented by bit pattern [xw.1,xw.2,...,x0]. Then for any k¡Ý 0, we claim the bit-level representation of x2k is given by [x .1,x .2,...,x0,0,...,0], where kzeros have been added to the right. This 

ww 

property can be derived using Equation 2.1: 
w.1 
 
2i+k

B2U +k([x .1,x .2,...,x0,0,...,0])= x i=0 
www i 
w.1 
 
2i.2k
= x
i i=0 
= x2k 

For k<w, we can truncate the shifted bit vector to be of length w, giving [x .k.1,x .k.2,...,x0,0,...,0]. By Equation 2.9, this bit vector has numeric 
ww value x2k mod 2w = x*u2k . Thus, for unsigned variable x, the C expression x<<k 
w 

is equivalent to x * pwr2k, where pwr2k equals 2k . In particular, we can compute pwr2k as 1U << k. 
By similar reasoning, we can show that for a two¡¯s-complement number x having bit pattern [x .1,x .2,...,x0], and any k in the range 0 ¡Ü k<w, bit
ww pattern [x .k.1,...,x0,0,...,0] will be the two¡¯s-complement representation 
w of x*t2k . Therefore, for signed variable x , the C expression x<<k is equivalent 
w 

to x * pwr2k, where pwr2k equals 2k . 
Note that multiplying by a power of 2 can cause over.ow with either unsigned or two¡¯s-complement arithmetic. Our result shows that even then we will get the same effect by shifting. 
Given that integer multiplication is much more costly than shifting and adding, many C compilers try to remove many cases where an integer is being multi-plied by a constant with combinations of shifting, adding, and subtracting. For example, suppose a program contains the expression x*14. Recognizing that 14 = 23 + 22 + 21, the compiler can rewrite the multiplication as (x<<3) + (x<<2) + (x<<1), replacing one multiplication with three shifts and two additions. The two computations will yield the same result, regardless of whether x is unsigned or two¡¯s complement, and even if the multiplication would cause an over.ow. (This can be shown from the properties of integer arithmetic.) Even better, the compiler can also use the property 14 = 24 . 21 to rewrite the multiplication as (x<<4) -(x<<1), requiring only two shifts and a subtraction. 
Practice Problem 2.38 
As we will see in Chapter 3, the lea instruction can perform computations of the form (a<<k) + b, where k is either 0, 1, 2, or 3, and b is either 0 or some program value. The compiler often uses this instruction to perform multiplications by constant factors. For example, we can compute 3*a as (a<<1) + a. 
Considering cases where b is either 0 or equal to a, and all possible values of k, what multiples of a can be computed with a single lea instruction? 
Generalizing from our example, consider the task of generating code for 
the expression x* K, for some constant K. The compiler can express the binary 
representation of K as an alternating sequence of zeros and ones: 
[(0 ...0)(1 ...1)(0 ...0)...(1 ...1)]. 
For example, 14 can be written as [(0 ...0)(111)(0)]. Consider a run of ones from bit position ndown to bit position m(n¡Ý m). (For the case of 14, we have n= 3 and m= 1.) We can compute the effect of these bits on the product using either of two different forms: 
Form A: (x<<n) + (x<<n.1)+ ...+ (x<<m) Form B: (x<<n+1) -(x<<m) 
By adding together the results for each run, we are able to compute x* K with-out any multiplications. Of course, the trade-off between using combinations of shifting, adding, and subtracting versus a single multiplication instruction depends on the relative speeds of these instructions, and these can be highly machine de-pendent. Most compilers only perform this optimization when a small number of shifts, adds, and subtractions suf.ce. 
Practice Problem 2.39 
How could we modify the expression for form B for the case where bit position n is the most signi.cant bit? 
Practice Problem 2.40 
For each of the following values of K, .nd ways to express x* K using only the speci.ed number of operations, where we consider both additions and subtrac-tions to have comparable cost. You may need to use some tricks beyond the simple form A and B rules we have considered so far. 
K  Shifts  Add/Subs  Expression  
6  2  1  
31  1  1  
.6  2  1  
55  2  2  

Practice Problem 2.41 
For a run of 1s starting at bit position ndown to bit position m(n¡Ý m), we saw that we can generate two forms of code, A and B. How should the compiler decide which form to use? 

k  >> k (Binary)  Decimal  12340/2k  
0  0011000000110100  12340  12340.0  
1  0001100000011010  6170  6170.0  
4  0000001100000011  771  771.25  
8  0000000000110000  48  48.203125  
Figure 2.27 Dividing unsigned numbers by powers of 2. The examples illustrate how performing a logical right shift by k has the same effect as dividing by 2k and then rounding toward zero. 


2.3.7 Dividing by Powers of Two 
Integer division on most machines is even slower than integer multiplication¡ª requiring 30 or more clock cycles. Dividing by a power of 2 can also be performed using shift operations, but we use a right shift rather than a left shift. The two dif-ferent shifts¡ªlogical and arithmetic¡ªserve this purpose for unsigned and two¡¯s-complement numbers, respectively. 
Integer division always rounds toward zero. For x¡Ý0 and y>0, the result should be x/y , where for any real number a, a is de.ned to be the unique integer a such that a ¡Üa<a +1. As examples, 3.14 =3, .3.14 =.4, and 
3 =3. 

Consider the effect of applying a logical right shift by kto an unsigned number. We claim this gives the same result as dividing by 2k . As examples, Figure 2.27 shows the effects of performing logical right shifts on a 16-bit representation of 12,340 to perform division by 1, 2, 16, and 256. The zeros shifted in from the left are shown in italics. We also show the result we would obtain if we did these divisions with real arithmetic. These examples show that the result of shifting consistently rounds toward zero, as is the convention for integer division. 
To show this relation between logical right shifting and dividing by a power of 2, let xbe the unsigned integer represented by bit pattern [xw.1,xw.2,...,x0], and k be in the range 0 ¡Ük<w. Let x be the unsigned number with w.k-bit representation [xw.1,xw.2,...,x ], and x be the unsigned number with
k k-bit representation [xk.1,...,x0]. We claim that x = x/2k . To see this, by 
 
w.1 w.1 k.1

Equation 2.1, we have x= x2i , x = x2i.k , and x = 2i.We 
i=0 ii=ki i=0 xi 
k.1

can therefore write xas x=2kx +x . Observe that 0 ¡Üx ¡Ü 02i =2k .1, and 
i= 

hence 0 ¡Üx<2k , implying that x/2k =0. Therefore, x/2k = x +x/2k = x + x/2k =x. 
Performing a logical right shift of bit vector [xw.1,xw.2,...,x0]by k yields the bit vector 
[0,...,0,xw.1,xw.2,...,x ]
k 

This bit vector has numeric value x. Therefore, for unsigned variable x, the C expression x>>k is equivalent to x / pwr2k, where pwr2k equals 2k . 
96  Chapter 2  Representing and Manipulating Information  
k  >> k (Binary)  Decimal  .12340/2k  
0 1 4 8  1100111111001100 1110011111100110 1111110011111100 1111111111001111  .12340 .6170 .772 .49  .12340.0 .6170.0 .771.25 .48.203125  
Figure 2.28 Applying arithmetic right shift. The examples illustrate that arithmetic right shift is similar to division by a power of 2, except that it rounds down rather than toward zero. 


Now consider the effect of performing an arithmetic right shift on a two¡¯s-complement number. For a positive number, we have 0 as the most signi.cant bit, and so the effect is the same as for a logical right shift. Thus, an arithmetic right shift by kis the same as division by 2k for a nonnegative number. As an example of a negative number, Figure 2.28 shows the effect of applying arithmetic right shift to a 16-bit representation of .12,340 for different shift amounts. As we can see, the result is almost the same as dividing by a power of 2. For the case when no rounding is required (k =1), the result is correct. But when rounding is required, shifting causes the result to be rounded downward rather than toward zero, as should be the convention. For example, the expression -7/2 should yield -3 rather than -4. 
Let us better understand the effect of arithmetic right shifting and how we can use it to perform division by a power of 2. Let x be the two¡¯s-complement integer represented by bit pattern [xw.1,xw.2,...,x0], and k be in the range 0 ¡Ük<w. Let x be the two¡¯s-complement number represented by the w.k bits [xw.1,xw.2,...,x ], and x be the unsigned number represented by the
k 
low-order k bits [xk.1,...,x0]. By a similar analysis as the unsigned case, we have x=2kx +x , and 0 ¡Üx<2k, giving x = x/2k . Furthermore, observe that shifting bit vector [xw.1,xw.2,...,x0]right arithmetically by kyields the bit vector 
[xw.1,...,xw.1,xw.1,xw.2,...,xk] 
which is the sign extension from w.k bits to w bits of [xw.1,xw.2,...,x ].
k Thus, this shifted bit vector is the two¡¯s-complement representation of x/2k . This analysis con.rms our .ndings from the examples of Figure 2.28. For x¡Ý0, or when no rounding is required (x =0), our analysis shows that this shifted result is the desired value. For x<0 and y>0, however, the result of integer division should be x/y , where for any real number a, a is de.ned to be the unique integer a such that a .1 <a¡Üa. That is, integer division should round negative results upward toward zero. Thus, right shifting a negative number by kis not equivalent to dividing it by 2k when rounding occurs. This analysis also con.rms our .ndings from the example of Figure 2.28. We can correct for this improper rounding by ¡°biasing¡± the value before shifting. This technique exploits the property that x/y = (x+y.1)/y for integers x and y such that y>0. As examples, when x=.30 and y=4, we have x+y.1 =.27, and .30/4 =.7 =.27/4 . When x=.32 and y=4, 

k  Bias  .12,340 + Bias (Binary)  >> k (Binary)  Decimal  .12340/2k  
0  0  1100111111001100  1100111111001100  .12340  .12340.0  
1  1  1100111111001101  1110011111100110  .6170  .6170.0  
4  15  1100111111011011  1111110011111101  .771  .771.25  
8  255  1101000011001011  1111111111010000  .48  .48.203125  
Figure 2.29 Dividing two¡¯s-complement numbers by powers of 2. By adding a bias before the right shift, the result is rounded toward zero. 


we have x +y .1 =.29, and .32/4 =.8 =.29/4 . To see that this relation holds in general, suppose that x =ky +r, where 0 ¡Ür<y, giving (x +y .1)/y = k +(r +y .1)/y, and so (x +y .1)/y =k + (r +y .1)/y . The latter term will equal 0 when r =0, and 1 when r> 0. That is, by adding a bias of y .1to x and then rounding the division downward, we will get k when y divides x and k +1 otherwise. Thus, for x< 0, if we .rst add 2k .1to x before right shifting, we will get a correctly rounded result. 
This analysis shows that for a two¡¯s-complement machine using arithmetic right shifts, the C expression 
(x<0 ? x+(1<<k)-1 : x) >> k 
is equivalent to x/pwr2k, where pwr2k equals 2k . 
Figure 2.29 demonstrates how adding the appropriate bias before performing the arithmetic right shift causes the result to be correctly rounded. In the third column, we show the result of adding the bias value to .12,340, with the lower k bits (those that will be shifted off to the right) shown in italics. We can see that the bits to the left of these may or may not be incremented. For the case where no rounding is required (k =1), adding the bias only affects bits that are shifted off. For the cases where rounding is required, adding the bias causes the upper bits to be incremented, so that the result will be rounded toward zero. 
Practice Problem 2.42 
Write a function div16 that returns the value x/16 for integer argument x. Your function should not use division, modulus, multiplication, any conditionals (if or ?:), any comparison operators (e.g., <, >,or ==), or any loops. You may assume that data type int is 32 bits long and uses a two¡¯s-complement representation, and that right shifts are performed arithmetically. 
We now see that division by a power of 2 can be implemented using logical or arithmetic right shifts. This is precisely the reason the two types of right shifts are available on most machines. Unfortunately, this approach does not generalize to division by arbitrary constants. Unlike multiplication, we cannot express division by arbitrary constants K in terms of division by powers of 2. 
Practice Problem 2.43 
In the following code, we have omitted the de.nitions of constants M and N: 
#define M /* Mystery number 1 */ #define N /* Mystery number 2 */ int arith(int x, int y) { 
int result = 0; result = x*M + y/N; /* M and N are mystery numbers. */ return result; 
} 
We compiled this code for particular values of M and N. The compiler opti-mized the multiplication and division using the methods we have discussed. The following is a translation of the generated machine code back into C: 
/* Translation of assembly code for arith */ 
int optarith(int x, int y) { intt=x; x <<= 5; x-=t; if (y<0)y+=7; y >>= 3; /* Arithmetic shift */ return x+y; 
} 
What are the values of M and N? 
2.3.8 Final Thoughts on Integer Arithmetic 
As we have seen, the ¡°integer¡± arithmetic performed by computers is really a form of modular arithmetic. The .nite word size used to represent numbers limits the range of possible values, and the resulting operations can over.ow. We have also seen that the two¡¯s-complement representation provides a clever way to be able to represent both negative and positive values, while using the same bit-level implementations as are used to perform unsigned arithmetic¡ªoperations such as addition, subtraction, multiplication, and even division have either identical or very similar bit-level behaviors whether the operands are in unsigned or two¡¯s-complement form. 
We have seen that some of the conventions in the C language can yield some surprising results, and these can be sources of bugs that are hard to recognize or understand. We have especially seen that the unsigned data type, while concep-tually straightforward, can lead to behaviors that even experienced programmers do not expect. We have also seen that this data type can arise in unexpected ways, for example, when writing integer constants and when invoking library routines. 

Practice Problem 2.44 
Assume we are running code on a 32-bit machine using two¡¯s-complement arith-metic for signed values. Right shifts are performed arithmetically for signed values and logically for unsigned values. The variables are declared and initialized as follows: 
int x = foo(); /* Arbitrary value */ int y = bar(); /* Arbitrary value */ 
unsigned ux = x; unsigned uy = y; 
For each of the following C expressions, either (1) argue that it is true (evalu-ates to 1) for all values of x and y, or (2) give values of x and y for which it is false (evaluates to 0): 
A. (x>0)||(x-1<0) 
B. (x & 7) != 7 || (x<<29 < 0) C. (x*x)>=0 D. x<0||-x<=0 E. x>0||-x>=0 
F. x+y == uy+ux 

G. x*~y + uy*ux == -x 
2.4 
Floating 
Point 

A .oating-point representation encodes rational numbers of the form V = x ¡Á 2y. It is useful for performing computations involving very large numbers (|V | 0), numbers very close to 0 (|V | 1), and more generally as an approximation to real arithmetic. 
Up until the 1980s, every computer manufacturer devised its own conventions for how .oating-point numbers were represented and the details of the operations performed on them. In addition, they often did not worry too much about the accuracy of the operations, viewing speed and ease of implementation as being more critical than numerical precision. 
All of this changed around 1985 with the advent of IEEE Standard 754, a carefully crafted standard for representing .oating-point numbers and the oper-ations performed on them. This effort started in 1976 under Intel¡¯s sponsorship with the design of the 8087, a chip that provided .oating-point support for the 8086 processor. They hired William Kahan, a professor at the University of California, Berkeley, as a consultant to help design a .oating-point standard for its future processors. They allowed Kahan to join forces with a committee generating an industry-wide standard under the auspices of the Institute of Electrical and Elec-tronics Engineers (IEEE). The committee ultimately adopted a standard close to the one Kahan had devised for Intel. Nowadays, virtually all computers support what has become known as IEEE .oating point. This has greatly improved the portability of scienti.c application programs across different machines. 
Aside The IEEE 
The Institute of Electrical and Electronic Engineers (IEEE¡ªpronounced ¡°Eye-Triple-Eee¡±) is a pro-fessional society that encompasses all of electronic and computer technology. It publishes journals, sponsors conferences, and sets up committees to de.ne standards on topics ranging from power trans-mission to software engineering. 
In this section, we will see how numbers are represented in the IEEE .oating-point format. We will also explore issues of rounding, when a number cannot be represented exactly in the format and hence must be adjusted upward or down-ward. We will then explore the mathematical properties of addition, multiplica-tion, and relational operators. Many programmers consider .oating point to be at best uninteresting and at worst arcane and incomprehensible. We will see that since the IEEE format is based on a small and consistent set of principles, it is really quite elegant and understandable. 
2.4.1 Fractional Binary Numbers 
A .rst step in understanding .oating-point numbers is to consider binary numbers having fractional values. Let us .rst examine the more familiar decimal notation. Decimal notation uses a representation of the form dd .1 ...d1d0.d.1d.2 ...d.n,
mm where each decimal digit d ranges between 0 and 9. This notation represents a
i value d de.ned as m 
 
d= 10i ¡Á d
i i=.n 
The weighting of the digits is de.ned relative to the decimal point symbol (¡®.¡¯), meaning that digits to the left are weighted by positive powers of 10, giving integral values, while digits to the right are weighted by negative powers of 10, giving fractional values. For example, 12.3410 represents the number 1 ¡Á 101 + 2 ¡Á 100 + 3 ¡Á 10.1 + 4 ¡Á 10.2 = 12 34 
100 . By analogy, consider a notation of the form bb .1 ...b1b0.b.1b.2 ... 
mm 
b.n.1b.n, where each binary digit, or bit, biranges between 0 and 1, as is illustrated 
in Figure 2.30. This notation represents a number bde.ned as 
m 
 
b= 2i ¡Á b (2.19)
i i=.n 
The symbol ¡®.¡¯ now becomes a binary point, with bits on the left being weighted by positive powers of 2, and those on the right being weighted by negative powers of 2. For example, 101.112 represents the number 1 ¡Á 22 + 0 ¡Á 21 + 1 ¡Á 20 + 1 ¡Á 2.1 + 1 ¡Á 2.2 = 4 + 0 + 1 + 21 + 1 = 543.
4 
Section 2.4 Floating Point 101 

Figure 2.30 
2m Fractional binary repre-2m¨C1 sentation. Digits to the left of the binary point have weights of the form 2i , 4 while those to the right ¡¤ ¡¤ ¡¤ 2 have weights of the form 1/2i . 1 
bb ¡¤ ¡¤ ¡¤ b2 b1 b0 ¡¤ ¡¤ ¡¤ ¡¤ bb
mm¨C1 b¨C1 b¨C2 b¨C3 ¨Cn¨C1 ¨Cn 
1/2 
1/4 
¡¤ ¡¤ ¡¤ 
1/8 
1/2n¨C1 
1/2n 

One can readily see from Equation 2.19 that shifting the binary point one position to the left has the effect of dividing the number by 2. For example, while 
101.112 represents the number 543,10.1112 represents the number 2 + 0 + 21 + 1 
4 + 1 = 278. Similarly, shifting the binary point one position to the right has the 
8 effect of multiplying the number by 2. For example, 1011.12 represents the number 
8 + 0 + 2 + 1 + 1 = 1121.
2 Note that numbers of the form 0.11 ...12 represent numbers just below 1. For 
example, 0.1111112 represents 63 0 .
64. We will use the shorthand notation 1. to represent such values. 
Assuming we consider only .nite-length encodings, decimal notation cannot 1
represent numbers such as and 5 exactly. Similarly, fractional binary notation 
37 can only represent numbers that can be written x¡Á 2y. Other values can only be 
approximated. For example, the number 1 can be represented exactly as the frac-
5 tional decimal number 0.20. As a fractional binary number, however, we cannot represent it exactly and instead must approximate it with increasing accuracy by lengthening the binary representation: 
Representation Value Decimal 
0

0.02 20.010 
1

0.012 40.2510 
0.0102 20.2510
8 3

0.00112 16 0.187510 
6

0.001102 32 0.187510 
13

0.0011012 64 0.20312510 
0.00110102 26 0.20312510
128 51

0.001100112 256 0.1992187510 
Practice Problem 2.45 
Fill in the missing information in the following table: 
Fractional value Binary representation Decimal representation 
1 
0.001 0.125
8 3 4 
25 16 10.1011 1.001 5.875 3.1875 
Practice Problem 2.46 
The imprecision of .oating-point arithmetic can have disastrous effects. On Febru-ary 25, 1991, during the .rst Gulf War, an American Patriot Missile battery in Dharan, Saudi Arabia, failed to intercept an incoming Iraqi Scud missile. The Scud struck an American Army barracks and killed 28 soldiers. The U.S. General Accounting Of.ce (GAO) conducted a detailed analysis of the failure [72] and de-termined that the underlying cause was an imprecision in a numeric calculation. In this exercise, you will reproduce part of the GAO¡¯s analysis. 
The Patriot system contains an internal clock, implemented as a counter that is incremented every 0.1 seconds. To determine the time in seconds, the program would multiply the value of this counter by a 24-bit quantity that was 
1
a fractional binary approximation to 10 . In particular, the binary representation 
1
of 10 is the nonterminating sequence 0.000110011[0011] ...2, where the portion in brackets is repeated inde.nitely. The program approximated 0.1, as a value x,by considering just the .rst 23 bits of the sequence to the right of the binary point: x= 0.00011001100110011001100. (See Problem 2.51 for a discussion of how they could have approximated 0.1 more precisely.) 
A. What is the binary representation of 0.1 . x? 
B. What is the approximate decimal value of 0.1 . x? 
C. The clock starts at 0 when the system is .rst powered up and keeps counting up from there. In this case, the system had been running for around 100 hours. What was the difference between the actual time and the time computed by the software? 
D. The system predicts where an incoming missile will appear based on its velocity and the time of the last radar detection. Given that a Scud travels at around 2000 meters per second, how far off was its prediction? 
Normally, a slight error in the absolute time reported by a clock reading would not affect a tracking computation. Instead, it should depend on the relative time between two successive readings. The problem was that the Patriot software had been upgraded to use a more accurate function for reading time, but not all of the function calls had been replaced by the new code. As a result, the tracking software used the accurate time for one reading and the inaccurate time for the other [100]. 

2.4.2 IEEE Floating-Point Representation 
Positional notation such as considered in the previous section would not be ef-.cient for representing very large numbers. For example, the representation of 5 ¡Á 2100 would consist of the bit pattern 101 followed by 100 zeros. Instead, we would like to represent numbers in a form x¡Á 2y by giving the values of xand y. 
The IEEE .oating-point standard represents a number in a form V = (.1)s ¡Á M¡Á 2E: 
. The sign s determines whether the number is negative (s= 1) or positive (s= 0), where the interpretation of the sign bit for numeric value 0 is handled as a special case. 
. The signi.cand M is a fractional binary number that ranges either between 1 and 2 . or between 0 and 1 . . 
. The exponent Eweights the value by a (possibly negative) power of 2. 
The bit representation of a .oating-point number is divided into three .elds to encode these values: 
. The single sign bit s directly encodes the sign s. 
. The k-bit exponent .eld exp = ek.1 ...e1e0 encodes the exponent E. 
. The n-bit fraction .eld frac = f .1 ...f1f0 encodes the signi.cand M, but the 
n 

value encoded also depends on whether or not the exponent .eld equals 0. 
Figure 2.31 shows the packing of these three .elds into words for the two most common formats. In the single-precision .oating-point format (a float in C), .elds s, exp, and frac are 1, k= 8, and n= 23 bits each, yielding a 32-bit representation. In the double-precision .oating-point format (a double in C), .elds s, exp, and frac are 1, k= 11, and n= 52 bits each, yielding a 64-bit representation. 
The value encoded by a given bit representation can be divided into three different cases (the latter having two variants), depending on the value of exp. These are illustrated in Figure 2.32 for the single-precision format. 
Case 1: Normalized Values 
This is the most common case. It occurs when the bit pattern of exp is neither all zeros (numeric value 0) nor all ones (numeric value 255 for single precision, 2047 for double). In this case, the exponent .eld is interpreted as representing a signed integer in biased form. That is, the exponent value is E= e. Bias where e is the unsigned number having bit representation ek.1 ...e1e0, and Bias is a bias 
Single precision 3130 2322 0 
s  exp  frac  

Double precision 63 62  52 51  32  

31  frac(31:0)  0  
Figure 2.31 Standard .oating-point formats. Floating-point numbers are represented by three .elds. For the two most common formats, these are packed in 32-bit (single precision) or 64-bit (double precision) words. 


s  exp  frac(51:32)  

1.
 Normalized 

2.
 Denormalized 


s  ¡Ù 0 & ¡Ù 255  f  


3a. Infinity 

3b. NaN 

Figure 2.32 Categories of single-precision, .oating-point values. The value of the exponent determines whether the number is (1) normalized, (2) denormalized, or a 
(3) special value. 
value equal to 2k.1 . 1 (127 for single precision and 1023 for double). This yields exponent ranges from .126 to +127 for single precision and .1022 to +1023 for double precision. 
The fraction .eld frac is interpreted as representing the fractional value f, where 0 ¡Ü f<1, having binary representation 0.f .1 ...f1f0, that is, with the 
n 
binary point to the left of the most signi.cant bit. The signi.cand is de.ned to be M= 1 + f. This is sometimes called an implied leading 1 representation, because we can view Mto be the number with binary representation 1.fn.1fn.2 ...f0. This representation is a trick for getting an additional bit of precision for free, since we can always adjust the exponent Eso that signi.cand M is in the range 1 ¡Ü M<2 (assuming there is no over.ow). We therefore do not need to explicitly represent the leading bit, since it always equals 1. 

Case 2: Denormalized Values 
When the exponent .eld is all zeros, the represented number is in denormalized form. In this case, the exponent value is E = 1 . Bias, and the signi.cand value is M = f , that is, the value of the fraction .eld without an implied leading 1. 
Aside Why set the bias this way for denormalized values? 
Having the exponent value be 1 . Bias rather than simply .Bias might seem counterintuitive. We will see shortly that it provides for smooth transition from denormalized to normalized values. 
Denormalized numbers serve two purposes. First, they provide a way to represent numeric value 0, since with a normalized number we must always have M ¡Ý 1, and hence we cannot represent 0. In fact the .oating-point representation of +0.0 has a bit pattern of all zeros: the sign bit is 0, the exponent .eld is all zeros (indicating a denormalized value), and the fraction .eld is all zeros, giving M = f = 0. Curiously, when the sign bit is 1, but the other .elds are all zeros, we get the value .0.0. With IEEE .oating-point format, the values .0.0 and +0.0 are considered different in some ways and the same in others. 
A second function of denormalized numbers is to represent numbers that are very close to 0.0. They provide a property known as gradual under.ow in which possible numeric values are spaced evenly near 0.0. 
Case 3: Special Values 
A .nal category of values occurs when the exponent .eld is all ones. When the fraction .eld is all zeros, the resulting values represent in.nity, either +¡Þ when s = 0, or .¡Þ when s = 1. In.nity can represent results that over.ow, as when we multiply two very large numbers, or when we divide by zero. When the fraction .eld is nonzero, the resulting value is called a ¡°NaN,¡± short for ¡°Not a Number.¡± Such values are returned as the result of an operation where the result cannot be
¡Ì 

given as a real number or as in.nity, as when computing .1or ¡Þ.¡Þ. They can also be useful in some applications for representing uninitialized data. 
2.4.3 Example Numbers 
Figure 2.33 shows the set of values that can be represented in a hypothetical 6-bit format having k = 3 exponent bits and n = 2 fraction bits. The bias is 23.1 . 1 = 
3. Part A of the .gure shows all representable values (other than NaN). The two in.nities are at the extreme ends. The normalized numbers with maximum magnitude are ¡À14. The denormalized numbers are clustered around 0. These can be seen more clearly in part B of the .gure, where we show just the numbers between .1.0 and +1.0. The two zeros are special cases of denormalized numbers. Observe that the representable numbers are not uniformly distributed¡ªthey are denser nearer the origin. 
Figure 2.34 shows some examples for a hypothetical 8-bit .oating-point format having k = 4 exponent bits and n = 3 fraction bits. The bias is 24.1 . 1 = 7. The 

10 50 5 10  

Denormalized 
Normalized 

Infinity 
(a)
 Complete range 

1 
0.8 0.6 0.4 0.2 0 0.2 0.4 0.6 0.8 1 




Denormalized 
Normalized 

Infinity 
(b) Values between 1.0 and 1.0 

Figure 2.33 Representable values for 6-bit .oating-point format. There are k = 3 exponent bits and n = 2 fraction bits. The bias is 3. 

Exponent  Fraction  Value  
Description  Bit representation  e  E  2E  f  M  2E ¡Á M  V  Decimal  
Zero Smallest pos. Largest denorm.  0 0000 000 0 0000 001 0 0000 010 0 0000 011 . . . 0 0000 111  0 0 0 0 0  .6 .6 .6 .6 .6  1 64 1 64 1 64 1 64 1 64  0 8 1 8 2 8 3 8 7 8  0 8 1 8 2 8 3 8 7 8  0 512 1 512 2 512 3 512 7 512  0 1 512 1 256 3 512 7 512  0.0 0.001953 0.003906 0.005859 0.013672  
Smallest norm. One Largest norm.  0 0001 000 0 0001 001 . . . 0 0110 110 0 0110 111 0 0111 000 0 0111 001 0 0111 010 . . . 0 1110 110 0 1110 111  1 1 6 6 7 7 7 14 14  .6 .6 .1 .1 0 0 0 7 7  1 64 1 64 1 2 1 2 1 1 1 128 128  0 8 1 8 6 8 7 8 0 8 1 8 2 8 6 8 7 8  8 8 9 8 14 8 15 8 8 8 9 8 10 8 14 8 15 8  8 512 9 512 14 16 15 16 8 8 9 8 10 8 1792 8 1920 8  1 64 9 512 7 8 15 16 1 9 8 5 4 224 240  0.015625 0.017578 0.875 0.9375 1.0 1.125 1.25 224.0 240.0  
In.nity  0 1111 000  ¡ª  ¡ª  ¡ª  ¡ª  ¡ª  ¡ª  ¡Þ  ¡ª  

Figure 2.34 Example nonnegative values for 8-bit .oating-point format. There are k = 4 exponent bits and n = 3 fraction bits. The bias is 7. 
Section 2.4 

.gure is divided into three regions representing the three classes of numbers. The different columns show how the exponent .eld encodes the exponent E, while the fraction .eld encodes the signi.cand M, and together they form the represented value V =2E ¡ÁM. Closest to 0 are the denormalized numbers, starting with 0 itself. Denormalized numbers in this format have E=1 .7 =.6, giving a weight 
1 17
2E = 64. The fractions f and signi.cands M range over the values 0, 8 ,..., 8, giving numbers V intherange0to 1 = 7 
64 ¡Á 78 512. The smallest normalized numbers in this format also have E=1 .7 =.6, 17
and the fractions also range over the values 0, 8 ,... 8. However, the signi.cands 15 81
then range from 1 +0 =1to 1 + 7 = 8 , giving numbers V in the range = 
8 512 64 15
to 

512. Observe the smooth transition between the largest denormalized number 7 
512 8

and the smallest normalized number 512. This smoothness is due to our de.nition of Efor denormalized values. By making it 1 .Bias rather than .Bias, we com-pensate for the fact that the signi.cand of a denormalized number does not have an implied leading 1. 
As we increase the exponent, we get successively larger normalized values, passing through 1.0 and then to the largest normalized number. This number has exponent E=7, giving a weight 2E =128. The fraction equals 78, giving a signi.-
15

cand M= 8 . Thus, the numeric value is V =240. Going beyond this over.ows to +¡Þ. 
One interesting property of this representation is that if we interpret the bit representations of the values in Figure 2.34 as unsigned integers, they occur in ascending order, as do the values they represent as .oating-point numbers. This is no accident¡ªthe IEEE format was designed so that .oating-point numbers could be sorted using an integer sorting routine. A minor dif.culty occurs when dealing with negative numbers, since they have a leading 1, and they occur in descending order, but this can be overcome without requiring .oating-point operations to perform comparisons (see Problem 2.83). 
Practice Problem 2.47 
Consider a 5-bit .oating-point representation based on the IEEE .oating-point format, with one sign bit, two exponent bits (k=2), and two fraction bits (n=2). The exponent bias is 22.1 .1 =1. 
The table that follows enumerates the entire nonnegative range for this 5-bit .oating-point representation. Fill in the blank table entries using the following directions: 
e: The value represented by considering the exponent .eld to be an unsigned integer 
E: The value of the exponent after biasing 2E: The numeric weight of the exponent 
f: The value of the fraction 
M: The value of the signi.cand 2E ¡ÁM: The (unreduced) fractional value of the number 
V : The reduced fractional value of the number Decimal: The decimal representation of the number 
Express the values of 2E , f , M,2E ¡ÁM, and V either as integers (when possible) or as fractions of the form x , where y is a power of 2. You need not .ll in entries 
y 
marked ¡°¡ª¡±. Bits eE 2E fM 2E ¡ÁMV Decimal 
00000 
00001 00010 00011 00100 
15 55
00101 101 
44 44 
00110 00111 01000 

01011  
01100  ¡ª  ¡ª  ¡ª  ¡ª  ¡ª  ¡ª  ¡ª  
01101  ¡ª  ¡ª  ¡ª  ¡ª  ¡ª  ¡ª  ¡ª  
01110  ¡ª  ¡ª  ¡ª  ¡ª  ¡ª  ¡ª  ¡ª  
01111  ¡ª  ¡ª  ¡ª  ¡ª  ¡ª  ¡ª  ¡ª  

Figure 2.35 shows the representations and numeric values of some important single-and double-precision .oating-point numbers. As with the 8-bit format shown in Figure 2.34, we can see some general properties for a .oating-point representation with a k-bit exponent and an n-bit fraction: 
. The value +0.0 always has a bit representation of all zeros. . The smallest positive denormalized value has a bit representation consisting of a 1 in the least signi.cant bit position and otherwise all zeros. It has a fraction (and signi.cand) value M =f =2.n and an exponent value E =.2k.1 +2. 
n.2k.1+2
The numeric value is therefore V =2. . . The largest denormalized value has a bit representation consisting of an exponent .eld of all zeros and a fraction .eld of all ones. It has a fraction 
(and signi.cand) value M =f =1 . 2.n (which we have written 1 . ) and an exponent value E =.2k.1 +2. The numeric value is therefore V =(1 . 2.n) ¡Á2.2k.1+2, which is just slightly smaller than the smallest normalized 
value. 

Single precision  Double precision  
Description  exp  frac  Value  Decimal  Value  Decimal  
Zero  00 ...00  0 ...00  0  0.0  0  0.0  
Smallest denorm.  00 ...00  0 ...01  2.23 ¡Á2.126  1.4 ¡Á10.45  2.52 ¡Á2.1022  9 ¡Á10.3244. 
Largest denorm.  00 ...00  1 ...11  (1 . )¡Á2.126  1.2 ¡Á10.38  (1 . )¡Á2.1022  2 ¡Á10.3082. 
Smallest norm.  00 ...01  0 ...00  1 ¡Á2.126  1.2 ¡Á10.38  1 ¡Á2.1022  2 ¡Á10.3082. 
One  01 ...11  0 ...00  1 ¡Á20  1.0  1 ¡Á20  1.0  
Largest norm.  11 ...10  1 ...11  (2 . )¡Á2127  3.4 ¡Á1038  (2 . )¡Á21023  8 ¡Á103081. 
Figure 2.35 Examples of nonnegative .oating-point numbers. 


. The smallest positive normalized value has a bit representation witha1in the least signi.cant bit of the exponent .eld and otherwise all zeros. It has a signi.cand value M=1 and an exponent value E=.2k.1 +2. The numeric value is therefore V =2.2k.1+2. 
. The value 1.0 has a bit representation with all but the most signi.cant bit of the exponent .eld equal to 1 and all other bits equal to 0. Its signi.cand value is M=1 and its exponent value is E=0. 
. The largest normalized value has a bit representation with a sign bit of 0, the least signi.cant bit of the exponent equal to 0, and all other bits equal to 1. It has a fraction value of f =1 . 2.n, giving a signi.cand M=2 . 2.n (which we have written 2 . ). It has an exponent value E=2k.1 .1, giving a numeric 
n)¡Á22k.1.1 n.1)¡Á22k.1

value V =(2 .2.=(1 .2. . 
One useful exercise for understanding .oating-point representations is to con-vert sample integer values into .oating-point form. For example, we saw in Figure 
2.14 that 12,345 has binary representation [11000000111001]. We create a normal-ized representation of this by shifting 13 positions to the right of a binary point, giving 12345 =1.10000001110012 ¡Á213. To encode this in IEEE single-precision format, we construct the fraction .eld by dropping the leading 1 and adding 10 zeros to the end, giving binary representation [10000001110010000000000]. To construct the exponent .eld, we add bias 127 to 13, giving 140, which has bi-nary representation [10001100]. We combine this with a sign bit of 0 to get the .oating-point representation in binary of [01000110010000001110010000000000]. Recall from Section 2.1.4 that we observed the following correlation in the bit-level representations of the integer value 12345 (0x3039) and the single-precision .oating-point value 12345.0 (0x4640E400): 
00003039 00000000000000000011000000111001 ************* 
4640E400 01000110010000001110010000000000 
We can now see that the region of correlation corresponds to the low-order bits of the integer, stopping just before the most signi.cant bit equal to 1 (this bit forms the implied leading 1), matching the high-order bits in the fraction part of the .oating-point representation. 
Practice Problem 2.48 
As mentioned in Problem 2.6, the integer 3,510,593 has hexadecimal representa-tion 0x00359141, while the single-precision, .oating-point number 3510593.0 has hexadecimal representation 0x4A564504. Derive this .oating-point representa-tion and explain the correlation between the bits of the integer and .oating-point representations. 
Practice Problem 2.49 
A. For a .oating-point format with an n-bit fraction, give a formula for the smallest positive integer that cannot be represented exactly (because it would require an n+1-bit fraction to be exact). Assume the exponent .eld size k is large enough that the range of representable exponents does not provide a limitation for this problem. 
B. What is the numeric value of this integer for single-precision format (n = 23)? 
2.4.4 Rounding 
Floating-point arithmetic can only approximate real arithmetic, since the repre-sentation has limited range and precision. Thus, for a value x, we generally want a systematic method of .nding the ¡°closest¡± matching value x that can be rep-resented in the desired .oating-point format. This is the task of the rounding operation. One key problem is to de.ne the direction to round a value that is halfway between two possibilities. For example, if I have $1.50 and want to round it to the nearest dollar, should the result be $1 or $2? An alternative approach is to maintain a lower and an upper bound on the actual number. For example, we could determine representable values x . and x + such that the value x is guaran-
+
teed to lie between them: x .¡Ü x ¡Ü x . The IEEE .oating-point format de.nes four different rounding modes. The default method .nds a closest match, while the other three can be used for computing upper and lower bounds. 
Figure 2.36 illustrates the four rounding modes applied to the problem of rounding a monetary amount to the nearest whole dollar. Round-to-even (also called round-to-nearest) is the default mode. It attempts to .nd a closest match. Thus, it rounds $1.40 to $1 and $1.60 to $2, since these are the closest whole dollar values. The only design decision is to determine the effect of rounding values that are halfway between two possible results. Round-to-even mode adopts the convention that it rounds the number either upward or downward such that the least signi.cant digit of the result is even. Thus, it rounds both $1.50 and $2.50 to $2. 

Mode  $1.40  $1.60  $1.50  $2.50  $.1.50  
Round-to-even  $1  $2  $2  $2  $.2  
Round-toward-zero  $1  $1  $1  $2  $.1  
Round-down  $1  $1  $1  $2  $.2  
Round-up  $2  $2  $2  $3  $.1  
Figure 2.36 Illustration of rounding modes for dollar rounding. The .rst rounds to a nearest value, while the other three bound the result above or below. 


The other three modes produce guaranteed bounds on the actual value. These can be useful in some numerical applications. Round-toward-zero mode rounds positive numbers downward and negative numbers upward, giving a value x.such that |.x|¡Ü|x|. Round-down mode rounds both positive and negative numbers downward, giving a value x . such that x .¡Üx. Round-up mode rounds both 
+

positive and negative numbers upward, giving a value x + such that x¡Üx . 
Round-to-even at .rst seems like it has a rather arbitrary goal¡ªwhy is there any reason to prefer even numbers? Why not consistently round values halfway between two representable values upward? The problem with such a convention is that one can easily imagine scenarios in which rounding a set of data values would then introduce a statistical bias into the computation of an average of the values. The average of a set of numbers that we rounded by this means would be slightly higher than the average of the numbers themselves. Conversely, if we always rounded numbers halfway between downward, the average of a set of rounded numbers would be slightly lower than the average of the numbers them-selves. Rounding toward even numbers avoids this statistical bias in most real-life situations. It will round upward about 50% of the time and round downward about 50% of the time. 
Round-to-even rounding can be applied even when we are not rounding to a whole number. We simply consider whether the least signi.cant digit is even or odd. For example, suppose we want to round decimal numbers to the nearest hundredth. We would round 1.2349999 to 1.23 and 1.2350001 to 1.24, regardless of rounding mode, since they are not halfway between 1.23 and 1.24. On the other hand, we would round both 1.2350000 and 1.2450000 to 1.24, since 4 is even. 
Similarly, round-to-even rounding can be applied to binary fractional num-bers. We consider least signi.cant bit value 0 to be even and 1 to be odd. In general, the rounding mode is only signi.cant when we have a bit pattern of the form XX...X.YY ...Y100 ..., where X and Y denote arbitrary bit values with the rightmost Y being the position to which we wish to round. Only bit patterns of this form denote values that are halfway between two possible results. As exam-ples, consider the problem of rounding values to the nearest quarter (i.e., 2 bits to the right of the binary point). We would round 10.000112 (2 323 ) down to 10.002 (2), 
and 10.001102(2 3)upto10.012 (2 1), because these values are not halfway be-
16 4 
tween two possible values. We would round 10.111002 (27)upto11.002 (3) and 
8 
10.101002 (25) down to 10.102 (2 1), since these values are halfway between two 
82 
possible results, and we prefer to have the least signi.cant bit equal to zero. 
Practice Problem 2.50 
Show how the following binary fractional values would be rounded to the nearest half (1 bit to the right of the binary point), according to the round-to-even rule. In each case, show the numeric values, both before and after rounding. 
A. 10.0102 B. 10.0112 C. 10.1102 D. 11.0012 
Practice Problem 2.51 
We saw in Problem 2.46 that the Patriot missile software approximated 0.1as x = 0.000110011001100110011002. Suppose instead that they had used IEEE round-to-even mode to determine an approximation x to 0.1 with 23 bits to the right of the binary point. 
A. What is the binary representation of x ? 
B. What is the approximate decimal value of x . 0.1? 
C. How far off would the computed clock have been after 100 hours of opera-tion? 
D. How far off would the program¡¯s prediction of the position of the Scud missile have been? 
Practice Problem 2.52 
Consider the following two 7-bit .oating-point representations based on the IEEE .oating point format. Neither has a sign bit¡ªthey can only represent nonnegative numbers. 
1. 
Format A There are k = 3 exponent bits. The exponent bias is 3. There are n = 4 fraction bits. 

2. 
Format B There are k = 4 exponent bits. The exponent bias is 7. There are n = 3 fraction bits. 


Below, you are given some bit patterns in Format A, and your task is to convert them to the closest value in Format B. If necessary, you should apply the round-to-even rounding rule. In addition, give the values of numbers given by the Format A and Format B bit patterns. Give these as whole numbers (e.g., 17) or as fractions (e.g., 17/64). 

Format A Format B Bits Value Bits Value 
011 0000 1 0111 000 1 101 1110 
010 1001 110 1111 000 0001 

2.4.5 Floating-Point Operations 

The IEEE standard speci.es a simple rule for determining the result of an arith-metic operation such as addition or multiplication. Viewing .oating-point values x and y as real numbers, and some operation de.ned over real numbers, the computation should yield Round(x y), the result of applying rounding to the exact result of the real operation. In practice, there are clever tricks .oating-point unit designers use to avoid performing this exact computation, since the compu-tation need only be suf.ciently precise to guarantee a correctly rounded result. When one of the arguments is a special value such as .0, ¡Þ,or NaN, the stan-dard speci.es conventions that attempt to be reasonable. For example, 1/ .0is de.ned to yield .¡Þ, while 1/ +0 is de.ned to yield +¡Þ. 
One strength of the IEEE standard¡¯s method of specifying the behavior of .oating-point operations is that it is independent of any particular hardware or software realization. Thus, we can examine its abstract mathematical properties without considering how it is actually implemented. 
We saw earlier that integer addition, both unsigned and two¡¯s complement, forms an abelian group. Addition over real numbers also forms an abelian group, but we must consider what effect rounding has on these properties. Let us de.ne x +f y to be Round(x +y). This operation is de.ned for all values of x and y, although it may yield in.nity even when both x and y are real numbers due to over.ow. The operation is commutative, with x +f y =y +f x for all values of x and 
y. On the other hand, the operation is not associative. For example, with single-precision .oating point the expression (3.14+1e10)-1e10 evaluates to 0.0¡ªthe value 3.14 is lost due to rounding. On the other hand, the expression 3.14+(1e10-1e10) evaluates to 3.14. As with an abelian group, most values have inverses under .oating-point addition, that is, x +f .x =0. The exceptions are in.nities (since +¡Þ.¡Þ=NaN), and NaN¡¯s, since NaN +f x =NaN for any x. 
The lack of associativity in .oating-point addition is the most important group property that is lacking. It has important implications for scienti.c programmers and compiler writers. For example, suppose a compiler is given the following code fragment: 
x=a+b+c; y=b+c+d; 

The compiler might be tempted to save one .oating-point addition by generating the following code: 
t=b+c; x=a+t; y=t+d; 
However, this computation might yield a different value for x than would the original, since it uses a different association of the addition operations. In most applications, the difference would be so small as to be inconsequential. Unfor-tunately, compilers have no way of knowing what trade-offs the user is willing to make between ef.ciency and faithfulness to the exact behavior of the original pro-gram. As a result, they tend to be very conservative, avoiding any optimizations that could have even the slightest effect on functionality. 
On the other hand, .oating-point addition satis.es the following monotonicity property: if a ¡Ý b then x + a ¡Ý x + b for any values of a, b, and x other than NaN. This property of real (and integer) addition is not obeyed by unsigned or two¡¯s-complement addition. 
Floating-point multiplication also obeys many of the properties one normally associates with multiplication. Let us de.ne x *f y to be Round(x ¡Á y). This oper-ation is closed under multiplication (although possibly yielding in.nity or NaN), it is commutative, and it has 1.0 as a multiplicative identity. On the other hand, it is not associative, due to the possibility of over.ow or the loss of precision due to rounding. For example, with single-precision .oating point, the expression (1e20*1e20)*1e-20 evaluates to +¡Þ, while 1e20*(1e20*1e-20) evaluates to 1e20. In addition, .oating-point multiplication does not distribute over addition. For example, with single-precision .oating point, the expression 1e20*(1e20-1e20) evaluates to 0.0, while 1e20*1e20-1e20*1e20 evaluates to NaN. 
On the other hand, .oating-point multiplication satis.es the following mono-tonicity properties for any values of a, b, and c other than NaN: 
a ¡Ý b and c ¡Ý 0 . a *f c ¡Ý b *f c 
a ¡Ý b and c ¡Ü 0 . a *f c ¡Ü b *f c 
In addition, we are also guaranteed that a *f a ¡Ý 0, as long as a = NaN.Aswe saw earlier, none of these monotonicity properties hold for unsigned or two¡¯s-complement multiplication. 
This lack of associativity and distributivity is of serious concern to scienti.c programmers and to compiler writers. Even such a seemingly simple task as writing code to determine whether two lines intersect in 3-dimensional space can be a major challenge. 
2.4.6 Floating Point in C 
All versions of C provide two different .oating-point data types: float and double. On machines that support IEEE .oating point, these data types corre-spond to single-and double-precision .oating point. In addition, the machines use the round-to-even rounding mode. Unfortunately, since the C standards do not require the machine to use IEEE .oating point, there are no standard methods to change the rounding mode or to get special values such as .0, +¡Þ, .¡Þ,or NaN. Most systems provide a combination of include (¡®.h¡¯) .les and procedure libraries to provide access to these features, but the details vary from one system to an-other. For example, the GNU compiler gcc de.nes program constants INFINITY (for +¡Þ) and NAN (for NaN) when the following sequence occurs in the program .le: 

#define _GNU_SOURCE 1 #include <math.h> 
More recent versions of C, including ISO C99, include a third .oating-point data type, long double. For many machines and compilers, this data type is equivalent to the double data type. For Intel-compatible machines, however, gcc implements this data type using an 80-bit ¡°extended precision¡± format, providing a much larger range and precision than does the standard 64-bit format. The properties of this format are investigated in Problem 2.85. 
Practice Problem 2.53 
Fill in the following macro de.nitions to generate the double-precision values +¡Þ, .¡Þ, and 0: 
#define POS_INFINITY #define NEG_INFINITY #define NEG_ZERO 
You cannot use any include .les (such as math.h), but you can make use of the fact that the largest .nite number that can be represented with double precision is around 1.8 ¡Á 10308. 
When casting values between int, float, and double formats, the program changes the numeric values and the bit representations as follows (assuming a 32-bit int): 
. From int to float, the number cannot over.ow, but it may be rounded. . From int or float to double, the exact numeric value can be preserved be-cause double has both greater range (i.e., the range of representable values), as well as greater precision (i.e., the number of signi.cant bits). 
. From double to float, the value can over.ow to +¡Þ or .¡Þ, since the range is smaller. Otherwise, it may be rounded, because the precision is smaller. . From float or double to int the value will be rounded toward zero. For 
example, 1.999 will be converted to 1, while .1.999 will be converted to .1. Furthermore, the value may over.ow. The C standards do not specify a .xed result for this case. Intel-compatible microprocessors designate the bit pattern [10 ...00] (TMin for word size w)asan integer inde.nite value. 
w 
Any conversion from .oating point to integer that cannot assign a reasonable integer approximation yields this value. Thus, the expression (int) +1e10 yields -21483648, generating a negative value from a positive one. 

Web Aside DATA:IA32-FP Intel IA32 .oating-point arithmetic 
In the next chapter, we will begin an in-depth study of Intel IA32 processors, the processor found in many of today¡¯s personal computers. Here we highlight an idiosyncrasy of these machines that can seriously affect the behavior of programs operating on .oating-point numbers when compiled with gcc. 
IA32 processors, like most other processors, have special memory elements called registers for holding .oating-point values as they are being computed and used. The unusual feature of IA32 is that the .oating-point registers use a special 80-bit extended-precision format to provide a greater range and precision than the normal 32-bit single-precision and 64-bit double-precision formats used for values held in memory. (See Problem 2.85.) All single-and double-precision numbers are converted to this format as they are loaded from memory into .oating-point registers. The arithmetic is always performed in extended precision. Numbers are converted from extended precision to single-or double-precision format as they are stored in memory. 
This extension to 80 bits for all register data and then contraction to a smaller format for memory data has some undesirable consequences for programmers. It means that storing a number from a register to memory and then retrieving it back into the register can cause it to change, due to rounding, under.ow, or over.ow. This storing and retrieving is not always visible to the C programmer, leading to some very peculiar results. 
More recent versions of Intel processors, including both IA32 and newer 64-bit machines, provide direct hardware support for single-and double-precision .oating-point operations. The peculiarities of the historic IA32 approach will diminish in importance with new hardware and with compilers that generate code based on the newer .oating-point instructions. 
Aside Ariane 5: the high cost of .oating-point over.ow 
Converting large .oating-point numbers to integers is a common source of programming errors. Such an error had disastrous consequences for the maiden voyage of the Ariane 5 rocket, on June 4, 1996. Just 37 seconds after liftoff, the rocket veered off its .ight path, broke up, and exploded. Communication satellites valued at $500 million were on board the rocket. 
A later investigation [69, 39] showed that the computer controlling the inertial navigation system had sent invalid data to the computer controlling the engine nozzles. Instead of sending .ight control information, it had sent a diagnostic bit pattern indicating that an over.ow had occurred during the conversion of a 64-bit .oating-point number to a 16-bit signed integer. 
The value that over.owed measured the horizontal velocity of the rocket, which could be more than 5 times higher than that achieved by the earlier Ariane 4 rocket. In the design of the Ariane 4 software, they had carefully analyzed the numeric values and determined that the horizontal velocity would never over.ow a 16-bit number. Unfortunately, they simply reused this part of the software in the Ariane 5 without checking the assumptions on which it had been based. 

. Fourmy/REA/SABA/Corbis 
Practice Problem 2.54 
Assume variables x, f, and d are of type int, float, and double, respectively. Their values are arbitrary, except that neither f nor d equals +¡Þ, .¡Þ,or NaN. For each of the following C expressions, either argue that it will always be true (i.e., evaluate to 1) or give a value for the variables such that it is not true (i.e., evaluates to 0). 
A. x == (int)(double) x 
B. x == (int)(float) x 
C. d == (double)(float) d 
D. f == (float)(double) f 
E. f == -(-f) F. 1.0/2 == 1/2.0 
G. d*d >= 0.0 
H. (f+d)-f == d 

2.5 
Summary 

Computers encode information as bits, generally organized as sequences of bytes. Different encodings are used for representing integers, real numbers, and charac-ter strings. Different models of computers use different conventions for encoding numbers and for ordering the bytes within multi-byte data. 
The C language is designed to accommodate a wide range of different imple-mentations in terms of word sizes and numeric encodings. Most current machines have 32-bit word sizes, although high-end machines increasingly have 64-bit words. Most machines use two¡¯s-complement encoding of integers and IEEE encod-ing of .oating point. Understanding these encodings at the bit level, as well as understanding the mathematical characteristics of the arithmetic operations, is im-portant for writing programs that operate correctly over the full range of numeric values. 
When casting between signed and unsigned integers of the same size, most C implementations follow the convention that the underlying bit pattern does not change. On a two¡¯s-complement machine, this behavior is characterized by functions T2U and U2T , for a w-bit value. The implicit casting of C gives results 
ww 
that many programmers do not anticipate, often leading to program bugs. 
Due to the .nite lengths of the encodings, computer arithmetic has properties quite different from conventional integer and real arithmetic. The .nite length can cause numbers to over.ow, when they exceed the range of the representation. Floating-point values can also under.ow, when they are so close to 0.0 that they are changed to zero. 
The .nite integer arithmetic implemented by C, as well as most other pro-gramming languages, has some peculiar properties compared to true integer arith-metic. For example, the expression x*x can evaluate to a negative number due to over.ow. Nonetheless, both unsigned and two¡¯s-complement arithmetic satisfy many of the other properties of integer arithmetic, including associativity, com-mutativity, and distributivity. This allows compilers to do many optimizations. For example, in replacing the expression 7*x by (x<<3)-x, we make use of the as-sociative, commutative, and distributive properties, along with the relationship between shifting and multiplying by powers of 2. 
We have seen several clever ways to exploit combinations of bit-level opera-tions and arithmetic operations. For example, we saw that with two¡¯s-complement arithmetic ~x+1 is equivalent to -x. As another example, suppose we want a bit pattern of the form [0,...,0,1,...,1], consisting of w. k zeros followed by k ones. Such bit patterns are useful for masking operations. This pattern can be gen-erated by the C expression (1<<k)-1, exploiting the property that the desired bit pattern has numeric value 2k . 1. For example, the expression (1<<8)-1 will generate the bit pattern 0xFF. 
Floating-point representations approximate real numbers by encoding num-bers of the form x¡Á 2y. The most common .oating-point representation is de.ned by IEEE Standard 754. It provides for several different precisions, with the most common being single (32 bits) and double (64 bits). IEEE .oating point also has representations for special values representing plus and minus in.nity, as well as not-a-number. 

Floating-point arithmetic must be used very carefully, because it has only limited range and precision, and because it does not obey common mathematical properties such as associativity. 
Bibliographic 
Notes 

Reference books on C [48, 58] discuss properties of the different data types and operations. (Of these two, only Steele and Harbison [48] cover the newer fea-tures found in ISO C99.) The C standards do not specify details such as pre-cise word sizes or numeric encodings. Such details are intentionally omitted to make it possible to implement C on a wide range of different machines. Several books have been written giving advice to C programmers [59, 70] that warn about problems with over.ow, implicit casting to unsigned, and some of the other pit-falls we have covered in this chapter. These books also provide helpful advice on variable naming, coding styles, and code testing. Seacord¡¯s book on security issues in C and C++ programs [94], combines information about C programs, how they are compiled and executed, and how vulnerabilities may arise. Books on Java (we recommend the one coauthored by James Gosling, the creator of the language [4]) describe the data formats and arithmetic operations supported by Java. 
Most books on logic design [56, 115] have a section on encodings and arith-metic operations. Such books describe different ways of implementing arithmetic circuits. Overton¡¯s book on IEEE .oating point [78] provides a detailed descrip-tion of the format as well as the properties from the perspective of a numerical applications programmer. 
Homework 
Problems 

2.55 ¡ô 

Compile and run the sample code that uses show_bytes (.le show-bytes.c)on different machines to which you have access. Determine the byte orderings used by these machines. 
2.56 ¡ô 

Try running the code for show_bytes for different sample values. 
2.57 ¡ô 

Write procedures show_short, show_long, and show_double that print the byte representations of C objects of types short int, long int, and double, respec-tively. Try these out on several machines. 
2.58 ¡ô¡ô 

Write a procedure is_little_endian that will return 1 when compiled and run on a little-endian machine, and will return 0 when compiled and run on a big-endian machine. This program should run on any machine, regardless of its word size. 
2.59 ¡ô¡ô 
Write a C expression that will yield a word consisting of the least signi.cant byte of x, and the remaining bytes of y. For operands x = 0x89ABCDEF and y = 0x76543210, this would give 0x765432EF. 
2.60 ¡ô¡ô 
Suppose we number the bytes in a w-bit word from 0 (least signi.cant) to w/8 . 1 (most signi.cant). Write code for the following C function, which will return an unsigned value in which byte i of argument x has been replaced by byte b: 
unsigned replace_byte (unsigned x, int i, unsigned char b); 
Here are some examples showing how the function should work: 
replace_byte(0x12345678, 2, 0xAB) --> 0x12AB5678 replace_byte(0x12345678, 0, 0xAB) --> 0x123456AB 
Bit-level integer coding rules 
In several of the following problems, we will arti.cially restrict what programming constructs you can use to help you gain a better understanding of the bit-level, logic, and arithmetic operations of C. In answering these problems, your code must follow these rules: 
. Assumptions Integers are represented in two¡¯s-complement form. Right shifts of signed data are performed arithmetically. Data type int is w bits long. For some of the problems, you will be given a speci.c value for w, but otherwise your code should work as long as w is a multiple of 8. You can use the expression sizeof(int)<<3 to compute w. 
. Forbidden Conditionals (if or ?:), loops, switch statements, function calls, and macro invocations. Division, modulus, and multiplication. Relative comparison operators (<, >, <=, and >=). Casting, either explicit or implicit. 
. Allowed operations All bit-level and logic operations. Left and right shifts, but only with shift amounts between 0 and w . 1. Addition and subtraction. Equality (==) and inequality (!=) tests. (Some of the problems do not allow these.) Integer constants INT_MIN and INT_MAX. 
Even with these rules, you should try to make your code readable by choosing descriptive variable names and using comments to describe the logic behind your solutions. As an example, the following code extracts the most signi.cant byte from integer argument x: 

/* Get most significant byte from x */ 
int get_msb(int x) { 
/* Shift by w-8 */ 
int shift_val = (sizeof(int)-1)<<3; 
/* Arithmetic shift */ 
int xright=x>> shift_val; 
/* Zero all but LSB */ 
return xright & 0xFF; } 
2.61 ¡ô¡ô 

Write C expressions that evaluate to 1 when the following conditions are true, and to 0 when they are false. Assume x is of type int. 
A. Any bit of x equals 1. 
B. Any bit of x equals 0. 
C. Any bit in the least signi.cant byte of x equals 1. 
D. Any bit in the most signi.cant byte of x equals 0. 
Your code should follow the bit-level integer coding rules (page 120), with the additional restriction that you may not use equality (==) or inequality (!=) tests. 
2.62 ¡ô¡ô¡ô 

Write a function int_shifts_are_arithmetic() that yields 1 when run on a machine that uses arithmetic right shifts for int¡¯s, and 0 otherwise. Your code should work on a machine with any word size. Test your code on several machines. 
2.63 ¡ô¡ô¡ô 

Fill in code for the following C functions. Function srl performs a logical right shift using an arithmetic right shift (given by value xsra), followed by other oper-ations not including right shifts or division. Function sra performs an arithmetic right shift using a logical right shift (given by value xsrl), followed by other operations not including right shifts or division. You may use the computation 8*sizeof(int) to determine w, the number of bits in data type int. The shift amount k can range from 0 to w . 1. 
unsigned srl(unsigned x, int k) { 
/* Perform shift arithmetically */ 
unsigned xsra = (int) x >> k; 
. 
. 
. 
} 

int sra(int x, int k) { 
/* Perform shift logically */ 
int xsrl = (unsigned) x >> k; . 
. 
. 
} 
2.64 ¡ô 
Write code to implement the following function: 
/* Return 1 when any odd bit of x equals 1; 0 otherwise. Assume w=32. */ 
int any_odd_one(unsigned x); 
Your function should follow the bit-level integer coding rules (page 120), except that you may assume that data type int has w= 32 bits. 
2.65 ¡ô¡ô¡ô¡ô 
Write code to implement the following function: 
/* Return 1 when x contains an odd number of 1s; 0 otherwise. Assume w=32. */ 
int odd_ones(unsigned x); 
Your function should follow the bit-level integer coding rules (page 120), except that you may assume that data type int has w= 32 bits. Your code should contain a total of at most 12 arithmetic, bit-wise, and logical operations. 
2.66 ¡ô¡ô¡ô 
Write code to implement the following function: 
/* 
* 
Generate mask indicating leftmost 1 in x. Assume w=32. 

* 
For example 0xFF00 -> 0x8000, and 0x6600 --> 0x4000. *Ifx=0, then return 0. */ 


int leftmost_one(unsigned x); 
Your function should follow the bit-level integer coding rules (page 120), except that you may assume that data type int has w= 32 bits. Your code should contain a total of at most 15 arithmetic, bit-wise, and logical operations. Hint: First transform x into a bit vector of the form [0 ...011 ...1]. 
2.67 ¡ô¡ô 
You are given the task of writing a procedure int_size_is_32() that yields 1 when run on a machine for which an int is 32 bits, and yields 0 otherwise. You are not allowed to use the sizeof operator. Here is a .rst attempt: 

1 /* The following code does not run properly on some machines */ 2 int bad_int_size_is_32() { 3 /* Set most significant bit (msb) of 32-bit machine */ 4 int set_msb=1<<31; 5 /* Shift past msb of 32-bit word */ 6 int beyond_msb=1<<32; 7 8 /* set_msb is nonzero when word size >= 32 9 beyond_msb is zero when word size <= 32 */ 
10 return set_msb && !beyond_msb; 
11 } 

When compiled and run on a 32-bit SUN SPARC, however, this procedure returns 
0. The following compiler message gives us an indication of the problem: 
warning: left shift count >= width of type 
A. In what way does our code fail to comply with the C standard? 
B. Modify the code to run properly on any machine for which data type int is at least 32 bits. 
C. Modify the code to run properly on any machine for which data type int is at least 16 bits. 
2.68 ¡ô¡ô 

Write code for a function with the following prototype: 
/* 

* 
Mask with least signficant n bits set to 1 * Examples:n=6--> 0x2F,n=17--> 0x1FFFF 

* 
Assume 1<= n <= w */ 


int lower_one_mask(int n); 
Your function should follow the bit-level integer coding rules (page 120). Be careful of the case n = w. 
2.69 ¡ô¡ô¡ô 

Write code for a function with the following prototype: 
/* 

* 
Do rotating left shift. Assume 0 <=n<w 

* 
Examples when x = 0x12345678 andw=32: * n=4 -> 0x23456781, n=20 -> 0x67812345 */ 


unsigned rotate_left(unsigned x, int n); 
Your function should follow the bit-level integer coding rules (page 120). Be careful of the case n = 0. 
2.70 ¡ô¡ô 
Write code for the function with the following prototype: 
/* 
* 
Return 1 when x can be represented as an n-bit, 2¡¯s complement 

* 
number; 0 otherwise 

* 
Assume1<= n<= w */ 


int fits_bits(int x, int n); 
Your function should follow the bit-level integer coding rules (page 120). 
2.71 ¡ô 
You just started working for a company that is implementing a set of procedures to operate on a data structure where 4 signed bytes are packed into a 32-bit unsigned. Bytes within the word are numbered from 0 (least signi.cant) to 3 (most signi.cant). You have been assigned the task of implementing a function for a machine using two¡¯s-complement arithmetic and arithmetic right shifts with the following prototype: 
/* Declaration of data type where 4 bytes are packed into an unsigned */ 
typedef unsigned packed_t; 
/* Extract byte from word. Return as signed integer */ 
int xbyte(packed_t word, int bytenum); 
That is, the function will extract the designated byte and sign extend it to be a 32-bit int. Your predecessor (who was .red for incompetence) wrote the following code: 
/* Failed attempt at xbyte */ 
int xbyte(packed_t word, int bytenum) { return (word >> (bytenum << 3)) & 0xFF; } 
A. What is wrong with this code? 
B. Give a correct implementation of the function that uses only left and right shifts, along with one subtraction. 
2.72 ¡ô¡ô 
You are given the task of writing a function that will copy an integer val into a buffer buf, but it should do so only if enough space is available in the buffer. Here is the code you write: 
/* Copy integer into buffer if space is available */ /* WARNING: The following code is buggy */ 

void copy_int(int val, void *buf, int maxbytes) { if (maxbytes-sizeof(val) >= 0) memcpy(buf, (void *) &val, sizeof(val)); } 
This code makes use of the library function memcpy. Although its use is a bit arti.cial here, where we simply want to copy an int, it illustrates an approach commonly used to copy larger data structures. 
You carefully test the code and discover that it always copies the value to the buffer, even when maxbytes is too small. 
A. Explain why the conditional test in the code always succeeds. Hint: The sizeof operator returns a value of type size_t. 
B. Show how you can rewrite the conditional test to make it work properly. 
2.73 ¡ô¡ô 

Write code for a function with the following prototype: 
/* Addition that saturates to TMin or TMax */ 
int saturating_add(int x, int y); 
Instead of over.owing the way normal two¡¯s-complement addition does, sat-urating addition returns TMax when there would be positive over.ow, and TMin when there would be negative over.ow. Saturating arithmetic is commonly used in programs that perform digital signal processing. 
Your function should follow the bit-level integer coding rules (page 120). 
2.74 ¡ô¡ô 

Write a function with the following prototype: 
/* Determine whether arguments can be subtracted without overflow */ 
int tsub_ok(int x, int y); 
This function should return 1 if the computation x . y does not over.ow. 
2.75 ¡ô¡ô¡ô 

Suppose we want to compute the complete 2w-bit representation of x.y, where both x and y are unsigned, on a machine for which data type unsigned is w bits. The low-order w bits of the product can be computed with the expression x*y,so we only require a procedure with prototype 
unsigned int unsigned_high_prod(unsigned x, unsigned y); 
that computes the high-order w bits of x.y for unsigned variables. We have access to a library function with prototype 
int signed_high_prod(int x, int y); 
that computes the high-order w bits of x.y for the case where x and y are in two¡¯s-complement form. Write code calling this procedure to implement the function for unsigned arguments. Justify the correctness of your solution. 
Hint: Look at the relationship between the signed product x.y and the unsigned product x .y in the derivation of Equation 2.18. 
2.76 ¡ô¡ô 
Suppose we are given the task of generating code to multiply integer variable x by various different constant factors K. To be ef.cient, we want to use only the operations +, -, and <<. For the following values of K, write C expressions to perform the multiplication using at most three operations per expression. 
A. K = 17: 
B. K =.7: 
C. K = 60: 
D. K =.112: 
2.77 ¡ô¡ô 
Write code for a function with the following prototype: 
/* Divide by power of two. Assume 0 <= k < w-1 */ 
int divide_power2(int x, int k); 
The function should compute x/2k with correct rounding, and it should follow the bit-level integer coding rules (page 120). 
2.78 ¡ô¡ô 
Write code for a function mul3div4 that, for integer argument x, computes 3*x/4, but following the bit-level integer coding rules (page 120). Your code should replicate the fact that the computation 3*x can cause over.ow. 
2.79 ¡ô¡ô¡ô 
Write code for a function threefourths which, for integer argument x, computes the value of 43 x, rounded toward zero. It should not over.ow. Your function should follow the bit-level integer coding rules (page 120). 
2.80 ¡ô¡ô 
Write C expressions to generate the bit patterns that follow, where ak represents k repetitions of symbol a. Assume a w-bit data type. Your code may contain references to parameters j and k, representing the values of j and k, but not a parameter representing w. 
A. 1w.k0k 0w.k.j1k0j
B. 
2.81 ¡ô 
We are running programs on a machine where values of type int are 32 bits. They are represented in two¡¯s complement, and they are right shifted arithmetically. Values of type unsigned are also 32 bits. 

We generate arbitrary values x and y, and convert them to unsigned values as follows: 
/* Create some arbitrary values */ 
int x = random(); int y = random(); 

/* Convert to unsigned */ 
unsigned ux = (unsigned) x; unsigned uy = (unsigned) y; 
For each of the following C expressions, you are to indicate whether or not the expression always yields 1. If it always yields 1, describe the underlying mathematical principles. Otherwise, give an example of arguments that make it yield 0. 
A. (x<y) == (-x>-y) 
B. ((x+y)<<4) + y-x == 17*y+15*x 
C. ~x+~y+1 == ~(x+y) 
D. (ux-uy) == -(unsigned)(y-x) 
E. ((x>> 2) << 2) <= x 
2.82 ¡ô¡ô 

Consider numbers having a binary representation consisting of an in.nite string of the form 0.yyyyyy..., where yis a k-bit sequence. For example, the binary representation of 1 is 0.01010101 ... (y= 01), while the representation of 1 is
35 
0.001100110011 ...(y= 0011). 
A. Let Y = B2Uk(y), that is, the number having binary representation y. Give a formula in terms of Y and kfor the value represented by the in.nite string. Hint: Consider the effect of shifting the binary point kpositions to the right. 
B. What is the numeric value of the string for the following values of y? (a) 101 (b) 0110 (c) 010011 
2.83 ¡ô 

Fill in the return value for the following procedure, which tests whether its .rst argument is less than or equal to its second. Assume the function f2u returns an unsigned 32-bit number having the same bit representation as its .oating-point argument. You can assume that neither argument is NaN. The two .avors of zero, +0 and .0, are considered equal. 
int float_le(float x, float y) { 
unsigned ux = f2u(x); 
unsigned uy = f2u(y); /* Get the sign bits */ 
unsigned sx = ux >> 31; 
unsigned sy = uy >> 31; 
/* Give an expression using only ux, uy, sx, and sy */ 
return ; } 
2.84 ¡ô 
Given a .oating-point format with a k-bit exponent and an n-bit fraction, write formulas for the exponent E, signi.cand M, the fraction f , and the value V for the quantities that follow. In addition, describe the bit representation. 
A. The number 7.0 
B. The largest odd integer that can be represented exactly 
C. The reciprocal of the smallest positive normalized value 
2.85 ¡ô 
Intel-compatible processors also support an ¡°extended precision¡± .oating-point format with an 80-bit word divided into a sign bit, k = 15 exponent bits, a single integer bit, and n = 63 fraction bits. The integer bit is an explicit copy of the implied bit in the IEEE .oating-point representation. That is, it equals 1 for normalized values and 0 for denormalized values. Fill in the following table giving the approximate values of some ¡°interesting¡± numbers in this format: 
Extended precision 
Description Value Decimal 
Smallest positive denormalized 
Smallest positive normalized Largest normalized 
2.86 ¡ô 
Consider a 16-bit .oating-point representation based on the IEEE .oating-point format, with one sign bit, seven exponent bits (k = 7), and eight fraction bits (n = 8). The exponent bias is 27.1 . 1 = 63. 
Fill in the table that follows for each of the numbers given, with the following instructions for each column: 
Hex:  The four hexadecimal digits describing the encoded form.  
M:  The value of the signi.cand. This should be a number of the  
xform x or , where x is an integer, and y is an integraly 1power of 2. Examples include: 0, 67 64 , and 256 .  
E:  The integer value of the exponent.  
V :  The numeric value represented. Use the notation x or  
x ¡Á 2z , where x and z are integers.  

7

As an example, to represent the number 7, we would have s = 0, M = , and 
84 

E =.1. Our number would therefore have an exponent .eld of 0x3E (decimal value 63 . 1 = 62) and a signi.cand .eld 0xC0 (binary 110000002), giving a hex representation 3EC0. 
You need not .ll in entries marked ¡°¡ª¡±. 
Description Hex MEV 
.0 Smallest value > 2 512 Largest denormalized .¡Þ Number with hex representation 3BB0  ¡ª  ¡ª  ¡ª  ¡ª ¡ª ¡ª  
2.87  ¡ô¡ô  

Consider the following two 9-bit .oating-point representations based on the IEEE .oating-point format. 
1. 
Format A There is one sign bit. There are k = 5 exponent bits. The exponent bias is 15. There are n = 3 fraction bits. 

2. 
Format B There is one sign bit. There are k = 4 exponent bits. The exponent bias is 7. There are n = 4 fraction bits. 


Below, you are given some bit patterns in Format A, and your task is to convert them to the closest value in Format B. If rounding is necessary, you should round toward +¡Þ. In addition, give the values of numbers given by the Format A and Format B bit patterns. Give these as whole numbers (e.g., 17) or as fractions (e.g., 17/64 or17/26). 
Format A Format B 
Bits Value Bits Value 
1 01111 001  .9 8  1 0111 0010  .9 8  
0 10110 011  
1 00111 010  
0 00000 111  
1 11100 000  
0 10111 100  
2.88  ¡ô  

We are running programs on a machine where values of type int have a 32-bit two¡¯s-complement representation. Values of type float use the 32-bit IEEE format, and values of type double use the 64-bit IEEE format. 
We generate arbitrary integer values x, y, and z, and convert them to values of type double as follows: 
/* Create some arbitrary values */ 
int x = random(); int y = random(); int z = random(); 
/* Convert to double */ 
double dx = (double) x; double dy = (double) y; double dz = (double) z; 
For each of the following C expressions, you are to indicate whether or not the expression always yields 1. If it always yields 1, describe the underlying mathematical principles. Otherwise, give an example of arguments that make it yield 0. Note that you cannot use an IA32 machine running gcc to test your answers, since it would use the 80-bit extended-precision representation for both float and double. 
A. (float) x == (float) dx 
B. dx -dy == (double) (x-y) 
C. (dx+dy)+dz==dx+(dy+dz) 
D. (dx*dy)*dz==dx*(dy*dz) 
E. dx/dx==dz/dz 
2.89 ¡ô 
You have been assigned the task of writing a C function to compute a .oating-point representation of 2x . You decide that the best way to do this is to directly construct the IEEE single-precision representation of the result. When x is too small, your routine will return 0.0. When x is too large, it will return +¡Þ. Fill in the blank portions of the code that follows to compute the correct result. Assume the function u2f returns a .oating-point value having an identical bit representation as its unsigned argument. 
float fpwr2(int x) { 
/* Result exponent and fraction */ 
unsigned exp, frac; 
unsigned u; 
if(x< ){ 
/* Too small. Return 0.0 */ 
exp= ; frac= ; 
}elseif(x< ){ 

/* Denormalized result */ 
exp= ; 
frac= ; 
}elseif(x< ){ 
/* Normalized result. */ 
exp= ; 
frac= ; 
} else { 

/* Too big. Return +oo */ 
exp= ; 
frac= ; 
} 

/* Pack exp and frac into 32 bits */ 
u = exp << 23 | frac; 
/* Return as float */ 
return u2f(u); } 
2.90 ¡ô 
22

Around 250 B.C., the Greek mathematician Archimedes proved that 223 <¦Ð< 7.
71 

Had he had access to a computer and the standard library <math.h>, he would have been able to determine that the single-precision .oating-point approximation of ¦Ð has the hexadecimal representation 0x40490FDB. Of course, all of these are just approximations, since ¦Ð is not rational. 
A. What is the fractional binary number denoted by this .oating-point value? 
B. What is the fractional binary representation of 227 ? Hint: See Problem 2.82. 
C. At what bit position (relative to the binary point) do these two approxima-tions to ¦Ð diverge? 
Bit-level .oating-point coding rules 
In the following problems, you will write code to implement .oating-point func-tions, operating directly on bit-level representations of .oating-point numbers. Your code should exactly replicate the conventions for IEEE .oating-point oper-ations, including using round-to-even mode when rounding is required. 
Toward this end, we de.ne data type float_bits to be equivalent to un-signed: 
/* Access bit-level representation floating-point number */ 
typedef unsigned float_bits; 
Rather than using data type float in your code, you will use float_bits. You may use both int and unsigned data types, including unsigned and integer constants and operations. You may not use any unions, structs, or arrays. Most signi.cantly, you may not use any .oating-point data types, operations, or con-stants. Instead, your code should perform the bit manipulations that implement the speci.ed .oating-point operations. 
The following function illustrates the use of these coding rules. For argument 
f , it returns ¡À0if f is denormalized (preserving the sign of f ) and returns f 
otherwise. 
/* If f is denorm, return 0. Otherwise, return f */ 
float_bits float_denorm_zero(float_bits f) { 
/* Decompose bit representation into parts */ 
unsigned sign = f>>31; unsigned exp = f>>23 & 0xFF; unsigned frac = f & 0x7FFFFF; if(exp==0) { 
/* Denormalized. Set fraction to 0 */ 
frac = 0; } 
/* Reassemble bits */ 
return (sign << 31) | (exp << 23) | frac; } 
2.91 ¡ô¡ô 
Following the bit-level .oating-point coding rules, implement the function with the following prototype: 
/* Compute -f. If f is NaN, then return f. */ 
float_bits float_negate(float_bits f); 
For .oating-point number f , this function computes .f .If f is NaN, your func-tion should simply return f . 
Test your function by evaluating it for all 232 values of argument f and com-
paring the result to what would be obtained using your machine¡¯s .oating-point 
operations. 
2.92 ¡ô¡ô 
Following the bit-level .oating-point coding rules, implement the function with the following prototype: 
/* Compute |f|. If f is NaN, then return f. */ 
float_bits float_absval(float_bits f); 
For .oating-point number f , this function computes |f |.If f is NaN, your function should simply return f . 
Test your function by evaluating it for all 232 values of argument f and com-
paring the result to what would be obtained using your machine¡¯s .oating-point 
operations. 
2.93 ¡ô¡ô¡ô 

Following the bit-level .oating-point coding rules, implement the function with the following prototype: 
/* Compute 2*f. If f is NaN, then return f. */ 
float_bits float_twice(float_bits f); 
For .oating-point number f , this function computes 2.0 .f .If f is NaN, your function should simply return f . 
Test your function by evaluating it for all 232 values of argument f and com-paring the result to what would be obtained using your machine¡¯s .oating-point operations. 
2.94 ¡ô¡ô¡ô 

Following the bit-level .oating-point coding rules, implement the function with the following prototype: 
/* Compute 0.5*f. If f is NaN, then return f. */ 
float_bits float_half(float_bits f); 
For .oating-point number f , this function computes 0.5 .f .If f is NaN, your function should simply return f . 
Test your function by evaluating it for all 232 values of argument f and com-paring the result to what would be obtained using your machine¡¯s .oating-point operations. 
2.95 ¡ô¡ô¡ô¡ô 

Following the bit-level .oating-point coding rules, implement the function with the following prototype: 
/* 

* 
Compute (int) f. 

* 
If conversion causes overflow or f is NaN, return 0x80000000 */ 


int float_f2i(float_bits f); 
For .oating-point number f , this function computes (int) f . Your function should round toward zero. If f cannot be represented as an integer (e.g., it is out of range, or it is NaN), then the function should return 0x80000000. 
Test your function by evaluating it for all 232 values of argument f and com-paring the result to what would be obtained using your machine¡¯s .oating-point operations. 
2.96 ¡ô¡ô¡ô¡ô 

Following the bit-level .oating-point coding rules, implement the function with the following prototype: 
/* Compute (float) i */ 
float_bits float_i2f(int i); 
For argument i, this function computes the bit-level representation of (float) i. 
Test your function by evaluating it for all 232 values of argument f and com-paring the result to what would be obtained using your machine¡¯s .oating-point operations. 
Solutions 
to 
Practice 
Problems 

Solution to Problem 2.1 (page 35) 
Understanding the relation between hexadecimal and binary formats will be im-portant once we start looking at machine-level programs. The method for doing these conversions is in the text, but it takes a little practice to become familiar. 
A. 0x39A7F8 to binary: 
Hexadecimal 39 A7F 8 Binary 0011 1001 1010 0111 1111 1000 
B. Binary 1100100101111011 to hexadecimal: 
Binary 1100 1001 0111 1011 
Hexadecimal C97B 
C. 0xD5E4C to binary: 
Hexadecimal D5 E 4 C Binary 1101 0101 1110 0100 1100 
D. Binary 1001101110011110110101 to hexadecimal: 
Binary 10 0110 1110 0111 1011 0101 
Hexadecimal 2 6 E7B5 
Solution to Problem 2.2 (page 35) 
This problem gives you a chance to think about powers of 2 and their hexadecimal representations. 

n  2n (Decimal)  2n (Hexadecimal)  
9  512  0x200  
19  524,288  0x80000  
14  16,384  0x4000  
16  65,536  0x10000  
17  131,072  0x20000  
5  32  0x20  
7  128  0x80  

Solution to Problem 2.3 (page 36) 
This problem gives you a chance to try out conversions between hexadecimal and decimal representations for some smaller numbers. For larger ones, it becomes much more convenient and reliable to use a calculator or conversion program. 
Decimal Binary Hexadecimal 
0 0000 0000 0x00 167 = 10 . 16 + 7 1010 0111 0xA7 62 = 3 . 16 + 14 0011 1110 0x3E 188 = 11 . 16 + 12 1011 1100 0xBC 
3 
. 16 + 7 = 55 0011 0111 0x37 

8 
. 16 + 8 = 136 1000 1000 0x88 

15 
. 16 + 3 = 243 1111 0011 0xF3 

5 
. 16 + 2 = 82 0101 0010 0x52 

10 
. 16 + 12 = 172 1010 1100 0xAC 

14 
. 16 + 7 = 231 1110 0111 0xE7 


Solution to Problem 2.4 (page 37) 
When you begin debugging machine-level programs, you will .nd many cases where some simple hexadecimal arithmetic would be useful. You can always convert numbers to decimal, perform the arithmetic, and convert them back, but being able to work directly in hexadecimal is more ef.cient and informative. 
A. 0x503c + 0x8 = 0x5044. Adding 8 to hex c gives 4 with a carry of 1. 
B. 0x503c . 0x40 = 0x4ffc. Subtracting 4 from 3 in the second digit position requires a borrow from the third. Since this digit is 0, we must also borrow from the fourth position. 
C. 0x503c + 64 = 0x507c. Decimal 64 (26) equals hexadecimal 0x40. 
D. 0x50ea . 0x503c = 0xae. To subtract hex c (decimal 12) from hex a (decimal 10), we borrow 16 from the second digit, giving hex e (decimal 14). In the second digit, we now subtract 3 from hex d (decimal 13), giving hex a (decimal 10). 
Solution to Problem 2.5 (page 45) 
This problem tests your understanding of the byte representation of data and the two different byte orderings. 
Little endian: 21 Big endian: 87 Little endian: 21 43 Big endian: 87 65 Little endian: 21 43 65 Big endian: 87 65 43 
Recall that show_bytes enumerates a series of bytes starting from the one with lowest address and working toward the one with highest address. On a little-endian machine, it will list the bytes from least signi.cant to most. On a big-endian machine, it will list bytes from the most signi.cant byte to the least. 
Solution to Problem 2.6 (page 46) 
This problem is another chance to practice hexadecimal to binary conversion. It also gets you thinking about integer and .oating-point representations. We will explore these representations in more detail later in this chapter. 
A. Using the notation of the example in the text, we write the two strings as follows: 
00359141 00000000001101011001000101000001 ********************* 
4A564504 01001010010101100100010100000100 
B. With the second word shifted two positions to the right relative to the .rst, we .nd a sequence with 21 matching bits. 
C. We .nd all bits of the integer embedded in the .oating-point number, except for the most signi.cant bit having value 1. Such is the case for the example in the text as well. In addition, the .oating-point number has some nonzero high-order bits that do not match those of the integer. 
Solution to Problem 2.7 (page 46) 
It prints 61 62 63 6465 66. Recall also that the library routine strlen does not count the terminating null character, and so show_bytes printed only through the character ¡®f¡¯. 
Solution to Problem 2.8 (page 49) 
This problem is a drill to help you become more familiar with Boolean operations. 
Operation Result a [01101001] b [01010101] 
~a [10010110] 
~b [10101010] 
a & b [01000001] a | b [01111101] a ^ b [00111100] 
Solution to Problem 2.9 (page 50) 
This problem illustrates how Boolean algebra can be used to describe and reason about real-world systems. We can see that this color algebra is identical to the Boolean algebra over bit vectors of length 3. 
A. Colors are complemented by complementing the values of R, G, and B. From this, we can see that White is the complement of Black, Yellow is the complement of Blue, Magenta is the complement of Green, and Cyan is the complement of Red. 

B. We perform Boolean operations based on a bit-vector representation of the colors. From this we get the following: 
Blue (001) | Green (010) = Cyan (011) Yellow (110) & Cyan (011) = Green (010) Red (100) ^ Magenta (101) = Blue (001) 
Solution to Problem 2.10 (page 51) 
This procedure relies on the fact that Exclusive-Or is commutative and associa-tive, and that a ^ a = 0 for any a. 
Step *x *y 
Initially ab Step 1 aa ^ b Step 2 a ^ (a ^ b) = (a ^ a) ^ b = ba ^ b Step 3 bb ^ (a ^ b) = (b ^ b) ^ a = a 
See Problem 2.11 for a case where this function will fail. 
Solution to Problem 2.11 (page 52) 
This problem illustrates a subtle and interesting feature of our inplace swap rou-tine. 
A. Both first and last have value k, so we are attempting to swap the middle element with itself. 
B. In this case, arguments x and y to inplace_swap both point to the same location. When we compute *x^*y, we get 0. We then store 0 as the middle element of the array, and the subsequent steps keep setting this element to 
0. We can see that our reasoning in Problem 2.10 implicitly assumed that x and y denote different locations. 
C. Simply replace the test in line 4 of reverse_array to be first < last, since there is no need to swap the middle element with itself. 
Solution to Problem 2.12 (page 53) 
Here are the expressions: 
A. x & 0xFF 
B. x ^ ~0xFF 
C. x | 0xFF 

These expressions are typical of the kind commonly found in performing low-level bit operations. The expression ~0xFF creates a mask where the 8 least-signi.cant bits equal 0 and the rest equal 1. Observe that such a mask will be generated regardless of the word size. By contrast, the expression 0xFFFFFF00 would only work on a 32-bit machine. 
Solution to Problem 2.13 (page 53) 
These problems help you think about the relation between Boolean operations and typical ways that programmers apply masking operations. Here is the code: 
/* Declarations of functions implementing operations bis and bic */ 
int bis(int x, int m); int bic(int x, int m); 
/* Compute x|y using only calls to functions bis and bic */ 
int bool_or(int x, int y) { int result = bis(x,y); return result; 
} 
/* Compute x^y using only calls to functions bis and bic */ 
int bool_xor(int x, int y) { int result = bis(bic(x,y), bic(y,x)); return result; 
} 
The bis operation is equivalent to Boolean Or¡ªa bit is set in z if either this bit is set in x or it is set in m. On the other hand, bic(x, m) is equivalent to x&~m; we want the result to equal 1 only when the corresponding bit of x is 1 and of m is 0. 
Given that, we can implement | with a single call to bis. To implement ^,we take advantage of the property 
x ^ y = (x &~y) | (~x & y). 
Solution to Problem 2.14 (page 54) 
This problem highlights the relation between bit-level Boolean operations and logic operations in C. A common programming error is to use a bit-level operation when a logic one is intended, or vice versa. 
Expression Value Expression Value 
x & y  0x20  x && y  0x01  
x | y  0x7F  x || y  0x01  
~x | ~y  0xDF  !x || !y  0x00  
x & !y  0x00  x && ~y  0x01  

Solution to Problem 2.15 (page 54) 
The expression is !(x^y). 
That is, x^y will be zero if and only if every bit of x matches the corresponding bit of y. We then exploit the ability of ! to determine whether a word contains any nonzero bit. 
There is no real reason to use this expression rather than simply writing x== y, but it demonstrates some of the nuances of bit-level and logical operations. 

Solution to Problem 2.16 (page 56) 
This problem is a drill to help you understand the different shift operations. 
(Logical) (Arithmetic) 
x<<3 x>>2 x>>2 

Hex Binary Binary Hex Binary Hex Binary Hex 
0xC3 [11000011] [00011000] 0x18 [00110000] 0x30 [11110000] 0xF0 0x75 [01110101] [10101000] 0xA8 [00011101] 0x1D [00011101] 0x1D 0x87 [10000111] [00111000] 0x38 [00100001] 0x21 [11100001] 0xE1 0x66 [01100110] [00110000] 0x30 [00011001] 0x19 [00011001] 0x19 
Solution to Problem 2.17 (page 61) 
In general, working through examples for very small word sizes is a very good way to understand computer arithmetic. 
The unsigned values correspond to those in Figure 2.2. For the two¡¯s-complement values, hex digits 0 through 7 have a most signi.cant bit of 0, yielding nonnegative values, while hex digits 8 through F have a most signi.cant bit of 1, yielding a negative value. 
x 

Hexadecimal Binary B2U4(x) B2T4(x) 
0xE [1110] 23 +22 +21 =14 .23 +22 +21 =.2 
0x0 [0000] 0 0 
0x5 [0101] 22 +20 =5 22 +20 =5 
0x8 [1000] 23 =8 .23 =.8 
0xD [1101] 23 +22 +20 =13 .23 +22 +20 =.3 
0xF [1111] 23 +22 +21 +20 =15 .23 +22 +21 +20 =.1 
Solution to Problem 2.18 (page 64) 
For a 32-bit machine, any value consisting of eight hexadecimal digits beginning with one of the digits 8 through f represents a negative number. It is quite com-mon to see numbers beginning with a string of f¡¯s, since the leading bits of a negative number are all ones. You must look carefully, though. For example, the number 0x8048337 has only seven digits. Filling this out with a leading zero gives 0x08048337, a positive number. 
8048337:  81  ec  b8  01  00  00  sub  $0x1b8,%esp  A.  440  
804833d:  8b  55  08  mov  0x8(%ebp),%edx  
8048340:  83  c2  14  add  $0x14,%edx  B.  20  
8048343:  8b  85  58  fe  ff  ff  mov  0xfffffe58(%ebp),%eax C.  -424  
8048349:  03  02  add  (%edx),%eax  
804834b:  89  85  74  fe  ff  ff  mov  %eax,0xfffffe74(%ebp) D.  -396  
8048351:  8b  55  08  mov  0x8(%ebp),%edx  
8048354:  83  c2  44  add  $0x44,%edx  E.  68  
8048357:  8b  85  c8  fe  ff  ff  mov  0xfffffec8(%ebp),%eax F.  -312  

804835d: 89 02 mov %eax,(%edx) 804835f: 8b 45 10 mov 0x10(%ebp),%eax G. 16 8048362: 03 45 0c add 0xc(%ebp),%eax H. 12 8048365: 89 85 ec fe ff ff mov %eax,0xfffffeec(%ebp) I. -276 804836b: 8b 45 08 mov 0x8(%ebp),%eax 804836e: 83 c0 20 add $0x20,%eax J. 32 8048371: 8b 00 mov (%eax),%eax 
Solution to Problem 2.19 (page 67) 
The functions T2U and U2T are very peculiar from a mathematical perspective. It is important to understand how they behave. 
We solve this problem by reordering the rows in the solution of Problem 2.17 according to the two¡¯s-complement value and then listing the unsigned value as the result of the function application. We show the hexadecimal values to make this process more concrete. 
x (hex) x T2U4(x) 
0x8 .88 
0xD .3 13 
0xE .2 14 
0xF .1 15 
0x0 00 
0x5 55 
Solution to Problem 2.20 (page 68) 
This exercise tests your understanding of Equation 2.6. 
For the .rst four entries, the values of x are negative and T2U4(x) = x + 24. For the remaining two entries, the values of x are nonnegative and T2U4(x) = x. 
Solution to Problem 2.21 (page 70) 
This problem reinforces your understanding of the relation between two¡¯s-complement and unsigned representations, and the effects of the C promotion rules. Recall that TMin32 is .2,147,483,648, and that when cast to unsigned it be-comes 2,147,483,648. In addition, if either operand is unsigned, then the other operand will be cast to unsigned before comparing. 

Expression  Type  Evaluation  
-2147483647-1 == 2147483648U  unsigned  1  
-2147483647-1 < 2147483647  signed  1  
-2147483647-1U < 2147483647  unsigned  0  
-2147483647-1 < -2147483647  signed  1  
-2147483647-1U < -2147483647  unsigned  1  

Solution to Problem 2.22 (page 74) 
This exercise provides a concrete demonstration of how sign extension preserves the numeric value of a two¡¯s-complement representation. 
A. [1011]: .23 + 21 + 20 =.8 + 2 + 1 =.5 
B. [11011]: .24 + 23 + 21 + 20 =.16 + 8 + 2 + 1 =.5 
C. [111011]: .25 + 24 + 23 + 21 + 20 =.32 + 16 + 8 + 2 + 1 =.5 
Solution to Problem 2.23 (page 74) 
The expressions in these functions are common program ¡°idioms¡± for extracting values from a word in which multiple bit .elds have been packed. They exploit the zero-.lling and sign-extending properties of the different shift operations. Note carefully the ordering of the cast and shift operations. In fun1, the shifts are performed on unsigned variable word, and hence are logical. In fun2, shifts are performed after casting word to int, and hence are arithmetic. 
A. w fun1(w) fun2(w) 0x00000076 0x00000076 0x00000076 0x87654321 0x00000021 0x00000021 0x000000C9 0x000000C9 0xFFFFFFC9 0xEDCBA987 0x00000087 0xFFFFFF87 
B. Function fun1 extracts a value from the low-order 8 bits of the argument, giving an integer ranging between 0 and 255. Function fun2 extracts a value from the low-order 8 bits of the argument, but it also performs sign extension. The result will be a number between .128 and 127. 
Solution to Problem 2.24 (page 76) 
The effect of truncation is fairly intuitive for unsigned numbers, but not for two¡¯s-complement numbers. This exercise lets you explore its properties using very small word sizes. 
Hex Unsigned Two¡¯s complement 
Original Truncated Original Truncated Original Truncated 
0  0  0  0  0  0  
2  2  2  2  2  2  
9  1  9  1  .7  1  
B  3  11  3  .5  3  
F  7  15  7  .1  .1  

As Equation 2.9 states, the effect of this truncation on unsigned values is to simply .nd their residue, modulo 8. The effect of the truncation on signed values is a bit more complex. According to Equation 2.10, we .rst compute the modulo 8 residue of the argument. This will give values 0 through 7 for arguments 0 through 7, and also for arguments .8 through .1. Then we apply function U2T3 to these residues, giving two repetitions of the sequences 0 through 3 and .4 through .1. 
Solution to Problem 2.25 (page 77) 
This problem is designed to demonstrate how easily bugs can arise due to the implicit casting from signed to unsigned. It seems quite natural to pass parameter length as an unsigned, since one would never want to use a negative length. The stopping criterion i <= length-1 also seems quite natural. But combining these two yields an unexpected outcome! 
Since parameter length is unsigned, the computation 0 . 1is performed using unsigned arithmetic, which is equivalent to modular addition. The result is then UMax.The ¡Ü comparison is also performed using an unsigned comparison, and since any number is less than or equal to UMax, the comparison always holds! Thus, the code attempts to access invalid elements of array a. 
The code can be .xed either by declaring length to be an int, or by changing the test of the for loop to be i < length. 
Solution to Problem 2.26 (page 77) 
This example demonstrates a subtle feature of unsigned arithmetic, and also the property that we sometimes perform unsigned arithmetic without realizing it. This can lead to very tricky bugs. 
A. For what cases will this function produce an incorrect result? The function will incorrectly return 1 when s is shorter than t. 
B. Explain how this incorrect result comes about. Since strlen is de.ned to yield an unsigned result, the difference and the comparison are both com-puted using unsigned arithmetic. When s is shorter than t, the difference strlen(s) -strlen(t) should be negative, but instead becomes a large, unsigned number, which is greater than 0. 
C. Show how to .x the code so that it will work reliably. Replace the test with the following: 
return strlen(s) > strlen(t); 
Solution to Problem 2.27 (page 81) 
This function is a direct implementation of the rules given to determine whether or not an unsigned addition over.ows. 
/* Determine whether arguments can be added without overflow */ 
int uadd_ok(unsigned x, unsigned y) { unsigned sum = x+y; return sum >= x; 
} 
Solution to Problem 2.28 (page 82) 
This problem is a simple demonstration of arithmetic modulo 16. The easiest way to solve it is to convert the hex pattern into its unsigned decimal value. For nonzero values of x, we must have (-4u x) + x = 16. Then we convert the complemented value back to hex. 

x  u-4 x  
Hex  Decimal  Decimal  Hex  
0  0  0  0  
5  5  11  B  
8  8  8  8  
D  13  3  3  
F  15  1  1  

Solution to Problem 2.29 (page 86) 
This problem is an exercise to make sure you understand two¡¯s-complement addition. 
xy x + yx +t5 y Case 
.12  .15  .27  5  1  
[10100]  [10001]  [100101]  [00101]  
.8  .8  .16  .16  2  
[11000]  [11000]  [110000]  [10000]  
.9  8  .1  .1  2  
[10111]  [01000]  [111111]  [11111]  
2  5  7  7  3  
[00010]  [00101]  [000111]  [00111]  
12  4  16  .16  4  
[01100]  [00100]  [010000]  [10000]  

Solution to Problem 2.30 (page 86) 
This function is a direct implementation of the rules given to determine whether or not a two¡¯s-complement addition over.ows. 
/* Determine whether arguments can be added without overflow */ 
int tadd_ok(int x, int y) { 
int sum = x+y; 
intneg_over=x< 0&&y< 0&&sum >= 0; 
intpos_over=x>=0&&y>=0&&sum < 0; 
return !neg_over && !pos_over; } 
Solution to Problem 2.31 (page 86) 
Your coworker could have learned, by studying Section 2.3.2, that two¡¯s-complement addition forms an abelian group, and so the expression (x+y)-x will evaluate to y regardless of whether or not the addition over.ows, and that (x+y)-y will always evaluate to x. 
Solution to Problem 2.32 (page 87) 
This function will give correct values, except when y is TMin. In this case, we will have -y also equal to TMin, and so function tadd_ok will consider there to be 
negative over.ow any time x is negative. In fact, x-y does not over.ow for these 
cases. 
One lesson to be learned from this exercise is that TMin should be included 
as one of the cases in any test procedure for a function. 
Solution to Problem 2.33 (page 87) 
This problem helps you understand two¡¯s-complement negation using a very small 
word size. 
For w =4, we have TMin4 =.8. So .8 is its own additive inverse, while other 
values are negated by integer negation. 
x -4t x 
Hex Decimal Decimal Hex 
0 00 0 5 5 .5 B 8 .8 .8 8 D .33 3 F .11 1 
The bit patterns are the same as for unsigned negation. 
Solution to Problem 2.34 (page 90) 
This problem is an exercise to make sure you understand two¡¯s-complement 
multiplication.  
Mode  x  y  x . y  Truncated x . y  
Unsigned  4  [100]  5  [101]  20  [010100]  4  [100]  
Two¡¯s comp.  .4  [100]  .3  [101]  12  [001100]  .4  [100]  
Unsigned  2  [010]  7  [111]  14  [001110]  6  [110]  
Two¡¯s comp.  2  [010]  .1  [111]  .2  [111110]  .2  [110]  
Unsigned  6  [110]  6  [110]  36  [100100]  4  [100]  
Two¡¯s comp.  .2  [110]  .2  [110]  4  [000100]  .4  [100]  

Solution to Problem 2.35 (page 90) 
It¡¯s not realistic to test this function for all possible values of x and y. Even if you could run 10 billion tests per second, it would require over 58 years to test all combinations when data type int is 32 bits. On the other hand, it is feasible to test your code by writing the function with data type short or char and then testing it exhaustively. 
Here¡¯s a more principled approach, following the proposed set of arguments: 
1. We know that x.y can be written as a 2w-bit two¡¯s-complement number. Let u denote the unsigned number represented by the lower w bits, and v denote the two¡¯s-complement number represented by the upper w bits. Then, based on Equation 2.3, we can see that x.y =v2w +u. 

We also know that u =T2U (p), since they are unsigned and two¡¯s-
w 

complement numbers arising from the same bit pattern, and so by Equa-tion 2.5, we can write u =p +pw.12w , where pw.1 is the most signi.cant bit of p. Letting t =v +pw.1, we have x.y =p +t2w . 
When t =0, we have x.y =p; the multiplication does not over.ow. When t =0, we have x.y =p; the multiplication does over.ow. 
2. 
By de.nition of integer division, dividing p by nonzero x gives a quotient q and a remainder r such that p =x.q +r, and |r|< |x|. (We use absolute values here, because the signs of x and r may differ. For example, dividing .7 by 2 gives quotient .3 and remainder .1.) 

3. 
Suppose q =y. Then we have x.y =x.y +r +t2w . From this, we can see that r +t2w =0. But |r|< |x|¡Ü2w, and so this identity can hold only if t =0, in which case r =0. 


Suppose r =t =0. Then we will have x.y =x.q, implying that y =q. 
When x equals 0, multiplication does not over.ow, and so we see that our code provides a reliable way to test whether or not two¡¯s-complement multiplication causes over.ow. 
Solution to Problem 2.36 (page 91) 
With 64 bits, we can perform the multiplication without over.owing. We then test whether casting the product to 32 bits changes the value: 
1 /* Determine whether arguments can be multiplied without overflow */ 2 int tmult_ok(int x, int y) { 3 /* Compute product without overflow */ 4 long long pll = (long long) x*y; 5 /* See if casting to int preserves value */ 6 return pll == (int) pll; 
7 } 

Note that the casting on the right-hand side of line 4 is critical. If we instead wrote the line as 
long long pll = x*y; 
the product would be computed as a 32-bit value (possibly over.owing) and then sign extended to 64 bits. 
Solution to Problem 2.37 (page 92) 
A. This change does not help at all. Even though the computation of asize will be accurate, the call to malloc will cause this value to be converted to a 32-bit unsigned number, and so the same over.ow conditions will occur. 
B. With malloc having a 32-bit unsigned number as its argument, it cannot possibly allocate a block of more than 232 bytes, and so there is no point attempting to allocate or copy this much memory. Instead, the function should abort and return NULL, as illustrated by the following replacement to the original call to malloc (line 10): 
long long unsigned required_size = 
ele_cnt * (long long unsigned) ele_size; size_t request_size = (size_t) required_size; if (required_size != request_size) 
/* Overflow must have occurred. Abort operation */ 
return NULL; 
void *result = malloc(request_size); 
if (result == NULL) 
/* malloc failed */ 
return NULL; 
Solution to Problem 2.38 (page 93) 
In Chapter 3, we will see many examples of the lea instruction in action. The instruction is provided to support pointer arithmetic, but the C compiler often uses it as a way to perform multiplication by small constants. 
For each value of k, we can compute two multiples: 2k (when b is 0) and 2k + 1 (when b is a). Thus, we can compute multiples 1, 2, 3, 4, 5, 8, and 9. 
Solution to Problem 2.39 (page 94) 
The expression simply becomes -(x<<m). To see this, let the word size be wso that n= w.1. Form B states that we should compute (x<<w) -(x<<m), but shifting x to the left by wwill yield the value 0. 
Solution to Problem 2.40 (page 94) 
This problem requires you to try out the optimizations already described and also to supply a bit of your own ingenuity. 
K Shifts Add/Subs Expression 
62 1 (x<<2) + (x<<1) 311 1 (x<<5) -x .62 1 (x<<1) -(x<<3) 552 2 (x<<6) -(x<<3) -x 
Observe that the fourth case uses a modi.ed version of form B. We can view the bit pattern [110111] as having a run of 6 ones with a zero in the middle, and so we apply the rule for form B, but then we subtract the term corresponding to the middle zero bit. 
Solution to Problem 2.41 (page 94) 
Assuming that addition and subtraction have the same performance, the rule is to choose form A when n= m, either form when n= m+ 1, and form B when n>m+ 1. 

The justi.cation for this rule is as follows. Assume .rst that m>1. When n= m, form A requires only a single shift, while form B requires two shifts and a subtraction. When n= m+ 1, both forms require two shifts and either an addition or a subtraction. When n>m+ 1, form B requires only two shifts and one subtraction, while form A requires n. m+ 1 >2 shifts and n. m>1 additions. For the case of m= 1, we get one fewer shift for both forms A and B, and so the same rules apply for choosing between the two. 
Solution to Problem 2.42 (page 97) 
The only challenge here is to compute the bias without any testing or conditional operations. We use the trick that the expression x>>31 generates a word with all ones if x is negative, and all zeros otherwise. By masking off the appropriate bits, we get the desired bias value. 
int div16(int x) { 
/* Compute bias to be either 0 (x >= 0) or 15 (x < 0) */ 
int bias = (x >> 31) & 0xF; 
return (x + bias) >> 4; } 
Solution to Problem 2.43 (page 98) 
We have found that people have dif.culty with this exercise when working di-rectly with assembly code. It becomes more clear when put in the form shown in optarith. 
We can see that M is 31; x*M is computed as (x<<5)-x. 
We can see that N is 8; a bias value of 7 is added when y is negative, and the right shift is by 3. 
Solution to Problem 2.44 (page 99) 
These ¡°C puzzle¡± problems provide a clear demonstration that programmers must understand the properties of computer arithmetic: 
A. (x>0)||(x-1<0) False. Let x be .2,147,483,648 (TMin32). We will then have x-1 equal to 2147483647 (TMax32). 
B. (x &7) != 7 || (x<<29 < 0) True.If (x&7)!=7 evaluates to 0, then we must have bit x2 equal to 1. When shifted left by 29, this will become the sign bit. 
C. (x*x)>=0 False. When x is 65,535 (0xFFFF), x*x is .131,071 (0xFFFE0001). D. x<0||-x<=0 True.If x is nonnegative, then -x is nonpositive. E. x>0||-x>=0 False. Let x be .2,147,483,648 (TMin32). Then both x and -x are negative. 
F. x+y == uy+ux True. Two¡¯s-complement and unsigned addition have the same bit-level be-havior, and they are commutative. 
G. x*~y + uy*ux == -x True. ~y equals -y-1. uy*ux equals x*y. Thus, the left hand side is equivalent to x*-y-x+x*y. 
Solution to Problem 2.45 (page 102) 
Understanding fractional binary representations is an important step to under-standing .oating-point encodings. This exercise lets you try out some simple ex-amples. 
Fractional value Binary representation Decimal representation 
1 
0.001 0.125
8 3 
0.11 0.75
4 25 
1.1001 1.5625
16 43 
10.1011 2.6875
16 9 
1.001 1.125
8 47 
101.111 5.875
8 51 
11.0011 3.1875
16 
One simple way to think about fractional binary representations is to repre-
x
sent a number as a fraction of the form 2k. We can write this in binary using the binary representation of x, with the binary point inserted k positions from the right. As an example, for 25 
16, we have 2510 = 110012. We then put the binary point four positions from the right to get 1.10012. 
Solution to Problem 2.46 (page 102) 
In most cases, the limited precision of .oating-point numbers is not a major problem, because the relative error of the computation is still fairly low. In this example, however, the system was sensitive to the absolute error. 
A. We can see that 0.1 . xhas binary representation 
0.000000000000000000000001100[1100] ...2 
B. Comparing this to the binary representation of 101 , we can see that it is simply 2.20 ¡Á 1 
10, which is around 9.54 ¡Á 10.8. 
C. 9.54 ¡Á 10.8 ¡Á 100 ¡Á 60 ¡Á 60 ¡Á 10 ¡Ö 0.343 seconds. 
D. 0.343 ¡Á 2000 ¡Ö 687 meters. 
Solution to Problem 2.47 (page 107) 
Working through .oating-point representations for very small word sizes helps clarify how IEEE .oating point works. Note especially the transition between denormalized and normalized values. 

Bits eE 2E fM 2E ¡Á MV Decimal 
00000  0  0  1  0 4  0 4  0 4  0  0.0  
00001  0  0  1  1 4  1 4  1 4  1 4  0.25  
00010  0  0  1  2 4  2 4  2 4  1 2  0.5  
00011  0  0  1  3 4  3 4  3 4  3 4  0.75  
00100  1  0  1  0 4  4 4  4 4  1  1.0  
00101  1  0  1  1 4  5 4  5 4  5 4  1.25  
00110  1  0  1  2 4  6 4  6 4  3 2  1.5  
00111  1  0  1  3 4  7 4  7 4  7 4  1.75  
01000  2  1  2  0 4  4 4  8 4  2  2.0  
01001  2  1  2  1 4  5 4  10 4  5 2  2.5  
01010  2  1  2  2 4  6 4  12 4  3  3.0  
01011  2  1  2  3 4  7 4  14 4  7 2  3.5  
01100  ¡ª  ¡ª  ¡ª  ¡ª  ¡ª  ¡ª  ¡Þ  ¡ª  
01101  ¡ª  ¡ª  ¡ª  ¡ª  ¡ª  ¡ª  NaN  ¡ª  
01110  ¡ª  ¡ª  ¡ª  ¡ª  ¡ª  ¡ª  NaN  ¡ª  
01111  ¡ª  ¡ª  ¡ª  ¡ª  ¡ª  ¡ª  NaN  ¡ª  

Solution to Problem 2.48 (page 110) 
Hexadecimal value 0x359141 is equivalent to binary [1101011001000101000001]. Shifting this right 21 places gives 1.1010110010001010000012 ¡Á 221. We form the fraction .eld by dropping the leading 1 and adding two 0s, giving [10101100100010100000100]. The exponent is formed by adding bias 127 to 21, giving 148 (binary [10010100]). We combine this with a sign .eld of 0 to give a binary representation 
[01001010010101100100010100000100]. 

We see that the matching bits in the two representations correspond to the low-order bits of the integer, up to the most signi.cant bit equal to 1 matching the high-order 21 bits of the fraction: 
00359141 00000000001101011001000101000001 ********************* 
4A564504 01001010010101100100010100000100 
Solution to Problem 2.49 (page 110) 
This exercise helps you think about what numbers cannot be represented exactly in .oating point. 
A. The number has binary representation 1, followed by n 0s, followed by 1, giving value 2n+1 + 1. 
B. When n = 23, the value is 224 + 1 = 16,777,217. 
Solution to Problem 2.50 (page 112) 
Performing rounding by hand helps reinforce the idea of round-to-even with binary numbers. 
Original Rounded 
10.0102 241 10.02 10.0112 283 10.1 221 10.1102 23 11.03
4 11.0012 31 11.03
8 
Solution to Problem 2.51 (page 112) 
A. Looking at the nonterminating sequence for 1/10, we can see that the 2 bits to the right of the rounding position are 1, and so a better ap-proximation to 1/10 would be obtained by incrementing x to get x = 0.000110011001100110011012, which is larger than 0.1. 
B. We can see that x . 0.1 has binary representation: 
0.0000000000000000000000000[1100]. 
1
Comparing this to the binary representation of 10, we can see that it is 2.22 ¡Á 1 
10 , which is around 2.38 ¡Á 10.8. 
C. 2.38 ¡Á 10.8 ¡Á 100 ¡Á 60 ¡Á 60 ¡Á 10 ¡Ö 0.086 seconds, a factor of 4 less than the error in the Patriot system. 
D. 0.343 ¡Á 2000 ¡Ö 171 meters. 
Solution to Problem 2.52 (page 112) 
This problem tests a lot of concepts about .oating-point representations, including the encoding of normalized and denormalized values, as well as rounding. 
Format A Format B 
Bits Value Bits Value Comments 
011 0000  1  0111 000  1  
101 1110  15 2  1001 111  15 2  
010 1001  25 32  0110 100  3 4  Round down  
110 1111  31 2  1011 000  16  Round up  
000 0001  1 64  0001 000  1 64  Denorm ¡ú norm  

Solution to Problem 2.53 (page 115) 
In general, it is better to use a library macro rather than inventing your own code. This code seems to work on a variety of machines, however. 

We assume that the value 1e400 over.ows to in.nity. 
#define POS_INFINITY 1e400 #define NEG_INFINITY (-POS_INFINITY) #define NEG_ZERO (-1.0/POS_INFINITY) 
Solution to Problem 2.54 (page 117) 
Exercises such as this one help you develop your ability to reason about .oating-point operations from a programmer¡¯s perspective. Make sure you understand each of the answers. 
A. x == (int)(double) x Yes, since double has greater precision and range than int. 
B. x == (int)(float) x No. For example, when x is TMax. 
C. d == (double)(float) d No. For example, when d is 1e40, we will get +¡Þ on the right. 
D. f == (float)(double) f Yes, since double has greater precision and range than float. 
E. f == -(-f) Yes, since a .oating-point number is negated by simply inverting its sign bit. 
F. 1.0/2 == 1/2.0 Yes, the numerators and denominators will both be converted to .oating-point representations before the division is performed. 
G. d*d >= 0.0 Yes, although it may over.ow to +¡Þ. 
H. (f+d)-f == d No, for example when f is 1.0e20 and d is 1.0, the expression f+d will be rounded to 1.0e20, and so the expression on the left-hand side will evaluate to 0.0, while the right-hand side will be 1.0. 
This page intentionally left blank 
CHAPTER 
3 

Machine-Level 
Representation 
of 
Programs 

3.1 A Historical Perspective 156 
3.2 Program Encodings 159 
3.3 Data Formats 167 
3.4 Accessing Information 168 
3.5 Arithmetic and Logical Operations 177 
3.6 Control 185 3.7 Procedures 219 
3.8 Array Allocation and Access 232 
3.9 Heterogeneous Data Structures 241 
3.10 Putting It Together: Understanding Pointers 252 
3.11 Life in the Real World: Using the gdb Debugger 254 
3.12 Out-of-Bounds Memory References and Buffer Over.ow 256 
3.13 x86-64: Extending IA32 to 64 Bits 267 
3.14 Machine-Level Representations of Floating-Point Programs 292 3.15 Summary 293 Bibliographic Notes 294 Homework Problems 294 Solutions to Practice Problems 308 
Computers execute machine code, sequences of bytes encoding the low-level op-erations that manipulate data, manage memory, read and write data on storage devices, and communicate over networks. A compiler generates machine code through a series of stages, based on the rules of the programming language, the instruction set of the target machine, and the conventions followed by the operat-ing system. The gcc C compiler generates its output in the form of assembly code, a textual representation of the machine code giving the individual instructions in the program. gcc then invokes both an assembler and a linker to generate the exe-cutable machine code from the assembly code. In this chapter, we will take a close look at machine code and its human-readable representation as assembly code. 
When programming in a high-level language such as C, and even more so in Java, we are shielded from the detailed, machine-level implementation of our pro-gram. In contrast, when writing programs in assembly code (as was done in the early days of computing) a programmer must specify the low-level instructions the program uses to carry out a computation. Most of the time, it is much more produc-tive and reliable to work at the higher level of abstraction provided by a high-level language. The type checking provided by a compiler helps detect many program errors and makes sure we reference and manipulate data in consistent ways. With modern, optimizing compilers, the generated code is usually at least as ef.cient as what a skilled, assembly-language programmer would write by hand. Best of all, a program written in a high-level language can be compiled and executed on a number of different machines, whereas assembly code is highly machine speci.c. 
So why should we spend our time learning machine code? Even though com-pilers do most of the work in generating assembly code, being able to read and understand it is an important skill for serious programmers. By invoking the com-piler with appropriate command-line parameters, the compiler will generate a .le showing its output in assembly-code form. By reading this code, we can under-stand the optimization capabilities of the compiler and analyze the underlying inef.ciencies in the code. As we will experience in Chapter 5, programmers seek-ing to maximize the performance of a critical section of code often try different variations of the source code, each time compiling and examining the generated assembly code to get a sense of how ef.ciently the program will run. Furthermore, there are times when the layer of abstraction provided by a high-level language hides information about the run-time behavior of a program that we need to un-derstand. For example, when writing concurrent programs using a thread package, as covered in Chapter 12, it is important to know what region of memory is used to hold the different program variables. This information is visible at the assembly-code level. As another example, many of the ways programs can be attacked, allowing worms and viruses to infest a system, involve nuances of the way pro-grams store their run-time control information. Many attacks involve exploiting weaknesses in system programs to overwrite information and thereby take control of the system. Understanding how these vulnerabilities arise and how to guard against them requires a knowledge of the machine-level representation of pro-grams. The need for programmers to learn assembly code has shifted over the years from one of being able to write programs directly in assembly to one of being able to read and understand the code generated by compilers. 

In this chapter, we will learn the details of two particular assembly languages and see how C programs get compiled into these forms of machine code. Reading the assembly code generated by a compiler involves a different set of skills than writing assembly code by hand. We must understand the transformations typical compilers make in converting the constructs of C into machine code. Relative to the computations expressed in the C code, optimizing compilers can rearrange execution order, eliminate unneeded computations, replace slow operations with faster ones, and even change recursive computations into iterative ones. Under-standing the relation between source code and the generated assembly can often be a challenge¡ªit¡¯s much like putting together a puzzle having a slightly differ-ent design than the picture on the box. It is a form of reverse engineering¡ªtrying to understand the process by which a system was created by studying the system and working backward. In this case, the system is a machine-generated assembly-language program, rather than something designed by a human. This simpli.es the task of reverse engineering, because the generated code follows fairly reg-ular patterns, and we can run experiments, having the compiler generate code for many different programs. In our presentation, we give many examples and provide a number of exercises illustrating different aspects of assembly language and compilers. This is a subject where mastering the details is a prerequisite to understanding the deeper and more fundamental concepts. Those who say ¡°I un-derstand the general principles, I don¡¯t want to bother learning the details¡± are deluding themselves. It is critical for you to spend time studying the examples, working through the exercises, and checking your solutions with those provided. 
Our presentation is based on two related machine languages: Intel IA32, the dominant language of most computers today, and x86-64, its extension to run on 64-bit machines. Our focus starts with IA32. Intel processors have grown from primitive 16-bit processors in 1978 to the mainstream machines for today¡¯s desk-top, laptop, and server computers. The architecture has grown correspondingly, with new features added and with the 16-bit architecture transformed to become IA32, supporting 32-bit data and addresses. The result is a rather peculiar design with features that make sense only when viewed from a historical perspective. It is also laden with features providing backward compatibility that are not used by modern compilers and operating systems. We will focus on the subset of the fea-tures used by gcc and Linux. This allows us to avoid much of the complexity and arcane features of IA32. 
Our technical presentation starts with a quick tour to show the relation be-tween C, assembly code, and machine code. We then proceed to the details of IA32, starting with the representation and manipulation of data and the imple-mentation of control. We see how control constructs in C, such as if, while, and switch statements, are implemented. We then cover the implementation of pro-cedures, including how the program maintains a run-time stack to support the passing of data and control between procedures, as well as storage for local vari-ables. Next, we consider how data structures such as arrays, structures, and unions are implemented at the machine level. With this background in machine-level pro-gramming, we can examine the problems of out of bounds memory references and the vulnerability of systems to buffer over.ow attacks. We .nish this part of the presentation with some tips on using the gdb debugger for examining the run-time behavior of a machine-level program. 
As we will discuss, the extension of IA32 to 64 bits, termed x86-64, was origi-nally developed by Advanced Micro Devices (AMD), Intel¡¯s biggest competitor. Whereas a 32-bit machine can only make use of around 4 gigabytes (232 bytes) of random-access memory, current 64-bit machines can use up to 256 terabytes (248 bytes). The computer industry is currently in the midst of a transition from 32-bit to 64-bit machines. Most of the microprocessors in recent server and desktop machines, as well as in many laptops, support either 32-bit or 64-bit operation. However, most of the operating systems running on these machines support only 32-bit applications, and so the capabilities of the hardware are not fully utilized. As memory prices drop, and the desire to perform computations involving very large data sets increases, 64-bit machines and applications will become common-place. It is therefore appropriate to take a close look at x86-64. We will see that in making the transition from 32 to 64 bits, the engineers at AMD also incorporated features that make the machines better targets for optimizing compilers and that improve system performance. 
We provide Web Asides to cover material intended for dedicated machine-language enthusiasts. In one, we examine the code generated when code is com-piled using higher degrees of optimization. Each successive version of the gcc compiler implements more sophisticated optimization algorithms, and these can radically transform a program to the point where it is dif.cult to understand the re-lation between the original source code and the generated machine-level program. Another Web Aside gives a brief presentation of ways to incorporate assembly code into C programs. For some applications, the programmer must drop down to assembly code to access low-level features of the machine. One approach is to write entire functions in assembly code and combine them with C functions during the linking stage. A second is to use gcc¡¯s support for embedding assembly code directly within C programs. We provide separate Web Asides for two different machine languages for .oating-point code. The ¡°x87¡± .oating-point instructions have been available since the early days of Intel processors. This implementation of .oating point is particularly arcane, and so we advise that only people deter-mined to work with .oating-point code on older machines attempt to study this section. The more recent ¡°SSE¡± instructions were developed to support multi-media applications, but in their more recent versions (version 2 and later), and with more recent versions of gcc, SSE has become the preferred method for map-ping .oating point onto both IA32 and x86-64 machines. 
3.1 
A 
Historical 
Perspective 

The Intel processor line, colloquially referred to as x86, has followed a long, evo-lutionary development. It started with one of the .rst single-chip, 16-bit micropro-cessors, where many compromises had to be made due to the limited capabilities of integrated circuit technology at the time. Since then, it has grown to take ad-vantage of technology improvements as well as to satisfy the demands for higher performance and for supporting more advanced operating systems. 

The list that follows shows some models of Intel processors and some of their key features, especially those affecting machine-level programming. We use the number of transistors required to implement the processors as an indication of how they have evolved in complexity (K denotes 1000, and M denotes 1,000,000). 
8086: (1978, 29 K transistors). One of the .rst single-chip, 16-bit microproces-sors. The 8088, a variant of the 8086 with an 8-bit external bus, formed the heart of the original IBM personal computers. IBM contracted with then-tiny Microsoft to develop the MS-DOS operating system. The orig-inal models came with 32,768 bytes of memory and two .oppy drives (no hard drive). Architecturally, the machines were limited to a 655,360-byte address space¡ªaddresses were only 20 bits long (1,048,576 bytes address-able), and the operating system reserved 393,216 bytes for its own use. In 1980, Intel introduced the 8087 .oating-point coprocessor (45 K tran-sistors) to operate alongside an 8086 or 8088 processor, executing the .oating-point instructions. The 8087 established the .oating-point model for the x86 line, often referred to as ¡°x87.¡± 
80286: (1982, 134 K transistors). Added more (and now obsolete) addressing modes. Formed the basis of the IBM PC-AT personal computer, the original platform for MS Windows. 
i386: (1985, 275 K transistors). Expanded the architecture to 32 bits. Added the .at addressing model used by Linux and recent versions of the Windows family of operating system. This was the .rst machine in the series that could support a Unix operating system. 
i486: (1989, 1.2 M transistors). Improved performance and integrated the .oating-point unit onto the processor chip but did not signi.cantly change the instruction set. 
Pentium: (1993, 3.1 M transistors). Improved performance, but only added minor extensions to the instruction set. 
PentiumPro: (1995, 5.5 M transistors). Introduced a radically new processor design, internally known as the P6 microarchitecture. Added a class of ¡°conditional move¡± instructions to the instruction set. 
Pentium II: (1997, 7 M transistors). Continuation of the P6 microarchitecture. Pentium III: (1999, 8.2 M transistors). Introduced SSE, a class of instructions for manipulating vectors of integer or .oating-point data. Each datum can be 1, 2, or 4 bytes, packed into vectors of 128 bits. Later versions of this chip went up to 24 M transistors, due to the incorporation of the level-2 cache on chip. Pentium 4: (2000, 42 M transistors). Extended SSE to SSE2, adding new data types (including double-precision .oating point), along with 144 new instructions for these formats. With these extensions, compilers can use SSE instructions, rather than x87 instructions, to compile .oating-point code. Introduced the NetBurst microarchitecture, which could operate at very high clock speeds, but at the cost of high power consumption. 
Pentium 4E: (2004, 125 M transistors). Added hyperthreading, a method to run two programs simultaneously on a single processor, as well as EM64T, Intel¡¯s implementation of a 64-bit extension to IA32 developed by Ad-vanced Micro Devices (AMD), which we refer to as x86-64. 
Core 2: (2006, 291 M transistors). Returned back to a microarchitecture similar to P6. First multi-core Intel microprocessor, where multiple processors are implemented on a single chip. Did not support hyperthreading. 
Core i7: (2008, 781 M transistors). Incorporated both hyperthreading and multi-core, with the initial version supporting two executing programs on each core and up to four cores on each chip. 
Each successive processor has been designed to be backward compatible¡ª able to run code compiled for any earlier version. As we will see, there are many strange artifacts in the instruction set due to this evolutionary heritage. Intel has had several names for their processor line, including IA32, for ¡°Intel Architecture 32-bit,¡± and most recently Intel64, the 64-bit extension to IA32, which we will refer to as x86-64. We will refer to the overall line by the commonly used colloquial name ¡°x86,¡± re.ecting the processor naming conventions up through the i486. 

Aside Moore¡¯s law 
Transistors 
Intel microprocessor complexity 
Core i7 1.0E09 1.0E08 1.0E07 1.0E06 1.0E05 1.0E04 

80286 8086  Core 2 Duo Pentium 4e Pentium 4 Pentium IIIPentiumPro Pentium II i486 Pentium i386  

1975 1980 1985 1990 1995 2000 2005 2010 Year 
If we plot the number of transistors in the different Intel processors versus the year of introduction, and use a logarithmic scale for the y-axis, we can see that the growth has been phenomenal. Fitting a line through the data, we see that the number of transistors increases at an annual rate of approximately 38%, meaning that the number of transistors doubles about every 26 months. This growth has been sustained over the multiple-decade history of x86 microprocessors. 
In 1965, Gordon Moore, a founder of Intel Corporation, extrapolated from the chip technology 
of the day, in which they could fabricate circuits with around 64 transistors on a single chip, to predict 
that the number of transistors per chip would double every year for the next 10 years. This predication 
became known as Moore¡¯s law. As it turns out, his prediction was just a little bit optimistic, but also too 
short-sighted. Over more than 45 years, the semiconductor industry has been able to double transistor 
counts on average every 18 months. 
Similar exponential growth rates have occurred for other aspects of computer technology¡ªdisk 
capacities, memory-chip capacities, and processor performance. These remarkable growth rates have 
been the major driving forces of the computer revolution. 
Over the years, several companies have produced processors that are com-patible with Intel processors, capable of running the exact same machine-level programs. Chief among these is Advanced Micro Devices (AMD). For years, AMD lagged just behind Intel in technology, forcing a marketing strategy where they produced processors that were less expensive although somewhat lower in performance. They became more competitive around 2002, being the .rst to break the 1-gigahertz clock-speed barrier for a commercially available microprocessor, and introducing x86-64, the widely adopted 64-bit extension to IA32. Although we will talk about Intel processors, our presentation holds just as well for the compatible processors produced by Intel¡¯s rivals. 
Much of the complexity of x86 is not of concern to those interested in programs for the Linux operating system as generated by the gcc compiler. The memory model provided in the original 8086 and its extensions in the 80286 are obsolete. Instead, Linux uses what is referred to as .at addressing, where the entire memory space is viewed by the programmer as a large array of bytes. 
As we can see in the list of developments, a number of formats and instructions have been added to x86 for manipulating vectors of small integers and .oating-point numbers. These features were added to allow improved performance on multimedia applications, such as image processing, audio and video encoding and decoding, and three-dimensional computer graphics. In its default invocation for 32-bit execution, gcc assumes it is generating code for an i386, even though there are very few of these 1985-era microprocessors running any longer. Only by giving speci.c command-line options, or by compiling for 64-bit operation, will the compiler make use of the more recent extensions. 
For the next part of our presentation, we will focus only on the IA32 instruc-tion set. We will then look at the extension to 64 bits via x86-64 toward the end of the chapter. 
3.2 
Program 
Encodings 

Suppose we write a C program as two .les p1.c and p2.c. We can then compile this code on an IA32 machine using a Unix command line: 
unix> gcc -O1 -o p p1.c p2.c 
The command gcc indicates the gcc C compiler. Since this is the default compiler on Linux, we could also invoke it as simply cc. The command-line option -O1 instructs the compiler to apply level-one optimizations. In general, increasing the level of optimization makes the .nal program run faster, but at a risk of increased compilation time and dif.culties running debugging tools on the code. As we will also see, invoking higher levels of optimization can generate code that is so heavily transformed that the relationship between the generated machine code and the original source code is dif.cult to understand. We will therefore use level-one optimization as a learning tool and then see what happens as we increase the level of optimization. In practice, level-two optimization (speci.ed with the option -O2) is considered a better choice in terms of the resulting program performance. 
The gcc command actually invokes a sequence of programs to turn the source code into executable code. First, the C preprocessor expands the source code to include any .les speci.ed with #include commands and to expand any macros, speci.ed with #define declarations. Second, the compiler generates assembly-code versions of the two source .les having names p1.s and p2.s. Next, the assembler converts the assembly code into binary object-code .les p1.o and p2.o. Object code is one form of machine code¡ªit contains binary representations of all of the instructions, but the addresses of global values are not yet .lled in. Finally, the linker merges these two object-code .les along with code implementing library functions (e.g., printf) and generates the .nal executable code .le p. Executable code is the second form of machine code we will consider¡ªit is the exact form of code that is executed by the processor. The relation between these different forms of machine code and the linking process is described in more detail in Chapter 7. 
3.2.1 Machine-Level Code 
As described in Section 1.9.2, computer systems employ several different forms of abstraction, hiding details of an implementation through the use of a sim-pler, abstract model. Two of these are especially important for machine-level programming. First, the format and behavior of a machine-level program is de-.ned by the instruction set architecture, or ¡°ISA,¡± de.ning the processor state, the format of the instructions, and the effect each of these instructions will have on the state. Most ISAs, including IA32 and x86-64, describe the behavior of a program as if each instruction is executed in sequence, with one instruction completing before the next one begins. The processor hardware is far more elab-orate, executing many instructions concurrently, but they employ safeguards to ensure that the overall behavior matches the sequential operation dictated by the ISA. Second, the memory addresses used by a machine-level program are vir-tual addresses, providing a memory model that appears to be a very large byte array. The actual implementation of the memory system involves a combination of multiple hardware memories and operating system software, as described in Chapter 9. 
The compiler does most of the work in the overall compilation sequence, transforming programs expressed in the relatively abstract execution model pro-vided by C into the very elementary instructions that the processor executes. The assembly-code representation is very close to machine code. Its main feature is that it is in a more readable textual format, as compared to the binary format of machine code. Being able to understand assembly code and how it relates to the original C code is a key step in understanding how computers execute programs. 

IA32 machine code differs greatly from the original C code. Parts of the processor state are visible that normally are hidden from the C programmer: 
. The program counter (commonly referred to as the ¡°PC,¡± and called %eip in IA32) indicates the address in memory of the next instruction to be executed. 
. The integer register .le contains eight named locations storing 32-bit values. These registers can hold addresses (corresponding to C pointers) or integer data. Some registers are used to keep track of critical parts of the program state, while others are used to hold temporary data, such as the local variables of a procedure, and the value to be returned by a function. 
. The condition code registers hold status information about the most recently executed arithmetic or logical instruction. These are used to implement con-ditional changes in the control or data .ow, such as is required to implement if and while statements. 
. A set of .oating-point registers store .oating-point data. 
Whereas C provides a model in which objects of different data types can be declared and allocated in memory, machine code views the memory as simply a large, byte-addressable array. Aggregate data types in C such as arrays and structures are represented in machine code as contiguous collections of bytes. Even for scalar data types, assembly code makes no distinctions between signed or unsigned integers, between different types of pointers, or even between pointers and integers. 
The program memory contains the executable machine code for the program, some information required by the operating system, a run-time stack for managing procedure calls and returns, and blocks of memory allocated by the user (for example, by using the malloc library function). As mentioned earlier, the program memory is addressed using virtual addresses. At any given time, only limited subranges of virtual addresses are considered valid. For example, although the 32-bit addresses of IA32 potentially span a 4-gigabyte range of address values, a typical program will only have access to a few megabytes. The operating system manages this virtual address space, translating virtual addresses into the physical addresses of values in the actual processor memory. 
A single machine instruction performs only a very elementary operation. For example, it might add two numbers stored in registers, transfer data between memory and a register, or conditionally branch to a new instruction address. The compiler must generate sequences of such instructions to implement program constructs such as arithmetic expression evaluation, loops, or procedure calls and returns. 
Aside The ever-changing forms of generated code 
In our presentation, we will show the code generated by a particular version of gcc with particular settings of the command-line options. If you compile code on your own machine, chances are you will be using a different compiler or a different version of gcc and hence will generate different code. The open-source community supporting gcc keeps changing the code generator, attempting to generate more ef.cient code according to changing code guidelines provided by the microprocessor manufacturers. 
Our goal in studying the examples shown in our presentation is to demonstrate how to examine assembly code and map it back to the constructs found in high-level programming languages. You will need to adapt these techniques to the style of code generated by your particular compiler. 
3.2.2 Code Examples 
Suppose we write a C code .le code.c containing the following procedure de.ni-tion: 
1 int accum = 0; 2 3 int sum(int x, int y) 4 { 5 intt=x+y; 6 accum += t; 7 return t; 
8 } 
To see the assembly code generated by the C compiler, we can use the ¡°-S¡± option on the command line: 
unix> gcc -O1 -S code.c 
This will cause gcc to run the compiler, generating an assembly .le code.s, and go no further. (Normally it would then invoke the assembler to generate an object-code .le.) 
The assembly-code .le contains various declarations including the set of lines: 
sum: pushl %ebp movl %esp, %ebp movl 12(%ebp), %eax addl 8(%ebp), %eax addl %eax, accum popl %ebp ret 
Each indented line in the above code corresponds to a single machine instruction. For example, the pushl instruction indicates that the contents of register %ebp should be pushed onto the program stack. All information about local variable names or data types has been stripped away. We still see a reference to the global variable accum, since the compiler has not yet determined where in memory this variable will be stored. 

If we use the ¡®-c¡¯ command-line option, gcc will both compile and assemble the code: 
unix> gcc -O1 -c code.c 
This will generate an object-code .le code.o that is in binary format and hence cannot be viewed directly. Embedded within the 800 bytes of the .le code.o is a 17-byte sequence having hexadecimal representation 
55 89e58b45 0c034508 01 05000000 00 5dc3 
This is the object-code corresponding to the assembly instructions listed above. A key lesson to learn from this is that the program actually executed by the machine is simply a sequence of bytes encoding a series of instructions. The machine has very little information about the source code from which these instructions were generated. 
Aside How do I .nd the byte representation of a program? 
To generate these bytes, we used a disassembler (to be described shortly) to determine that the code for sum is 17 bytes long. Then we ran the GNU debugging tool gdb on .le code.o and gave it the command 
(gdb) x/17xb sum 
telling it to examine (abbreviated ¡®x¡¯) 17 hex-formatted (also abbreviated ¡®x¡¯) bytes (abbreviated ¡®b¡¯). You will .nd that gdb has many useful features for analyzing machine-level programs, as will be discussed in Section 3.11. 
To inspect the contents of machine-code .les, a class of programs known as disassemblers can be invaluable. These programs generate a format similar to assembly code from the machine code. With Linux systems, the program objdump (for ¡°object dump¡±) can serve this role given the ¡®-d¡¯ command-line .ag: 
unix> objdump -d code.o 
The result is (where we have added line numbers on the left and annotations in italicized text) as follows: 
Disassembly of function sum in binary file code.o 
1  00000000  <sum>:  
Offset  Bytes  Equivalent assembly language  
2  0:  55  push  %ebp  
3  1:  89  e5  mov  %esp,%ebp  
4  3:  8b  45  0c  mov  0xc(%ebp),%eax  
5  6:  03  45  08  add  0x8(%ebp),%eax  
6  9:  01  05  00 00 00 00 add  %eax,0x0  
7  f:  5d  pop  %ebp  

8 10: c3 ret 
On the left, we see the 17 hexadecimal byte values listed in the byte sequence earlier, partitioned into groups of 1 to 6 bytes each. Each of these groups is a single instruction, with the assembly-language equivalent shown on the right. 
Several features about machine code and its disassembled representation are worth noting: 
. IA32 instructions can range in length from 1 to 15 bytes. The instruction encoding is designed so that commonly used instructions and those with fewer operands require a smaller number of bytes than do less common ones or ones with more operands. 
. The instruction format is designed in such a way that from a given starting position, there is a unique decoding of the bytes into machine instructions. For example, only the instruction pushl %ebp can start with byte value 55. 
. The disassembler determines the assembly code based purely on the byte sequences in the machine-code .le. It does not require access to the source or assembly-code versions of the program. 
. The disassembler uses a slightly different naming convention for the instruc-tions than does the assembly code generated by gcc. In our example, it has omitted the suf.x ¡®l¡¯ from many of the instructions. These suf.xes are size designators and can be omitted in most cases. 
Generating the actual executable code requires running a linker on the set 
of object-code .les, one of which must contain a function main. Suppose in .le 
main.c we had the following function: 
1 int main() 2 { 3 return sum(1, 3); 
4 } 
Then, we could generate an executable program prog as follows: 
unix> gcc -O1 -o prog code.o main.c 
The .le prog has grown to 9,123 bytes, since it contains not just the code for our two procedures but also information used to start and terminate the program as well as to interact with the operating system. We can also disassemble the .le prog: 
unix> objdump -d prog 
The disassembler will extract various code sequences, including the following: 
Disassembly of function sum in executable file prog 1 08048394 <sum>: 
Offset Bytes Equivalent assembly language 
2 8048394: 55 push %ebp 3 8048395: 89 e5 mov %esp,%ebp 4 8048397: 8b 45 0c mov 0xc(%ebp),%eax 

5  804839a:  03  45  08  add  0x8(%ebp),%eax  
6  804839d:  01  05  18  a0  04  08  add  %eax,0x804a018  
7  80483a3:  5d  pop  %ebp  
8  80483a4:  c3  ret  

This code is almost identical to that generated by the disassembly of code.c. One important difference is that the addresses listed along the left are different¡ªthe linker has shifted the location of this code to a different range of addresses. A second difference is that the linker has determined the location for storing global variable accum. On line 6 of the disassembly for code.o, the address of accum was listed as 0. In the disassembly of prog, the address has been set to 0x804a018. This is shown in the assembly-code rendition of the instruction. It can also be seen in the last 4 bytes of the instruction, listed from least-signi.cant to most as 18 a0 04 08. 
3.2.3 Notes on Formatting 
The assembly code generated by gcc is dif.cult for a human to read. On one hand, it contains information with which we need not be concerned, while on the other hand, it does not provide any description of the program or how it works. For example, suppose the .le simple.c contains the following code: 
1 int simple(int *xp, int y) 
2 { 

3 intt=*xp+y; 
4 *xp=t; 
5 return t; 
6 } 

When gcc is run with .ags ¡®-S¡¯ and ¡®-O1¡¯, it generates the following .le for simple.s: 
.file "simple.c" 
.text .globl simple 

.type simple, @function simple: 
pushl %ebp 

movl %esp, %ebp 
movl 8(%ebp), %edx 
movl 12(%ebp), %eax 
addl (%edx), %eax 
movl %eax, (%edx) 
popl %ebp 
ret 

.size simple, .-simple 
.ident "GCC: (Ubuntu 4.3.2-1ubuntu11) 4.3.2" 
.section .note.GNU-stack,"",@progbits 
All of the lines beginning with ¡®.¡¯ are directives to guide the assembler and linker. We can generally ignore these. On the other hand, there are no explanatory remarks about what the instructions do or how they relate to the source code. 
To provide a clearer presentation of assembly code, we will show it in a form that omits most of the directives, while including line numbers and explanatory annotations. For our example, an annotated version would appear as follows: 

1  simple:  
2  pushl  %ebp  Save  frame  pointer  
3  movl  %esp,  %ebp  Create  new  frame  pointer  
4  movl  8(%ebp),  %edx  Retrieve  xp  
5  movl  12(%ebp),  %eax  Retrieve  y  
6  addl  (%edx),  %eax  Add  *xp  to  get  t  
7  movl  %eax,  (%edx)  Store  t  at  xp  
8  popl  %ebp  Restore  frame  pointer  
9  ret  Return  

We typically show only the lines of code relevant to the point being discussed. Each line is numbered on the left for reference and annotated on the right by a brief description of the effect of the instruction and how it relates to the computa-tions of the original C code. This is a stylized version of the way assembly-language programmers format their code. 

Aside ATT versus Intel assembly-code formats 
In our presentation, we show assembly code in ATT (named after ¡°AT&T,¡± the company that operated Bell Laboratories for many years) format, the default format for gcc, objdump, and the other tools we will consider. Other programming tools, including those from Microsoft as well as the documentation from Intel, show assembly code in Intel format. The two formats differ in a number of ways. As an example, gcc can generate code in Intel format for the sum function using the following command line: 
unix> gcc -O1 -S -masm=intel code.c 
This gives the following assembly code: 
Assembly code for simple in Intel format 1 simple: 2 push ebp 3 mov ebp, esp 4 mov edx, DWORD PTR [ebp+8] 5 mov eax, DWORD PTR [ebp+12] 6 add eax, DWORD PTR [edx] 7 mov DWORD PTR [edx], eax 8 pop ebp 9 ret 
We see that the Intel and ATT formats differ in the following ways: 
. The Intel code omits the size designation suf.xes. We see instruction mov instead of movl. . The Intel code omits the ¡®%¡¯ character in front of register names, using esp instead of %esp. . The Intel code has a different way of describing locations in memory, for example ¡®DWORD PTR 
[ebp+8]¡¯ rather than ¡®8(%ebp)¡¯. . Instructions with multiple operands list them in the reverse order. This can be very confusing when switching between the two formats. 
Although we will not be using Intel format in our presentation, you will encounter it in IA32 documen-tation from Intel and Windows documentation from Microsoft. 
3.3 
Data 
Formats 

Due to its origins as a 16-bit architecture that expanded into a 32-bit one, Intel uses the term ¡°word¡± to refer to a 16-bit data type. Based on this, they refer to 32-bit quantities as ¡°double words.¡± They refer to 64-bit quantities as ¡°quad words.¡± Most instructions we will encounter operate on bytes or double words. 
Figure 3.1 shows the IA32 representations used for the primitive data types of 
C. Most of the common data types are stored as double words. This includes both regular and long int¡¯s, whether or not they are signed. In addition, all pointers (shown here as char *) are stored as 4-byte double words. Bytes are commonly used when manipulating string data. As we saw in Section 2.1, more recent ex-tensions of the C language include the data type long long, which is represented using 8 bytes. IA32 does not support this data type in hardware. Instead, the com-piler must generate sequences of instructions that operate on these data 32 bits at a time. Floating-point numbers come in three different forms: single-precision (4-byte) values, corresponding to C data type float; double-precision (8-byte) values, corresponding to C data type double; and extended-precision (10-byte) values. gcc uses the data type long double to refer to extended-precision .oating-point values. It also stores them as 12-byte quantities to improve memory system performance, as will be discussed later. Using the long double data type (intro-duced in ISO C99) gives us access to the extended-precision capability of x86. For most other machines, this data type will be represented using the same 8-byte format of the ordinary double data type. 
C declaration  Intel data type  Assembly code suf.x  Size (bytes)  
char  Byte  b  1  
short  Word  w  2  
int  Double word  l  4  
long int  Double word  l  4  
long long int  ¡ª  ¡ª  4  
char *  Double word  l  4  
float  Single precision  s  4  
double  Double precision  l  8  
long double  Extended precision  t  10/12  
Figure 3.1 Sizes of C data types in IA32. IA32 does not provide hardware support for 64-bit integer arithmetic. Compiling code with long long data requires generating sequences of operations to perform the arithmetic in 32-bit chunks. 


As the table indicates, most assembly-code instructions generated by gcc have a single-character suf.x denoting the size of the operand. For example, the data movement instruction has three variants: movb (move byte), movw (move word), and movl (move double word). The suf.x ¡®l¡¯ is used for double words, since 32-bit quantities are considered to be ¡°long words,¡± a holdover from an era when 16-bit word sizes were standard. Note that the assembly code uses the suf.x ¡®l¡¯ to denote both a 4-byte integer as well as an 8-byte double-precision .oating-point number. This causes no ambiguity, since .oating point involves an entirely different set of instructions and registers. 
3.4 
Accessing 
Information 

An IA32 central processing unit (CPU) contains a set of eight registers storing 32-bit values. These registers are used to store integer data as well as pointers. Figure 3.2 diagrams the eight registers. Their names all begin with %e, but other-wise, they have peculiar names. With the original 8086, the registers were 16 bits and each had a speci.c purpose. The names were chosen to re.ect these different purposes. With .at addressing, the need for specialized registers is greatly reduced. For the most part, the .rst six registers can be considered general-purpose regis-
Figure 3.2 

IA32 integer registers. 
All eight registers can be accessed as either 16 bits (word) or 32 bits (double word). The 2 low-order bytes of the .rst four registers can be accessed independently. 


ters with no restrictions placed on their use. We said ¡°for the most part,¡± because some instructions use .xed registers as sources and/or destinations. In addition, within procedures there are different conventions for saving and restoring the .rst three registers (%eax, %ecx, and %edx) than for the next three (%ebx, %edi, and %esi). This will be discussed in Section 3.7. The .nal two registers (%ebp and %esp) contain pointers to important places in the program stack. They should only be altered according to the set of standard conventions for stack management. 
As indicated in Figure 3.2, the low-order 2 bytes of the .rst four registers can be independently read or written by the byte operation instructions. This feature was provided in the 8086 to allow backward compatibility to the 8008 and 8080¡ªtwo 8-bit microprocessors that date back to 1974. When a byte instruction updates one of these single-byte ¡°register elements,¡± the remaining 3 bytes of the register do not change. Similarly, the low-order 16 bits of each register can be read or written by word operation instructions. This feature stems from IA32¡¯s evolutionary heritage as a 16-bit microprocessor and is also used when operating on integers with size designator short. 
3.4.1 Operand Speci.ers 
Most instructions have one or more operands, specifying the source values to reference in performing an operation and the destination location into which to place the result. IA32 supports a number of operand forms (see Figure 3.3). Source values can be given as constants or read from registers or memory. Results can be stored in either registers or memory. Thus, the different operand possibilities can be classi.ed into three types. The .rst type, immediate, is for constant values. In ATT-format assembly code, these are written with a ¡®$¡¯ followed by an integer using standard C notation, for example, $-577 or $0x1F. Any value that .ts into a 32-bit word can be used, although the assembler will use 1-or 2-byte encodings when possible. The second type, register, denotes the contents of one of the registers, either one of the eight 32-bit registers (e.g., %eax) for a double-word operation, one of the eight 16-bit registers (e.g., %ax) for a word operation, or one of the eight single-byte register elements (e.g., %al) for a byte operation. In Figure 3.3, we use the notation E to denote an arbitrary register a, and indicate 
Type  Form  Operand value  Name  
Immediate  $Imm  Imm  Immediate  
Register  Ea  R[Ea]  Register  
Memory Memory Memory Memory Memory Memory Memory Memory Memory  Imm (Ea) Imm(Eb) (Eb,Ei) Imm(Eb,Ei) (,Ei,s) Imm(,Ei,s) (Eb,Ei,s) Imm(Eb,Ei,s)  M[Imm] M[R[Ea]] M[Imm + R[Eb]] M[R[Eb] + R[Ei]] M[Imm + R[Eb] + R[Ei]] M[R[Ei] . s] M[Imm + R[Ei] . s] M[R[Eb] + R[Ei] . s] M[Imm + R[Eb] + R[Ei] . s]  Absolute Indirect Base + displacement Indexed Indexed Scaled indexed Scaled indexed Scaled indexed Scaled indexed  
Figure 3.3 Operand forms. Operands can denote immediate (constant) values, register values, or values from memory. The scaling factor s must be either 1, 2, 4, or 8. 


a 
its value with the reference R[E ], viewing the set of registers as an array R indexed
a 
by register identi.ers. 
The third type of operand is a memory reference, in which we access some memory location according to a computed address, often called the effective ad-dress. Since we view the memory as a large array of bytes, we use the notation Mb[Addr] to denote a reference to the b-byte value stored in memory starting at address Addr. To simplify things, we will generally drop the subscript b. 
As Figure 3.3 shows, there are many different addressing modes allowing dif-ferent forms of memory references. The most general form is shown at the bottom of the table with syntax Imm(Eb,Ei,s). Such a reference has four components: an immediate offset Imm, a base register Eb, an index register Ei, and a scale factor s, where s must be 1, 2, 4, or 8. The effective address is then computed as Imm + R[Eb] + R[Ei] . s. This general form is often seen when referencing el-ements of arrays. The other forms are simply special cases of this general form where some of the components are omitted. As we will see, the more complex addressing modes are useful when referencing array and structure elements. 
Practice Problem 3.1 
Assume the following values are stored at the indicated memory addresses and 
registers:  
Address 0x100 0x104 0x108 0x10C  Value 0xFF 0xAB 0x13 0x11  Register %eax %ecx %edx  Value 0x100 0x1 0x3  

Fill in the following table showing the values for the indicated operands: 
Operand Value 
%eax 
0x104 $0x108 (%eax) 4(%eax) 9(%eax,%edx) 260(%ecx,%edx) 0xFC(,%ecx,4) (%eax,%edx,4) 

Section 3.4 Accessing Information 171 

Instruction  Effect  Description  
mov movb movw  S, D  D ¡û S Move byte Move word  Move  
movl  Move double word  
movs movsbw movsbl movswl  S, D  D ¡û SignExtend(S) Move sign-extended byte to word Move sign-extended byte to double word Move sign-extended word to double word  Move with sign extension  
movz movzbw movzbl movzwl  S, D  D ¡û ZeroExtend(S) Move zero-extended byte to word Move zero-extended byte to double word Move zero-extended word to double word  Move with zero extension  
pushl  S  R[%esp] ¡û R[%esp] . 4; M[R[%esp]] ¡û S  Push double word  
popl  D  D ¡û M[R[%esp]]; R[%esp] ¡û R[%esp] + 4  Pop double word  

Figure 3.4 Data movement instructions. 
3.4.2 Data Movement Instructions 
Among the most heavily used instructions are those that copy data from one location to another. The generality of the operand notation allows a simple data movement instruction to perform what in many machines would require a number of instructions. Figure 3.4 lists the important data movement instructions. As can be seen, we group the many different instructions into instruction classes, where the instructions in a class perform the same operation, but with different operand sizes. For example, the mov class consists of three instructions: movb, movw, and movl. All three of these instructions perform the same operation; they differ only in that they operate on data of size 1, 2, and 4 bytes, respectively. 
The instructions in the mov class copy their source values to their destinations. The source operand designates a value that is immediate, stored in a register, or stored in memory. The destination operand designates a location that is either a register or a memory address. IA32 imposes the restriction that a move instruction cannot have both operands refer to memory locations. Copying a value from one memory location to another requires two instructions¡ªthe .rst to load the source value into a register, and the second to write this register value to the destination. Referring to Figure 3.2, the register operands for these instructions can be any of the eight 32-bit registers (%eax¨C%ebp) for movl, any of the eight 16-bit regis-ters (%ax¨C%bp) for movw, and any of the single-byte register elements (%ah¨C%bh, %al¨C%bl) for movb. The following mov instruction examples show the .ve possible combinations of source and destination types. Recall that the source operand comes .rst and the destination second: 
1  movl  $0x4050,%eax  Immediate--Register,  4  bytes  
2  movw  %bp,%sp  Register--Register,  2  bytes  
3  movb  (%edi,%ecx),%ah  Memory--Register,  1  byte  
4  movb  $-17,(%esp)  Immediate--Memory,  1  byte  
5  movl  %eax,-12(%ebp)  Register--Memory,  4  bytes  

Both the movs and the movz instruction classes serve to copy a smaller amount of source data to a larger data location, .lling in the upper bits by either sign expansion (movs) or by zero expansion (movz). With sign expansion, the upper bits of the destination are .lled in with copies of the most signi.cant bit of the source value. With zero expansion, the upper bits are .lled with zeros. As can be seen, there are three instructions in each of these classes, covering all cases of 1-and 2-byte source sizes and 2-and 4-byte destination sizes (omitting the redundant combinations movsww and movzww, of course). 

Aside Comparing byte movement instructions 
Observe that the three byte-movement instructions movb, movsbl, and movzbl differ from each other in subtle ways. Here is an example: 
Assume initially that %dh = CD, %eax = 98765432 1 movb %dh,%al %eax = 987654CD 2 movsbl %dh,%eax %eax = FFFFFFCD 3 movzbl %dh,%eax %eax = 000000CD 
In these examples, all set the low-order byte of register %eax to the second byte of %edx.The movb instruction does not change the other 3 bytes. The movsbl instruction sets the other 3 bytes to either all ones or all zeros, depending on the high-order bit of the source byte. The movzbl instruction sets the other 3 bytes to all zeros in any case. 
The .nal two data movement operations are used to push data onto and pop data from the program stack. As we will see, the stack plays a vital role in the handling of procedure calls. By way of background, a stack is a data structure where values can be added or deleted, but only according to a ¡°last-in, .rst-out¡± discipline. We add data to a stack via a push operation and remove it via a pop op-eration, with the property that the value popped will always be the value that was most recently pushed and is still on the stack. A stack can be implemented as an array, where we always insert and remove elements from one end of the array. This end is called the top of the stack. With IA32, the program stack is stored in some region of memory. As illustrated in Figure 3.5, the stack grows downward such that the top element of the stack has the lowest address of all stack elements. (By con-vention, we draw stacks upside down, with the stack ¡°top¡± shown at the bottom of the .gure). The stack pointer %esp holds the address of the top stack element. 

Initially pushl %eax popl %edx 
%eax  0x123  
%edx  0  
%esp  0x108  

%eax  0x123  
%edx  0  
%esp  0x104  

%eax  0x123  
%edx  0x123  
%esp  0x108  


Stack ¡°bottom¡± Stack ¡°bottom¡± Stack ¡°bottom¡± 
Increasing address 
0x108 0x108 0x108 
0x104 
Stack ¡°top¡± 


The pushl instruction provides the ability to push data onto the stack, while the popl instruction pops it. Each of these instructions takes a single operand¡ªthe data source for pushing and the data destination for popping. 
Pushing a double-word value onto the stack involves .rst decrementing the stack pointer by 4 and then writing the value at the new top of stack address. Therefore, the behavior of the instruction pushl %ebp is equivalent to that of the pair of instructions 
subl $4,%esp Decrement stack pointer 
movl %ebp,(%esp) Store %ebp on stack 
except that the pushl instruction is encoded in the machine code as a single byte, whereas the pair of instructions shown above requires a total of 6 bytes. The .rst two columns in Figure 3.5 illustrate the effect of executing the instruction pushl %eax when %esp is 0x108 and %eax is 0x123. First %esp is decremented by 4, giving 0x104, and then 0x123 is stored at memory address 0x104. 
Popping a double word involves reading from the top of stack location and then incrementing the stack pointer by 4. Therefore, the instruction popl %eax is equivalent to the following pair of instructions: 
movl (%esp),%eax Read %eax from stack 
addl $4,%esp Increment stack pointer 
The third column of Figure 3.5 illustrates the effect of executing the instruction popl %edx immediately after executing the pushl. Value 0x123 is read from memory and written to register %edx. Register %esp is incremented back to 0x108. As shown in the .gure, the value 0x123 remains at memory location 0x104 until it is overwritten (e.g., by another push operation). However, the stack top is always considered to be the address indicated by %esp. Any value stored beyond the stack top is considered invalid. 





Since the stack is contained in the same memory as the program code and other forms of program data, programs can access arbitrary positions within the stack using the standard memory addressing methods. For example, assuming the topmost element of the stack is a double word, the instruction movl 4(%esp),%edx will copy the second double word from the stack to register %edx. 
Practice Problem 3.2 
For each of the following lines of assembly language, determine the appropriate instruction suf.x based on the operands. (For example, mov can be rewritten as movb, movw,or movl.) 
1  mov  %eax, (%esp)  
2  mov  (%eax), %dx  
3  mov  $0xFF, %bl  
4  mov  (%esp,%edx,4), %dh  
5  push  $0xFF  
6  mov  %dx, (%eax)  
7  pop  %edi  

Practice Problem 3.3 
Each of the following lines of code generates an error message when we invoke the assembler. Explain what is wrong with each line. 
1  movb  $0xF, (%bl)  
2  movl  %ax, (%esp)  
3  movw  (%eax),4(%esp)  
4  movb  %ah,%sh  
5  movl  %eax,$0x123  
6  movl  %eax,%dx  
7  movb  %si, 8(%ebp)  

3.4.3 Data Movement Example 
As an example of code that uses data movement instructions, consider the data exchange routine shown in Figure 3.6, both as C code and as assembly code generated by gcc. We omit the portion of the assembly code that allocates space on the run-time stack on procedure entry and deallocates it prior to return. The details of this set-up and completion code will be covered when we discuss procedure linkage. The code we are left with is called the ¡°body.¡± 

New to C? Some examples of pointers 
Function exchange (Figure 3.6) provides a good illustration of the use of pointers in C. Argument xp is a pointer to an integer, while y is an integer itself. The statement 
int x=*xp; 

indicates that we should read the value stored in the location designated by xp and store it as a local variable named x. This read operation is known as pointer dereferencing. The C operator * performs pointer dereferencing. 
The statement 
*xp=y; 

does the reverse¡ªit writes the value of parameter y at the location designated by xp. This is also a form of pointer dereferencing (and hence the operator *), but it indicates a write operation since it is on the left-hand side of the assignment. 
The following is an example of exchange in action: 
inta=4; 

int b = exchange(&a, 3); 
printf("a = %d, b = %d\n", a, b); 
This code will print 
a=3,b=4 

The C operator & (called the ¡°address of¡± operator) creates a pointer, in this case to the location holding local variable a. Function exchange then overwrote the value stored in a with 3 but returned 4 as the function value. Observe how by passing a pointer to exchange, it could modify data held at some remote location. 
When the body of the procedure starts execution, procedure parameters xp and y are stored at offsets 8 and 12 relative to the address in register %ebp. Instructions 1 and 2 read parameter xp from memory and store it in register %edx. Instruction 2 uses register %edx and reads x into register %eax, a direct implementation of the operation x = *xp in the C program. Later, register %eax will be used to return a value from this function, and so the return value will be 
(a) C code  (b) Assembly code  
1  int  exchange(int  *xp,  int  y)  xp  at  %ebp+8,yat %ebp+12  
2  {  1  movl  8(%ebp),  %edx  Get  xp  
3  int  x  =  *xp;  By copying  to  %eax  below,  x  becomes  the  return  value  
4  2  movl  (%edx),  %eax  Getxatxp  
5  *xp=y;  3  movl  12(%ebp),  %ecx  Get  y  
6  return  x;  4  movl  %ecx,  (%edx)  Store  y  at  xp  
7  }  
Figure 3.6 C and assembly code for exchange routine body. The stack set-up and completion portions have been omitted. 


x. Instruction 3 loads parameter y into register %ecx. Instruction 4 then writes this value to the memory location designated by xp in register %edx, a direct implementation of the operation *xp=y. This example illustrates how the mov instructions can be used to read from memory to a register (instructions 1 to 3), and to write from a register to memory (instruction 4.) 
Two features about this assembly code are worth noting. First, we see that what we call ¡°pointers¡± in C are simply addresses. Dereferencing a pointer involves copying that pointer into a register, and then using this register in a memory reference. Second, local variables such as x are often kept in registers rather than stored in memory locations. Register access is much faster than memory access. 
Practice Problem 3.4 
Assume variables v and p declared with types 
src_t v; dest_t *p; 
where src_t and dest_t are data types declared with typedef. We wish to use the appropriate data movement instruction to implement the operation 
*p = (dest_t) v; 
where v is stored in the appropriately named portion of register %eax (i.e., %eax, %ax,or %al), while pointer p is stored in register %edx. 
For the following combinations of src_t and dest_t, write a line of assembly code that does the appropriate transfer. Recall that when performing a cast that involves both a size change and a change of ¡°signedness¡± in C, the operation should change the signedness .rst (Section 2.2.6). 
src_t  dest_t  Instruction  
int  int  movl %eax, (%edx)  
char  int  
char  unsigned  
unsigned char  int  
int  char  
unsigned  unsigned char  
unsigned  int  

Practice Problem 3.5 
You are given the following information. A function with prototype 
void decode1(int *xp, int *yp, int *zp); 

is compiled into assembly code. The body of the code is as follows: 
xp at %ebp+8, yp at %ebp+12, zp at %ebp+16 1 movl 8(%ebp), %edi 2 movl 12(%ebp), %edx 3 movl 16(%ebp), %ecx 4 movl (%edx), %ebx 5 movl (%ecx), %esi 6 movl (%edi), %eax 7 movl %eax, (%edx) 8 movl %ebx, (%ecx) 9 movl %esi, (%edi) 
Parameters xp, yp, and zp are stored at memory locations with offsets 8, 12, and 16, respectively, relative to the address in register %ebp. Write C code for decode1 that will have an effect equivalent to the assembly code above. 
3.5 
Arithmetic 
and 
Logical 
Operations 

Figure 3.7 lists some of the integer and logic operations. Most of the operations are given as instruction classes, as they can have different variants with different operand sizes. (Only leal has no other size variants.) For example, the instruction class add consists of three addition instructions: addb, addw, and addl, adding bytes, words, and double words, respectively. Indeed, each of the instruction classes shown has instructions for operating on byte, word, and double-word data. The operations are divided into four groups: load effective address, unary, binary, and shifts. Binary operations have two operands, while unary operations have one operand. These operands are speci.ed using the same notation as described in Section 3.4. 
3.5.1 Load Effective Address 
The load effective address instruction leal is actually a variant of the movl instruc-tion. It has the form of an instruction that reads from memory to a register, but it does not reference memory at all. Its .rst operand appears to be a memory refer-ence, but instead of reading from the designated location, the instruction copies the effective address to the destination. We indicate this computation in Figure 3.7 using the C address operator &S. This instruction can be used to generate point-ers for later memory references. In addition, it can be used to compactly describe common arithmetic operations. For example, if register %edx contains value x, then the instruction leal 7(%edx,%edx,4), %eax will set register %eax to 5x + 7. Compilers often .nd clever uses of leal that have nothing to do with effective address computations. The destination operand must be a register. 
Instruction  Effect  Description  
leal  S, D  D ¡û &S  Load effective address  
inc  D  D ¡û D + 1  Increment  
dec  D  D ¡û D -1  Decrement  
neg not  D D  D ¡û -D D ¡û ~D  Negate Complement  
add sub imul xor or and  S, D S, D S, D S, D S, D S, D  D ¡û D + S D ¡û D -S D ¡û D * S D ¡û D ^ S D ¡û D | S D ¡û D & S  Add Subtract Multiply Exclusive-or Or And  
sal shl sar shr  k, D k, D k, D k, D  D ¡û D << k D ¡û D << k D ¡û D >>A k D ¡û D >>L k  Left shift Left shift (same as sal) Arithmetic right shift Logical right shift  
Figure 3.7 Integer arithmetic operations. The load effective address (leal) instruction is commonly used to perform simple arithmetic. The remaining ones are more standard unary or binary operations. We use the notation >>A and >>L to denote arithmetic and logical right shift, respectively. Note the nonintuitive ordering of the operands with ATT-format assembly code. 


Practice Problem 3.6 
Suppose register %eax holds value x and %ecx holds value y. Fill in the table below with formulas indicating the value that will be stored in register %edx for each of the given assembly code instructions: 
Instruction Result 
leal 6(%eax), %edx 
leal (%eax,%ecx), %edx leal (%eax,%ecx,4), %edx leal 7(%eax,%eax,8), %edx leal 0xA(,%ecx,4), %edx leal 9(%eax,%ecx,2), %edx 
3.5.2 Unary and Binary Operations 
Operations in the second group are unary operations, with the single operand serving as both source and destination. This operand can be either a register or a memory location. For example, the instruction incl (%esp) causes the 4-byte element on the top of the stack to be incremented. This syntax is reminiscent of the C increment (++) and decrement (--) operators. 

The third group consists of binary operations, where the second operand is used as both a source and a destination. This syntax is reminiscent of the C assignment operators, such as x+=y. Observe, however, that the source operand is given .rst and the destination second. This looks peculiar for noncommutative operations. For example, the instruction subl %eax,%edx decrements register %edx by the value in %eax. (It helps to read the instruction as ¡°Subtract %eax from %edx.¡±) The .rst operand can be either an immediate value, a register, or a memory location. The second can be either a register or a memory location. As with the movl instruction, however, the two operands cannot both be memory locations. 
Practice Problem 3.7 
Assume the following values are stored at the indicated memory addresses and 
registers:  
Address 0x100 0x104 0x108 0x10C  Value 0xFF 0xAB 0x13 0x11  Register %eax %ecx %edx  Value 0x100 0x1 0x3  

Fill in the following table showing the effects of the following instructions, both in terms of the register or memory location that will be updated and the resulting value: 
Instruction Destination Value 

3.5.3 Shift Operations 
The .nal group consists of shift operations, where the shift amount is given .rst, and the value to shift is given second. Both arithmetic and logical right shifts are possible. The shift amount is encoded as a single byte, since only shift amounts between 0 and 31 are possible (only the low-order 5 bits of the shift amount are considered). The shift amount is given either as an immediate or in the single-byte register element %cl. (These instructions are unusual in only allowing this speci.c register as operand.) As Figure 3.7 indicates, there are two names for the left shift instruction: sal and shl. Both have the same effect, .lling from the right with zeros. The right shift instructions differ in that sar performs an arithmetic shift (.ll with copies of the sign bit), whereas shr performs a logical shift (.ll with zeros). The destination operand of a shift operation can be either a register or a memory location. We denote the two different right shift operations in Figure 3.7 as >>A (arithmetic) and >>L (logical). 
Practice Problem 3.8 
Suppose we want to generate assembly code for the following C function: 
int  shift_left2_rightn(int  x,  int  n)  
{  
x  <<=  2;  
x  >>=  n;  
return  x;  
}  

The code that follows is a portion of the assembly code that performs the actual shifts and leaves the .nal value in register %eax. Two key instructions have been omitted. Parameters x and n are stored at memory locations with offsets 8 and 12, respectively, relative to the address in register %ebp. 
1 movl 8(%ebp), %eax Get x 
2 x <<= 2 
3 movl 12(%ebp), %ecx Get n 
4 x >>= n 
Fill in the missing instructions, following the annotations on the right. The right shift should be performed arithmetically. 
3.5.4 Discussion 
We see that most of the instructions shown in Figure 3.7 can be used for either unsigned or two¡¯s-complement arithmetic. Only right shifting requires instructions that differentiate between signed versus unsigned data. This is one of the features that makes two¡¯s-complement arithmetic the preferred way to implement signed integer arithmetic. 
Figure 3.8 shows an example of a function that performs arithmetic operations and its translation into assembly code. As before, we have omitted the stack set-up and completion portions. Function arguments x, y, and z are stored in memory at offsets 8, 12, and 16 relative to the address in register %ebp, respectively. 
The assembly code instructions occur in a different order than in the C source code. Instructions 2 and 3 compute the expression z*48 by a combination of leal and shift instructions. Line 5 computes the value of x+y. Line 6 computes the and of t1 and 0xFFFF. The .nal multiply is computed by line 7. Since the destination of the multiply is register %eax, this will be the value returned by the function. 

(a) C code (b) Assembly code 1 int arith(int x, xat %ebp+8,yat %ebp+12, z at %ebp+16 2 int y, 1 movl 16(%ebp), %eax z 3 int z) 2 leal (%eax,%eax,2), %eax z*3 4 { 3 sall $4, %eax t2 = z*48 5 int t1 = x+y; 4 movl 12(%ebp), %edx y 6 int t2 = z*48; 5 addl 8(%ebp), %edx t1 = x+y 7 int t3 = t1 & 0xFFFF; 6 andl $65535, %edx t3 = t1&0xFFFF 
8 int t4 = t2 * t3; 7 imull %edx, %eax Return t4 = t2*t3 9 return t4; 
10 } 

Figure 3.8 C and assembly code for arithmetic routine body. The stack set-up and completion portions have been omitted. 
In the assembly code of Figure 3.8, the sequence of values in register %eax corresponds to program values z, 3*z, z*48, and t4 (as the return value). In gen-eral, compilers generate code that uses individual registers for multiple program values and moves program values among the registers. 
Practice Problem 3.9 
In the following variant of the function of Figure 3.8(a), the expressions have been replaced by blanks: 
1  int arith(int  x,  
2  int  y,  
3  int  z)  
4  {  
5  int  t1  =  ;  
6  int  t2  =  ;  
7  int  t3  =  ;  
8  int  t4  =  ;  
9  return  t4;  
10  }  

The portion of the generated assembly code implementing these expressions is as follows: 
xat %ebp+8,yat %ebp+12, z at %ebp+16 1 movl 12(%ebp), %eax 2 xorl 8(%ebp), %eax 3 sarl $3, %eax 4 notl %eax 5 subl 16(%ebp), %eax 
Based on this assembly code, .ll in the missing portions of the C code. 
Practice Problem 3.10 
It is common to .nd assembly code lines of the form 
xorl %edx,%edx 
in code that was generated from C where no Exclusive-Or operations were present. 
A. Explain the effect of this particular Exclusive-Or instruction and what useful operation it implements. 
B. What would be the more straightforward way to express this operation in assembly code? 
C. Compare the number of bytes to encode these two different implementa-tions of the same operation. 
3.5.5 Special Arithmetic Operations 
Figure 3.9 describes instructions that support generating the full 64-bit product of two 32-bit numbers, as well as integer division. 
The imull instruction, a member of the imul instruction class listed in Fig-ure 3.7, is known as a ¡°two-operand¡± multiply instruction. It generates a 32-bit product from two 32-bit operands, implementing the operations *u and *t de-
32 32 
scribed in Sections 2.3.4 and 2.3.5. Recall that when truncating the product to 32 bits, both unsigned multiply and two¡¯s-complement multiply have the same bit-level behavior. IA32 also provides two different ¡°one-operand¡± multiply instruc-tions to compute the full 64-bit product of two 32-bit values¡ªone for unsigned (mull), and one for two¡¯s-complement (imull) multiplication. For both of these, one argument must be in register %eax, and the other is given as the instruction source operand. The product is then stored in registers %edx (high-order 32 bits) and %eax (low-order 32 bits). Although the name imull is used for two distinct multiplication operations, the assembler can tell which one is intended by counting the number of operands. 
Instruction  Effect  Description  
imull  S  R[%edx]:R[%eax] ¡û S ¡Á R[%eax]  Signed full multiply  
mull  S  R[%edx]:R[%eax] ¡û S ¡Á R[%eax]  Unsigned full multiply  
cltd  R[%edx]:R[%eax] ¡û SignExtend(R[%eax])  Convert to quad word  
idivl  S  R[%edx] ¡û R[%edx]:R[%eax] mod S;  Signed divide  
R[%eax] ¡û R[%edx]:R[%eax] ¡Â S  
divl  S  R[%edx] ¡û R[%edx]:R[%eax] mod S;  Unsigned divide  
R[%eax] ¡û R[%edx]:R[%eax] ¡Â S  
Figure 3.9 Special arithmetic operations. These operations provide full 64-bit multi-plication and division, for both signed and unsigned numbers. The pair of registers %edx and %eax are viewed as forming a single 64-bit quad word. 



As an example, suppose we have signed numbers x and y stored at positions 8 and 12 relative to %ebp, and we want to store their full 64-bit product as 8 bytes on top of the stack. The code would proceed as follows: 
xat %ebp+8,yat %ebp+12 1 movl 12(%ebp), %eax Putyin %eax 2 imull 8(%ebp) Multiply by x 3 movl %eax, (%esp) Store low-order 32 bits 4 movl %edx, 4(%esp) Store high-order 32 bits 
Observe that the locations in which we store the two registers are correct for a little-endian machine¡ªthe high-order bits in register %edx are stored at offset 4 relative to the low-order bits in %eax. With the stack growing toward lower addresses, that means that the low-order bits are at the top of the stack. 
Our earlier table of arithmetic operations (Figure 3.7) does not list any divi-sion or modulus operations. These operations are provided by the single-operand divide instructions similar to the single-operand multiply instructions. The signed division instruction idivl takes as dividend the 64-bit quantity in registers %edx (high-order 32 bits) and %eax (low-order 32 bits). The divisor is given as the in-struction operand. The instruction stores the quotient in register %eax and the remainder in register %edx. 
As an example, suppose we have signed numbers x and y stored at positions 8 and 12 relative to %ebp, and we want to store values x/y and x mod y on the stack. gcc generates the following code: 
xat %ebp+8,yat %ebp+12 1 movl 8(%ebp), %edx Putxin %edx 2 movl %edx, %eax Copy x to %eax 3 sarl $31, %edx Sign extend x in %edx 4 idivl 12(%ebp) Divide by y 5 movl %eax, 4(%esp) Storex/y 6 movl %edx, (%esp) Storex%y 
The move instruction on line 1 and the arithmetic shift on line 3 have the combined effect of setting register %edx to either all zeros or all ones depending on the sign of x, while the move instruction on line 2 copies x into %eax. Thus, we have the combined registers %edx and %eax storing a 64-bit, sign-extended version of x. Following the idivl instruction, the quotient and remainder are copied to the top two stack locations (instructions 5 and 6). 
A more conventional method of setting up the divisor makes use of the cltd1 instruction. This instruction sign extends %eax into %edx. With this instruction, the code sequence shown above becomes 
xat %ebp+8,yat %ebp+12 1 movl 8(%ebp),%eax Load x into %eax 2 cltd Sign extend into %edx 3 idivl 12(%ebp) Divide by y 4 movl %eax, 4(%esp) Storex/y 5 movl %edx, (%esp) Storex%y 
We can see that the .rst two instructions have the same overall effect as the .rst three instructions in our earlier code sequence. Different versions of gcc generate these two different ways of setting up the dividend for integer division. 
Unsigned division makes use of the divl instruction. Typically register %edx is set to 0 beforehand. 
Practice Problem 3.11 
Modify the assembly code shown for signed division so that it computes the unsigned quotient and remainder of numbers x and y and stores the results on the stack. 
Practice Problem 3.12 
Consider the following C function prototype, where num_t is a data type declared using typedef: 
void store_prod(num_t *dest, unsigned x, num_t y) { *dest = x*y; 
} 
gcc generates the following assembly code implementing the body of the compu-tation: 
dest at %ebp+8,xat %ebp+12, y at %ebp+16 1 movl 12(%ebp), %eax 2 movl 20(%ebp), %ecx 3 imull %eax, %ecx 4 mull 16(%ebp) 5 leal (%ecx,%edx), %edx 6 movl 8(%ebp), %ecx 7 movl %eax, (%ecx) 8 movl %edx, 4(%ecx) 
1. This instruction is called cdq in the Intel documentation, one of the few cases where the ATT-format name for an instruction bears no relation to the Intel name. 

Observe that this code requires two memory reads to fetch argument y (lines 2 and 4), two multiplies (lines 3 and 4), and two memory writes to store the result (lines 7 and 8). 
A. What data type is num_t? 
B. Describe the algorithm used to compute the product and argue that it is correct. 
3.6 
Control 


So far, we have only considered the behavior of straight-line code, where instruc-tions follow one another in sequence. Some constructs in C, such as conditionals, loops, and switches, require conditional execution, where the sequence of opera-tions that gets performed depends on the outcomes of tests applied to the data. Machine code provides two basic low-level mechanisms for implementing condi-tional behavior: it tests data values and then either alters the control .ow or the data .ow based on the result of these tests. 
Data-dependent control .ow is the more general and more common approach for implementing conditional behavior, and so we will examine this .rst. Normally, both statements in C and instructions in machine code are executed sequentially, in the order they appear in the program. The execution order of a set of machine-code instructions can be altered with a jump instruction, indicating that control should pass to some other part of the program, possibly contingent on the result of some test. The compiler must generate instruction sequences that build upon this low-level mechanism to implement the control constructs of C. 
In our presentation, we .rst cover the machine-level mechanisms and then show how the different control constructs of C are implemented with them. We then return to the use of conditional data transfer to implement data-dependent behavior. 
3.6.1 Condition Codes 
In addition to the integer registers, the CPU maintains a set of single-bit condition code registers describing attributes of the most recent arithmetic or logical opera-tion. These registers can then be tested to perform conditional branches. The most useful condition codes are: 
CF: Carry Flag. The most recent operation generated a carry out of the most signi.cant bit. Used to detect over.ow for unsigned operations. ZF: Zero Flag. The most recent operation yielded zero. SF: Sign Flag. The most recent operation yielded a negative value. OF: Over.ow Flag. The most recent operation caused a two¡¯s-complement over.ow¡ªeither negative or positive. 
186  Chapter 3  Machine-Level Representation of Programs  
Instruction cmp S2, S1 cmpb cmpw cmpl test S2, S1 testb testw testl  Based on S1 -S2 Compare byte Compare word Compare double word S1 & S2 Test byte Test word Test double word  Description Compare Test  
Figure 3.10 Comparison and test instructions. These instructions set the condition codes without updating any other registers. 


For example, suppose we used one of the add instructions to perform the equivalent of the C assignment t=a+b, where variables a, b, and t are integers. Then the condition codes would be set according to the following C expressions: 
CF: (unsigned) t < (unsigned) a Unsigned over.ow ZF: (t == 0) Zero SF: (t<0) Negative OF: (a<0==b<0)&&(t<0!=a<0) Signed over.ow 
The leal instruction does not alter any condition codes, since it is intended to be used in address computations. Otherwise, all of the instructions listed in Figure 3.7 cause the condition codes to be set. For the logical operations, such as Xor, the carry and over.ow .ags are set to 0. For the shift operations, the carry .ag is set to the last bit shifted out, while the over.ow .ag is set to 0. For reasons that we will not delve into, the inc and dec instructions set the over.ow and zero .ags, but they leave the carry .ag unchanged. 
In addition to the setting of condition codes by the instructions of Figure 3.7, there are two instruction classes (having 8, 16, and 32-bit forms) that set condition codes without altering any other registers; these are listed in Figure 3.10. The cmp instructions set the condition codes according to the differences of their two operands. They behave in the same way as the sub instructions, except that they set the condition codes without updating their destinations. With ATT format, the operands are listed in reverse order, making the code dif.cult to read. These instructions set the zero .ag if the two operands are equal. The other .ags can be used to determine ordering relations between the two operands. The test instructions behave in the same manner as the and instructions, except that they set the condition codes without altering their destinations. Typically, the same operand is repeated (e.g., testl %eax,%eax to see whether %eax is negative, zero, or positive), or one of the operands is a mask indicating which bits should be tested. 

Section 3.6 Control 187 

Instruction  Synonym  Effect  Set condition  
sete setne  D D  setz setnz  D ¡û ZF D ¡û ~ZF  Equal / zero Not equal / not zero  
sets setns  D D  D ¡û SF D ¡û ~SF  Negative Nonnegative  
setg setge setl setle  D D D D  setnle setnl setnge setng  D ¡û ~(SF ^ OF) & ~ZF D ¡û ~(SF ^ OF) D ¡û SF ^ OF D ¡û (SF ^ OF) |ZF  Greater (signed >) Greater or equal (signed >=) Less (signed <) Less or equal (signed <=)  
seta setae setb setbe  D D D D  setnbe setnb setnae setna  D ¡û ~CF & ~ZF D ¡û ~CF D ¡û CF D ¡û CF | ZF  Above (unsigned >) Above or equal (unsigned >=) Below (unsigned <) Below or equal (unsigned <=)  

Figure 3.11 The set instructions. Each instruction sets a single byte to 0 or 1 based on some combination of the condition codes. Some instructions have ¡°synonyms,¡± i.e., alternate names for the same machine instruction. 
3.6.2 Accessing the Condition Codes 
Rather than reading the condition codes directly, there are three common ways of using the condition codes: (1) we can set a single byte to 0 or 1 depending on some combination of the condition codes, (2) we can conditionally jump to some other part of the program, or (3) we can conditionally transfer data. For the .rst case, the instructions described in Figure 3.11 set a single byte to 0 or to 1 depending on some combination of the condition codes. We refer to this entire class of instructions as the set instructions; they differ from one another based on which combinations of condition codes they consider, as indicated by the different suf.xes for the instruction names. It is important to recognize that the suf.xes for these instructions denote different conditions and not different operand sizes. For example, instructions setl and setb denote ¡°set less¡± and ¡°set below,¡± not ¡°set long word¡± or ¡°set byte.¡± 
A set instruction has either one of the eight single-byte register elements (Figure 3.2) or a single-byte memory location as its destination, setting this byte to either 0 or 1. To generate a 32-bit result, we must also clear the high-order 24 bits. A typical instruction sequence to compute the C expression a<b, where a and b are both of type int, proceeds as follows: 
aisin %edx,bisin %eax  
1  cmpl  %eax,  %edx  Compare  a:b  
2  setl  %al  Set  low  order  byte of %eax to 0 or 1  
3  movzbl  %al,  %eax  Set  remaining bytes  of  %eax  to  0  

The movzbl instruction clears the high-order 3 bytes of %eax. 
For some of the underlying machine instructions, there are multiple possible names, which we list as ¡°synonyms.¡± For example, both setg (for ¡°set greater¡±) and setnle (for ¡°set not less or equal¡±) refer to the same machine instruction. Compilers and disassemblers make arbitrary choices of which names to use. 
Although all arithmetic and logical operations set the condition codes, the de-scriptions of the different set instructions apply to the case where a comparison instruction has been executed, setting the condition codes according to the com-putation t = a-b. More speci.cally, let a, b, and t be the integers represented in two¡¯s-complement form by variables a, b, and t, respectively, and so t= a-t b,
w 
where wdepends on the sizes associated with a and b. 
Consider the sete, or ¡°set when equal¡± instruction. When a= b, we will have t= 0, and hence the zero .ag indicates equality. Similarly, consider testing for signed comparison with the setl, or ¡°set when less,¡± instruction. When no over.ow occurs (indicated by having OF set to 0), we will have a<bwhen a-t b< 
w 
0, indicated by having SF set to 1, and a¡Ý bwhen a-t b¡Ý 0, indicated by having 
w 
SF set to 0. On the other hand, when over.ow occurs, we will have a<bwhen a-t b>0 (positive over.ow) and a>bwhen a-t b<0 (negative over.ow). We 
ww 
cannot have over.ow when a= b. Thus, when OF is set to 1, we will have a<bif and only if SF is set to 0. Combining these cases, the Exclusive-Or of the over.ow and sign bits provides a test for whether a<b. The other signed comparison tests are based on other combinations of SF ^ OF and ZF. 
For the testing of unsigned comparisons, we now let aand bbe the integers represented in unsigned form by variables a and b. In performing the computation t = a-b, the carry .ag will be set by the cmp instruction when the a. b<0, and so the unsigned comparisons use combinations of the carry and zero .ags. 
It is important to note how machine code distinguishes between signed and unsigned values. Unlike in C, it does not associate a data type with each program value. Instead, it mostly uses the same instructions for the two cases, because many arithmetic operations have the same bit-level behavior for unsigned and two¡¯s-complement arithmetic. Some circumstances require different instructions to handle signed and unsigned operations, such as using different versions of right shifts, division and multiplication instructions, and different combinations of condition codes. 
Practice Problem 3.13 
The following C code 
int comp(data_t a, data_t b) { return a COMP b; } 
shows a general comparison between arguments a and b, where we can set the data type of the arguments by declaring data_t with a typedef declaration, and we can set the comparison by de.ning COMP with a #define declaration. 
Suppose a is in %edx and b is in %eax. For each of the following instruction sequences, determine which data types data_t and which comparisons COMP could cause the compiler to generate this code. (There can be multiple correct answers; you should list them all.) 

A. cmpl %eax, %edx setl %al 
B. cmpw %ax, %dx setge %al 
C. cmpb %al, %dl setb %al 
D. cmpl %eax, %edx setne %al 
Practice Problem 3.14 
The following C code 
int test(data_t a) { 
return a TEST 0; } 
shows a general comparison between argument a and 0, where we can set the data type of the argument by declaring data_t with a typedef, and the nature of the comparison by declaring TEST with a #define declaration. For each of the following instruction sequences, determine which data types data_t and which comparisons TEST could cause the compiler to generate this code. (There can be multiple correct answers; list all correct ones.) 
A. testl %eax, %eax setne %al 
B. testw %ax, %ax sete %al 
C. testb %al, %al setg %al 
D. testw %ax, %ax seta %al 
3.6.3 Jump Instructions and Their Encodings 
Under normal execution, instructions follow each other in the order they are listed. A jump instruction can cause the execution to switch to a completely new position in the program. These jump destinations are generally indicated in assembly code by a label. Consider the following (very contrived) assembly-code sequence: 
190  Chapter 3  Machine-Level Representation of Programs  
Instruction jmp Label jmp *Operand je Label jne Label js Label jns Label jg Label jge Label jl Label jle Label ja Label jae Label jb Label jbe Label  Synonym jz jnz jnle jnl jnge jng jnbe jnb jnae jna  Jump condition 1 1 ZF ~ZF SF ~SF ~(SF ^ OF) & ~ZF ~(SF ^ OF) SF ^ OF (SF ^ OF) |ZF ~CF & ~ZF ~CF CF CF | ZF  Description Direct jump Indirect jump Equal / zero Not equal / not zero Negative Nonnegative Greater (signed >) Greater or equal (signed >=) Less (signed <) Less or equal (signed <=) Above (unsigned >) Above or equal (unsigned >=) Below (unsigned <) Below or equal (unsigned <=)  
Figure 3.12 The jump instructions. These instructions jump to a labeled destination when the jump condition holds. Some instructions have ¡°synonyms,¡± alternate names for the same machine instruction. 


1  movl  $0,%eax  Set  %eax  to  0  
2  jmp  .L1  Goto  .L1  
3  movl  (%eax),%edx  Null  pointer dereference  
4  .L1:  
5  popl  %edx  

The instruction jmp .L1 will cause the program to skip over the movl instruc-tion and instead resume execution with the popl instruction. In generating the object-code .le, the assembler determines the addresses of all labeled instruc-tions and encodes the jump targets (the addresses of the destination instructions) as part of the jump instructions. 
Figure 3.12 shows the different jump instructions. The jmp instruction jumps unconditionally. It can be either a direct jump, where the jump target is encoded as part of the instruction, or an indirect jump, where the jump target is read from a register or a memory location. Direct jumps are written in assembly by giving a label as the jump target, e.g., the label ¡°.L1¡± in the code shown. Indirect jumps are written using ¡®*¡¯ followed by an operand speci.er using one of the formats described in Section 3.4.1. As examples, the instruction jmp *%eax uses the value in register %eax as the jump target, and the instruction jmp *(%eax) 

reads the jump target from memory, using the value in %eax as the read address. 
The remaining jump instructions in the table are conditional¡ªthey either jump or continue executing at the next instruction in the code sequence, depending on some combination of the condition codes. The names of these instructions and the conditions under which they jump match those of the set instructions (see Figure 3.11). As with the set instructions, some of the underlying machine instructions have multiple names. Conditional jumps can only be direct. 
Although we will not concern ourselves with the detailed format of machine code, understanding how the targets of jump instructions are encoded will become important when we study linking in Chapter 7. In addition, it helps when inter-preting the output of a disassembler. In assembly code, jump targets are written using symbolic labels. The assembler, and later the linker, generate the proper encodings of the jump targets. There are several different encodings for jumps, but some of the most commonly used ones are PC relative. That is, they encode the difference between the address of the target instruction and the address of the instruction immediately following the jump. These offsets can be encoded using 1, 2, or 4 bytes. A second encoding method is to give an ¡°absolute¡± address, using 4 bytes to directly specify the target. The assembler and linker select the appropriate encodings of the jump destinations. 
As an example of PC-relative addressing, the following fragment of assembly code was generated by compiling a .le silly.c. It contains two jumps: the jle instruction on line 1 jumps forward to a higher address, while the jg instruction on line 8 jumps back to a lower one. 
1 jle .L2 if <=, goto dest2 
2 .L5: dest1: 
3 movl %edx, %eax 
4 sarl %eax 
5 subl %eax, %edx 
6 leal (%edx,%edx,2), %edx 
7 testl %edx, %edx 
8 jg .L5 if >, goto dest1 
9 .L2: dest2: 10 movl %edx, %eax 
The disassembled version of the ¡°.o¡± format generated by the assembler is as follows: 
1 
8: 7e 0d jle 17 <silly+0x17> Target = dest2 2 a: 89 d0 mov %edx,%eax dest1: 3 c: d1 f8 sar %eax 4 e: 29 c2 sub %eax,%edx 5 10: 8d 14 52 lea (%edx,%edx,2),%edx 6 13: 85 d2 test %edx,%edx 

7 
15: 7f f3 jg a <silly+0xa> Target = dest1 8 17: 89 d0 mov %edx,%eax dest2: 


In the annotations generated by the disassembler on the right, the jump targets are indicated as 0x17 for the jump instruction on line 1 and 0xa for the jump instruction on line 7. Looking at the byte encodings of the instructions, however, we see that the target of the .rst jump instruction is encoded (in the second byte) as 0xd (decimal 13). Adding this to 0xa (decimal 10), the address of the following instruction, we get jump target address 0x17 (decimal 23), the address of the instruction on line 8. 
Similarly, the target of the second jump instruction is encoded as 0xf3 (dec-imal .13) using a single-byte, two¡¯s-complement representation. Adding this to 0x17 (decimal 23), the address of the instruction on line 8, we get 0xa (decimal 10), the address of the instruction on line 2. 
As these examples illustrate, the value of the program counter when perform-ing PC-relative addressing is the address of the instruction following the jump, not that of the jump itself. This convention dates back to early implementations, when the processor would update the program counter as its .rst step in executing an instruction. 
The following shows the disassembled version of the program after linking: 
1 804839c: 7e 0d jle 80483ab <silly+0x17> 2 804839e: 89 d0 mov %edx,%eax 3 80483a0: d1 f8 sar %eax 4 80483a2: 29 c2 sub %eax,%edx 5 80483a4: 8d 14 52 lea (%edx,%edx,2),%edx 6 80483a7: 85 d2 test %edx,%edx 7 80483a9: 7f f3 jg 804839e <silly+0xa> 8 80483ab: 89 d0 mov %edx,%eax 
The instructions have been relocated to different addresses, but the encodings of the jump targets in lines 1 and 7 remain unchanged. By using a PC-relative encoding of the jump targets, the instructions can be compactly encoded (requiring just 2 bytes), and the object code can be shifted to different positions in memory without alteration. 
Practice Problem 3.15 
In the following excerpts from a disassembled binary, some of the information has been replaced by Xs. Answer the following questions about these instructions. 
A. What is the target of the je instruction below? (You don¡¯t need to know 
anything about the call instruction here.)  
804828f:  74 05  je  XXXXXXX  
8048291:  e8 1e 00 00 00  call  80482b4  
B. What is the target of the jb instruction below?  
8048357:  72 e7  jb  XXXXXXX  

8048359: c6 05 10 a0 04 08 01 movb $0x1,0x804a010 

C. What is the address of the mov instruction? 
XXXXXXX: 74 12 je 8048391 
XXXXXXX: b8 00 00 00 00 mov $0x0,%eax 
D. In the code that follows, the jump target is encoded in PC-relative form as a 4-byte, two¡¯s-complement number. The bytes are listed from least signi.cant to most, re.ecting the little-endian byte ordering of IA32. What is the address of the jump target? 
80482bf: e9 e0 ff ff ff jmp XXXXXXX 80482c4: 90 nop 
E. Explain the relation between the annotation on the right and the byte coding on the left. 
80482aa: ff 25 fc 9f 04 08 jmp *0x8049ffc 
To implement the control constructs of C via conditional control transfer, the compiler must use the different types of jump instructions we have just seen. We will go through the most common constructs, starting from simple conditional branches, and then consider loops and switch statements. 
3.6.4 Translating Conditional Branches 
The most general way to translate conditional expressions and statements from C into machine code is to use combinations of conditional and unconditional jumps. (As an alternative, we will see in Section 3.6.6 that some conditionals can be implemented by conditional transfers of data rather than control.) For example, Figure 3.13(a) shows the C code for a function that computes the absolute value of the difference of two numbers.2 gcc generates the assembly code shown as Figure 3.13(c). We have created a version in C, called gotodiff (Figure 3.13(b)), that more closely follows the control .ow of this assembly code. It uses the goto statement in C, which is similar to the unconditional jump of assembly code. The statement goto x_ge_y on line 4 causes a jump to the label x_ge_y (since it occurs when x ¡Ý y) on line 7, skipping the computation of y-x on line 5. If the test fails, the program computes the result as y-x and then transfers unconditionally to the end of the code. Using goto statements is generally considered a bad programming style, since their use can make code very dif.cult to read and debug. We use them in our presentation as a way to construct C programs that describe the control .ow of assembly-code programs. We call this style of programming ¡°goto code.¡± 
The assembly-code implementation .rst compares the two operands (line 3), setting the condition codes. If the comparison result indicates that x is greater 
2. Actually, it can return a negative value if one of the subtractions over.ows. Our interest here is to demonstrate machine code, not to implement robust code. 
(a) Original C code (b) Equivalent goto version 1 int absdiff(int x, int y) { 1 int gotodiff(int x, int y) { 2 if(x <y) 2 int result; 3 returny-x; 3 if(x>= y) 
4 else 4 goto x_ge_y; 5 returnx-y; 5 result= y-x; 
6 } 6 goto done; 7 x_ge_y: 8 result=x-y; 9 done: 
10 return result; 
11 } 
(c) Generated assembly code 
xat %ebp+8,yat %ebp+12 1 movl 8(%ebp), %edx Get x 2 movl 12(%ebp), %eax Get y 3 cmpl %eax, %edx Compare x:y 4 jge .L2 if >= goto x_ge_y 5 subl %edx, %eax Compute result = y-x 6 jmp .L3 Goto done 7 .L2: x_ge_y: 8 subl %eax, %edx Compute result = x-y 9 movl %edx, %eax Set result as return value 
10 .L3: done: Begin completion code Figure 3.13 Compilation of conditional statements. C procedure absdiff (part (a)) contains an if-else statement. The generated assembly code is shown (part (c)), along with a C procedure gotodiff (part (b)) that mimics the control .ow of the assembly code. The stack set-up and completion portions of the assembly code have been omitted. 
than or equal to y, it then jumps to a block of code that computes x-y (line 8). Otherwise, it continues with the execution of code that computes y-x (line 5). In both cases, the computed result is stored in register %eax, and the program reaches line 10, at which point it executes the stack completion code (not shown). 
The general form of an if-else statement in C is given by the template 
if (test-expr) then-statement 
else 
else-statement 
where test-expr is an integer expression that evaluates either to 0 (interpreted as meaning ¡°false¡±) or to a nonzero value (interpreted as meaning ¡°true¡±). Only one of the two branch statements (then-statement or else-statement) is executed. 

For this general form, the assembly implementation typically adheres to the following form, where we use C syntax to describe the control .ow: 
t= test-expr; if (!t) goto false; 
then-statement 
goto done; false: 

else-statement 
done: 

That is, the compiler generates separate blocks of code for then-statement and else-statement. It inserts conditional and unconditional branches to make sure the correct block is executed. 
Practice Problem 3.16 
When given the C code 
1 void cond(int a, int *p) 2 { 3 if (p &&a>0) 4 *p+= a; 
5 } 

gcc generates the following assembly code for the body of the function: 
a %ebp+8,pat %ebp+12 1 movl 8(%ebp), %edx 2 movl 12(%ebp), %eax 3 testl %eax, %eax 4 je .L3 5 testl %edx, %edx 6 jle .L3 7 addl %edx, (%eax) 8 .L3: 
A. Write a goto version in C that performs the same computation and mimics the control .ow of the assembly code, in the style shown in Figure 3.13(b). You might .nd it helpful to .rst annotate the assembly code as we have done in our examples. 
B. Explain why the assembly code contains two conditional branches, even though the C code has only one if statement. 
Practice Problem 3.17 
An alternate rule for translating if statements into goto code is as follows: 
t= test-expr; if (t) goto true; 
else-statement 
goto done; true: 
then-statement 
done: 
A. Rewrite the goto version of absdiff based on this alternate rule. 
B. Can you think of any reasons for choosing one rule over the other? 
Practice Problem 3.18 
Starting with C code of the form 
1  int  test(int  x,  int  y)  {  
2  int  val  =  ;  
3  if  (  ){  
4  if  (  )  
5  val  =  ;  
6  else  
7  val  =  ;  
8  }  else  if  (  )  
9  val  =  ;  
10  return  val;  
11  }  

gcc generates the following assembly code: 
xat%ebp+8,yat%ebp+12 1 movl 8(%ebp), %eax 2 movl 12(%ebp), %edx 3 cmpl $-3, %eax 4 jge .L2 5 cmpl %edx, %eax 6 jle .L3 7 imull %edx, %eax 8 jmp .L4 9 .L3: 
10 leal (%edx,%eax), %eax 11 jmp .L4 12 .L2: 

13  cmpl  $2, %eax  
14  jg  .L5  
15  xorl  %edx,  %eax  
16  jmp  .L4  
17  .L5:  
18  subl  %edx,  %eax  
19  .L4:  

Fill in the missing expressions in the C code. To make the code .t into the C code template, you will need to undo some of the reordering of computations done by gcc. 
3.6.5 Loops 

C provides several looping constructs¡ªnamely, do-while, while, and for.No corresponding instructions exist in machine code. Instead, combinations of condi-tional tests and jumps are used to implement the effect of loops. Most compilers generate loop code based on the do-while form of a loop, even though this form is relatively uncommon in actual programs. Other loops are transformed into do-while form and then compiled into machine code. We will study the translation of loops as a progression, starting with do-while and then working toward ones with more complex implementations. 
Do-While Loops 

The general form of a do-while statement is as follows: 
do 

body-statement while (test-expr); 
The effect of the loop is to repeatedly execute body-statement, evaluate test-expr, and continue the loop if the evaluation result is nonzero. Observe that body-statement is executed at least once. 
This general form can be translated into conditionals and goto statements as follows: 
loop: 
body-statement 
t= test-expr; 
if (t) 
goto loop; 

That is, on each iteration the program evaluates the body statement and then the test expression. If the test succeeds, we go back for another iteration. 
As an example, Figure 3.14(a) shows an implementation of a routine to com-pute the factorial of its argument, written n!, with a do-while loop. This function only computes the proper value for n>0. 
(a) C code (c) Corresponding assembly-language code 1 int fact_do(int n) Argument: n at %ebp+8 2 { Registers: n in %edx, result in %eax 3 int result = 1; 1 movl 8(%ebp), %edx Get n 4 do { 2 movl $1, %eax Set result = 1 
5 result *= n; 3 .L2: loop: 6 n = n-1; 4 imull %edx, %eax Compute result *= n 
7 } while (n > 1); 5 subl $1, %edx Decrement n 8 return result; 6 cmpl $1, %edx Compare n:1 
9 } 7 jg .L2 If >, goto loop Return result 
(b) Register usage 

Register Variable Initially %eax result 1 %edx n n 
Figure 3.14 Code for do-while version of factorial program. The C code, the generated assembly code, and a table of register usage is shown. 
Practice Problem 3.19 
A. What is the maximum value of nfor which we can represent n! with a 32-bit int? 
B. What about for a 64-bit long long int? 
The assembly code shown in Figure 3.14(c) shows a standard implementation of a do-while loop. Following the initialization of register %edx to hold nand %eax to hold result, the program begins looping. It .rst executes the body of the loop, consisting here of the updates to variables result and n(lines 4¨C5). It then tests whether n>1, and, if so, it jumps back to the beginning of the loop. We see here that the conditional jump (line 7) is the key instruction in implementing a loop. It determines whether to continue iterating or to exit the loop. 
Determining which registers are used for which program values can be chal-lenging, especially with loop code. We have shown such a mapping in Figure 3.14. In this case, the mapping is fairly simple to determine: we can see ngetting loaded into register %edx on line 1, getting decremented on line 5, and being tested on line 6. We therefore conclude that this register holds n. 
We can see register %eax getting initialized to 1 (line 2), and being updated by multiplication on line 4. Furthermore, since %eax is used to return the function value, it is often chosen to hold program values that are returned. We therefore conclude that %eax corresponds to program value result. 

Aside Reverse engineering loops 
A key to understanding how the generated assembly code relates to the original source code is to .nd a mapping between program values and registers. This task was simple enough for the loop of Figure 3.14, but it can be much more challenging for more complex programs. The C compiler will often rearrange the computations, so that some variables in the C code have no counterpart in the machine code, and new values are introduced into the machine code that do not exist in the source code. Moreover, it will often try to minimize register usage by mapping multiple program values onto a single register. 
The process we described for fact_do works as a general strategy for reverse engineering loops. Look at how registers are initialized before the loop, updated and tested within the loop, and used after the loop. Each of these provides a clue that can be combined to solve a puzzle. Be prepared for surprising transformations, some of which are clearly cases where the compiler was able to optimize the code, and others where it is hard to explain why the compiler chose that particular strategy. In our experience, gcc often makes transformations that provide no performance bene.t and can even decrease code performance. 
Practice Problem 3.20 
For the C code 

1  int dw_loop(int  x,  int y, int  n) {  
2  do  {  
3  x+=n;  
4  y*=n;  
5  n--;  
6  }  while  ((n  >  0)  &&  (y  <  n));  
7  return  x;  
8  }  

gcc generates the following assembly code: 
xat %ebp+8,yat %ebp+12, n at %ebp+16 
1 movl 8(%ebp), %eax 
2 movl 12(%ebp), %ecx 
3 movl 16(%ebp), %edx 
4 .L2: 

5 addl %edx, %eax 
6 imull %edx, %ecx 
7 subl $1, %edx 
8 testl %edx, %edx 
9 jle .L5 
10 cmpl %edx, %ecx 
11 jl .L2 
12 .L5: 

A. Make a table of register usage, similar to the one shown in Figure 3.14(b). 
B. Identify test-expr and body-statement in the C code, and the corresponding lines in the assembly code. 
C. Add annotations to the assembly code describing the operation of the pro-gram, similar to those shown in Figure 3.14(b). 
While Loops 
The general form of a while statement is as follows: 
while (test-expr) body-statement 
It differs from do-while in that test-expr is evaluated and the loop is potentially terminated before the .rst execution of body-statement. There are a number of ways to translate a while loop into machine code. One common approach, also used by gcc, is to transform the code into a do-while loop by using a conditional branch to skip the .rst execution of the body if needed: 
if (!test-expr) goto done; do 
body-statement while (test-expr); done: 
This, in turn, can be transformed into goto code as 
t= test-expr; if (!t) goto done; loop: 
body-statement t= test-expr; if (t) 
goto loop; done: 
Using this implementation strategy, the compiler can often optimize the initial test, for example determining that the test condition will always hold. 
As an example, Figure 3.15 shows an implementation of the factorial func-tion using a while loop (Figure 3.15(a)). This function correctly computes 0! = 1. The adjacent function fact_while_goto (Figure 3.15(b)) is a C rendition of the assembly code generated by gcc. Comparing the code generated for fact_while (Figure 3.15) to that for fact_do (Figure 3.14), we see that they are nearly iden-tical. The only difference is the initial test (line 3) and the jump around the loop (line 4). The compiler closely followed our template for converting a while loop to a do-while loop, and for translating this loop to goto code. 

(a) C code (b) Equivalent goto version 1 int fact_while(int n) 1 int fact_while_goto(int n) 2 { 2 { 3 int result = 1; 3 int result = 1; 4 while (n>1) { 4 if(n<= 1) 
5 result *= n; 5 goto done; 6 n = n-1; 6 loop: 
7 
} 7 result *= n; 8 return result; 8 n = n-1; 

9 
} 9 if (n > 1) 10 goto loop; 11 done: 12 return result; 


13 } 

(c) Corresponding assembly-language code 
Argument: n at %ebp+8 
Registers: n in %edx, result in %eax 1 movl 8(%ebp), %edx Get n 2 movl $1, %eax Set result = 1 3 cmpl $1, %edx Compare n:1 4 jle .L7 If <=, goto done 5 .L10: loop: 6 imull %edx, %eax Compute result *= n 7 subl $1, %edx Decrement n 8 cmpl $1, %edx Compare n:1 9 jg .L10 If >, goto loop 
10 .L7: done: 
Return result 
Figure 3.15 C and assembly code for while version of factorial. The fact_while_ goto function illustrates the operation of the assembly code version. 
Practice Problem 3.21 
For the C code 

1  int  loop_while(int  a,  int  b)  
2  {  
3  int  result  =  1;  
4  while  (a  <  b)  {  
5  result  *=  (a+b);  
6  a++;  
7  }  
8  return  result;  
9  }  

gcc generates the following assembly code: 
aat%ebp+8,bat%ebp+12 1 movl 8(%ebp), %ecx 2 movl 12(%ebp), %ebx 3 movl $1, %eax 4 cmpl %ebx, %ecx 5 jge .L11 6 leal (%ebx,%ecx), %edx 7 movl $1, %eax 8 .L12: 9 imull %edx, %eax 
10 addl $1, %ecx 11 addl $1, %edx 12 cmpl %ecx, %ebx 13 jg .L12 14 .L11: 
In generating this code, gcc makes an interesting transformation that, in effect, introduces a new program variable. 
A. Register %edx is initialized on line 6 and updated within the loop on line 11. Consider this to be a new program variable. Describe how it relates to the variables in the C code. 
B. Create a table of register usage for this function. 
C. Annotate the assembly code to describe how it operates. 
D. Write a goto version of the function (in C) that mimics how the assembly code program operates. 
Practice Problem 3.22 
A function, fun_a, has the following overall structure: 
int  fun_a(unsigned  x)  {  
int  val  =  0;  
while  (  ){  
;  
}  
return  ;  
}  

The gcc C compiler generates the following assembly code: 
xat%ebp+8 1 movl 8(%ebp), %edx 2 movl $0, %eax 3 testl %edx, %edx 

4  je  .L7  
5  .L10:  
6  xorl  %edx,  %eax  
7  shrl  %edx  Shift right by 1  
8  jne  .L10  
9  .L7:  
10  andl  $1,  %eax  

Reverse engineer the operation of this code and then do the following: 
A. Use the assembly-code version to .ll in the missing parts of the C code. 
B. Describe in English what this function computes. 
For Loops 

The general form of a for loop is as follows: 
for (init-expr; test-expr; update-expr) body-statement 
The C language standard states (with one exception, highlighted in Problem 3.24) that the behavior of such a loop is identical to the following code, which uses a while loop: 
init-expr; 

while (test-expr){ body-statement update-expr; 
} 

The program .rst evaluates the initialization expression init-expr. It enters a loop where it .rst evaluates the test condition test-expr, exiting if the test fails, then executes the body of the loop body-statement, and .nally evaluates the update expression update-expr. 
The compiled form of this code is based on the transformation from while to do-while described previously, .rst giving a do-while form: 
init-expr; if (!test-expr) goto done; do { 

body-statement 
update-expr; } while (test-expr); done: 
This, in turn, can be transformed into goto code as 
init-expr; t= test-expr; if (!t) 
goto done; loop: 
body-statement update-expr; t= test-expr; if (t) 
goto loop; done: 
As an example, consider a factorial function written with a for loop: 
1 int fact_for(int n) 2 { 3 int i; 4 int result = 1; 5 for (i= 2; i<= n; i++) 6 result *= i; 7 return result; 
8 } 
As shown, the natural way of writing a factorial function with a for loop is to multiply factors from 2 up to n, and so this function is quite different from the code we showed using either a while or a do-while loop. 
We can identify the different components of the for loop in this code as follows: 
init-expr i=2 test-expr i<=n update-expr i++ body-statement result *= i; 
Substituting these components into the template we have shown yields the following version in goto code: 
1 int fact_for_goto(int n) 2 { 3 inti=2; 4 int result = 1; 5 if (!(i <= n)) 6 goto done; 7 loop: 8 result *= i; 9 i++; 

10 if (i <=n) 11 goto loop; 12 done: 13 return result; 
14 } 

Indeed, a close examination of the assembly code produced by gcc closely follows this template: 
Argument:  n  at  %ebp+8  
Registers:  n  in  %ecx,iin  %edx,  result  in  %eax  
1  movl  8(%ebp),  %ecx  Get  n  
2  movl  $2,  %edx  Set  i  to  2  (init)  
3  movl  $1,  %eax  Set  result  to  1  
4  cmpl  $1,  %ecx  Compare  n:1  (!test)  
5  jle  .L14  If  <=,  goto  done  
6  .L17:  loop:  
7  imull  %edx,  %eax  Compute  result  *=  i  (body)  
8  addl  $1,  %edx  Increment  i  (update)  
9  cmpl  %edx,  %ecx  Compare  n:i  (test)  
10  jge  .L17  If  >=,  goto  loop  
11  .L14:  done:  

We see from this presentation that all three forms of loops in C¡ªdo-while, while, and for¡ªcan be translated by a single strategy, generating code that con-tains one or more conditional branches. Conditional transfer of control provides the basic mechanism for translating loops into machine code. 
Practice Problem 3.23 
A function fun_b has the following overall structure: 
int  fun_b(unsigned int val = 0; int i; for (  x) ;  {  ;  ){  
}  } return  val;  

The gcc C compiler generates the following assembly code: 
xat %ebp+8 1 movl 8(%ebp), %ebx 2 movl $0, %eax 3 movl $0, %ecx 4 .L13: 
5 leal (%eax,%eax), %edx 6 movl %ebx, %eax 7 andl $1, %eax 8 orl %edx, %eax 9 shrl %ebx Shift right by 1 
10 addl $1, %ecx 11 cmpl $32, %ecx 12 jne .L13 
Reverse engineer the operation of this code and then do the following: 
A. Use the assembly-code version to .ll in the missing parts of the C code. 
B. Describe in English what this function computes. 
Practice Problem 3.24 
Executing a continue statement in C causes the program to jump to the end of the current loop iteration. The stated rule for translating a for loop into a while loop needs some re.nement when dealing with continue statements. For example, consider the following code: 
/* Example of for loop using a continue statement */ /* Sum even numbers between 0 and 9 */ 
int sum=0; int i; for (i = 0; i <10;i++){ 
if (i& 1) continue; sum += i; } 
A. What would we get if we naively applied our rule for translating the for loop into a while loop? What would be wrong with this code? 
B. How could you replace the continue statement with a goto statement to ensure that the while loop correctly duplicates the behavior of the for loop? 
3.6.6 Conditional Move Instructions 
The conventional way to implement conditional operations is through a condi-tional transfer of control, where the program follows one execution path when a condition holds and another when it does not. This mechanism is simple and general, but it can be very inef.cient on modern processors. 
An alternate strategy is through a conditional transfer of data. This approach computes both outcomes of a conditional operation, and then selects one based on whether or not the condition holds. This strategy makes sense only in restricted cases, but it can then be implemented by a simple conditional move instruction that is better matched to the performance characteristics of modern processors. 

We will examine this strategy and its implementation with more recent versions of IA32 processors. 
Starting with the PentiumPro in 1995, recent generations of IA32 processors have had conditional move instructions that either do nothing or copy a value to a register, depending on the values of the condition codes. For years, these instructions have been largely unused. With its default settings, gcc did not gen-erate code that used them, because that would prevent backward compatibility, even though almost all x86 processors manufactured by Intel and its competitors since 1997 have supported these instructions. More recently, for systems running on processors that are certain to support conditional moves, such as Intel-based Apple Macintosh computers (introduced in 2006) and the 64-bit versions of Linux and Windows, gcc will generate code using conditional moves. By giving special command-line parameters on other machines, we can indicate to gcc that the tar-get machine supports conditional move instructions. 
As an example, Figure 3.16(a) shows a variant form of the function absdiff we used in Figure 3.13 to illustrate conditional branching. This version uses a conditional expression rather than a conditional statement to illustrate the concepts behind conditional data transfers more clearly, but in fact gcc 
(b) Implementation using conditional (a) Original C code 

assignment 1 int absdiff(int x, int y) { 1 int cmovdiff(int x, int y) { 2 returnx<y?y-x : x-y; 2 int tval = y-x; 
3 } 3 int rval = x-y; 4 int test=x<y; 5 /* Line below requires 6 single instruction: */ 7 if (test) rval = tval; 8 return rval; 
9 } 

(c) Generated assembly code 
xat %ebp+8,yat %ebp+12 1 movl 8(%ebp), %ecx Get x 2 movl 12(%ebp), %edx Get y 3 movl %edx, %ebx Copy y 4 subl %ecx, %ebx Compute y-x 5 movl %ecx, %eax Copy x 6 subl %edx, %eax Compute x-y and set as return value 7 cmpl %edx, %ecx Compare x:y 8 cmovl %ebx, %eax If <, replace return value with y-x 
Figure 3.16 Compilation of conditional statements using conditional assignment. C function absdiff (a) contains a conditional expression. The generated assembly code is shown (c), along with a C function cmovdiff (b) that mimics the operation of the assembly code. The stack set-up and completion portions of the assembly code have been omitted. 
generates identical code for this version as it does for the version of Figure 3.13. If we compile this giving gcc the command-line option ¡®-march=i686¡¯,3 we generate the assembly code shown in Figure 3.16(c), having an approximate form shown by the C function cmovdiff shown in Figure 3.16(b). Studying the C version, we can see that it computes both y-x and x-y, naming these tval and rval, respec-tively. It then tests whether x is less than y, and if so, copies tval to rval before returning rval. The assembly code in Figure 3.16(c) follows the same logic. The key is that the single cmovl instruction (line 8) of the assembly code implements the conditional assignment (line 7) of cmovdiff. This instruction has the same syntax as a mov instruction, except that it only performs the data movement if the speci.ed condition holds. (The suf.x ¡®l¡¯in cmovl stands for ¡°less,¡± not for ¡°long.¡±) 
To understand why code based on conditional data transfers can outperform code based on conditional control transfers (as in Figure 3.13), we must understand something about how modern processors operate. As we will see in Chapters 4 and 5, processors achieve high performance through pipelining, where an instruc-tion is processed via a sequence of stages, each performing one small portion of the required operations (e.g., fetching the instruction from memory, determining the instruction type, reading from memory, performing an arithmetic operation, writing to memory, and updating the program counter.) This approach achieves high performance by overlapping the steps of the successive instructions, such as fetching one instruction while performing the arithmetic operations for a pre-vious instruction. To do this requires being able to determine the sequence of instructions to be executed well ahead of time in order to keep the pipeline full of instructions to be executed. When the machine encounters a conditional jump (referred to as a ¡°branch¡±), it often cannot determine yet whether or not the jump will be followed. Processors employ sophisticated branch prediction logic to try to guess whether or not each jump instruction will be followed. As long as it can guess reliably (modern microprocessor designs try to achieve success rates on the order of 90%), the instruction pipeline will be kept full of instructions. Mispredicting a jump, on the other hand, requires that the processor discard much of the work it has already done on future instructions and then begin .lling the pipeline with in-structions starting at the correct location. As we will see, such a misprediction can incur a serious penalty, say, 20¨C40 clock cycles of wasted effort, causing a serious degradation of program performance. 
As an example, we ran timings of the absdiff function on an Intel Core i7 processor using both methods of implementing the conditional operation. In a typical application, the outcome of the test x<y is highly unpredictable, and so even the most sophisticated branch prediction hardware will guess correctly only around 50% of the time. In addition, the computations performed in each of the two code sequences require only a single clock cycle. As a consequence, the branch misprediction penalty dominates the performance of this function. For the IA32 code with conditional jumps, we found that the function requires around 13 clock 
3. In gcc terminology, the Pentium should be considered model ¡°586¡± and the PentiumPro should be considered model ¡°686¡± of the x86 line. 

cycles per call when the branching pattern is easily predictable, and around 35 clock cycles per call when the branching pattern is random. From this we can infer that the branch misprediction penalty is around 44 clock cycles. That means time required by the function ranges between around 13 and 57 cycles, depending on whether or not the branch is predicted correctly. 
Aside How did you determine this penalty? 
Assume the probability of misprediction is p, the time to execute the code without misprediction is TOK, and the misprediction penalty is TMP. Then the average time to execute the code as a function of p is T (p) = (1 . p)TOK + p(TOK + TMP) = TOK + pTMP. We are given TOK and T , the average time 
avg ran when p = 0.5, and we want to determine TMP. Substituting into the equation, we get T = T(0.5) = 
ran avg TOK + 0.5TMP, and therefore TMP = 2(T . TMP). So, for TOK =13 and T =35, we get TMP =44.
ran ran 

On the other hand, the code compiled using conditional moves requires around 14 clock cycles regardless of the data being tested. The .ow of control does not depend on data, and this makes it easier for the processor to keep its pipeline full. 
Practice Problem 3.25 
Running on a Pentium 4, our code required around 16 cycles when the branching pattern was highly predictable, and around 31 cycles when the pattern was random. 
A. What is the approximate miss penalty? 
B. How many cycles would the function require when the branch is mispre-dicted? 
Figure 3.17 illustrates some of the conditional move instructions added to the IA32 instruction set with the introduction of the PentiumPro microprocessor and supported by most IA32 processors manufactured by Intel and its competitors since 1997. Each of these instructions has two operands: a source register or mem-ory location S, and a destination register R. As with the different set (Section 3.6.2) and jump instructions (Section 3.6.3), the outcome of these instructions depends on the values of the condition codes. The source value is read from either mem-ory or the source register, but it is copied to the destination only if the speci.ed condition holds. 
For IA32, the source and destination values can be 16 or 32 bits long. Single-byte conditional moves are not supported. Unlike the unconditional instructions, where the operand length is explicitly encoded in the instruction name (e.g., movw and movl), the assembler can infer the operand length of a conditional move instruction from the name of the destination register, and so the same instruction name can be used for all operand lengths. 
Unlike conditional jumps, the processor can execute conditional move in-structions without having to predict the outcome of the test. The processor simply reads the source value (possibly from memory), checks the condition code, and then either updates the destination register or keeps it the same. We will explore the implementation of conditional moves in Chapter 4. 
210  Chapter 3  Machine-Level Representation of Programs  
Instruction cmove S, R cmovne S, R cmovs S, R cmovns S, R cmovg S, R cmovge S, R cmovl S, R cmovle S, R cmova S, R cmovae S, R cmovb S, R cmovbe S, R  Synonym cmovz cmovnz cmovnle cmovnl cmovnge cmovng cmovnbe cmovnb cmovnae cmovna  Move condition ZF ~ZF SF ~SF ~(SF ^ OF) & ~ZF ~(SF ^ OF) SF ^ OF (SF ^ OF) |ZF ~CF & ~ZF ~CF CF CF | ZF  Description Equal / zero Not equal / not zero Negative Nonnegative Greater (signed >) Greater or equal (signed >=) Less (signed <) Less or equal (signed <=) Above (unsigned >) Above or equal (Unsigned >=) Below (unsigned <) below or equal (unsigned <=)  
Figure 3.17 The conditional move instructions. These instructions copy the source value S to its destination R when the move condition holds. Some instructions have ¡°synonyms,¡± alternate names for the same machine instruction. 


To understand how conditional operations can be implemented via condi-tional data transfers, consider the following general form of conditional expression and assignment: 
v= test-expr ? then-expr : else-expr; 
With traditional IA32, the compiler generates code having a form shown by the following abstract code: 
if (!test-expr) 
goto false; 
v= true-expr; 
goto done; 
false: v= else-expr; done: 
This code contains two code sequences¡ªone evaluating then-expr and one evalu-ating else-expr. A combination of conditional and unconditional jumps is used to ensure that just one of the sequences is evaluated. 

For the code based on conditional move, both the then-expr and the else-expr are evaluated, with the .nal value chosen based on the evaluation test-expr. This can be described by the following abstract code: 
vt = then-expr; v= else-expr; t= test-expr; if(t) v=vt; 

The .nal statement in this sequence is implemented with a conditional move¡ª value vt is copied to v only if test condition t holds. 
Not all conditional expressions can be compiled using conditional moves. Most signi.cantly, the abstract code we have shown evaluates both then-expr and else-expr regardless of the test outcome. If one of those two expressions could possibly generate an error condition or a side effect, this could lead to invalid behavior. As an illustration, consider the following C function: 
int cread(int *xp) { return (xp ? *xp : 0); } 
At .rst, this seems like a good candidate to compile using a conditional move to read the value designated by pointer xp, as shown in the following assembly code: 
Invalid implementation of function cread 
xp in register %edx 1 movl $0, %eax Set 0 as return value 2 testl %edx, %edx Test xp 3 cmovne (%edx), %eax if !0, dereference xp to get return value 
This implementation is invalid, however, since the dereferencing of xp by the cmovne instruction (line 3) occurs even when the test fails, causing a null pointer dereferencing error. Instead, this code must be compiled using branching code. 
A similar case holds when either of the two branches causes a side effect, as illustrated by the following function: 
1 /* Global variable */ 2 int lcount = 0; 3 int absdiff_se(int x, int y) { 4 returnx<y? (lcount++, y-x) : x-y; 
5 } 

This function increments global variable lcount as part of then-expr. Thus, branching code must be used to ensure this side effect only occurs when the test condition holds. 
Using conditional moves also does not always improve code ef.ciency. For example, if either the then-expr or the else-expr evaluation requires a signi.cant computation, then this effort is wasted when the corresponding condition does not hold. Compilers must take into account the relative performance of wasted computation versus the potential for performance penalty due to branch mispre-diction. In truth, they do not really have enough information to make this decision reliably; for example, they do not know how well the branches will follow pre-dictable patterns. Our experiments with gcc indicate that it only uses conditional moves when the two expressions can be computed very easily, for example, with single add instructions. In our experience, gcc uses conditional control transfers even in many cases where the cost of branch misprediction would exceed even more complex computations. 
Overall, then, we see that conditional data transfers offer an alternative strategy to conditional control transfers for implementing conditional operations. They can only be used in restricted cases, but these cases are fairly common and provide a much better match to the operation of modern processors. 
Practice Problem 3.26 
In the following C function, we have left the de.nition of operation OP incomplete: 
#define OP /* Unknown operator */ 
int arith(int x) { return x OP 4; 
} 
When compiled, gcc generates the following assembly code: 
Register: x in %edx 1 leal 3(%edx), %eax 2 testl %edx, %edx 3 cmovns %edx, %eax 4 sarl $2, %eax Return value in %eax 
A. What operation is OP? 
B. Annotate the code to explain how it works. 
Practice Problem 3.27  
Starting with C code of the form  
1  int test(int  x, int  y) {  
2  int  val =  ;  
3  if (  ){  
4  if (  )  
5  val =  ;  
6  else  
7  val =  ;  


8  }  else  if  ( )  
9  val  =  ;  
10  return  val;  
11 }  

gcc, with the command-line setting ¡®-march=i686¡¯, generates the following as-sembly code: 
xat%ebp+8,yat%ebp+12 1 movl 8(%ebp), %ebx 2 movl 12(%ebp), %ecx 3 testl %ecx, %ecx 4 jle .L2 5 movl %ebx, %edx 6 subl %ecx, %edx 7 movl %ecx, %eax 8 xorl %ebx, %eax 9 cmpl %ecx, %ebx 
10 cmovl %edx, %eax 11 jmp .L4 12 .L2: 13 leal 0(,%ebx,4), %edx 14 leal (%ecx,%ebx), %eax 15 cmpl $-2, %ecx 16 cmovge %edx, %eax 17 .L4: 
Fill in the missing expressions in the C code. 
3.6.7 Switch Statements 
A switch statement provides a multi-way branching capability based on the value of an integer index. They are particularly useful when dealing with tests where there can be a large number of possible outcomes. Not only do they make the C code more readable, they also allow an ef.cient implementation using a data structure called a jump table. A jump table is an array where entry i is the address of a code segment implementing the action the program should take when the switch index equals i. The code performs an array reference into the jump table using the switch index to determine the target for a jump instruction. The advantage of using a jump table over a long sequence of if-else statements is that the time taken to perform the switch is independent of the number of switch cases. gcc selects the method of translating a switch statement based on the number of cases and the sparsity of the case values. Jump tables are used when there are a number of cases (e.g., four or more) and they span a small range of values. 
Figure 3.18(a) shows an example of a C switch statement. This example has a number of interesting features, including case labels that do not span a contiguous 
(a) Switch statement (b) Translation into extended C 
1 int switch_eg(int x, int n) { 1 2 int result = x; 2 33 4 switch (n) { 4 55 6 case 100: 6 7 result *= 13; 7 8 break; 8 99 10 case 102: 10 11 result += 10; 11 12 /* Fall through */ 12 13 13 14 case 103: 14 15 result += 11; 15 16 break; 16 17 17 18 case 104: 18 19 case 106: 19 20 result *= result; 20 21 break; 21 22 22 23 default: 23 24 result = 0; 24 25 } 25 26 26 27 return result; 27 28 } 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 
int switch_eg_impl(int x, int n) { 
/* Table of code pointers */ 
static void *jt[7] = { &&loc_A, &&loc_def, &&loc_B, &&loc_C, &&loc_D, &&loc_def, &&loc_D 
}; 
unsigned index=n-100; int result; 
if (index > 6) goto loc_def; 
/* Multiway branch */ 
goto *jt[index]; 
loc_def: /* Default case*/ result = 0; goto done; 
loc_C: /* Case 103 */ result = x; goto rest; 
loc_A: /* Case 100 */ result=x*13; goto done; 
loc_B: /* Case 102 */ result=x+10; 
/* Fall through */ 
rest: /* Finish case 103 */ result += 11; goto done; 
loc_D: /* Cases 104, 106 */ result=x*x; 
/* Fall through */ 
done: return result; } 

Figure 3.18 Switch statement example with translation into extended C. The translation shows the structure of jump table jt and how it is accessed. Such tables are supported by gcc as an extension to the C language. 
Section 3.6 Control 215 

xat  %ebp+8,nat  %ebp+12  
1  movl  8(%ebp),  %edx  Get  x  
2  movl  12(%ebp),  %eax  Get  n  
Set  up  jump  table  access  
3  subl  $100,  %eax  Compute  index  =  n-100  
4  cmpl  $6,  %eax  Compare  index:6  
5  ja  .L2  If  >,  goto  loc_def  
6  jmp  *.L7(,%eax,4)  Goto  *jt[index]  
Default  case  
7  .L2:  loc_def:  
8  movl  $0,  %eax  result  =  0;  
9  jmp  .L8  Goto  done  
Case 103  
10  .L5:  loc_C:  
11  movl  %edx,  %eax  result  =  x;  
12  jmp  .L9  Goto  rest  
Case 100  
13  .L3:  loc_A:  
14  leal  (%edx,%edx,2),  %eax  result  =  x*3;  
15  leal  (%edx,%eax,4),  %eax  result  =  x+4*result  
16  jmp  .L8  Goto  done  
Case 102  
17  .L4:  loc_B:  
18  leal  10(%edx),  %eax  result  =  x+10  
Fall  through  
19  .L9:  rest:  
20  addl  $11,  %eax  result  +=  11;  
21  jmp  .L8  Goto  done  
Cases  104,  106  
22  .L6:  loc_D  
23  movl  %edx,  %eax  result  =  x  
24  imull  %edx,  %eax  result  *=  x  
Fall  through  
25  .L8:  done:  
Return  result  

Figure 3.19 Assembly code for switch statement example in Figure 3.18. 
range (there are no labels for cases 101 and 105), cases with multiple labels (cases 104 and 106), and cases that fall through to other cases (case 102) because the code for the case does not end with a break statement. 
Figure 3.19 shows the assembly code generated when compiling switch_eg. The behavior of this code is shown in C as the procedure switch_eg_impl in Figure 3.18(b). This code makes use of support provided by gcc for jump tables, 
as an extension to the C language. The array jt contains seven entries, each of which is the address of a block of code. These locations are de.ned by labels in the code, and indicated in the entries in jt by code pointers, consisting of the labels pre.xed by ¡®&&.¡¯ (Recall that the operator & creates a pointer for a data value. In making this extension, the authors of gcc created a new operator && to create a pointer for a code location.) We recommend that you study the C procedure switch_eg_impl and how it relates assembly code version. 
Our original C code has cases for values 100, 102¨C104, and 106, but the switch variable n can be an arbitrary int. The compiler .rst shifts the range to between 0 and 6 by subtracting 100 from n, creating a new program variable that we call index in our C version. It further simpli.es the branching possibilities by treating index as an unsigned value, making use of the fact that negative numbers in a two¡¯s-complement representation map to large positive numbers in an unsigned representation. It can therefore test whether index is outside of the range 0¨C6 by testing whether it is greater than 6. In the C and assembly code, there are .ve distinct locations to jump to, based on the value of index. These are: loc_ A (identi.ed in the assembly code as .L3), loc_B (.L4), loc_C (.L5), loc_D (.L6), and loc_def (.L2), where the latter is the destination for the default case. Each of these labels identi.es a block of code implementing one of the case branches. In both the C and the assembly code, the program compares index to 6 and jumps to the code for the default case if it is greater. 
The key step in executing a switch statement is to access a code location through the jump table. This occurs in line 16 in the C code, with a goto statement that references the jump table jt. This computed goto is supported by gcc as an extension to the C language. In our assembly-code version, a similar operation occurs on line 6, where the jmp instruction¡¯s operand is pre.xed with ¡®*¡¯, indicating an indirect jump, and the operand speci.es a memory location indexed by register %eax, which holds the value of index. (We will see in Section 3.8 how array references are translated into machine code.) 
Our C code declares the jump table as an array of seven elements, each of which is a pointer to a code location. These elements span values 0¨C6 of index, corresponding to values 100¨C106 of n. Observe the jump table handles duplicate cases by simply having the same code label (loc_D) for entries 4 and 6, and it handles missing cases by using the label for the default case (loc_def) as entries 1 and 5. 
In the assembly code, the jump table is indicated by the following declarations, to which we have added comments: 
1 .section .rodata 2 .align 4 Align address to multiple of 4 3 .L7: 4 .long .L3 Case 100: loc_A 5 .long .L2 Case 101: loc_def 6 .long .L4 Case 102: loc_B 7 .long .L5 Case 103: loc_C 

8  .long  .L6  Case 104: loc_D  
9  .long  .L2  Case 105: loc_def  
10  .long  .L6  Case 106: loc_D  

These declarations state that within the segment of the object-code .le called ¡°.rodata¡± (for ¡°Read-Only Data¡±), there should be a sequence of seven ¡°long¡± (4-byte) words, where the value of each word is given by the instruction address associated with the indicated assembly code labels (e.g., .L3). Label .L7 marks the start of this allocation. The address associated with this label serves as the base for the indirect jump (line 6). 
The different code blocks (C labels loc_A through loc_D and loc_def) im-plement the different branches of the switch statement. Most of them simply compute a value for result and then go to the end of the function. Similarly, the assembly-code blocks compute a value for register %eax and jump to the po-sition indicated by label .L8 at the end of the function. Only the code for case labels 102 and 103 do not follow this pattern, to account for the way that case 102 falls through to 103 in the original C code. This is handled in the assembly code and switch_eg_impl by having separate destinations for the two cases (loc_C and loc_B in C, .L5 and .L4 in assembly), where both of these blocks then converge on code that increments result by 11 (labeled rest in C and .L9 in assembly). 
Examining all of this code requires careful study, but the key point is to see that the use of a jump table allows a very ef.cient way to implement a multiway branch. In our case, the program could branch to .ve distinct locations with a single jump table reference. Even if we had a switch statement with hundreds of cases, they could be handled by a single jump table access. 
Practice Problem 3.28 
In the C function that follows, we have omitted the body of the switch statement. In the C code, the case labels did not span a contiguous range, and some cases had multiple labels. 
int switch2(int x) { 
int result = 0; 
switch (x) { 

/* Body of switch statement omitted */ 
} 

return result; } 
In compiling the function, gcc generates the assembly code that follows for the initial part of the procedure and for the jump table. Variable x is initially at offset 8 relative to register %ebp. 
xat %ebp+8 
1  movl  8(%ebp),  %eax  
Set up  jump table access  
2  addl  $2, %eax  
3  cmpl  $6, %eax  
4  ja  .L2  
5  jmp  *.L8(,%eax,4)  

Jump table  for switch2  
1  .L8:  
2  .long  .L3  
3  .long  .L2  
4  .long  .L4  
5  .long  .L5  
6  .long  .L6  
7  .long  .L6  
8  .long  .L7  

Based on this information, answer the following questions: 
A. What were the values of the case labels in the switch statement body? 
B. What cases had multiple labels in the C code? 
Practice Problem 3.29 
For a C function switcher with the general structure 
1 int switcher(int a, int b, int c) 2 { 3 int answer; 4 switch(a) { 5 case : /*CaseA*/ 6 c= ; 7 /* Fall through */ 8 case : /*CaseB*/ 9 answer = ; 
10 break; 11 case : /*CaseC*/ 12 case : /*CaseD*/ 13 answer = ; 14 break; 15 case : /*CaseE*/ 16 answer = ; 17 break; 18 default: 19 answer = ; 
20 } 21 return answer; 
22 } 
gcc generates the assembly code and jump table shown in Figure 3.20. Fill in the missing parts of the C code. Except for the ordering of case labels C and D, there is only one way to .t the different cases into the template. 

Section 3.7 Procedures 219 

aat %ebp+8,bat %ebp+12, c at  %ebp+16  
1  movl  8(%ebp), %eax  1  .L7:  
2  cmpl  $7, %eax  2  .long  .L3  
3  ja  .L2  3  .long  .L2  
4  jmp  *.L7(,%eax,4)  4  .long  .L4  
5  .L2:  5  .long  .L2  
6  movl  12(%ebp), %eax  6  .long  .L5  
7  jmp  .L8  7  .long  .L6  
8  .L5:  8  .long  .L2  
9  movl  $4, %eax  9  .long  .L4  
10  jmp  .L8  
11  .L6:  
12  movl  12(%ebp), %eax  
13  xorl  $15, %eax  
14  movl  %eax, 16(%ebp)  
15  .L3:  
16  movl  16(%ebp), %eax  
17  addl  $112, %eax  
18  jmp  .L8  
19  .L4:  
20  movl  16(%ebp), %eax  
21  addl  12(%ebp), %eax  
22  sall  $2, %eax  
23  .L8:  

Figure 3.20 Assembly code and jump table for Problem 3.29. 
3.7 
Procedures 

A procedure call involves passing both data (in the form of procedure parame-ters and return values) and control from one part of a program to another. In addition, it must allocate space for the local variables of the procedure on entry and deallocate them on exit. Most machines, including IA32, provide only simple instructions for transferring control to and from procedures. The passing of data and the allocation and deallocation of local variables is handled by manipulating the program stack. 
3.7.1 Stack Frame Structure 
IA32 programs make use of the program stack to support procedure calls. The machine uses the stack to pass procedure arguments, to store return information, to save registers for later restoration, and for local storage. The portion of the stack allocated for a single procedure call is called a stack frame. Figure 3.21 diagrams the general structure of a stack frame. The topmost stack frame is delimited by two pointers, with register %ebp serving as the frame pointer, and register %esp 
Figure 3.21 Stack ¡°bottom¡± 
Stack frame structure. The stack is used for passing arguments, for storing return information, for saving registers, and for local storage. 

Earlier frames 

 
. . .  
Argument n  
. . .  
Argument 1  
Return address  
Saved %ebp  
Saved registers, local variables, and temporaries  
Argument build area  


Increasing address 44n Caller¡¯s frame 
8 
4 
Frame pointer 
%ebp 

4 
Current frame 
Stack pointer 

%esp 
Stack ¡°top¡± 
serving as the stack pointer. The stack pointer can move while the procedure is executing, and hence most information is accessed relative to the frame pointer. 
Suppose procedure P (the caller) calls procedure Q (the callee). The arguments to Q are contained within the stack frame for P. In addition, when P calls Q, the return address within P where the program should resume execution when it returns from Q is pushed onto the stack, forming the end of P¡¯s stack frame. The stack frame for Q starts with the saved value of the frame pointer (a copy of register %ebp), followed by copies of any other saved register values. 

Procedure Q also uses the stack for any local variables that cannot be stored in registers. This can occur for the following reasons: 
. There are not enough registers to hold all of the local data. . Some of the local variables are arrays or structures and hence must be accessed by array or structure references. . The address operator ¡®&¡¯ is applied to a local variable, and hence we must be able to generate an address for it. 
In addition, Q uses the stack frame for storing arguments to any procedures it calls. As illustrated in Figure 3.21, within the called procedure, the .rst argument is positioned at offset 8 relative to %ebp, and the remaining arguments (assuming their data types require no more than 4 bytes) are stored in successive 4-byte blocks, so that argument i is at offset 4 + 4i relative to %ebp. Larger arguments (such as structures and larger numeric formats) require larger regions on the stack. 
As described earlier, the stack grows toward lower addresses and the stack pointer %esp points to the top element of the stack. Data can be stored on and retrieved from the stack using the pushl and popl instructions. Space for data with no speci.ed initial value can be allocated on the stack by simply decrementing the stack pointer by an appropriate amount. Similarly, space can be deallocated by incrementing the stack pointer. 
3.7.2 Transferring Control 
The instructions supporting procedure calls and returns are shown in the following table: 
Instruction Description call Label Procedure call call *Operand Procedure call leave Prepare stack for return ret Return from call 
The call instruction has a target indicating the address of the instruction where the called procedure starts. Like jumps, a call can either be direct or indirect. In assembly code, the target of a direct call is given as a label, while the target of an indirect call is given by a * followed by an operand speci.er using one of the formats described in Section 3.4.1. 
The effect of a call instruction is to push a return address on the stack and jump to the start of the called procedure. The return address is the address of the instruction immediately following the call in the program, so that execution will resume at this location when the called procedure returns. The ret instruction pops an address off the stack and jumps to this location. The proper use of this instruction is to have prepared the stack so that the stack pointer points to the place where the preceding call instruction stored its return address. 


(a) Executing call  (b) After call  (c) After ret  
Figure 3.22 Illustration of call and ret functions. The call instruction transfers control to the start of a function, while the ret instruction returns back to the instruction following the call.  
Figure 3.22 illustrates the execution of the call and ret instructions for the sum and main functions introduced in Section 3.2.2. The following are excerpts of the disassembled code for the two functions:  
1 2  Beginning of function 08048394 <sum>: 8048394: 55  sum  push  %ebp  
3  ... Return from 80483a4:  function c3  sum  ret  
4 5  ... Call to sum 80483dc: 80483e1:  from main e8 b3 ff 83 c4 14  ff  ff  call add  8048394 <sum> $0x14,%esp  
In this code, we can see that the call instruction with address 0x080483dc in main calls function sum. This status is shown in Figure 3.22(a), with the indicated values for the stack pointer %esp and the program counter %eip. The effect of the call is to push the return address 0x080483e1 onto the stack and to jump to the .rst instruction in function sum, at address 0x08048394 (Figure 3.22(b)). The execution of function sum continues until it hits the ret instruction at address 0x080483a4. This instruction pops the value 0x080483e1 from the stack and jumps to this address, resuming the execution of main just after the call instruction in sum (Figure 3.22(c)). The leave instruction can be used to prepare the stack for returning. It is equivalent to the following code sequence:  
1 2  movl popl  %ebp, %ebp  %esp  Set stack pointer to beginning of frame Restore saved %ebp and set stack ptr to  end  of  caller¡¯s  frame  

Alternatively, this preparation can be performed by an explicit sequence of move and pop operations. Register %eax is used for returning the value from any function that returns an integer or pointer. 
Practice Problem 3.30 
The following code fragment occurs often in the compiled version of library routines: 
1 call next 2 next: 3 popl %eax 

A. To what value does register %eax get set? 
B. Explain why there is no matching ret instruction to this call. 
C. What useful purpose does this code fragment serve? 
3.7.3 Register Usage Conventions 
The set of program registers acts as a single resource shared by all of the proce-dures. Although only one procedure can be active at a given time, we must make sure that when one procedure (the caller) calls another (the callee), the callee does not overwrite some register value that the caller planned to use later. For this reason, IA32 adopts a uniform set of conventions for register usage that must be respected by all procedures, including those in program libraries. 
By convention, registers %eax, %edx, and %ecx are classi.ed as caller-save registers. When procedure Q is called by P, it can overwrite these registers without destroying any data required by P. On the other hand, registers %ebx, %esi, and %edi are classi.ed as callee-save registers. This means that Q must save the values of any of these registers on the stack before overwriting them, and restore them before returning, because P (or some higher-level procedure) may need these values for its future computations. In addition, registers %ebp and %esp must be maintained according to the conventions described here. 
As an example, consider the following code: 
1  int P(int  x)  
2  {  
3  int y  = x*x;  
4  int z  = Q(y);  
5  returny+z;  
6  }  

Procedure P computes y before calling Q, but it must also ensure that the value of y is available after Q returns. It can do this by one of two means: 
. It can store the value of y in its own stack frame before calling Q; when Q returns, procedure P can then retrieve the value of y from the stack. In other words, P, the caller, saves the value. 
. It can store the value of y in a callee-save register. If Q, or any procedure called by Q, wants to use this register, it must save the register value in its stack frame and restore the value before it returns (in other words, the callee saves the value). When Q returns to P, the value of y will be in the callee-save register, either because the register was never altered or because it was saved and restored. 
Either convention can be made to work, as long as there is agreement as to which function is responsible for saving which value. IA32 follows both approaches, partitioning the registers into one set that is caller-save, and another set that is callee-save. 
Practice Problem 3.31 
The following code sequence occurs right near the beginning of the assembly code generated by gcc for a C procedure: 
1  subl  $12, %esp  
2  movl  %ebx, (%esp)  
3  movl  %esi, 4(%esp)  
4  movl  %edi, 8(%esp)  
5  movl  8(%ebp), %ebx  
6  movl  12(%ebp), %edi  
7  movl  (%ebx), %esi  
8  movl  (%edi), %eax  
9  movl  16(%ebp), %edx  
10  movl  (%edx), %ecx  

We see that just three registers (%ebx, %esi, and %edi) are saved on the stack (lines 2¨C4). The program modi.es these and three other registers (%eax, %ecx, and %edx). At the end of the procedure, the values of registers %edi, %esi, and %ebx are restored (not shown), while the other three are left in their modi.ed states. 
Explain this apparent inconsistency in the saving and restoring of register states. 
3.7.4 Procedure Example 
As an example, consider the C functions de.ned in Figure 3.23, where function caller includes a call to function swap_add. Figure 3.24 shows the stack frame structure both just before caller calls function swap_add and while swap_add 

1  int  swap_add(int  *xp,  int  *yp)  
2  {  
3  int  x  =  *xp;  
4  int  y  =  *yp;  
5  
6  *xp=y;  
7  *yp=x;  

8 returnx+y; 
9 } 10 11 int caller() 12 { 13 int arg1 = 534; 14 int arg2 = 1057; 15 int sum = swap_add(&arg1, &arg2); 16 int diff = arg1 -arg2; 17 18 return sum * diff; 
19 } Figure 3.23 Example of procedure de.nition and call. 
Just before call In body of 
to swap_add
Frame pointer %ebp 
0 ¨C4 ¨C8 

Saved %ebp  
arg1  
arg2  
Unused  
&arg2  
&arg1  

Stack frame for caller 
+4 

+12 %esp 
0 
+8 Stack pointer 
+4 Frame pointer %ebp 
0 Stack pointer %esp 

Saved %ebp  
arg1  
arg2  
Unused  
&arg2  
&arg1  
Return address  
Saved %ebp  
Saved %ebx  
Figure 3.24 Stack frames for caller and swap_add. Procedure swap_add retrieves its arguments from the stack frame for caller. 


swap_add 

Stack frame for swap_add 

is running. Some of the instructions access stack locations relative to the stack pointer %esp while others access locations relative to the base pointer %ebp. These offsets are identi.ed by the lines shown relative to the two pointers. 
New to C? Passing parameters to a function 
Some languages, such as Pascal, provide two different ways to pass parameters to procedures¡ªby value, where the caller provides the actual parameter value, and by reference, where the caller provides a pointer to the value. In C, all parameters are passed by value, but we can mimic the effect of a reference parameter by explicitly generating a pointer to a value and passing this pointer to a procedure. We can see this with the call by caller to swap_add (Figure 3.23). By passing pointers to arg1 and arg2, caller provides a way for swap_add to modify these values. 
One of the ways in which C++ extends C is the inclusion of reference parameters. 
The stack frame for caller includes storage for local variables arg1 and arg2, at positions .4 and .8 relative to the frame pointer. These variables must be stored on the stack, since the code must associate an address with them. The following assembly code from the compiled version of caller shows how it calls swap_add: 

1  caller:  
2  pushl  %ebp  Save  old  %ebp  
3  movl  %esp,  %ebp  Set  %ebp  as  frame  pointer  
4  subl  $24,  %esp  Allocate  24  bytes  on  stack  
5  movl  $534,  -4(%ebp)  Set  arg1  to  534  
6  movl  $1057,  -8(%ebp)  Set  arg2  to  1057  
7  leal  -8(%ebp),  %eax  Compute &arg2  
8  movl  %eax,  4(%esp)  Store  on  stack  
9  leal  -4(%ebp),  %eax  Compute &arg1  
10  movl  %eax,  (%esp)  Store  on  stack  
11  call  swap_add  Call  the  swap_add  function  

This code saves a copy of %ebp and sets %ebp to the beginning of the stack frame (lines 2¨C3). It then allocates 24 bytes on the stack by decrementing the stack pointer (recall that the stack grows toward lower addresses). It initializes arg1 and arg2 to 534 and 1057, respectively (lines 5¨C6), and computes the values of &arg2 and &arg1 and stores these on the stack to form the arguments to swap_ add (lines 7¨C10). It stores these arguments relative to the stack pointer, at offsets 0 and +4 for later access by swap_add. It then calls swap_add. Of the 24 bytes allocated for the stack frame, 8 are used for the local variables, 8 are used for passing parameters to swap_add, and 8 are not used for anything. 

Aside Why does gcc allocate space that never gets used? 
We see that the code generated by gcc for caller allocates 24 bytes on the stack even though it only makes use of 16 of them. We will see many examples of this apparent wastefulness. gcc adheres to an x86 programming guideline that the total stack space used by the function should be a multiple of 16 bytes. Including the 4 bytes for the saved value of %ebp and the 4 bytes for the return address, caller uses a total of 32 bytes. The motivation for this convention is to ensure a proper alignment for accessing data. We will explain the reason for having alignment conventions and how they are implemented in Section 3.9.3. 
The compiled code for swap_add has three parts: the ¡°setup,¡± where the stack frame is initialized; the ¡°body,¡± where the actual computation of the procedure is performed; and the ¡°.nish,¡± where the stack state is restored and the procedure returns. 
The following is the setup code for swap_add. Recall that before reaching this part of the code, the call instruction will have pushed the return address onto the stack. 
1 swap_add: 2 pushl %ebp Save old %ebp 3 movl %esp, %ebp Set %ebp as frame pointer 4 pushl %ebx Save %ebx 
Function swap_add requires register %ebx for temporary storage. Since this is a callee-save register, it pushes the old value onto the stack as part of the stack frame setup. At this point, the state of the stack is as shown on the right-hand side of Figure 3.24. Register %ebp has been shifted to serve as the frame pointer for swap_add. 
The following is the body code for swap_add: 
5 movl 8(%ebp), %edx Get xp 6 movl 12(%ebp), %ecx Get yp 7 movl (%edx), %ebx Get x 8 movl (%ecx), %eax Get y 9 movl %eax, (%edx) Store y at xp 
10 movl %ebx, (%ecx) Store x at yp 11 addl %ebx, %eax Return value = x+y 
This code retrieves its arguments from the stack frame for caller. Since the frame pointer has shifted, the locations of these arguments has shifted from positions +4 and 0 relative to the old value of %esp to positions +12 and +8 relative to new value of %ebp. The sum of variables x and y is stored in register %eax to be passed as the returned value. 
The following is the .nishing code for swap_add: 
12 popl %ebx Restore %ebx 13 popl %ebp Restore %ebp 14 ret Return 
This code restores the values of registers %ebx and %ebp, while also resetting the stack pointer so that it points to the stored return address, so that the ret instruction transfers control back to caller. 
The following code in caller comes immediately after the instruction calling swap_add: 
12 movl -4(%ebp), %edx 13 subl -8(%ebp), %edx 14 imull %edx, %eax 15 leave 16 ret 
This code retrieves the values of arg1 and arg2 from the stack in order to compute diff, and uses register %eax as the return value from swap_add. Observe the use of the leave instruction to reset both the stack and the frame pointer prior to return. We have seen in our code examples that the code generated by gcc sometimes uses a leave instruction to deallocate a stack frame, and sometimes it uses one or two popl instructions. Either approach is acceptable, and the guidelines from Intel and AMD as to which is preferable change over time. 
We can see from this example that the compiler generates code to manage the stack structure according to a simple set of conventions. Arguments are passed to a function on the stack, where they can be retrieved using positive offsets (+8,+12,...) relative to %ebp. Space can be allocated on the stack either by using push instructions or by subtracting offsets from the stack pointer. Before returning, a function must restore the stack to its original condition by restoring any callee-saved registers and %ebp, and by resetting %esp so that it points to the return address. It is important for all procedures to follow a consistent set of conventions for setting up and restoring the stack in order for the program to execute properly. 
Practice Problem 3.32 
A C function fun has the following code body: 
*p=d; 
return x-c; 
The IA32 code implementing this body is as follows: 
1  movsbl  12(%ebp),%edx  
2  movl  16(%ebp),  %eax  
3  movl  %edx,  (%eax)  
4  movswl  8(%ebp),%eax  
5  movl  20(%ebp),  %edx  
6  subl  %eax,  %edx  
7  movl  %edx,  %eax  

Write a prototype for function fun, showing the types and ordering of the arguments p, d, x, and c. 
Practice Problem 3.33 
Given the C function 
1 int proc(void) 2 { 3 int x,y; 4 scanf("%x %x", &y, &x); 5 return x-y; 
6 } 

gcc generates the following assembly code: 
1 proc: 2 pushl %ebp 3 movl %esp, %ebp 4 subl $40, %esp 5 leal -4(%ebp), %eax 6 movl %eax, 8(%esp) 7 leal -8(%ebp), %eax 8 movl %eax, 4(%esp) 9 movl $.LC0, (%esp) Pointer to string "%x %x" 
10 call scanf 
Diagram stack frame at this point 
11 movl -4(%ebp), %eax 12 subl -8(%ebp), %eax 13 leave 14 ret 
Assume that procedure proc starts executing with the following register val-ues: 
Register Value 

%esp 0x800040 %ebp 0x800060 
Suppose proc calls scanf (line 10), and that scanf reads values 0x46 and 0x53 from the standard input. Assume that the string ¡°%x %x¡± is stored at memory location 0x300070. 
A. What value does %ebp get set to on line 3? 
B. What value does %esp get set to on line 4? 
C. At what addresses are local variables x and y stored? 
D. Draw a diagram of the stack frame for proc right after scanf returns. Include as much information as you can about the addresses and the contents of the stack frame elements. 
E. Indicate the regions of the stack frame that are not used by proc. 
3.7.5 Recursive Procedures 
The stack and linkage conventions described in the previous section allow pro-cedures to call themselves recursively. Since each call has its own private space on the stack, the local variables of the multiple outstanding calls do not interfere with one another. Furthermore, the stack discipline naturally provides the proper policy for allocating local storage when the procedure is called and deallocating it when it returns. 
1  int  rfact(int  n)  
2  {  
3  int  result;  
4  if  (n  <=  1)  
5  result  =  1;  
6  else  
7  result=n*  rfact(n-1);  
8  return  result;  
9  }  
Figure 3.25 C code for recursive factorial program. 


Figure 3.25 shows the C code for a recursive factorial function. The assembly code generated by gcc is shown in Figure 3.26. Let us examine how the machine code will operate when called with argument n. The set-up code (lines 2¨C 5) creates a stack frame containing the old version of %ebp, the saved value for callee-save register %ebx, and 4 bytes to hold the argument when it calls itself recursively, as illustrated in Figure 3.27. It uses register %ebx to save a copy of n (line 6). It sets the return value in register %eax to 1 (line 7) in anticipation of the case where n ¡Ü 1, in which event it will jump to the completion code. 
For the recursive case, it computes n . 1, stores it on the stack, and calls itself (lines 10¨C12). Upon completion of the code, we can assume (1) register %eax holds 
Argument: n at %ebp+8 Registers: n in %ebx, result in %eax 
1  rfact:  
2  pushl  
3  movl  
4  pushl  
5  subl  
6  movl  
7  movl  
8  cmpl  
9  jle  
10  leal  
11  movl  
12  call  
13  imull  
14  .L53:  
15  addl  
16  popl  
17  popl  
18  ret  
Figure 3.26 Assembly code for the recursive factorial program in Figure 3.25. 


%ebp Save old %ebp 
%esp, %ebp  Set %ebp as frame pointer  
%ebx  Save callee save register %ebx  
$4, %esp  Allocate 4 bytes on stack  
8(%ebp), %ebx  Get n  
$1, %eax  result = 1  
$1, %ebx  Compare n:1  
.L53  If <=, goto done  
-1(%ebx), %eax  Compute n-1  
%eax, (%esp)  Store at top of stack  
rfact  Call rfact(n-1)  
%ebx, %eax  Compute result = return value *  n  
done:  
$4, %esp  Deallocate 4 bytes from stack  
%ebx  Restore %ebx  
%ebp  Restore %ebp  
Return result  




Figure 3.27 

Stack frame for recursive factorial function. The 

n  
Return address  
Saved %ebp  
Saved %ebx  
n-1  

Stack frame 

state of the frame is shown 
for calling 

just before the recursive procedure
+8
call. 
+4
Frame pointer %ebp 0 
Stack frame Stack pointer 
for rfact %esp 


the value of (n . 1)! and (2) callee-save register %ebx holds the parameter n.It therefore multiplies these two quantities (line 13) to generate the return value of the function. 
For both cases¡ªthe terminal condition and the recursive call¡ªthe code pro-ceeds to the completion section (lines 15¨C17) to restore the stack and callee-saved register, and then it returns. 
We can see that calling a function recursively proceeds just like any other function call. Our stack discipline provides a mechanism where each invocation of a function has its own private storage for state information (saved values of the return location, frame pointer, and callee-save registers). If need be, it can also provide storage for local variables. The stack discipline of allocation and deallocation naturally matches the call-return ordering of functions. This method of implementing function calls and returns even works for more complex patterns, including mutual recursion (for example, when procedure P calls Q, which in turn calls P). 
Practice Problem 3.34 
For a C function having the general structure 
int  rfun(unsigned  x)  {  
if  (  )  
return  ;  
unsigned  nx  =  ;  
int  rv  =  rfun(nx);  
return  ;  
}  

gcc generates the following assembly code (with the setup and completion code omitted): 
1 movl 8(%ebp), %ebx 2 movl $0, %eax 3 testl %ebx, %ebx 4 je .L3 
5 movl %ebx, %eax 6 shrl %eax Shift right by 1 7 movl %eax, (%esp) 8 call rfun 9 movl %ebx, %edx 
10 andl $1, %edx 11 leal (%edx,%eax), %eax 12 .L3: 
A. What value does rfun store in the callee-save register %ebx? 
B. Fill in the missing expressions in the C code shown above. 
C. Describe in English what function this code computes. 
3.8 
Array 
Allocation 
and 
Access 

Arrays in C are one means of aggregating scalar data into larger data types. C uses a particularly simple implementation of arrays, and hence the translation into machine code is fairly straightforward. One unusual feature of C is that we can generate pointers to elements within arrays and perform arithmetic with these pointers. These are translated into address computations in machine code. 
Optimizing compilers are particularly good at simplifying the address compu-tations used by array indexing. This can make the correspondence between the C code and its translation into machine code somewhat dif.cult to decipher. 
3.8.1 Basic Principles 
For data type T and integer constant N, the declaration 
T A[N]; 
has two effects. First, it allocates a contiguous region of L.N bytes in memory, where L is the size (in bytes) of data type T . Let us denote the starting location as xA. Second, it introduces an identi.er A that can be used as a pointer to the beginning of the array. The value of this pointer will be xA. The array elements can be accessed using an integer index ranging between 0 and N .1. Array element i will be stored at address xA + L.i. 
As examples, consider the following declarations: 
char A[12]; char *B[8]; double C[6]; double *D[5]; 

These declarations will generate arrays with the following parameters: Array Element size Total size Start address Element i 
A 1 12 xA xA + i B 4 32 xB xB + 4i C 8 48 xC xC + 8i D 4 20 xD xD + 4i 
Array A consists of 12 single-byte (char) elements. Array C consists of six double-precision .oating-point values, each requiring 8 bytes. B and D are both arrays of pointers, and hence the array elements are 4 bytes each. 
The memory referencing instructions of IA32 are designed to simplify array access. For example, suppose E is an array of int¡¯s, and we wish to evaluate E[i], where the address of E is stored in register %edx and i is stored in register %ecx. Then the instruction 
movl (%edx,%ecx,4),%eax 
will perform the address computation xE + 4i, read that memory location, and copy the result to register %eax. The allowed scaling factors of 1, 2, 4, and 8 cover the sizes of the common primitive data types. 
Practice Problem 3.35 
Consider the following declarations: 
short  S[7];  
short  *T[3];  
short  **U[6];  
long  double  V[8];  
long  double  *W[4];  

Fill in the following table describing the element size, the total size, and the address of element i for each of these arrays. 
Array  Element size  Total size  Start address  Element i  
S  xS  
T  xT  
U  xU  
V  xV  
W  xW  

3.8.2 Pointer Arithmetic 
C allows arithmetic on pointers, where the computed value is scaled according to the size of the data type referenced by the pointer. That is, if p is a pointer to data of type T , and the value of p is x , then the expression p+i has value x + L.i,
pp 
where L is the size of data type T . 
The unary operators & and * allow the generation and dereferencing of point-ers. That is, for an expression Expr denoting some object, &Expr is a pointer giving the address of the object. For an expression AExpr denoting an address, *AExpr gives the value at that address. The expressions Expr and *&Expr are therefore equivalent. The array subscripting operation can be applied to both arrays and pointers. The array reference A[i] is identical to the expression *(A+i). It com-putes the address of the ith array element and then accesses this memory location. 
Expanding on our earlier example, suppose the starting address of integer array E and integer index i are stored in registers %edx and %ecx, respectively. The following are some expressions involving E. We also show an assembly-code implementation of each expression, with the result being stored in register %eax. 
Expression Type Value Assembly code 
E int * xE movl %edx,%eax E[0] int M[xE] movl (%edx),%eax E[i] int M[xE + 4i] movl (%edx,%ecx,4),%eax &E[2] int * xE + 8 leal 8(%edx),%eax E+i-1 int * xE + 4i . 4 leal -4(%edx,%ecx,4),%eax *(E+i-3) int * M[xE + 4i . 12] movl -12(%edx,%ecx,4),%eax &E[i]-E int i movl %ecx,%eax 
In these examples, the leal instruction is used to generate an address, while movl is used to reference memory (except in the .rst and last cases, where the former copies an address and the latter copies the index). The .nal example shows that one can compute the difference of two pointers within the same data structure, with the result divided by the size of the data type. 
Practice Problem 3.36 
Suppose the address of short integer array S and integer index i are stored in registers %edx and %ecx, respectively. For each of the following expressions, give its type, a formula for its value, and an assembly code implementation. The result should be stored in register %eax if it is a pointer and register element %ax if it is a short integer. 
Expression Type Value Assembly code 
S+1 
S[3] &S[i] S[4*i+1] S+i-5 

3.8.3 Nested Arrays 
The general principles of array allocation and referencing hold even when we create arrays of arrays. For example, the declaration 
int A[5][3]; 

is equivalent to the declaration 
typedef int row3_t[3]; row3_t A[5]; 
Data type row3_t is de.ned to be an array of three integers. Array A contains .ve such elements, each requiring 12 bytes to store the three integers. The total array size is then 4 . 5 . 3 = 60 bytes. 
Array A can also be viewed as a two-dimensional array with .ve rows and three columns, referenced as A[0][0] through A[4][2]. The array elements are ordered in memory in ¡°row major¡± order, meaning all elements of row 0, which can be written A[0], followed by all elements of row 1 (A[1]), and so on. 
Row Element Address 
A[0] A[0][0] xA A[0][1] xA + 4 A[0][2] xA + 8 
A[1] A[1][0] xA + 12 A[1][1] xA + 16 A[1][2] xA + 20 
A[2] A[2][0] xA + 24 A[2][1] xA + 28 A[2][2] xA + 32 
A[3] A[3][0] xA + 36 A[3][1] xA + 40 A[3][2] xA + 44 
A[4] A[4][0] xA + 48 A[4][1] xA + 52 A[4][2] xA + 56 
This ordering is a consequence of our nested declaration. Viewing A as an array of .ve elements, each of which is an array of three int¡¯s, we .rst have A[0], followed by A[1], and so on. 
To access elements of multidimensional arrays, the compiler generates code to compute the offset of the desired element and then uses one of the mov instructions with the start of the array as the base address and the (possibly scaled) offset as an index. In general, for an array declared as 
T D[R][C]; 

array element D[i][j] is at memory address 
&D[i][j] = xD + L(C .i + j), (3.1) 
where L is the size of data type T in bytes. As an example, consider the 5 ¡Á 3 integer array A de.ned earlier. Suppose xA, i, and j are at offsets 8, 12, and 16 relative to %ebp, respectively. Then array element A[i][j] can be copied to register %eax by the following code: 
Aat %ebp+8,iat %ebp+12,j at %ebp+16 1 movl 12(%ebp), %eax Get i 2 leal (%eax,%eax,2), %eax Compute 3*i 3 movl 16(%ebp), %edx Get j 4 sall $2, %edx Compute j*4 5 addl 8(%ebp), %edx Compute xA + 4j 6 movl (%edx,%eax,4), %eax Read from M[xA + 4j + 12i] 
As can be seen, this code computes the element¡¯s address as xA + 4j + 12i = xA + 4(3i + j) using a combination of shifting, adding, and scaling to avoid more costly multiplication instructions. 
Practice Problem 3.37 
Consider the following source code, where M and N are constants declared with #define: 
1 int mat1[M][N]; 2 int mat2[N][M]; 3 4 int sum_element(int i, int j) { 5 return mat1[i][j] + mat2[j][i]; 
6 } 
In compiling this program, gcc generates the following assembly code: 
iat %ebp+8,jat %ebp+12  
1  movl  8(%ebp),  %ecx  
2  movl  12(%ebp),  %edx  
3  leal  0(,%ecx,8),  %eax  
4  subl  %ecx, %eax  
5  addl  %edx, %eax  
6  leal  (%edx,%edx,4),  %edx  
7  addl  %ecx,  %edx  
8  movl  mat1(,%eax,4),  %eax  
9  addl  mat2(,%edx,4),  %eax  

Use your reverse engineering skills to determine the values of M and N based on this assembly code. 

3.8.4 Fixed-Size Arrays 
The C compiler is able to make many optimizations for code operating on multi-dimensional arrays of .xed size. For example, suppose we declare data type fix_ matrix tobe 16 ¡Á 16 arrays of integers as follows: 

1 #define N 16 

2 typedef int fix_matrix[N][N]; 
(This example illustrates a good coding practice. Whenever a program uses some constant as an array dimension or buffer size, it is best to associate a name with it via a #define declaration, and then use this name consistently, rather than the numeric value. That way, if an occasion ever arises to change the value, it can be done by simply modifying the #define declaration.) The code in Figure 3.28(a) computes element i, k of the product of arrays A and B, according to the formula 
 

0¡Üj<N ai,j . bj,k. The C compiler generates code that we then recoded into C, shown as function fix_prod_ele_opt in Figure 3.28(b). This code contains a number of clever optimizations. It recognizes that the loop will access just the elements of row i of array A, and so it creates a local pointer variable, which we have named Arow, to provide direct access to row i of the array. Arow is initialized to &A[i][0], and so array element A[i][j] can be accessed as Arow[j]. It also recognizes that the loop will access the elements of array B as B[0][k], B[1][k],..., B[15][k] in sequence. These elements occupy positions in memory starting with the address of array element B[0][k] and spaced 64 bytes apart. The program can therefore use a pointer variable Bptr to access these successive locations. In C, this pointer is shown as being incremented by N (16), although in fact the actual address is incremented by 4 . 16 = 64. 
The following is the actual assembly code for the loop. We see that four variables are maintained in registers within the loop: Arow, Bptr, j, and result. 
Registers:  Arow  in  %esi,  Bptr  in  %ecx,  j  in  %edx,  result  in  %ebx  
1  .L6:  loop:  
2  movl  (%ecx),  %eax  Get  *Bptr  
3  imull  (%esi,%edx,4),  %eax  Multiply by Arow[j]  
4  addl  %eax, %ebx  Add to result  
5  addl  $1,  %edx  Increment  j  
6  addl  $64,  %ecx  Add  64  to  Bptr  
7  cmpl  $16,  %edx  Compare j:16  
8  jne  .L6  If !=, goto loop  

As can be seen, register %ecx is incremented by 64 within the loop (line 6). Machine code considers every pointer to be a byte address, and so in compiling pointer arithmetic, it must scale every increment by the size of the underlying data type. 
Practice Problem 3.38 
The following C code sets the diagonal elements of one of our .xed-size arrays to val: 
1 /* Set all diagonal elements to val */ 2 void fix_set_diag(fix_matrix A, int val) { 3 int i; 4 for (i=0;i<N; i++) 5 A[i][i] = val; 
6 } 
When compiled, gcc generates the following assembly code: 
Aat  %ebp+8,  val  at  %ebp+12  
1  movl  8(%ebp),  %ecx  
2  movl  12(%ebp),  %edx  
3  movl  $0, %eax  
4  .L14:  
5  movl  %edx,  (%ecx,%eax)  
6  addl  $68,  %eax  
7  cmpl  $1088,  %eax  
8  jne  .L14  

Create a C-code program fix_set_diag_opt that uses optimizations similar to those in the assembly code, in the same style as the code in Figure 3.28(b). Use expressions involving the parameter N rather than integer constants, so that your code will work correctly if N is rede.ned. 
3.8.5 Variable-Size Arrays 
Historically, C only supported multidimensional arrays where the sizes (with the possible exception of the .rst dimension) could be determined at compile time. Programmers requiring variable-sized arrays had to allocate storage for these arrays using functions such as malloc or calloc, and had to explicitly encode the mapping of multidimensional arrays into single-dimension ones via row-major indexing, as expressed in Equation 3.1. ISO C99 introduced the capability to have array dimensions be expressions that are computed as the array is being allocated, and recent versions of gcc support most of the conventions for variable-sized arrays in ISO C99. 
In the C version of variable-size arrays, we can declare an array int A[expr1][expr2], either as a local variable or as an argument to a function, and then the dimensions of the array are determined by evaluating the expres-sions expr1 and expr2 at the time the declaration is encountered. So, for example, we can write a function to access element i, j of an n ¡Á n array as follows: 
1  int  var_ele(int  n,  int  A[n][n],  int  i,  int  j)  {  
2  return  A[i][j];  
3  }  

(a) Original C code 

1 /* Compute i,k of fixed matrix product */ 2 int fix_prod_ele (fix_matrix A, fix_matrix B, int i, int k) { 3 int j; 4 int result = 0; 5 6 for (j=0;j<N; j++) 7 result += A[i][j] * B[j][k]; 8 9 return result; 
10 
} 


(b) 
Optimized C code 


1 /* Compute i,k of fixed matrix product */ 2 int fix_prod_ele_opt(fix_matrix A, fix_matrix B, int i, int k) { 3 int *Arow = &A[i][0]; 4 int *Bptr = &B[0][k]; 5 int result = 0; 6 int j; 7 for(j=0; j!= N; j++) { 8 result += Arow[j] * *Bptr; 9 Bptr += N; 
10 } 11 return result; 
12 } 

Figure 3.28 Original and optimized code to compute element i, k of matrix product for .xed-length arrays. The compiler performs these optimizations automatically. 
The parameter n must precede the parameter A[n][n], so that the function can 
compute the array dimensions as the parameter is encountered. gcc generates code for this referencing function as 
nat  %ebp+8,Aat  %ebp+12,  i  at  %ebp+16,  j  at  %ebp+20  
1  movl  8(%ebp),  %eax  Get  n  
2  sall  $2,  %eax  Compute  4*n  
3  movl  %eax,  %edx  Copy  4*n  
4  imull  16(%ebp),  %edx  Compute  4*n*i  
5  movl  20(%ebp),  %eax  Get  j  
6  sall  $2,  %eax  Compute 4*j  
7  addl  12(%ebp),  %eax  Compute  xA + 4 . j  
8  movl  (%eax,%edx),  %eax  Read  from  xA + 4 . (n . i + j)  

As the annotations show, this code computes the address of element i, j as xA + 4(n . i + j). The address computation is similar to that of the .xed-size array (page 236), except that (1) the positions of the arguments on the stack are shifted due to the addition of parameter n, and (2) a multiply instruction is used (line 4) to 
1 /* Compute i,k of variable matrix product */ 2 int var_prod_ele(int n, int A[n][n], int B[n][n], int i, int k) { 3 int j; 4 int result = 0; 5 6 for (j=0;j<n; j++) 7 result += A[i][j] * B[j][k]; 8 9 return result; 
10 } 
Figure 3.29 Code to compute element i, k of matrix product for variable-sized arrays. The compiler performs optimizations similar to those for .xed-size arrays. 
compute n.i, rather than an leal instruction to compute 3i. We see therefore that referencing variable-size arrays requires only a slight generalization over .xed-size ones. The dynamic version must use a multiplication instruction to scale i by n, rather than a series of shifts and adds. In some processors, this multiplication can incur a signi.cant performance penalty, but it is unavoidable in this case. 
When variable-sized arrays are referenced within a loop, the compiler can often optimize the index computations by exploiting the regularity of the access patterns. For example, Figure 3.29 shows C code to compute element i, k of the product of two n ¡Á n arrays A and B. The compiler generates code similar to what we saw for .xed-size arrays. In fact, the code bears close resemblance to that of Figure 3.28(b), except that it scales Bptr, the pointer to element B[j][k], by the variable value n rather than the .xed value N on each iteration. 
The following is the assembly code for the loop of var_prod_ele: 
n stored at %ebp+8 Registers: Arow in %esi, Bptr in %ecx,jin %edx, 
result in %ebx, %edi holds 4*n 1 .L30: loop: 2 movl (%ecx), %eax Get *Bptr 3 imull (%esi,%edx,4), %eax Multiply by Arow[j] 4 addl %eax, %ebx Add to result 5 addl $1, %edx Increment j 6 addl %edi, %ecx Add 4*n to Bptr 7 cmpl %edx, 8(%ebp) Compare n:j 8 jg .L30 If >, goto loop 
We see that the program makes use of both a scaled value 4n (register %edi) for incrementing Bptr and the actual value of n stored at offset 8 from %ebp to check the loop bounds. The need for two values does not show up in the C code, due to the scaling of pointer arithmetic. The code retrieves the value of n from memory on each iteration to check for loop termination (line 7). This is an example of register spilling: there are not enough registers to hold all of the needed temporary data, and hence the compiler must keep some local variables in memory. In this case the compiler chose to spill n, because it is a ¡°read-only¡± value¡ªit does not change value within the loop. IA32 must often spill loop values to memory, since the processor has so few registers. In general, reading from memory can be done more readily than writing to memory, and so spilling read-only variables is preferable. See Problem 3.61 regarding how to improve this code to avoid register spilling. 

3.9 
Heterogeneous 
Data 
Structures 

C provides two mechanisms for creating data types by combining objects of dif-ferent types: structures, declared using the keyword struct, aggregate multiple objects into a single unit; unions, declared using the keyword union, allow an object to be referenced using several different types. 
3.9.1 Structures 

The C struct declaration creates a data type that groups objects of possibly different types into a single object. The different components of a structure are referenced by names. The implementation of structures is similar to that of arrays in that all of the components of a structure are stored in a contiguous region of memory, and a pointer to a structure is the address of its .rst byte. The compiler maintains information about each structure type indicating the byte offset of each .eld. It generates references to structure elements using these offsets as displacements in memory referencing instructions. 
New to C? Representing an object as a struct 
The struct data type constructor is the closest thing C provides to the objects of C++ and Java. It allows the programmer to keep information about some entity in a single data structure, and reference that information with names. 
For example, a graphics program might represent a rectangle as a structure: 
struct rect { int llx; /* X coordinate of lower-left corner */ int lly; /* Y coordinate of lower-left corner */ int color; /* Coding of color */ int width; /* Width (in pixels) */ int height; /* Height (in pixels) */ 
}; 

We could declare a variable r of type struct rect and set its .eld values as follows: 
struct rect r; 
r.llx = r.lly = 0; r.color = 0xFF00FF; r.width = 10; r.height = 20; 
where the expression r.llx selects .eld llx of structure r. 
Alternatively, we can both declare the variable and initialize its .elds with a single statement: 
struct rectr={0,0, 0xFF00FF, 10, 20 }; 
It is common to pass pointers to structures from one place to another rather than copying them. 
For example, the following function computes the area of a rectangle, where a pointer to the rectangle 
struct is passed to the function: 
int area(struct rect *rp) 
{ return (*rp).width * (*rp).height; 
} 

The expression (*rp).width dereferences the pointer and selects the width .eld of the resulting structure. Parentheses are required, because the compiler would interpret the expression *rp.width as *(rp.width), which is not valid. This combination of dereferencing and .eld selection is so common that C provides an alternative notation using ->. That is, rp->width is equivalent to the expression (*rp).width. For example, we could write a function that rotates a rectangle counterclockwise by 90 degrees as 
void rotate_left(struct rect *rp) { 
/* Exchange width and height */ 
int t = rp->height; rp->height = rp->width; rp->width = t; 
/* Shift to new lower-left corner */ 
rp->llx -= t; } 
The objects of C++ and Java are more elaborate than structures in C, in that they also associate 
a set of methods with an object that can be invoked to perform computation. In C, we would simply 
write these as ordinary functions, such as the functions area and rotate_left shown above. 
As an example, consider the following structure declaration: 
struct rec { int i; int j; int a[3]; int *p; 
}; 
This structure contains four .elds: two 4-byte int¡¯s, an array consisting of three 4-byte int¡¯s, and a 4-byte integer pointer, giving a total of 24 bytes: 
Offset 0 4 8 20 24 
Contents 
i  j  a[0]  a[1]  a[2]  p  


Observe that array a is embedded within the structure. The numbers along the top of the diagram give the byte offsets of the .elds from the beginning of the structure. 
To access the .elds of a structure, the compiler generates code that adds the appropriate offset to the address of the structure. For example, suppose variable r of type struct rec * is in register %edx. Then the following code copies element r->i to element r->j: 
1 movl (%edx), %eax Get r->i 
2 movl %eax, 4(%edx) Store in r->j 
Since the offset of .eld i is 0, the address of this .eld is simply the value of r.To store into .eld j, the code adds offset 4 to the address of r. 
To generate a pointer to an object within a structure, we can simply add the .eld¡¯s offset to the structure address. For example, we can generate the pointer &(r->a[1]) by adding offset 8 + 4 . 1 = 12. For pointer r in register %eax and integer variable i in register %edx, we can generate the pointer value &(r->a[i]) with the single instruction 
Registers: r in %edx,iin %eax 1 leal 8(%edx,%eax,4), %eax Set %eax to &r->a[i] 
As a .nal example, the following code implements the statement 
r->p = &r->a[r->i + r->j]; 
starting with r in register %edx: 
1 movl 4(%edx), %eax Get r->j 
2 addl (%edx), %eax Add r->i 
3 leal 8(%edx,%eax,4), %eax Compute &r->a[r->i + r->j] 
4 movl %eax, 20(%edx) Store in r->p 
As these examples show, the selection of the different .elds of a structure is handled completely at compile time. The machine code contains no information about the .eld declarations or the names of the .elds. 
Practice Problem 3.39 
Consider the following structure declaration: 
struct prob { int *p; struct { 
int x; 

int y; }s; struct prob *next; 
}; 

This declaration illustrates that one structure can be embedded within another, just as arrays can be embedded within structures, and arrays can be embedded within arrays. 
The following procedure (with some expressions omitted) operates on this structure: 
void sp_init(struct prob *sp) 
{ sp->s.x = ; sp->p = ; sp->next = ; 
} 
A. What are the offsets (in bytes) of the following .elds? 
p: 
s.x: s.y: next: 
B. How many total bytes does the structure require? 
C. The compiler generates the following assembly code for the body of sp_ init: 
sp at %ebp+8 1 movl 8(%ebp), %eax 2 movl 8(%eax), %edx 3 movl %edx, 4(%eax) 4 leal 4(%eax), %edx 5 movl %edx, (%eax) 6 movl %eax, 12(%eax) 
On the basis of this information, .ll in the missing expressions in the code for sp_init. 
3.9.2 Unions 
Unions provide a way to circumvent the type system of C, allowing a single object to be referenced according to multiple types. The syntax of a union declaration is identical to that for structures, but its semantics are very different. Rather than having the different .elds reference different blocks of memory, they all reference the same block. 
Consider the following declarations: 
struct S3 { char c; int i[2]; 
double v; }; 
union U3 { char c; int i[2]; double v; 
}; 

When compiled on an IA32 Linux machine, the offsets of the .elds, as well as the total size of data types S3 and U3, are as shown in the following table: 
Type  c  i  v  Size  
S3  0  4  12  20  
U3  0  0  0  8  

(We will see shortly why i has offset 4 in S3 rather than 1, and we will discuss why the results would be different for a machine running Microsoft Windows.) For pointer p of type union U3 *, references p->c, p->i[0], and p->v would all reference the beginning of the data structure. Observe also that the overall size of a union equals the maximum size of any of its .elds. 
Unions can be useful in several contexts. However, they can also lead to nasty bugs, since they bypass the safety provided by the C type system. One application is when we know in advance that the use of two different .elds in a data structure will be mutually exclusive. Then, declaring these two .elds as part of a union rather than a structure will reduce the total space allocated. 
For example, suppose we want to implement a binary tree data structure where each leaf node has a double data value, while each internal node has pointers to two children, but no data. If we declare this as 
struct NODE_S { 

struct NODE_S *left; 
struct NODE_S *right; 
double data; }; 

then every node requires 16 bytes, with half the bytes wasted for each type of node. On the other hand, if we declare a node as 
union NODE_U { struct { 

union NODE_U *left; 
union NODE_U *right; } internal; 
double data; }; 

then every node will require just 8 bytes. If n is a pointer to a node of type union NODE *, we would reference the data of a leaf node as n->data, and the children of an internal node as n->internal.left and n->internal.right. 
With this encoding, however, there is no way to determine whether a given node is a leaf or an internal node. A common method is to introduce an enumer-ated type de.ning the different possible choices for the union, and then create a structure containing a tag .eld and the union: 
typedef enum { N_LEAF, N_INTERNAL } nodetype_t; 
struct NODE_T { nodetype_t type; union { 
struct { struct NODE_T *left; struct NODE_T *right; 
} internal; double data; } info; }; 
This structure requires a total of 12 bytes: 4 for type, and either 4 each for info.internal.left and info.internal.right, or 8 for info.data. In this case, the savings gain of using a union is small relative to the awkwardness of the resulting code. For data structures with more .elds, the savings can be more compelling. 
Unions can also be used to access the bit patterns of different data types. 
For example, the following code returns the bit representation of a float as an 
unsigned: 
1 unsigned float2bit(float f) 2 { 3 union { 4 float f; 5 unsigned u; 
6 } temp; 7 temp.f = f; 8 return temp.u; 9 }; 
In this code, we store the argument in the union using one data type, and access it using another. Interestingly, the code generated for this procedure is identical to that for the following procedure: 
1 unsigned copy(unsigned u) 2 { 3 return u; 
4 } 

The body of both procedures is just a single instruction: 
1 movl 8(%ebp), %eax 
This demonstrates the lack of type information in machine code. The argu-ment will be at offset 8 relative to %ebp regardless of whether it is a float or an unsigned. The procedure simply copies its argument as the return value without modifying any bits. 
When using unions to combine data types of different sizes, byte-ordering issues can become important. For example, suppose we write a procedure that will create an 8-byte double using the bit patterns given by two 4-byte unsigned¡¯s: 
1 double bit2double(unsigned word0, unsigned word1) 
2 { 
3 union { 

4 double d; 
5 unsigned u[2]; 
6 } temp; 7 8 temp.u[0] = word0; 9 temp.u[1] = word1; 
10 return temp.d; 
11 } 

On a little-endian machine such as IA32, argument word0 will become the low-order 4 bytes of d, while word1 will become the high-order 4 bytes. On a big-endian machine, the role of the two arguments will be reversed. 
Practice Problem 3.40 
Suppose you are given the job of checking that a C compiler generates the proper code for structure and union access. You write the following structure declaration: 
typedef union { 
struct { short v; short d; int s; 
} t1; 
struct { int a[2]; char *p; 
} t2; } u_type; 

You write a series of functions of the form 
void get(u_type *up, TYPE *dest) { *dest = EXPR; } 
with different access expressions EXPR, and with destination data type TYPE set according to type associated with EXPR. You then examine the code generated when compiling the functions to see if they match your expectations. 
Suppose in these functions that up and dest are loaded into registers %eax and %edx, respectively. Fill in the following table with data type TYPE and sequences of 1¨C3 instructions to compute the expression and store the result at dest. Try to use just registers %eax and %edx, using register %ecx when these do not suf.ce. 
EXPR TYPE Code 
up->t1.s int movl 4(%eax), %eax movl %eax, (%edx) 
up->t1.v 
&up->t1.d 
up->t2.a 
up->t2.a[up->t1.s] 

3.9.3 Data Alignment 
Many computer systems place restrictions on the allowable addresses for the primitive data types, requiring that the address for some type of object must be a multiple of some value K (typically 2, 4, or 8). Such alignment restrictions simplify the design of the hardware forming the interface between the processor and the memory system. For example, suppose a processor always fetches 8 bytes from memory with an address that must be a multiple of 8. If we can guarantee that any double will be aligned to have its address be a multiple of 8, then the value can be read or written with a single memory operation. Otherwise, we may need to perform two memory accesses, since the object might be split across two 8-byte memory blocks. 

The IA32 hardware will work correctly regardless of the alignment of data. However, Intel recommends that data be aligned to improve memory system performance. Linux follows an alignment policy where 2-byte data types (e.g., short) must have an address that is a multiple of 2, while any larger data types (e.g., int, int *, float, and double) must have an address that is a multiple of 
4. Note that this requirement means that the least signi.cant bit of the address of an object of type short must equal zero. Similarly, any object of type int, or any pointer, must be at an address having the low-order 2 bits equal to zero. 
Aside A case of mandatory alignment 
For most IA32 instructions, keeping data aligned improves ef.ciency, but it does not affect program behavior. On the other hand, some of the SSE instructions for implementing multimedia operations will not work correctly with unaligned data. These instructions operate on 16-byte blocks of data, and the instructions that transfer data between the SSE unit and memory require the memory addresses to be multiples of 16. Any attempt to access memory with an address that does not satisfy this alignment will lead to an exception, with the default behavior for the program to terminate. 
This is the motivation behind the IA32 convention of making sure that every stack frame is a multiple of 16 bytes long (see the aside of page 226). The compiler can allocate storage within a stack frame in such a way that a block can be stored with a 16-byte alignment. 
Aside Alignment with Microsoft Windows 
Microsoft Windows imposes a stronger alignment requirement¡ªany primitive object of K bytes, for K = 2, 4, or 8, must have an address that is a multiple of K. In particular, it requires that the address of a double or a long long be a multiple of 8. This requirement enhances the memory performance at the expense of some wasted space. The Linux convention, where 8-byte values are aligned on 4-byte boundaries was probably good for the i386, back when memory was scarce and memory interfaces were only 4 bytes wide. With modern processors, Microsoft¡¯s alignment is a better design decision. Data type long double, for which gcc generates IA32 code allocating 12 bytes (even though the actual data type requires only 10 bytes) has a 4-byte alignment requirement with both Windows and Linux. 
Alignment is enforced by making sure that every data type is organized and allocated in such a way that every object within the type satis.es its alignment restrictions. The compiler places directives in the assembly code indicating the desired alignment for global data. For example, the assembly-code declaration of the jump table beginning on page 217 contains the following directive on line 2: 
.align 4 

This ensures that the data following it (in this case the start of the jump table) will start with an address that is a multiple of 4. Since each table entry is 4 bytes long, the successive elements will obey the 4-byte alignment restriction. 
Library routines that allocate memory, such as malloc, must be designed so that they return a pointer that satis.es the worst-case alignment restriction for the machine it is running on, typically 4 or 8. For code involving structures, the compiler may need to insert gaps in the .eld allocation to ensure that each structure element satis.es its alignment requirement. The structure then has some required alignment for its starting address. 
For example, consider the following structure declaration: 
struct S1 { int i; char c; int j; 
}; 
Suppose the compiler used the minimal 9-byte allocation, diagrammed as follows: 
Offset 0 45 9 Contents 
i  c  j  

Then it would be impossible to satisfy the 4-byte alignment requirement for both .elds i (offset 0) and j (offset 5). Instead, the compiler inserts a 3-byte gap (shown here as shaded in blue) between .elds c and j: 
Offset 0 45 8 12 Contents 
i  c  j  

As a result, j has offset 8, and the overall structure size is 12 bytes. Further-
more, the compiler must ensure that any pointer p of type struct S1* satis.es 
a 4-byte alignment. Using our earlier notation, let pointer p have value x . Then 
p 
x must be a multiple of 4. This guarantees that both p->i (address x ) and p->j
pp 
(address x + 8) will satisfy their 4-byte alignment requirements. 
p 
In addition, the compiler may need to add padding to the end of the structure 
so that each element in an array of structures will satisfy its alignment requirement. 
For example, consider the following structure declaration: 
struct S2 { int i; int j; char c; 
}; 
If we pack this structure into 9 bytes, we can still satisfy the alignment requirements for .elds i and j by making sure that the starting address of the structure satis.es a 4-byte alignment requirement. Consider, however, the following declaration: 
struct S2 d[4]; 

With the 9-byte allocation, it is not possible to satisfy the alignment requirement for each element of d, because these elements will have addresses xd, xd + 9, xd + 18, and xd + 27. Instead, the compiler allocates 12 bytes for structure S2, with the .nal 3 bytes being wasted space: 
Offset 0 4 89 12 Contents 
j  c  

That way the elements of d will have addresses xd, xd + 12, xd + 24, and xd + 36. As long as xd is a multiple of 4, all of the alignment restrictions will be satis.ed. 
Practice Problem 3.41 
For each of the following structure declarations, determine the offset of each .eld, the total size of the structure, and its alignment requirement under Linux/IA32. 
A. struct P1 { int i; char c; int j; char d; }; 
B. struct P2 { int i; char c; char d; int j; }; 
C. struct P3 { short w[3]; char c[3] }; 
D. struct P4 { short w[3]; char *c[3] }; 
E. struct P3 { struct P1 a[2]; struct P2 *p }; 
Practice Problem 3.42 
For the structure declaration 
struct { char *a; short b; double c; char d; float e; char f; long long g; void *h; 
} foo; 

suppose it was compiled on a Windows machine, where each primitive data type of K bytes must have an offset that is a multiple of K. 
A. What are the byte offsets of all the .elds in the structure? 
B. What is the total size of the structure? 
C. Rearrange the .elds of the structure to minimize wasted space, and then show the byte offsets and total size for the rearranged structure. 
3.10 
Putting 
It 
Together: 
Understanding 
Pointers 

Pointers are a central feature of the C programming language. They serve as a uniform way to generate references to elements within different data structures. Pointers are a source of confusion for novice programmers, but the underlying concepts are fairly simple. Here we highlight some key principles of pointers and their mapping into machine code. 
. Every pointer has an associated type. This type indicates what kind of object the pointer points to. Using the following pointer declarations as illustrations, 
int *ip; 
char **cpp; 
variable ip is a pointer to an object of type int, while cpp is a pointer to an object that itself is a pointer to an object of type char. In general, if the object has type T , then the pointer has type *T . The special void * type represents a generic pointer. For example, the malloc function returns a generic pointer, which is converted to a typed pointer via either an explicit cast or by the implicit casting of the assignment operation. Pointer types are not part of machine code; they are an abstraction provided by C to help programmers avoid addressing errors. 
. Every pointer has a value. This value is an address of some object of the designated type. The special NULL (0) value indicates that the pointer does not point anywhere. 
. Pointers are created with the & operator. This operator can be applied to any C expression that is categorized as an lvalue, meaning an expression that can appear on the left side of an assignment. Examples include variables and the elements of structures, unions, and arrays. We have seen that the machine-code realization of the & operator often uses the leal instruction to compute the expression value, since this instruction is designed to compute the address of a memory reference. 
. Pointers are dereferenced with the * operator. The result is a value having the type associated with the pointer. Dereferencing is implemented by a memory reference, either storing to or retrieving from the speci.ed address. 
. Arrays and pointers are closely related. The name of an array can be referenced (but not updated) as if it were a pointer variable. Array referencing (e.g., a[3]) has the exact same effect as pointer arithmetic and dereferencing (e.g., *(a+3)). Both array referencing and pointer arithmetic require scaling the offsets by the object size. When we write an expression p+i for pointer p with value p, the resulting address is computed as p + L.i, where L is the size of the data type associated with p. 
. Casting from one type of pointer to another changes its type but not its value. 
One effect of casting is to change any scaling of pointer arithmetic. So for example, if p is a pointer of type char * having value p, then the expression 

(int *) p+7 computes p + 28, while (int *) (p+7) computes p + 7. (Recall that casting has higher precedence than addition.) 
. Pointers can also point to functions. This provides a powerful capability for storing and passing references to code, which can be invoked in some other part of the program. For example, if we have a function de.ned by the proto-type 
int fun(int x, int *p); 
then we can declare and assign a pointer fp to this function by the following code sequence: 
(int) (*fp)(int, int *); fp = fun; 
We can then invoke the function using this pointer: 
inty=1; int result = fp(3, &y); 
The value of a function pointer is the address of the .rst instruction in the machine-code representation of the function. 
New to C? Function pointers 
The syntax for declaring function pointers is especially dif.cult for novice programmers to understand. For a declaration such as 
int (*f)(int*); 

it helps to read it starting from the inside (starting with ¡°f¡±) and working outward. Thus, we see that f is a pointer, as indicated by ¡°(*f).¡± It is a pointer to a function that has a single int * as an argument, as indicated by ¡°(*f)(int*)¡±. Finally, we see that it is a pointer to a function that takes an int * as an argument and returns int. 
The parentheses around *f are required, because otherwise the declaration 
int *f(int*); 
would be read as 

(int *) f(int*); 
That is, it would be interpreted as a function prototype, declaring a function f that has an int * as its argument and returns an int *. Kernighan & Ritchie [58, Sect. 5.12] present a helpful tutorial on reading C declarations. 
3.11 
Life 
in 
the 
Real 
World: 
Using 
the 
gdb 
Debugger 

The GNU debugger gdb provides a number of useful features to support the run-time evaluation and analysis of machine-level programs. With the examples and exercises in this book, we attempt to infer the behavior of a program by just looking at the code. Using gdb, it becomes possible to study the behavior by watching the program in action, while having considerable control over its execution. 
Figure 3.30 shows examples of some gdb commands that help when working with machine-level, IA32 programs. It is very helpful to .rst run objdump to get a disassembled version of the program. Our examples are based on running gdb on the .le prog, described and disassembled on page 164. We start gdb with the following command line: 
unix> gdb prog 
The general scheme is to set breakpoints near points of interest in the pro-gram. These can be set to just after the entry of a function, or at a program address. When one of the breakpoints is hit during program execution, the program will halt and return control to the user. From a breakpoint, we can examine different registers and memory locations in various formats. We can also single-step the program, running just a few instructions at a time, or we can proceed to the next breakpoint. 
As our examples suggest, gdb has an obscure command syntax, but the on-line help information (invoked within gdb with the help command) overcomes this shortcoming. Rather than using the command-line interface to gdb, many programmers prefer using ddd, an extension to gdb that provides a graphic user interface. 

Web Aside ASM:OPT Machine code generated with higher levels of optimization 
In our presentation, we have looked at machine code generated with level-one optimization (speci.ed 
with the command-line option ¡®-O1¡¯). In practice, most heavily used programs are compiled with higher 
levels of optimization. For example, all of the GNU libraries and packages are compiled with level-two 
optimization, speci.ed with the command-line option ¡®-O2¡¯. Recent versions of gcc employ an extensive set of optimizations at level two, making the mapping 
between the source code and the generated code more dif.cult to discern. Here are some examples of 
the optimizations that can be found at level two: 
. The control structures become more entangled. Most procedures have multiple return points, and the stack management code to set up and complete a function is intermixed with the code implementing the operations of the procedure. 
. Procedure calls are often inlined, replacing them by the instructions implementing the procedures. This eliminates much of the overhead involved in calling and returning from a function, and it enables optimizations that are speci.c to individual function calls. On the other hand, if we try to set a breakpoint for a function in a debugger, we might never encounter a call to this function. 
Section 3.11 Life in the Real World: Using the gdb Debugger 255 

Command  Effect  
Starting and stopping  
quit  Exit gdb  
run  Run your program (give command line arguments here)  
kill  Stop your program  
Breakpoints  
break sum  Set breakpoint at entry to function sum  
break *0x8048394  Set breakpoint at address 0x8048394  
delete 1  Delete breakpoint 1  
delete  Delete all breakpoints  
Execution  
stepi  Execute one instruction  
stepi 4  Execute four instructions  
nexti  Like stepi, but proceed through function calls  
continue  Resume execution  
finish  Run until current function returns  
Examining code  
disas  Disassemble current function  
disas sum  Disassemble function sum  
disas 0x8048397  Disassemble function around address 0x8048397  
disas 0x8048394 0x80483a4  Disassemble code within speci.ed address range  
print /x $eip  Print program counter in hex  
Examining data  
print $eax  Print contents of %eax in decimal  
print /x $eax  Print contents of %eax in hex  
print /t $eax  Print contents of %eax in binary  
print 0x100  Print decimal representation of 0x100  
print /x 555  Print hex representation of 555  
print /x ($ebp+8)  Print contents of %ebp plus 8 in hex  
print *(int *) 0xfff076b0  Print integer at address 0xfff076b0  
print *(int *) ($ebp+8)  Print integer at address %ebp +8  
x/2w 0xfff076b0  Examine two (4-byte) words starting at address 0xfff076b0  
x/20b sum  Examine .rst 20 bytes of function sum  
Useful information  
info frame  Information about current stack frame  
info registers  Values of all the registers  
help  Get information about gdb  

Figure 3.30 Example gdb commands. These examples illustrate some of the ways gdb supports debugging of machine-level programs. 
. Recursion is often replaced by iteration. For example, the recursive factorial function rfact (Fig-ure 3.25) is compiled into code very similar to that generated for the while loop implementation (Figure 3.15). Again, this can lead to some surprises when we try to monitor program execution with a debugger. 
These optimizations can signi.cantly improve program performance, but they make the mapping between source and machine code much more dif.cult to discern. This can make the programs more dif.cult to debug. Nonetheless, these higher level optimizations have now become standard, and so those who study programs at the machine level must become familiar with the possible optimizations they may encounter. 
3.12 
Out-of-Bounds 
Memory 
References 
and 
Buffer 
Over.ow 

We have seen that C does not perform any bounds checking for array references, and that local variables are stored on the stack along with state information such as saved register values and return addresses. This combination can lead to serious program errors, where the state stored on the stack gets corrupted by a write to an out-of-bounds array element. When the program then tries to reload the register or execute a ret instruction with this corrupted state, things can go seriously wrong. 
A particularly common source of state corruption is known as buffer over.ow. Typically some character array is allocated on the stack to hold a string, but the size of the string exceeds the space allocated for the array. This is demonstrated by the following program example: 
1 /* Sample implementation of library function gets() */ 2 char *gets(char *s) 3 { 4 int c; 5 char *dest = s; 6 int gotchar = 0; /* Has at least one character been read? */ 7 while ((c = getchar()) != ¡¯\n¡¯ && c != EOF) { 8 *dest++ = c; /* No bounds checking! */ 9 gotchar = 1; 
10 } 11 *dest++ = ¡¯\0¡¯; /* Terminate string */ 12 if (c == EOF && !gotchar) 13 return NULL; /* End of file or error */ 14 return s; 
15 } 16 

17 /* Read input line and write it back */ 18 void echo() 19 { 20 char buf[8]; /* Way too small! */ 21 gets(buf); 22 puts(buf); 
23 } 

The preceding code shows an implementation of the library function gets to demonstrate a serious problem with this function. It reads a line from the standard input, stopping when either a terminating newline character or some error condition is encountered. It copies this string to the location designated by argument s, and terminates the string with a null character. We show the use of gets in the function echo, which simply reads a line from standard input and echoes it back to standard output. 
The problem with gets is that it has no way to determine whether suf.cient space has been allocated to hold the entire string. In our echo example, we have purposely made the buffer very small¡ªjust eight characters long. Any string longer than seven characters will cause an out-of-bounds write. 
Examining the assembly code generated by gcc for echo shows how the stack is organized. 
1 echo: 

2 pushl %ebp Save %ebp on stack 
3 movl %esp, %ebp 
4 pushl %ebx Save %ebx 
5 subl $20, %esp Allocate 20 bytes on stack 
6 leal -12(%ebp), %ebx Compute buf as %ebp-12 
7 movl %ebx, (%esp) Store buf at top of stack 
8 call gets Call gets 
9 movl %ebx, (%esp) Store buf at top of stack 10 call puts Call puts 11 addl $20, %esp Deallocate stack space 12 popl %ebx Restore %ebx 13 popl %ebp Restore %ebp 14 ret Return 
We can see in this example that the program stores the contents of registers %ebp and %ebx on the stack, and then allocates an additional 20 bytes by subtracting 20 from the stack pointer (line 5). The location of character array buf is computed as 12 bytes below %ebp (line 6), just below the stored value of %ebx, as illustrated in Figure 3.31. As long as the user types at most seven characters, the string returned by gets (including the terminating null) will .t within the space allocated for buf. A longer string, however, will cause gets to overwrite some of the information 
Figure 3.31 

Stack organization for 
Stack frame

echo function. Character 
for caller 

array buf is just below part of the saved state. An out-of-bounds write to buf can 

%ebp

corrupt the program state. 
Stack frame for echo 
stored on the stack. As the string gets longer, the following information will get 
corrupted:  
Characters typed  Additional corrupted state  
0¨C7 8¨C11 12¨C15 16¨C19 20+  None Saved value of %ebx Saved value of %ebp Return address Saved state in caller  

As this table indicates, the corruption is cumulative¡ªas the number of char-acters increases, more state gets corrupted. Depending on which portions of the state are affected, the program can misbehave in several different ways: 
. If the stored value of %ebx is corrupted, then this register will not be restored properly in line 12, and so the caller will not be able to rely on the integrity of this register, even though it should be callee-saved. 
. If the stored value of %ebp is corrupted, then this register will not be restored properly on line 13, and so the caller will not be able to reference its local variables or parameters properly. 
. If the stored value of the return address is corrupted, then the ret instruction (line 14) will cause the program to jump to a totally unexpected location. 
None of these behaviors would seem possible based on the C code. The impact of out-of-bounds writing to memory by functions such as gets can only be under-stood by studying the program at the machine-code level. 
Our code for echo is simple but sloppy. A better version involves using the function fgets, which includes as an argument a count on the maximum number of bytes to read. Problem 3.68 asks you to write an echo function that can handle an input string of arbitrary length. In general, using gets or any function that can over.ow storage is considered a bad programming practice. The C compiler even produces the following error message when compiling a .le containing a call to gets:¡°The gets function is dangerous and should not be used.¡± Unfortunately, a number of commonly used library functions, including strcpy, strcat, and sprintf, have the property that they can generate a byte sequence without being given any indication of the size of the destination buffer [94]. Such conditions can lead to vulnerabilities to buffer over.ow. 

Practice Problem 3.43 
Figure 3.32 shows a (low-quality) implementation of a function that reads a line from standard input, copies the string to newly allocated storage, and returns a pointer to the result. 
Consider the following scenario. Procedure getline is called with the return address equal to 0x8048643, register %ebp equal to 0xbffffc94, register %ebx equal to 0x1, register %edi is equal to 0x2, and register %esi is equal to 0x3.You type in the string ¡° 012345678901234567890123¡±. The program terminates with 
(a) C code 

1 /* This is very low-quality code. 2 It is intended to illustrate bad programming practices. 3 See Problem 3.43. */ 4 char *getline() 5 { 6 char buf[8]; 7 char *result; 8 gets(buf); 9 result = malloc(strlen(buf)); 
10 strcpy(result, buf); 11 return result; 
12 
} 


(b) 
Disassembly up through call to gets 


1  080485c0  <getline>:  
2  80485c0:  55  push  %ebp  
3  80485c1:  89  e5  mov  %esp,%ebp  
4  80485c3:  83  ec  28  sub  $0x28,%esp  
5  80485c6:  89  5d  f4  mov  %ebx,-0xc(%ebp)  
6  80485c9:  89  75  f8  mov  %esi,-0x8(%ebp)  
7  80485cc:  89  7d  fc  mov  %edi,-0x4(%ebp)  
Diagram  stack  at  this  point  
8  80485cf:  8d  75  ec  lea  -0x14(%ebp),%esi  
9  80485d2:  89  34  24  mov  %esi,(%esp)  
10  80485d5:  e8  a3  ff  ff ff  call  804857d <gets>  
Modify diagram  to  show  stack  contents  at  this  point  
Figure 3.32 C and disassembled code for Problem 3.43. 


a segmentation fault. You run gdb and determine that the error occurs during the execution of the ret instruction of getline. 
A. Fill in the diagram that follows, indicating as much as you can about the stack just after executing the instruction at line 7 in the disassembly. Label the quantities stored on the stack (e.g., ¡°Return address¡±) on the right, and their hexadecimal values (if known) within the box. Each box represents 4 bytes. Indicate the position of %ebp. 
Return address
08 04 86 43  








B. Modify your diagram to show the effect of the call to gets (line 10). 
C. To what address does the program attempt to return? 
D. What register(s) have corrupted value(s) when getline returns? 
E. Besides the potential for buffer over.ow, what two other things are wrong with the code for getline? 
A more pernicious use of buffer over.ow is to get a program to perform a function that it would otherwise be unwilling to do. This is one of the most common methods to attack the security of a system over a computer network. Typically, the program is fed with a string that contains the byte encoding of some executable code, called the exploit code, plus some extra bytes that overwrite the return address with a pointer to the exploit code. The effect of executing the ret instruction is then to jump to the exploit code. 
In one form of attack, the exploit code then uses a system call to start up a shell program, providing the attacker with a range of operating system functions. In another form, the exploit code performs some otherwise unauthorized task, repairs the damage to the stack, and then executes ret a second time, causing an (apparently) normal return to the caller. 
As an example, the famous Internet worm of November 1988 used four dif-ferent ways to gain access to many of the computers across the Internet. One was a buffer over.ow attack on the .nger daemon fingerd, which serves requests by the .nger command. By invoking .nger with an appropriate string, the worm could make the daemon at a remote site have a buffer over.ow and execute code that gave the worm access to the remote system. Once the worm gained access to a system, it would replicate itself and consume virtually all of the machine¡¯s comput-ing resources. As a consequence, hundreds of machines were effectively paralyzed until security experts could determine how to eliminate the worm. The author of the worm was caught and prosecuted. He was sentenced to 3 years probation, 400 hours of community service, and a $10,500 .ne. Even to this day, however, people continue to .nd security leaks in systems that leave them vulnerable to buffer over.ow attacks. This highlights the need for careful programming. Any interface to the external environment should be made ¡°bullet proof¡± so that no behavior by an external agent can cause the system to misbehave. 

Aside Worms and viruses 
Both worms and viruses are pieces of code that attempt to spread themselves among computers. As described by Spafford [102], a worm is a program that can run by itself and can propagate a fully working version of itself to other machines. A virus is a piece of code that adds itself to other programs, including operating systems. It cannot run independently. In the popular press, the term ¡°virus¡± is used to refer to a variety of different strategies for spreading attacking code among systems, and so you will hear people saying ¡°virus¡± for what more properly should be called a ¡°worm.¡± 
3.12.1 Thwarting Buffer Over.ow Attacks 
Buffer over.ow attacks have become so pervasive and have caused so many problems with computer systems that modern compilers and operating systems have implemented mechanisms to make it more dif.cult to mount these attacks and to limit the ways by which an intruder can seize control of a system via a buffer over.ow attack. In this section, we will present ones that are provided by recent versions of gcc for Linux. 
Stack Randomization 
In order to insert exploit code into a system, the attacker needs to inject both the code as well as a pointer to this code as part of the attack string. Generating this pointer requires knowing the stack address where the string will be located. Historically, the stack addresses for a program were highly predictable. For all systems running the same combination of program and operating system version, the stack locations were fairly stable across many machines. So, for example, if an attacker could determine the stack addresses used by a common Web server, it could devise an attack that would work on many machines. Using infectious disease as an analogy, many systems were vulnerable to the exact same strain of a virus, a phenomenon often referred to as a security monoculture [93]. 
The idea of stack randomization is to make the position of the stack vary from one run of a program to another. Thus, even if many machines are running identical code, they would all be using different stack addresses. This is implemented by allocating a random amount of space between 0 and n bytes on the stack at the start of a program, for example, by using the allocation function alloca, which allocates space for a speci.ed number of bytes on the stack. This allocated space is not used by the program, but it causes all subsequent stack locations to vary from one execution of a program to another. The allocation range n needs to be large enough to get suf.cient variations in the stack addresses, yet small enough that it does not waste too much space in the program. 
The following code shows a simple way to determine a ¡°typical¡± stack address: 
1 int main() { 2 int local; 3 printf("local at %p\n", &local); 4 return 0; 
5 } 
This code simply prints the address of a local variable in the main function. Running the code 10,000 times on a Linux machine in 32-bit mode, the addresses ranged from 0xff7fa7e0 to 0xffffd7e0, a range of around 223. By comparison, running on an older Linux system, the same address occurred every time. Running in 64-bit mode on the newer machine, the addresses ranged from 0x7fff00241914 to 0x7ffffff98664, a range of nearly 232. 
Stack randomization has become standard practice in Linux systems. It is one of a larger class of techniques known as address-space layout randomization, or ASLR [95]. With ASLR, different parts of the program, including program code, library code, stack, global variables, and heap data, are loaded into different regions of memory each time a program is run. That means that a program running on one machine will have very different address mappings than the same program running on other machines. This can thwart some forms of attack. 
Overall, however, a persistent attacker can overcome randomization by brute force, repeatedly attempting attacks with different addresses. A common trick is to include a long sequence of nop (pronounced ¡°no op,¡± short for ¡°no operation¡±) instructions before the actual exploit code. Executing this instruction has no ef-fect, other than incrementing the program counter to the next instruction. As long as the attacker can guess an address somewhere within this sequence, the program will run through the sequence and then hit the exploit code. The common term for this sequence is a ¡°nop sled¡± [94], expressing the idea that the program ¡°slides¡± through the sequence. If we set up a 256-byte nop sled, then the randomization over n = 223 can be cracked by enumerating 215 = 32,768 starting addresses, which is entirely feasible for a determined attacker. For the 64-bit case, trying to enumer-ate 224 = 16,777,216 is a bit more daunting. We can see that stack randomization and other aspects of ASLR can increase the effort required to successfully attack a system, and therefore greatly reduce the rate at which a virus or worm can spread, but it cannot provide a complete safeguard. 
Practice Problem 3.44 
Running our stack-checking code 10,000 times on a system running Linux ver-sion 2.6.16, we obtained addresses ranging from a minimum of 0xffffb754 to a maximum of 0xffffd754. 
A. What is the approximate range of addresses? 
B. If we attempted a buffer overrun with a 128-byte nop sled, how many attempts would it take to exhaustively test all starting addresses? 
Figure 3.33 

Stack organization for echo function with stack protector enabled. A special ¡°canary¡± value is positioned between array buf and the saved state. The code checks the canary value to determine whether or not the stack state has been corrupted. 
Stack frame for caller 

%ebp 

Stack frame for echo 

Return address  
Saved %ebp  
Saved %ebx  
Canary  
[7]  [6]  [5]  [4]  
[3]  [2]  [1]  [0] buf  



Stack Corruption Detection 
A second line of defense is to be able to detect when a stack has been corrupted. We saw in the example of the echo function (Figure 3.31) that the corruption typically occurs when we overrun the bounds of a local buffer. In C, there is no reliable way to prevent writing beyond the bounds of an array. Instead, we can try to detect when such a write has occurred before any harmful effects can occur. 
Recent versions of gcc incorporate a mechanism known as stack protector into the generated code to detect buffer overruns. The idea is to store a special canary value4 in the stack frame between any local buffer and the rest of the stack state, as illustrated in Figure 3.33 [32, 94]. This canary value, also referred to as a guard value, is generated randomly each time the program is run, and so there is no easy way for an attacker to determine what it is. Before restoring the register state and returning from the function, the program checks if the canary has been altered by some operation of this function or one that it has called. If so, the program aborts with an error. 
Recent versions of gcc try to determine whether a function is vulnerable to a stack over.ow, and insert this type of over.ow detection automatically. In fact, for our earlier demonstration of stack over.ow, we had to give the command-line option ¡°-fno-stack-protector¡± to prevent gcc from inserting this code. When we compile the function echo without this option, and hence with stack protector enabled, we get the following assembly code: 
1  echo:  
2  pushl  %ebp  
3  movl  %esp,  %ebp  
4  pushl  %ebx  
5  subl  $20,  %esp  
6  movl  %gs:20,  %eax  Retrieve  canary  
7  movl  %eax,  -8(%ebp)  Store  on  stack  

4. The term ¡°canary¡± refers to the historic use of these birds to detect the presence of dangerous gasses in coal mines. 
8 xorl %eax, %eax Zero out register 
9 leal -16(%ebp), %ebx Compute buf as %ebp-16 
10 movl %ebx, (%esp) Store buf at top of stack 
11 call gets Call gets 
12 movl %ebx, (%esp) Store buf at top of stack 
13 call puts Call puts 
14 movl -8(%ebp), %eax Retrieve canary 
15 xorl %gs:20, %eax Compare to stored value 
16 je .L19 If =, goto ok 
17 call __stack_chk_fail Stack corrupted! 
18 .L19: ok: 
19 addl $20, %esp Normal return ... 
20 popl %ebx 
21 popl %ebp 
22 ret 
We see that this version of the function retrieves a value from memory (line 6) and stores it on the stack at offset .8 from %ebp. The instruction argument %gs:20 is an indication that the canary value is read from memory using segmented addressing, an addressing mechanism that dates back to the 80286 and is seldom found in programs running on modern systems. By storing the canary in a special segment, it can be marked as ¡°read only,¡± so that an attacker cannot overwrite the stored canary value. Before restoring the register state and returning, the function compares the value stored at the stack location with the canary value (via the xorl instruction on line 15.) If the two are identical, the xorl instruction will yield 0, and the function will complete in the normal fashion. A nonzero value indicates that the canary on the stack has been modi.ed, and so the code will call an error routine. 
Stack protection does a good job of preventing a buffer over.ow attack from corrupting state stored on the program stack. It incurs only a small performance penalty, especially because gcc only inserts it when there is a local buffer of type char in the function. Of course, there are other ways to corrupt the state of an executing program, but reducing the vulnerability of the stack thwarts many common attack strategies. 
Practice Problem 3.45 
The function intlen, along with the functions len and iptoa, provides a very convoluted way of computing the number of decimal digits required to represent an integer. We will use this as a way to study some aspects of the gcc stack protector facility. 
int len(char *s) { 
return strlen(s); } 
void iptoa(char *s, int *p) 

{ int val = *p; sprintf(s, "%d", val); 
} 

int intlen(int x) { int v; char buf[12]; v=x; iptoa(buf, &v); return len(buf); 
} 

The following show portions of the code for intlen, compiled both with and without stack protector: 
Without protector 

1 subl $36, %esp 2 movl 8(%ebp), %eax 3 movl %eax, -8(%ebp) 4 leal -8(%ebp), %eax 5 movl %eax, 4(%esp) 6 leal -20(%ebp), %ebx 7 movl %ebx, (%esp) 8 call iptoa 
With protector 

1 subl $52, %esp 2 movl %gs:20, %eax 3 movl %eax, -8(%ebp) 4 xorl %eax, %eax 5 movl 8(%ebp), %eax 6 movl %eax, -24(%ebp) 7 leal -24(%ebp), %eax 8 movl %eax, 4(%esp) 9 leal -20(%ebp), %ebx 
10 movl %ebx, (%esp) 11 call iptoa 
A. For both versions: What are the positions in the stack frame for buf, v, and (when present) the canary value? 
B. How would the rearranged ordering of the local variables in the protected code provide greater security against a buffer overrun attack? 
Limiting Executable Code Regions 
A .nal step is to eliminate the ability of an attacker to insert executable code into a system. One method is to limit which memory regions hold executable code. In typical programs, only the portion of memory holding the code generated by the compiler need be executable. The other portions can be restricted to allow just reading and writing. As we will see in Chapter 9, the virtual memory space is logically divided into pages, typically with 2048 or 4096 bytes per page. The hardware supports different forms of memory protection, indicating the forms of access allowed by both user programs and by the operating system kernel. Many systems allow control over three forms of access: read (reading data from memory), write (storing data into memory), and execute (treating the memory contents as machine-level code). Historically, the x86 architecture merged the read and execute access controls into a single 1-bit .ag, so that any page marked as readable was also executable. The stack had to be kept both readable and writable, and therefore the bytes on the stack were also executable. Various schemes were implemented to be able to limit some pages to being readable but not executable, but these generally introduced signi.cant inef.ciencies. 
More recently, AMD introduced an ¡°NX¡± (for ¡°no-execute¡±) bit into the memory protection for its 64-bit processors, separating the read and execute access modes, and Intel followed suit. With this feature, the stack can be marked as being readable and writable, but not executable, and the checking of whether a page is executable is performed in hardware, with no penalty in ef.ciency. 
Some types of programs require the ability to dynamically generate and ex-ecute code. For example, ¡°just-in-time¡± compilation techniques dynamically gen-erate code for programs written in interpreted languages, such as Java, to improve execution performance. Whether or not we can restrict the executable code to just that part generated by the compiler in creating the original program depends on the language and the operating system. 
The techniques we have outlined¡ªrandomization, stack protection, and lim-iting which portions of memory can hold executable code¡ªare three of the most common mechanisms used to minimize the vulnerability of programs to buffer over.ow attacks. They all have the properties that they require no special effort on the part of the programmer and incur very little or no performance penalty. Each separately reduces the level of vulnerability, and in combination they be-come even more effective. Unfortunately, there are still ways to attack computers [81, 94], and so worms and viruses continue to compromise the integrity of many machines. 

Web Aside ASM:EASM Combining assembly code with C programs 
Although a C compiler does a good job of converting the computations we express in a program into machine code, there are some features of a machine that cannot be accessed by a C program. For example, IA32 machines have a condition code PF (for ¡°parity .ag¡±) that is set to 1 when there is an even number of ones in the low-order 8 bits of the computed result. Computing this information in C requires at least seven shifting, masking, and exclusive-or operations (see Problem 2.65). It is ironic 
that the hardware performs this computation as part of every arithmetic or logical operation, but there 
is no way for a C program to determine the value of the PF condition code. 
There are two ways to incorporate assembly code into C programs. First, we can write an entire 
function as a separate assembly-code .le and let the assembler and linker combine this with code we 
have written in C. Second, we can use the inline assembly feature of gcc, where brief sections of assembly 
code can be incorporated into a C program using the asm directive. This approach has the advantage 
that it minimizes the amount of machine-speci.c code. 
Of course, including assembly code in a C program makes the code speci.c to a particular class of 
machines (such as IA32), and so it should only be used when the desired feature can only be accessed 
in this way. 

3.13 
x86-64: 
Extending 
IA32 
to 
64 
Bits 

Intel¡¯s IA32 instruction set architecture (ISA) has been the dominant instruction format for the world¡¯s computers for many years. IA32 has been the platform of choice for most Windows, Linux, and, since 2006, even Macintosh computers. The IA32 format used today was, for the most part, de.ned in 1985 with the introduc-tion of the i386 microprocessor, extending the 16-bit instruction set de.ned by the original 8086 to 32 bits. Even though subsequent processor generations have in-troduced new instruction types and formats, many compilers, including gcc, have avoided using these features in the interest of maintaining backward compatibility. For example, we saw in Section 3.6.6 that the conditional move instructions, intro-duced by Intel in 1995, can yield signi.cant ef.ciency improvements over more traditional conditional branches, yet in most con.gurations gcc will not generate these instructions. 
A shift is underway to a 64-bit version of the Intel instruction set. Originally developed by Advanced Micro Devices (AMD) and named x86-64, itisnow supported by most processors from AMD (who now call it AMD64) and by Intel, who refer to it as Intel64. Most people still refer to it as ¡°x86-64,¡± and we follow this convention. (Some vendors have shortened this to simply ¡°x64¡±.) Newer versions of Linux and Windows support this extension, although systems still run only 32-bit versions of these operating systems. In extending gcc to support x86-64, the developers saw an opportunity to also make use of some of the instruction-set features that had been added in more recent generations of IA32 processors. 
This combination of new hardware and revised compiler makes x86-64 code substantially different in form and in performance than IA32 code. In creating the 64-bit extension, the AMD engineers adopted some of the features found in reduced instruction set computers (RISC) [49] that made them the favored targets for optimizing compilers. For example, there are now 16 general-purpose registers, rather than the performance-limiting 8 of the original 8086. The developers of gcc were able to exploit these features, as well as those of more recent generations of the IA32 architecture, to obtain substantial performance improvements. For example, procedure parameters are now passed via registers rather than on the stack, greatly reducing the number of memory read and write operations. 
This section serves as a supplement to our description of IA32, describing the extensions in both the hardware and the software support to accommodate x86-64. We assume readers are already familiar with IA32. We start with a brief history of how AMD and Intel arrived at x86-64, followed by a summary of the main features that distinguish x86-64 code from IA32 code, and then work our way through the individual features. 
3.13.1 History and Motivation for x86-64 
Over the many years since introduction of the i386 in 1985, the capabilities of microprocessors have changed dramatically. In 1985, a fully con.gured high-end desktop computer, such as the Sun-3 workstation sold by Sun Microsystems, had at most 8 megabytes of random-access memory (RAM) and 100 megabytes of disk storage. It used a Motorola 68020 microprocessor (Intel microprocessors of that era did not have the necessary features and performance for high-end ma-chines) with a 12.5-megahertz clock and ran around 4 million instructions per second. Nowadays, a typical high-end desktop system has 4 gigabytes of RAM (512¡Á increase), 1 terabyte of disk storage (10,000¡Á increase), and a nearly 4-gigahertz clock, running around 5 billion instructions per second (1250¡Á increase). Microprocessor-based systems have become pervasive. Even today¡¯s supercom-puters are based on harnessing the power of many microprocessors computing in parallel. Given these large quantitative improvements, it is remarkable that the world¡¯s computing base mostly runs code that is binary compatible with machines that existed back in 1985 (except that they did not have nearly enough memory to handle today¡¯s operating systems and applications). 
The 32-bit word size of the IA32 has become a major limitation in growing the capacity of microprocessors. Most signi.cantly, the word size of a machine de.nes the range of virtual addresses that programs can use, giving a 4-gigabyte virtual address space in the case of 32 bits. It is now feasible to buy more than this amount of RAM for a machine, but the system cannot make effective use of it. For applications that involve manipulating large data sets, such as scienti.c computing, databases, and data mining, the 32-bit word size makes life dif.cult for programmers. They must write code using out-of-core algorithms,5 where the data reside on disk and are explicitly read into memory for processing. 
Further progress in computing technology requires shifting to a larger word size. Following the tradition of growing word sizes by doubling, the next logical step is 64 bits. In fact, 64-bit machines have been available for some time. Digital Equipment Corporation introduced its Alpha processor in 1992, and it became a popular choice for high-end computing. Sun Microsystems introduced a 64-bit version of its SPARC architecture in 1995. At the time, however, Intel was not a serious contender for high-end computers, and so the company was under less pressure to switch to 64 bits. 
5. The physical memory of a machine is often referred to as core memory, dating to an era when each bit of a random-access memory was implemented with a magnetized ferrite core. 

Intel¡¯s .rst foray into 64-bit computers were the Itanium processors, based on a totally new instruction set, known as ¡°IA64.¡± Unlike Intel¡¯s historic strategy of maintaining backward compatibility as it introduced each new generation of microprocessor, IA64 is based on a radically new approach jointly developed with Hewlett-Packard. Its Very Large Instruction Word (VLIW) format packs multiple instructions into bundles, allowing higher degrees of parallel execution. Implementing IA64 proved to be very dif.cult, and so the .rst Itanium chips did not appear until 2001, and these did not achieve the expected level of performance on real applications. Although the performance of Itanium-based systems has improved, they have not captured a signi.cant share of the computer market. Itanium machines can execute IA32 code in a compatibility mode, but not with very good performance. Most users have preferred to make do with less expensive, and often faster, IA32-based systems. 
Meanwhile, Intel¡¯s archrival, Advanced Micro Devices (AMD), saw an op-portunity to exploit Intel¡¯s misstep with IA64. For years, AMD had lagged just behind Intel in technology, and so they were relegated to competing with Intel on the basis of price. Typically, Intel would introduce a new microprocessor at a price premium. AMD would come along 6 to 12 months later and have to undercut Intel signi.cantly to get any sales¡ªa strategy that worked but yielded very low pro.ts. In 2003, AMD introduced a 64-bit microprocessor based on its ¡°x86-64¡± instruction set. As the name implies, x86-64 is an evolution of the Intel instruc-tion set to 64 bits. It maintains full backward compatibility with IA32, but it adds new data formats, as well as other features that enable higher capacity and higher performance. With x86-64, AMD captured some of the high-end market that had historically belonged to Intel. AMD¡¯s recent generations of processors have in-deed proved very successful as high-performance machines. Most recently, AMD has renamed this instruction set AMD64, but ¡°x86-64¡± persists as a favored name. 
Intel realized that its strategy of a complete shift from IA32 to IA64 was not working, and so began supporting their own variant of x86-64 in 2004 with processors in the Pentium 4 Xeon line. Since they had already used the name ¡°IA64¡± to refer to Itanium, they then faced a dif.culty in .nding their own name for this 64-bit extension. In the end, they decided to describe x86-64 as an enhancement to IA32, and so they referred to it as IA32-EM64T, for ¡°Enhanced Memory 64-bit Technology.¡± In late 2006, they adopted the name Intel64. 
On the compiler side, the developers of gcc steadfastly maintained binary compatibility with the i386, even as useful features were being added to the IA32 instruction set, including conditional moves and a more modern set of .oating-point instructions. These features would only be used when code was compiled with special settings of command-line options. Switching to x86-64 as a target provided an opportunity for gcc to give up backward compatibility and instead exploit these newer features even with standard command-line options. 
In this text, we use ¡°IA32¡± to refer to the combination of hardware and gcc code found in traditional 32-bit versions of Linux running on Intel-based machines. We use ¡°x86-64¡± to refer to the hardware and code combination running on the newer 64-bit machines from AMD and Intel. In the worlds of Linux and gcc, these two platforms are referred to as ¡°i386¡± and ¡°x86_64,¡± respectively. 
3.13.2 An Overview of x86-64 
The combination of the new hardware supplied by Intel and AMD, and the new versions of gcc targeting these machines makes x86-64 code substantially different from that generated for IA32 machines. The main features include: 
. Pointers and long integers are 64 bits long. Integer arithmetic operations support 8, 16, 32, and 64-bit data types. . The set of general-purpose registers is expanded from 8 to 16. 
. Much of the program state is held in registers rather than on the stack. Integer and pointer procedure arguments (up to 6) are passed via registers. Some procedures do not need to access the stack at all. 
. Conditional operations are implemented using conditional move instructions when possible, yielding better performance than traditional branching code. 
. Floating-point operations are implemented using the register-oriented in-struction set introduced with SSE version 2, rather than the stack-based ap-proach supported by IA32. 
Data Types 
Figure 3.34 shows the sizes of different C data types for x86-64, and compares them to the sizes for IA32 (rightmost column). We see that pointers (shown here as data type char *) require 8 bytes rather than 4. These are referred to as quad words by Intel, since they are 4 times longer than the nominal 16-bit ¡°word.¡± In principle, this gives programs the ability to access 264 bytes, or 16 exabytes, of memory (around 18.4 ¡Á 1018 bytes). That seems like an astonishing amount of memory, but keep in mind that 4 gigabytes seemed like an extremely large amount of memory when the .rst 32-bit machines appeared in the late 1970s. In practice, most machines do not really support the full address range¡ªthe current generations of AMD and Intel x86-64 machines support 256 terabytes (248 bytes) of virtual memory¡ªbut allocating a full 64 bits for pointers is a good idea for long-term compatibility. 
Assembly  x86-64  
C declaration  Intel data type  code suf.x  size (bytes)  IA32 Size  
char  Byte  b  1  1  
short  Word  w  2  2  
int  Double word  l  4  4  
long int  Quad word  q  8  4  
long long int  Quad word  q  8  8  
char *  Quad word  q  8  4  
float  Single precision  s  4  4  
double  Double precision  d  8  8  
long double  Extended precision  t  10/16  10/12  
Figure 3.34 Sizes of standard data types with x86-64. These are compared to the sizes for IA32. Both long integers and pointers require 8 bytes, as compared to 4 for IA32. 



We also see that the pre.x ¡°long¡± changes integers to 64 bits, allowing a considerably larger range of values. In fact, data type long becomes identical to long long. Moreover, the hardware provides registers that can hold 64-bit integers and instructions that can operate on these quad words. 
As with IA32, the long pre.x also changes a .oating-point double to use the 80-bit format supported by IA32 (Section 2.4.6). These are stored in memory with an allocation of 16 bytes for x86-64, compared to 12 bytes for IA32. This improves the performance of memory read and write operations, which typically fetch 8 or 16 bytes at a time. Whether 12 or 16 bytes are allocated, only the low-order 10 bytes are actually used. Moreover, the long double data type is only supported by an older class of .oating-point instructions that have some idiosyn-cratic properties (see Web Aside data:ia32-fp), while both the float and double data types are supported by the more recent SSE instructions. The long double data type should only be used by programs requiring the additional precision and range the extended-precision format provides over the double-precision format. 
Practice Problem 3.46 
As shown in Figure 6.17(b), the cost of DRAM, the memory technology used to implement the main memories of microprocessors, has dropped from around $8,000 per megabyte in 1980 to around $0.06 in 2010, roughly a factor of 1.48 every year, or around 51 every 10 years. Let us assume these trends will continue inde.nitely (which may not be realistic), and that our budget for a machine¡¯s memory is around $1,000, so that we would have con.gured a machine with 128 kilobytes in 1980 and with 16.3 gigabytes in 2010. 
A. Estimate when our $1,000 budget would pay for 256 terabytes of memory. 
B. Estimate when our $1,000 budget would pay for 16 exabytes of memory. 
C. How much earlier would these transition points occur if we raised our DRAM budget to $10,000? 
Assembly-Code Example 
In Section 3.2.3, we presented the IA32 assembly code generated by gcc for a function simple. Below is the C code for simple_l, similar to simple, except that it uses long integers: 
long int simple_l(long int *xp, long int y) 
{ long intt=*xp+y; *xp=t; return t; 
} 

When gcc is run on an x86-64 Linux machine with the command line 
unix> gcc -O1 -S -m32 code.c 
it generates code that is compatible with any IA32 machine (we annotate the code to highlight which instructions read (R) data from memory and which instructions write (W) data to memory): 
IA32 implementation of function simple_l. 
xp at %ebp+8,yat %ebp+12 1 simple_l: 2 pushl %ebp Save frame pointer (W) 3 movl %esp, %ebp Create new frame pointer 4 movl 8(%ebp), %edx Retrieve xp (R) 5 movl 12(%ebp), %eax Retrieve yp (R) 6 addl (%edx), %eax Add *xp to get t (R) 7 movl %eax, (%edx) Store t at xp (W) 8 popl %ebp Restore frame pointer (R) 9 ret Return (R) 
When we instruct gcc to generate x86-64 code 
unix> gcc -O1 -S -m64 code.c 
(on most machines, the .ag -m64 is not required), we get very different code: 
x86-64 version of function simple_l. xp in %rdi, y in %rsi 
1 simple_l: 2 movq %rsi, %rax Copy y 3 addq (%rdi), %rax Add *xp to get t (R) 4 movq %rax, (%rdi) Store t at xp (W) 5 ret Return (R) 
Some of the key differences include: 
. Instead of movl and addl instructions, we see movq and addq. The pointers and variables declared as long integers are now 64 bits (quad words) rather than 32 bits (long words). 
. We see the 64-bit versions of registers (e.g., %rsi and %rdi, rather than %esi and %edi). The procedure returns a value by storing it in register %rax. 
. No stack frame gets generated in the x86-64 version. This eliminates the instructions that set up (lines 2¨C3) and remove (line 8) the stack frame in the IA32 code. 
. Arguments xp and y are passed in registers (%rdi and %rsi, respectively) rather than on the stack. This eliminates the need to fetch the arguments from memory. 

The net effect of these changes is that the IA32 code consists of eight instruc-tions making seven memory references (.ve reads, two writes), while the x86-64 code consists of four instructions making three memory references (two reads, one write). The relative performance of the two versions depends greatly on the hardware on which they are executed. Running on an Intel Pentium 4E, one of the .rst Intel machines to support x86-64, we found that the IA32 version requires around 18 clock cycles per call to simple_l, while the x86-64 version requires only 
12. This 50% performance improvement on the same machine with the same C code is quite striking. On a newer Intel Core i7 processor, we found that both ver-sions required around 12 clock cycles, indicating no performance improvement. On other machines we have tried, the performance difference lies somewhere be-tween these two extremes. In general, x86-64 code is more compact, requires fewer memory accesses, and runs more ef.ciently than the corresponding IA32 code. 
3.13.3 Accessing Information 
Figure 3.35 shows the set of general-purpose registers under x86-64. Compared to the registers for IA32 (Figure 3.2), we see a number of differences: 
. The number of registers has been doubled to 16. . All registers are 64 bits long. The 64-bit extensions of the IA32 registers are named %rax, %rcx, %rdx, %rbx, %rsi, %rdi, %rsp, and %rbp. The new registers are named %r8¨C%r15. . The low-order 32 bits of each register can be accessed directly. This gives us the familiar registers from IA32: %eax, %ecx, %edx, %ebx, %esi, %edi, %esp, and %ebp, as well as eight new 32-bit registers: %r8d¨C%r15d. 
. The low-order 16 bits of each register can be accessed directly, as is the case for IA32. The word-size versions of the new registers are named %r8w¨C%r15w. . The low-order 8 bits of each register can be accessed directly. This is true 
in IA32 only for the .rst four registers (%al, %cl, %dl, %bl). The byte-size versions of the other IA32 registers are named %sil, %dil, %spl, and %bpl. The byte-size versions of the new registers are named %r8b¨C%r15b. . For backward compatibility, the second byte of registers %rax, %rcx, %rdx, and %rbx can be directly accessed by instructions having single-byte operands. 
As with IA32, most of the registers can be used interchangeably, but there are some special cases. Register %rsp has special status, in that it holds a pointer to the top stack element. Unlike in IA32, however, there is no frame pointer register; register %rbp is available for use as a general-purpose register. Particular conventions are used for passing procedure arguments via registers and for how registers are to be saved and restored during procedure calls, as is discussed in Section 3.13.4. In addition, some arithmetic instructions make special use of registers %rax and %rdx. 
For the most part, the operand speci.ers of x86-64 are just the same as those in IA32 (see Figure 3.3), except that the base and index register identi.ers must 

use the ¡®r¡¯ version of a register (e.g., %rax) rather than the ¡®e¡¯ version. In addition to the IA32 addressing forms, some forms of PC-relative operand addressing are supported. With IA32, this form of addressing is only supported for jump and other control transfer instructions (see Section 3.6.3). This mode is provided to compensate for the fact that the offsets (shown in Figure 3.3 as Imm) are only 32 bits long. By viewing this .eld as a 32-bit two¡¯s-complement number, instructions can access data within a window of around ¡À2.15 ¡Á 109 relative to the program counter. With x86-64, the program counter is named %rip. 
As an example of PC-relative data addressing, consider the following proce-dure, which calls the function simple_l examined earlier: 
long int gval1 = 567; long int gval2 = 763; 
long int call_simple_l() 
{ 

long int z = simple_l(&gval1, 12L); 
return z + gval2; } 
This code references global variables gval1 and gval2. When this function is compiled, assembled, and linked, we get the following executable code (as generated by the disassembler objdump): 
1  0000000000400541  <call_simple_l>:  
2  400541:  be  0c  00  00  00  mov  $0xc,%esi  Load  12  as  2nd  argument  
3  400546:  bf  20  10  60  00  mov  $0x601020,%edi  Load  &gval1  as  1st  argument  
4  40054b:  e8  c3  ff  ff  ff  callq  400513  <simple_l>  Call  simple_l  
5  400550:  48  03  05  d1  0a  20  00  add  0x200ad1(%rip),%rax  Add  gval2  to  result  
6  400557:  c3  retq  Return  

The instruction on line 3 stores the address of global variable gval1 in register %rdi. It does this by copying the constant value 0x601020 into register %edi.The upper 32 bits of %rdi are automatically set to zero. The instruction on line 5 retrieves the value of gval2 and adds it to the value returned by the call to simple_l. Here we see PC-relative addressing¡ªthe immediate value 0x200ad1 is added to the address of the following instruction to get 0x200ad1 + 0x400557 = 0x601028. 
Figure 3.36 documents some of the data movement instructions available with x86-64 beyond those found in IA32 (see Figure 3.4). Some instructions require the destination to be a register, indicated by R. Others can have either a register or a memory location as destination, indicated by D. Most of these instructions fall within a class of instructions seen with IA32. The movabsq instruction, on the other hand, has no counterpart in IA32. This instruction can copy a full 64-bit immediate value to its destination register. When the movq instruction has an immediate value as its source operand, it is limited to a 32-bit value, which is sign-extended to 64 bits. 
Instruction  Effect  Description  
movabsq  I , R  R ¡û I  Move absolute quad word  
mov  S, D  D ¡û S  Move  
movq  Move quad word  
movs  S, D  D ¡û SignExtend(S)  Move with sign extension  
movsbq  Move sign-extended byte to quad word  
movswq  Move sign-extended word to quad word  
movslq  Move sign-extended double word to quad word  
movz  S, D  D ¡û ZeroExtend(S)  Move with zero extension  
movzbq  Move zero-extended byte to quad word  
movzwq  Move zero-extended word to quad word  
pushq  S  R[%rsp] ¡û R[%rsp] . 8;  Push quad word  
M[R[%rsp]] ¡û S  
popq  D  D ¡û M[R[%rsp]];  Pop quad word  
R[%rsp] ¡û R[%rsp] + 8  
Figure 3.36 Data movement instructions. These supplement the movement instructions of IA32 (Figure 3.4). The movabsq instruction only allows immediate data (shown as I ) as the source value. Others allow immediate data, a register, or memory (shown as S). Some instructions require the destination to be a register (shown as R), while others allow both register and memory destinations (shown as D). 


Moving from a smaller data size to a larger one can involve either sign ex-tension (movs) or zero extension (movz). Perhaps unexpectedly, instructions that move or generate 32-bit register values also set the upper 32 bits of the register to zero. Consequently there is no need for an instruction movzlq. Similarly, the instruction movzbq has the exact same behavior as movzbl when the destination is a register¡ªboth set the upper 56 bits of the destination register to zero. This is in contrast to instructions that generate 8-or 16-bit values, such as movb; these instructions do not alter the other bits in the register. The new stack instructions pushq and popq allow pushing and popping of 64-bit values. 
Practice Problem 3.47 
The following C function converts an argument of type src_t to a return value of type dst_t, where these two types are de.ned using typedef: 
dest_t cvt(src_t x) 
{ dest_t y = (dest_t) x; return y; 
} 

Assume argument x is in the appropriately named portion of register %rdi (i.e., %rdi, %edi, %di,or %dil), and that some form of data movement instruction is to be used to perform the type conversion and to copy the value to the ap-propriately named portion of register %rax. Fill in the following table indicating the instruction, the source register, and the destination register for the following combinations of source and destination type: 
src_t dest_t Instruction SD 
long long movq %rdi %rax 

Arithmetic Instructions 
In Figure 3.7, we listed a number of arithmetic and logic instructions, using a class name, such as ¡°add¡±, to represent instructions for different operand sizes, such as addb (byte), addw (word), and addl (long word). To each of these classes we now add instructions that operate on quad words with the suf.x ¡®q¡¯. Examples of these quad-word instructions include leaq (load effective address), incq (increment), addq (add), and salq (shift left). These quad-word instructions have the same argument types as their shorter counterparts. As mentioned earlier, instructions that generate 32-bit register results, such as addl, also set the upper 32 bits of the register to zero. Instructions that generate 16-bit results, such as addw, only affect their 16-bit destination registers, and similarly for instructions that generate 8-bit results. As with the movq instruction, immediate operands are limited to 32-values, which are sign extended to 64 bits. 
When mixing operands of different sizes, gcc must choose the right combina-tions of arithmetic instructions, sign extensions, and zero extensions. These depend on subtle aspects of type conversion and the behavior of the instructions for dif-ferent operand sizes. This is illustrated by the following C function: 
1 long int gfun(int x, int y) 
2 { 

3 long int t1 = (long)x+y; /* 64-bit addition */ 
4 long int t2 = (long) (x + y); /* 32-bit addition */ 
5 return t1 | t2; 
6 } 

Given that integers are 32 bits and long integers are 64, the two additions in this function proceed as follows. Recall that casting has higher precedence than addition, and so line 3 calls for x to be converted to 64 bits, and by operand promotion y is also converted. Value t1 is then computed using 64-bit addition. On the other hand, t2 is computed in line 4 by performing 32-bit addition and then extending this value to 64 bits. 
The assembly code generated for this function is as follows: 
1 gfun: xin %rdi,yin %rsi 
2 leal (%rsi,%rdi), %eax Compute t2 as 32-bit sum of x and y 
cltq is equivalent to movslq %eax,%rax 
3 cltq Sign extend to 64 bits 
4 movslq %esi,%rsi Convert y to long 
5 movslq %edi,%rdi Convert x to long 
6 addq %rdi, %rsi Compute t1 (64-bit addition) 
7 orq %rsi, %rax Set t1 | t2 as return value 
8 ret Return 
Local value t2 is computed with an leal instruction (line 2), which uses 32-bit arithmetic. It is then sign-extended to 64 bits using the cltq instruction, which we will see is a special instruction equivalent to executing the instruction movslq %eax,%rax.The movslq instructions on lines 4¨C5 take the lower 32 bits of the arguments and sign extend them to 64 bits in the same registers. The addq instruction on line 6 then performs 64-bit addition to get t1. 
Practice Problem 3.48 
A C function arithprob with arguments a, b, c, and d has the following body: 
return a*b + c*d; 
It compiles to the following x86-64 code: 
1  arithprob:  
2  movslq  %ecx,%rcx  
3  imulq  %rdx, %rcx  
4  movsbl  %sil,%esi  
5  imull  %edi, %esi  
6  movslq  %esi,%rsi  
7  leaq  (%rcx,%rsi),  %rax  
8  ret  

The arguments and return value are all signed integers of various lengths. Arguments a, b, c, and d are passed in the appropriate regions of registers %rdi, %rsi, %rdx, and %rcx, respectively. Based on this assembly code, write a function prototype describing the return and argument types for arithprob. 
Figure 3.37 show instructions used to generate the full 128-bit product of two 64-bit words, as well as ones to support 64-bit division. They are similar to their 32-bit counterparts (Figure 3.9). Several of these instructions view the combination of registers %rdx and %rax as forming a 128-bit oct word. For example, the imulq and mulq instructions store the result of multiplying two 64-bit values¡ªthe .rst as given by the source operand and the second from register %rax. 

Section 3.13 x86-64: Extending IA32 to 64 Bits 279 

Instruction  Effect  Description  
imulq  S  R[%rdx]:R[%rax] ¡û S ¡Á R[%rax]  Signed full multiply  
mulq  S  R[%rdx]:R[%rax] ¡û S ¡Á R[%rax]  Unsigned full multiply  
cltq  R[%rax] ¡û SignExtend(R[%eax])  Convert %eax to quad word  
cqto  R[%rdx]:R[%rax] ¡û SignExtend(R[%rax])  Convert to oct word  
idivq  S  R[%rdx] ¡û R[%rdx]:R[%rax] mod S;  Signed divide  
R[%rax] ¡û R[%rdx]:R[%rax] ¡Â S  
divq  S  R[%rdx] ¡û R[%rdx]:R[%rax] mod S;  Unsigned divide  
R[%rax] ¡û R[%rdx]:R[%rax] ¡Â S  

Figure 3.37 Special arithmetic operations. These operations support full 64-bit multiplication and division, for both signed and unsigned numbers. The pair of registers %rdx and %rax are viewed as forming a single 128-bit oct word. 
The two divide instructions idivq and divq start with %rdx:%rax as the 128-bit dividend and the source operand as the 64-bit divisor. They then store the quotient in register %rax and the remainder in register %rdx. Preparing the dividend depends on whether unsigned (divq) or signed (idivq) division is to be performed. In the former case, register %rdx is simply set to zero. In the latter case, the instruction cqto is used to perform sign extension, copying the sign bit of %rax into every bit of %rdx.6 Figure 3.37 also shows an instruction cltq to sign extend register %eax to %rax.7 This instruction is just a shorthand for the instruction movslq %eax,%rax. 
3.13.4 Control 

The control instructions and methods of implementing control transfers in x86-64 are the same as those in IA32 (Section 3.6.) As shown in Figure 3.38, two new instructions, cmpq and testq, are added to compare and test quad words, aug-menting those for byte, word, and double word sizes (Figure 3.10). gcc uses both conditional data transfer and conditional control transfer, since all x86-64 ma-chines support conditional moves. 
To illustrate the similarity between IA32 and x86-64 code, consider the as-sembly code generated by compiling an integer factorial function implemented with a while loop (Figure 3.15), as is shown in Figure 3.39. As can be seen, these 
6. 
ATT-format instruction cqto is called cqo in Intel and AMD documentation. 

7. 
Instruction cltq is called cdqe in Intel and AMD documentation. 


Instruction  Based on  Description  
cmp cmpq  S2, S1  S1 -S2 Compare Compare quad word  
test testq  S2, S1  S1 & S2 Test Test quad word  
Figure 3.38 64-bit comparison and test instructions. These instructions set the condition codes without updating any other registers. 


(a) IA32 version 
1  fact_while:  
nat %ebp+8  
2  pushl  %ebp  Save  frame  pointer  
3  movl  %esp,  %ebp  Create  new  frame  pointer  
4  movl  8(%ebp),  %edx  Get  n  
5  movl  $1,  %eax  Set  result  =  1  
6  cmpl  $1,  %edx  Compare  n:1  
7  jle  .L7  If  <=,  goto  done  
8  .L10:  loop:  
9  imull  %edx,  %eax  Compute  result  *=  n  
10  subl  $1,  %edx  Decrement  n  
11  cmpl  $1,  %edx  Compare  n:1  
12  jg  .L10  If  >,  goto  loop  
13  .L7:  done:  
14  popl  %ebp  Restore  frame  pointer  
15  ret  Return  result  

(b) x86-64 version 1 fact_while: nin %rdi 2 movl $1, %eax Set result = 1 3 cmpl $1, %edi Compare n:1 4 jle .L7 If <=, goto done 5 .L10: loop: 6 imull %edi, %eax Compute result *= n 7 subl $1, %edi Decrement n 8 cmpl $1, %edi Compare n:1 9 jg .L10 If >, goto loop 10 .L7: done: 11 rep (See explanation in aside) 12 ret Return result 
Figure 3.39 IA32 and x86-64 versions of factorial. Both were compiled from the C code shown in Figure 3.15. 

two versions are very similar. They differ only in how arguments are passed (on the stack vs. in registers), and the absence of a stack frame or frame pointer in the x86-64 code. 
Aside Why is there a rep instruction in this code? 
On line 11 of the x86-64 code, we see the instruction rep precedes the return instruction ret. Looking at the Intel and AMD documentation for the rep instruction, we .nd that it is normally used to implement a repeating string operation [3, 29]. It seems completely inappropriate here. The answer to this puzzle can be seen in AMD¡¯s guidelines to compiler writers [1]. They recommend using the combination of rep followed by ret to avoid making the ret instruction be the destination of a conditional jump instruction. Without the rep instruction, the jg instruction would proceed to the ret instruction when the branch is not taken. According to AMD, their processors cannot properly predict the destination of a ret instruction when it is reached from a jump instruction. The rep instruction serves as a form of no-operation here, and so inserting it as the jump destination does not change behavior of the code, except to make it faster on AMD processors. 
Practice Problem 3.49 
A function fun_c has the following overall structure: 
long fun_c(unsigned long x) { long val = 0; int i; for( ; ; ){ 
; } ; return ; } 
The gcc C compiler generates the following assembly code: 
1 fun_c: 

xin %rdi 2 movl $0, %ecx 3 movl $0, %edx 4 movabsq $72340172838076673, %rsi 5 .L2: 6 movq %rdi, %rax 7 andq %rsi, %rax 8 addq %rax, %rcx 9 shrq %rdi Shift right by 1 
10 addl $1, %edx 11 cmpl $8, %edx 12 jne .L2 
13 movq %rcx, %rax 14 sarq $32, %rax 15 addq %rcx, %rax 16 movq %rax, %rdx 17 sarq $16, %rdx 18 addq %rax, %rdx 19 movq %rdx, %rax 20 sarq $8, %rax 21 addq %rdx, %rax 22 andl $255, %eax 23 ret 
Reverse engineer the operation of this code. You will .nd it useful to convert the decimal constant on line 4 to hexadecimal. 
A. Use the assembly-code version to .ll in the missing parts of the C code. 
B. Describe in English what this code computes. 
Procedures 
We have already seen in our code samples that the x86-64 implementation of procedure calls differs substantially from that of IA32. By doubling the register set, programs need not be so dependent on the stack for storing and retrieving procedure information. This can greatly reduce the overhead for procedure calls and returns. 
Here are some of the highlights of how procedures are implemented with x86-64: 
. Arguments (up to the .rst six) are passed to procedures via registers, rather than on the stack. This eliminates the overhead of storing and retrieving values on the stack. 
. The callq instruction stores a 64-bit return address on the stack. . Many functions do not require a stack frame. Only functions that cannot keep all local variables in registers need to allocate space on the stack. . Functions can access storage on the stack up to 128 bytes beyond (i.e., at a lower address than) the current value of the stack pointer. This allows some functions to store information on the stack without altering the stack pointer. . There is no frame pointer. Instead, references to stack locations are made relative to the stack pointer. Most functions allocate their total stack storage needs at the beginning of the call and keep the stack pointer at a .xed position. . As with IA32, some registers are designated as callee-save registers. These must be saved and restored by any procedure that modi.es them. 

Operand  Argument Number  
size (bits)  1  2  3  4  5  6  
64  %rdi  %rsi  %rdx  %rcx  %r8  %r9  
32  %edi  %esi  %edx  %ecx  %r8d  %r9d  
16  %di  %si  %dx  %cx  %r8w  %r9w  
8  %dil  %sil  %dl  %cl  %r8b  %r9b  
Figure 3.40 Registers for passing function arguments. The registers are used in a speci.ed order and named according to the argument sizes. 


Argument Passing 
Up to six integral (i.e., integer and pointer) arguments can be passed via registers. The registers are used in a speci.ed order, with the name used for a register de-pending on the size of the data type being passed. These are shown in Figure 3.40. Arguments are allocated to these registers according to their ordering in the ar-gument list. Arguments smaller than 64 bits can be accessed using the appropriate subsection of the 64-bit register. For example, if the .rst argument is 32 bits, it can be accessed as %edi. 
As an example of argument passing, consider the following C function having eight arguments: 
void proc(long a1, long *a1p, int a2, int *a2p, short a3, short *a3p, char a4, char *a4p) 
{ *a1p += a1; *a2p += a2; *a3p += a3; *a4p += a4; 
} 

The arguments include a range of different-sized integers (64, 32, 16, and 8 bits) as well as different types of pointers, each of which is 64 bits. This function is implemented in x86-64 as follows: 
x86-64 implementation of function proc 
Arguments passed as follows: a1 in %rdi (64 bits) a1p in %rsi (64 bits) a2 in %edx (32 bits) a2p in %rcx (64 bits) a3 in %r8w (16 bits) a3p in %r9 (64 bits) 
a4 at %rsp+8 (8 bits) 
a4p at %rsp+16 (64 bits) 1 proc: 2 movq 16(%rsp), %r10 Fetch a4p (64 bits) 3 addq %rdi, (%rsi) *a1p += a1 (64 bits) 4 addl %edx, (%rcx) *a2p += a2 (32 bits) 5 addw %r8w, (%r9) *a3p += a3 (16 bits) 6 movzbl 8(%rsp), %eax Fetch a4 (8 bits) 7 addb %al, (%r10) *a4p += a4 (8 bits) 8 ret 
The .rst six arguments are passed in registers, while the last two are at positions 8 and 16 relative to the stack pointer. Different versions of the add instruction are used according to the sizes of the operands: addq for a1 (long), addl for a2 (int), addw for a3 (short), and addb for a4 (char). 
Practice Problem 3.50 
A C function incrprob has arguments q, t, and x of different sizes, and each may be signed or unsigned. The function has the following body: 
*t += x; *q += *t; 
It compiles to the following x86-64 code: 
1  incrprob:  
2  addl  (%rdx),  %edi  
3  movl  %edi,  (%rdx)  
4  movslq  %edi,%rdi  
5  addq  %rdi,  (%rsi)  
6  ret  

Determine all four valid function prototypes for incrprob by determining the ordering and possible types of the three parameters. 
Stack Frames 
We have already seen that many compiled functions do not require a stack frame. If all of the local variables can be held in registers, and the function does not call any other functions (sometimes referred to as a leaf procedure, in reference to the tree structure of procedure calls), then the only need for the stack is to save the return address. 
On the other hand, there are several reasons a function may require a stack frame: 
. There are too many local variables to hold in registers. 
. Some local variables are arrays or structures. 

. The function uses the address-of operator (&) to compute the address of a local variable. . The function must pass some arguments on the stack to another function. . The function needs to save the state of a callee-save register before modify-ing it. 
When any of these conditions hold, we .nd the compiled code for the function creating a stack frame. Unlike the code for IA32, where the stack pointer .uctuates back and forth as values are pushed and popped, the stack frames for x86-64 procedures usually have a .xed size, set at the beginning of the procedure by decrementing the stack pointer (register %rsp). The stack pointer remains at a .xed position during the call, making it possible to access data using offsets relative to the stack pointer. As a consequence, the frame pointer (register %ebp) seen in IA32 code is no longer needed. 
Whenever one function (the caller) calls another (the callee), the return ad-dress gets pushed onto the stack. By convention, we consider this part of the caller¡¯s stack frame, in that it encodes part of the caller¡¯s state. But this infor-mation gets popped from the stack as control returns to the caller, and so it does not affect the offsets used by the caller for accessing values within the stack frame. 
The following function illustrates many aspects of the x86-64 stack discipline. Despite the length of this example, it is worth studying carefully. 
long int call_proc() 
{ long x1=1; int x2=2; short x3 = 3; charx4=4; proc(x1, &x1, x2, &x2, x3, &x3, x4, &x4); return (x1+x2)*(x3-x4); 
} 

gcc generates the following x86-64 code. 
x86-64 implementation of call_proc 
1  call_proc:  
2  subq  $32,  %rsp  Allocate 32-byte stack frame  
3  movq  $1,  16(%rsp)  Store 1 in &x1  
4  movl  $2,  24(%rsp)  Store 2 in &x2  
5  movw  $3,  28(%rsp)  Store 3 in &x3  
6  movb  $4,  31(%rsp)  Store 4 in &x4  
7  leaq  24(%rsp),  %rcx  Pass &x2 as argument  4  
8  leaq  16(%rsp),  %rsi  Pass &x1 as argument  2  
9  leaq  31(%rsp),  %rax  Compute &x4  
10  movq  %rax,  8(%rsp)  Pass &x4 as argument  8  
11  movl  $4,  (%rsp)  Pass  4 as argument  7  
12  leaq  28(%rsp),  %r9  Pass &x3 as argument  6  
13  movl  $3,  %r8d  Pass  3 as argument  5  
14  movl  $2,  %edx  Pass  2 as argument  3  
15  movl  $1,  %edi  Pass  1 as argument  1  

16 call proc Call 17 movswl 28(%rsp),%eax Get x3 and convert to int 18 movsbl 31(%rsp),%edx Get x4 and convert to int 19 subl %edx, %eax Compute x3-x4 20 cltq Sign extend to long int 21 movslq 24(%rsp),%rdx Get x2 22 addq 16(%rsp), %rdx Compute x1+x2 23 imulq %rdx, %rax Compute (x1+x2)*(x3-x4) 24 addq $32, %rsp Deallocate stack frame 25 ret Return 
Figure 3.41(a) illustrates the stack frame set up during the execution of call_ proc. Function call_proc allocates 32 bytes on the stack by decrementing the stack pointer. It uses bytes 16¨C31 to hold local variables x1 (bytes 16¨C23), x2 (bytes 24¨C27), x3 (bytes 28¨C29), and x4 (byte 31). These allocations are sized according to the variable types. Byte 30 is unused. Bytes 0¨C7 and 8¨C15 of the stack frame are used to hold the seventh and eighth arguments to call_proc, since there are not enough argument registers. The parameters are allocated eight bytes each, even though parameter x4 requires only a single byte. In the code for call_proc,we can see instructions initializing the local variables and setting up the parameters (both in registers and on the stack) for the call to call_proc. After proc returns, the local variables are combined to compute the .nal expression, which is returned in register %rax. The stack space is deallocated by simply incrementing the stack pointer before the ret instruction. 
Figure 3.41(b) illustrates the stack during the execution of proc.The call 
instruction pushed the return address onto the stack, and so the stack pointer 
is shifted down by 8 relative to its position during the execution of call_proc. 

Figure 3.41 31 28 Stack frame structure for 24 call_proc. The frame 16is required to hold local variables x1 through x4, Stack pointer 8 as well as the seventh and %rsp 
0 
x4  x3  x2  
x1  
Argument 8  
Argument 7  


eighth arguments to proc. 
(a) Before call to proc

During the execution of proc (b), the stack pointer is shifted down by 8. 
32 24 16 8
Stack pointer %rsp 
0 
x4  x3  x2  
x1  
Argument 8  
Argument 7  
Return address  

(b) During call to proc 

Hence, within the code for proc, arguments 7 and 8 are accessed by offsets of 8 and 16 from the stack pointer. 
Observe how call_proc changed the stack pointer only once during its execu-tion. gcc determined that 32 bytes would suf.ce for holding all local variables and for holding the additional arguments to proc. Minimizing the amount of move-ment by the stack pointer simpli.es the compiler¡¯s task of generating reference to stack elements using offsets from the stack pointer. 
Register Saving Conventions 
We saw in IA32 (Section 3.7.3) that some registers used for holding temporary values are designated as caller-saved, where a function is free to overwrite their values, while others are callee-saved, where a function must save their values on the stack before writing to them. With x86-64, the following registers are designated as being callee-saved: %rbx, %rbp, and %r12¨C%r15. 
Aside Are there any caller-saved temporary registers? 
Of the 16 general-purpose registers, we¡¯ve seen that 6 are designated for passing arguments, 6 are for callee-saved temporaries, 1 (%rax) holds the return value for a function, and 1 (%rsp) serves as the stack pointer. Only %r10 and %r11 are left as caller-saved temporary registers. Of course, an argument register can be used when there are fewer than six arguments or when the function is done using that argument, and %rax can be used multiple times before the .nal result is generated. 
We illustrate the use of callee-saved registers with a somewhat unusual version of a recursive factorial function: 
/* Compute x! and store at resultp */ 
void sfact_helper(long int x, long int *resultp) { if(x <= 1) *resultp = 1; 
else { long int nresult; sfact_helper(x-1, &nresult); *resultp=x* nresult; 
} } 

To compute the factorial of a value x, this function would be called at the top level as follows: 
long int sfact(long int x) 
{ long int result; sfact_helper(x, &result); return result; 
} 

The x86-64 code for sfact_helper is shown below. 
Arguments: x in %rdi, resultp in %rsi 
1 sfact_helper: 
2 movq %rbx, -16(%rsp) Save %rbx (callee save) 
3 movq %rbp, -8(%rsp) Save %rbp (callee save) 
4 subq $40, %rsp Allocate 40 bytes on stack 
5 movq %rdi, %rbx Copy x to %rbx 
6 movq %rsi, %rbp Copy resultp to %rbp 
7 cmpq $1, %rdi Compare x:1 
8 jg .L14 If >, goto recur 
9 movq $1, (%rsi) Store 1 in *resultp 
10 jmp .L16 Goto done 
11 .L14: recur: 
12 leaq 16(%rsp), %rsi Compute &nresult as second argument 
13 leaq -1(%rdi), %rdi Compute xm1 = x-1 as first argument 
14 call sfact_helper Call sfact_helper(xm1, &nresult) 
15 movq %rbx, %rax Copy x 
16 imulq 16(%rsp), %rax Compute x*nresult 
17 movq %rax, (%rbp) Store at resultp 
18 .L16: done: 
19 movq 24(%rsp), %rbx Restore %rbx 
20 movq 32(%rsp), %rbp Restore %rbp 
21 addq $40, %rsp Deallocate stack 
22 ret Return 
Figure 3.42 illustrates how sfact_helper uses the stack to store the values of callee-saved registers and to hold the local variable nresult. This implementation 
¨C8 

function decrements the stack pointer after saving ¨C16 some of the state. 
Figure 3.42 Stack pointer Stack frame for function %rsp 
0 sfact_helper. This 

Saved %rbp  
Saved %rbx  

(a) Before decrementing the stack pointer 
+32 
+24 
+16 
+8
Stack pointer %rsp 
0 
Saved %rbp  
Saved %rbx  
nresult  
Unused  
Unused  

(b) After decrementing the stack pointer 

has the interesting feature that the two callee-saved registers it uses (%rbx and %rbp) are saved on the stack (lines 2¨C3) before the stack pointer is decremented (line 4) to allocate the stack frame. As a consequence, the stack offset for %rbx shifts from .16 at the beginning to +24 at the end (line 19). Similarly, the offset for %rbp shifts from .8to +32. 
Being able to access memory beyond the stack pointer is an unusual feature of x86-64. It requires that the virtual memory management system allocate memory for that region. The x86-64 ABI [73] speci.es that programs can use the 128 bytes beyond (i.e., at lower addresses than) the current stack pointer. The ABI refers to this area as the red zone. It must be kept available for reading and writing as the stack pointer moves. 
Practice Problem 3.51 
For the C program 

long int local_array(int i) 
{ long int a[4] = {2L, 3L, 5L, 7L}; int idx=i&3; return a[idx]; 
} 

gcc generates the following code: 
x86-64 implementation of local_array Argument: i in %edi 
1  local_array:  
2  movq  $2,  -40(%rsp)  
3  movq  $3,  -32(%rsp)  
4  movq  $5,  -24(%rsp)  
5  movq  $7,  -16(%rsp)  
6  andl  $3,  %edi  
7  movq  -40(%rsp,%rdi,8), %rax  
8  ret  

A. Draw a diagram indicating the stack locations used by this function and their offsets relative to the stack pointer. 
B. Annotate the assembly code to describe the effect of each instruction. 
C. What interesting feature does this example illustrate about the x86-64 stack discipline? 
Practice Problem 3.52 
For the recursive factorial program 
long int rfact(long int x) { if(x<= 0) return 1; 
else { long int xm1 = x-1; return x * rfact(xm1); 
} } 
gcc generates the following code: 
x86-64 implementation of recursive factorial function rfact Argument x in %rdi 
1 rfact: 2 pushq %rbx 3 movq %rdi, %rbx 4 movl $1, %eax 5 testq %rdi, %rdi 6 jle .L11 7 leaq -1(%rdi), %rdi 8 call rfact 9 imulq %rbx, %rax 
10 .L11: 11 popq %rbx 12 ret 
A. What value does the function store in %rbx? 
B. What are the purposes of the pushq (line 2) and popq (line 11) instructions? 
C. Annotate the assembly code to describe the effect of each instruction. 
D. How does this function manage the stack frame differently from others we have seen? 
3.13.5 Data Structures 
Data structures follow the same principles in x86-64 as they do in IA32: arrays are allocated as sequences of identically sized blocks holding the array elements, structures are allocated as sequences of variably sized blocks holding the structure elements, and unions are allocated as a single block big enough to hold the largest union element. 

One difference is that x86-64 follows a more stringent set of alignment re-quirements. For any scalar data type requiring K bytes, its starting address must be a multiple of K. Thus, data types long and double as well as pointers, must be aligned on 8-byte boundaries. In addition, data type long double uses a 16-byte alignment (and size allocation), even though the actual representation requires only 10 bytes. These alignment conditions are imposed to improve memory sys-tem performance¡ªthe memory interface is designed in most processors to read or write aligned blocks that are 8 or 16 bytes long. 
Practice Problem 3.53 
For each of the following structure declarations, determine the offset of each .eld, the total size of the structure, and its alignment requirement under x86-64. 
A. struct P1 { int i; char c; long j; char d; }; 
B. struct P2 { long i; char c; char d; int j; }; 
C. struct P3 { short w[3]; char c[3] }; 
D. struct P4 { short w[3]; char *c[3] }; 
E. struct P3 { struct P1 a[2]; struct P2 *p }; 
3.13.6 Concluding Observations about x86-64 
Both AMD and the authors of gcc deserve credit for moving x86 processors into a new era. The formulation of both the x86-64 hardware and the programming conventions changed the processor from one that relied heavily on the stack to hold program state to one where the most heavily used part of the state is held in the much faster and expanded register set. Finally, x86 has caught up to ideas developed for RISC processors in the early 1980s! 
Processors capable of running either IA32 or x86-64 code are becoming com-monplace. Many current desktop and laptop systems are still running 32-bit ver-sions of their operating systems, and these machines are restricted to running only 32-bit applications, as well. Machines running 64-bit operating systems, and therefore capable of running both 32-and 64-bit applications, have become the widespread choice for high-end machines, such as for database servers and scien-ti.c computing. The biggest drawback in transforming applications from 32 bits to 64 bits is that the pointer variables double in size, and since many data struc-tures contain pointers, this means that the overall memory requirement can nearly double. The transition from 32-to 64-bit applications has only occurred for ones having memory needs that exceed the 4-gigabyte address space limitation of IA32. History has shown that applications grow to use all available processing power and memory size, and so we can reliably predict that 64-bit processors running 64-bit operating systems and applications will become increasingly more commonplace. 
3.14 
Machine-Level 
Representations 
of 
Floating-Point 
Programs 

Thus far, we have only considered programs that represent and operate on inte-ger data types. In order to implement programs that make use of .oating-point data, we must have some method of storing .oating-point data and additional in-structions to operate on .oating-point values, to convert between .oating-point and integer values, and to perform comparisons between .oating-point values. We also require conventions on how to pass .oating-point values as function ar-guments and to return them as function results. We call this combination of storage model, instructions, and conventions the .oating-point architecture for a machine. 
Due to its long evolutionary heritage, x86 processors provide multiple .oating-point architectures, of which two are in current use. The .rst, referred to as ¡°x87,¡± dates back to the earliest days of Intel microprocessors and until recently was the standard implementation. The second, referred to as ¡°SSE,¡± is based on recent additions to x86 processors to support multimedia applications. 

Web Aside ASM:X87 The x87 .oating-point architecture 
The historical x87 .oating-point architecture is one of the least elegant features of the x87 architecture. In the original Intel machines, .oating point was performed by a separate coprocessor, a unit with its own registers and processing capabilities that executes a subset of the instructions. This coprocessor was implemented as a separate chip, named the 8087, 80287, and i387, to accompany the processor chips 8086, 80286, and i386, respectively, and hence the colloquial name ¡°x87.¡± All x86 processors support the x87 architecture, and so this continues to be a possible target for compiling .oating-point code. 
x87 instructions operate on a shallow stack of .oating-point registers. In a stack model, some instructions read values from memory and push them onto the stack; others pop operands from the stack, perform an operation, and then push the result; while others pop values from the stack and store them to memory. This approach has the advantage that there is a simple algorithm by which a compiler can map the evaluation of arithmetic expressions into stack code. 
Modern compilers can make many optimizations that do not .t well within a stack model, for example, making use of a single computed result multiple times. Consequently, the x87 architecture implements an odd hybrid between a stack and a register model, where the different elements of the stack can be read and written explicitly, as well as shifted up and down by pushing and popping. In addition, the x87 stack is limited to a depth of eight values; when additional values are pushed, the ones at the bottom are simply discarded. Hence, the compiler must keep track of the stack depth. Furthermore, a compiler must treat all .oating-point registers as being caller-save, since their values might disappear off the bottom if other procedures push more values onto the stack. 
Web Aside ASM:SSE The SSE .oating-point architecture 
Starting with the Pentium 4, the SSE2 instruction set, added to support multimedia applications, becomes a viable .oating-point architecture for compiled C code. Unlike the stack-based architecture of x87, SSE-based .oating point uses a straightforward register-based approach, a much better target for optimizing compilers. With SSE2, .oating-point code is similar to integer code, except that it uses a different set of registers and instructions. When compiling for x86-64, gcc generates SSE code. On the other hand, its default is to generate x87 code for IA32, but it can be directed to generate SSE code by a suitable setting of the command-line parameters. 
3.15 
Summary 

In this chapter, we have peered beneath the layer of abstraction provided by the C language to get a view of machine-level programming. By having the compiler generate an assembly-code representation of the machine-level program, we gain insights into both the compiler and its optimization capabilities, along with the ma-chine, its data types, and its instruction set. In Chapter 5, we will see that knowing the characteristics of a compiler can help when trying to write programs that have ef.cient mappings onto the machine. We have also gotten a more complete picture of how the program stores data in different memory regions. In Chapter 12, we will see many examples where application programmers need to know whether a program variable is on the run-time stack, in some dynamically allocated data structure, or part of the global program data. Understanding how programs map onto machines makes it easier to understand the differences between these kinds of storage. 
Machine-level programs, and their representation by assembly code, differ in many ways from C programs. There is minimal distinction between different data types. The program is expressed as a sequence of instructions, each of which performs a single operation. Parts of the program state, such as registers and the run-time stack, are directly visible to the programmer. Only low-level operations are provided to support data manipulation and program control. The compiler must use multiple instructions to generate and operate on different data structures and to implement control constructs such as conditionals, loops, and procedures. We have covered many different aspects of C and how it gets compiled. We have seen that the lack of bounds checking in C makes many programs prone to buffer over.ows. This has made many systems vulnerable to attacks by malicious intruders, although recent safeguards provided by the run-time system and the compiler help make programs more secure. 
We have only examined the mapping of C onto IA32 and x86-64, but much of what we have covered is handled in a similar way for other combinations of language and machine. For example, compiling C++ is very similar to compiling 
C. In fact, early implementations of C++ .rst performed a source-to-source con-version from C++ to C and generated object-code by running a C compiler on the result. C++ objects are represented by structures, similar to a C struct. Methods are represented by pointers to the code implementing the methods. By contrast, Java is implemented in an entirely different fashion. The object code of Java is a special binary representation known as Java byte code. This code can be viewed as a machine-level program for a virtual machine. As its name suggests, this machine is not implemented directly in hardware. Instead, software interpreters process the byte code, simulating the behavior of the virtual machine. Alternatively, an approach known as just-in-time compilation dynamically translates byte code se-quences into machine instructions. This approach provides faster execution when code is executed multiple times, such as in loops. The advantage of using byte code as the low-level representation of a program is that the same code can be ¡°exe-cuted¡± on many different machines, whereas the machine code we have considered runs only on x86 machines. 
Bibliographic 
Notes 

Both Intel and AMD provide extensive documentation on their processors. This includes general descriptions of an assembly-language programmer¡¯s view of the hardware [2, 27], as well as detailed references about the individual instructions [3, 28, 29]. Reading the instruction descriptions is complicated by the facts that 
(1) all documentation is based on the Intel assembly-code format, (2) there are many variations for each instruction due to the different addressing and execution modes, and (3) there are no illustrative examples. Still, these remain the authori-tative references about the behavior of each instruction. 
The organization amd64.org has been responsible for de.ning the Application Binary Interface (ABI) for x86-64 code running on Linux systems [73]. This inter-face describes details for procedure linkages, binary code .les, and a number of other features that are required for machine-code programs to execute properly. 
As we have discussed, the ATT format used by gcc is very different from the Intel format used in Intel documentation and by other compilers (including the Microsoft compilers). Blum¡¯s book [9] is one of the few references based on ATT format, and it provides an extensive description of how to embed assembly code into C programs using the asm directive. 
Muchnick¡¯s book on compiler design [76] is considered the most comprehen-sive reference on code-optimization techniques. It covers many of the techniques we discuss here, such as register usage conventions and the advantages of gener-ating code for loops based on their do-while form. 
Much has been written about the use of buffer over.ow to attack systems over the Internet. Detailed analyses of the 1988 Internet worm have been published by Spafford [102] as well as by members of the team at MIT who helped stop its spread [40]. Since then a number of papers and projects have generated ways both to create and to prevent buffer over.ow attacks. Seacord¡¯s book [94] provides a wealth of information about buffer over.ow and other attacks on code generated by C compilers. 
Homework 
Problems 

3.54 ¡ô 
A function with prototype 
int decode2(int x, int y, int z); 
is compiled into IA32 assembly code. The body of the code is as follows: 

xat  %ebp+8,yat %ebp+12,  z  at  %ebp+16  
1  movl  12(%ebp),  %edx  
2  subl  16(%ebp),  %edx  
3  movl  %edx,  %eax  
4  sall  $31,  %eax  
5  sarl  $31,  %eax  
6  imull  8(%ebp),  %edx  
7  xorl  %edx,  %eax  

Parameters x, y, and z are stored at memory locations with offsets 8, 12, and 16 relative to the address in register %ebp. The code stores the return value in register %eax. 
Write C code for decode2 that will have an effect equivalent to our assembly code. 
3.55 ¡ô 

The following code computes the product of x and y and stores the result in memory. Data type ll_t is de.ned to be equivalent to long long. 
typedef long long ll_t; 
void store_prod(ll_t *dest, int x, ll_t y) { *dest = x*y; } 
gcc generates the following assembly code implementing the computation: 
dest at %ebp+8,xat %ebp+12, y at %ebp+16 1 movl 16(%ebp), %esi 2 movl 12(%ebp), %eax 3 movl %eax, %edx 4 sarl $31, %edx 5 movl 20(%ebp), %ecx 6 imull %eax, %ecx 7 movl %edx, %ebx 8 imull %esi, %ebx 9 addl %ebx, %ecx 
10 mull %esi 11 leal (%ecx,%edx), %edx 12 movl 8(%ebp), %ecx 13 movl %eax, (%ecx) 14 movl %edx, 4(%ecx) 
This code uses three multiplications to implement the multiprecision arith-metic required to implement 64-bit arithmetic on a 32-bit machine. Describe the algorithm used to compute the product, and annotate the assembly code to show how it realizes your algorithm. Hint: See Problem 3.12 and its solution. 
3.56 ¡ô¡ô 
Consider the following assembly code: 
xat%ebp+8,nat%ebp+12 1 movl 8(%ebp), %esi 2 movl 12(%ebp), %ebx 3 movl $-1, %edi 4 movl $1, %edx 5 .L2: 6 movl %edx, %eax 7 andl %esi, %eax 8 xorl %eax, %edi 9 movl %ebx, %ecx 
10 sall %cl, %edx 11 testl %edx, %edx 12 jne .L2 13 movl %edi, %eax 
The preceding code was generated by compiling C code that had the following overall form: 
1 int loop(int x, int n) 2 { 3 int result = ; 4 int mask; 5 for(mask= ;mask ;mask= ){ 6 result ^= ; 
7 } 8 return result; 
9 } 
Your task is to .ll in the missing parts of the C code to get a program equivalent to the generated assembly code. Recall that the result of the function is returned in register %eax. You will .nd it helpful to examine the assembly code before, during, and after the loop to form a consistent mapping between the registers and the program variables. 
A. Which registers hold program values x, n, result, and mask? 
B. What are the initial values of result and mask? 
C. What is the test condition for mask? 
D. How does mask get updated? 
E. How does result get updated? 
F. Fill in all the missing parts of the C code. 
3.57 ¡ô¡ô 
In Section 3.6.6, we examined the following code as a candidate for the use of conditional data transfer: We showed a trial implementation using a conditional move instruction but argued that it was not valid, since it could attempt to read from a null address. 

int  cread(int  *xp)  {  
return  (xp  ?  *xp  :  0);  
}  

Write a C function cread_alt that has the same behavior as cread, except that it can be compiled to use conditional data transfer. When compiled with the command-line option ¡®-march=i686¡¯, the generated code should use a conditional move instruction rather than one of the jump instructions. 
3.58 ¡ô¡ô 

The code that follows shows an example of branching on an enumerated type value in a switch statement. Recall that enumerated types in C are simply a way to introduce a set of names having associated integer values. By default, the values assigned to the names go from zero upward. In our code, the actions associated with the different case labels have been omitted. 
/* Enumerated type creates set of constants numbered 0 and upward */ 
typedef enum {MODE_A, MODE_B, MODE_C, MODE_D, MODE_E} mode_t; 
int switch3(int *p1, int *p2, mode_t action) 
{ 

int result = 0; 
switch(action) { 
case MODE_A: 
case MODE_B: 
case MODE_C: 
case MODE_D: 
case MODE_E: 
default: 
} 

return result; } 
The part of the generated assembly code implementing the different actions is shown in Figure 3.43. The annotations indicate the argument locations, the register values, and the case labels for the different jump destinations. Register %edx corresponds to program variable result and is initialized to .1. 
Fill in the missing parts of the C code. Watch out for cases that fall through. 
Arguments: p1 at %ebp+8, p2 at %ebp+12, action at %ebp+16 Registers: result in %edx (initialized to -1) 
The jump targets: 
1  .L17:  MODE_E  
2  movl  $17, %edx  
3  jmp  .L19  
4  .L13:  MODE_A  
5  movl  8(%ebp), %eax  
6  movl  (%eax), %edx  
7  movl  12(%ebp), %ecx  
8  movl  (%ecx), %eax  
9  movl  8(%ebp), %ecx  
10  movl  %eax, (%ecx)  
11  jmp  .L19  
12  .L14:  MODE_B  
13  movl  12(%ebp), %edx  
14  movl  (%edx), %eax  
15  movl  %eax, %edx  
16  movl  8(%ebp), %ecx  
17  addl  (%ecx), %edx  
18  movl  12(%ebp), %eax  
19  movl  %edx, (%eax)  
20  jmp  .L19  
21  .L15:  MODE_C  
22  movl  12(%ebp), %edx  
23  movl  $15, (%edx)  
24  movl  8(%ebp), %ecx  
25  movl  (%ecx), %edx  
26  jmp  .L19  
27  .L16:  MODE_D  
28  movl  8(%ebp), %edx  
29  movl  (%edx), %eax  
30  movl  12(%ebp), %ecx  
31  movl  %eax, (%ecx)  
32  movl  $17, %edx  
33  .L19:  default  
34  movl  %edx, %eax  Set return  value  
Figure 3.43 Assembly code for Problem 3.58. This code implements the different branches of a switch statement. 


3.59 ¡ô¡ô 
This problem will give you a chance to reverse engineer a switch statement from machine code. In the following procedure, the body of the switch statement has been removed: 

1 int switch_prob(int x, int n) 2 { 3 int result = x; 4 5 switch(n) { 6 7 /* Fill in code here */ 
8 } 

9 10 return result; 
11 } 

Figure 3.44 shows the disassembled machine code for the procedure. We can see in lines 4 and 5 that parameters x and n are loaded into registers %eax and %edx, respectively. 
The jump table resides in a different area of memory. We can see from the indirect jump on line 9 that the jump table begins at address 0x80485d0. Using the gdb debugger, we can examine the six 4-byte words of memory comprising the jump table with the command x/6w 0x80485d0. gdb prints the following: 
(gdb) x/6w 0x80485d0 0x80485d0: 0x08048438 0x08048448 0x08048438 0x0804843d 0x80485e0: 0x08048442 0x08048445 
Fill in the body of the switch statement with C code that will have the same behavior as the machine code. 
1  08048420  <switch_prob>:  
2  8048420:  55  push  %ebp  
3  8048421:  89  e5  mov  %esp,%ebp  
4  8048423:  8b  45  08  mov  0x8(%ebp),%eax  
5  8048426:  8b  55  0c  mov  0xc(%ebp),%edx  
6  8048429:  83  ea  32  sub  $0x32,%edx  
7  804842c:  83  fa  05  cmp  $0x5,%edx  
8  804842f:  77  17  ja  8048448  <switch_prob+0x28>  
9  8048431:  ff  24  95  d0 85 04  08 jmp  *0x80485d0(,%edx,4)  
10  8048438:  c1  e0  02  shl  $0x2,%eax  
11  804843b:  eb  0e  jmp  804844b  <switch_prob+0x2b>  
12  804843d:  c1  f8  02  sar  $0x2,%eax  
13  8048440:  eb  09  jmp  804844b  <switch_prob+0x2b>  
14  8048442:  8d  04  40  lea  (%eax,%eax,2),%eax  
15  8048445:  0f  af  c0  imul  %eax,%eax  
16  8048448:  83  c0  0a  add  $0xa,%eax  
17  804844b:  5d  pop  %ebp  
18  804844c:  c3  ret  
Figure 3.44 Disassembled code for Problem 3.59. 


3.60 ¡ô¡ô¡ô 
Consider the following source code, where R, S, and T are constants declared with #define: 
int A[R][S][T]; 
int store_ele(int i, int j, int k, int *dest) 
{ *dest = A[i][j][k]; return sizeof(A); 
} 
In compiling this program, gcc generates the following assembly code: 
iat %ebp+8,jat %ebp+12, k at %ebp+16, dest at %ebp+20 1 movl 12(%ebp), %edx 2 leal (%edx,%edx,4), %eax 3 leal (%edx,%eax,2), %eax 4 imull $99, 8(%ebp), %edx 5 addl %edx, %eax 6 addl 16(%ebp), %eax 7 movl A(,%eax,4), %edx 8 movl 20(%ebp), %eax 9 movl %edx, (%eax) 
10 movl $1980, %eax 
A. Extend Equation 3.1 from two dimensions to three to provide a formula for the location of array element A[i][j][k]. 
B. Use your reverse engineering skills to determine the values of R, S, and T based on the assembly code. 
3.61 ¡ô¡ô 
The code generated by the C compiler for var_prod_ele (Figure 3.29) cannot .t all of the values it uses in the loop in registers, and so it must retrieve the value of n from memory on each iteration. Write C code for this function that incorporates optimizations similar to those performed by gcc, but such that the compiled code does not spill any loop values into memory. 
Recall that the processor only has six registers available to hold temporary data, since registers %ebp and %esp cannot be used for this purpose. One of these registers must be used to hold the result of the multiply instruction. Hence, you must reduce the number of values in the loop from six (result, Arow, Bcol, j, n, and 4*n) to .ve. 
You will need to .nd a strategy that works for your particular compiler. Keep trying different strategies until you .nd one that works. 
3.62 ¡ô¡ô 
The following code transposes the elements of an M ¡Á M array, where M is a constant de.ned by #define: 

void transpose(Marray_t A) { int i, j; for (i=0;i<M; i++) 
for (j=0;j<i; j++) { int t = A[i][j]; A[i][j] = A[j][i]; A[j][i] = t; 
} } 

When compiled with optimization level -O2, gcc generates the following code for the inner loop of the function: 
1 .L3: 2 movl (%ebx), %eax 3 movl (%esi,%ecx,4), %edx 4 movl %eax, (%esi,%ecx,4) 5 addl $1, %ecx 6 movl %edx, (%ebx) 7 addl $52, %ebx 8 cmpl %edi, %ecx 9 jl .L3 
A. What is the value of M? 
B. What registers hold program values i and j ? 
C. Write a C code version of transpose that makes use of the optimizations that occur in this loop. Use the parameter M in your code rather than numeric constants. 
3.63 ¡ô¡ô 

Consider the following source code, where E1 and E2 are macro expressions de-clared with #define that compute the dimensions of array A in terms of parameter 
n. This code computes the sum of the elements of column j of the array. 
1 int sum_col(int n, int A[E1(n)][E2(n)], int j) { 2 int i; 3 int result = 0; 4 for (i = 0; i < E1(n); i++) 5 result += A[i][j]; 6 return result; 
7 } 

In compiling this program, gcc generates the following assembly code: 
nat %ebp+8,Aat %ebp+12, j at %ebp+16 1 movl 8(%ebp), %eax 2 leal (%eax,%eax), %edx 
3  leal  (%edx,%eax), %ecx  
4  movl  %edx, %ebx  
5  leal  1(%edx), %eax  
6  movl  $0, %edx  
7  testl  %eax, %eax  
8  jle  .L3  
9  leal  0(,%ecx,4), %esi  
10  movl  16(%ebp), %edx  
11  movl  12(%ebp), %ecx  
12  leal  (%ecx,%edx,4),  %eax  
13  movl  $0, %edx  
14  movl  $1, %ecx  
15  addl  $2, %ebx  
16  .L4:  
17  addl  (%eax), %edx  
18  addl  $1, %ecx  
19  addl  %esi, %eax  
20  cmpl  %ebx, %ecx  
21  jne  .L4  
22  .L3:  
23  movl  %edx, %eax  

Use your reverse engineering skills to determine the de.nitions of E1 and E2. 
3.64 ¡ô¡ô 
For this exercise, we will examine the code generated by gcc for functions that have structures as arguments and return values, and from this see how these language features are typically implemented. 
The following C code has a function word_sum having structures as argument and return values, and a function prod that calls word_sum: 
typedef struct { int a; int *p; 
} str1; 
typedef struct { int sum; int diff; 
} str2; 
str2 word_sum(str1 s1) { str2 result; result.sum = s1.a + *s1.p; result.diff = s1.a -*s1.p; 

return result; } 
int prod(int x, int y) 
{ 
str1 s1; 
str2 s2; 
s1.a = x; 
s1.p = &y; 

s2 = word_sum(s1); 
return s2.sum * s2.diff; } 
gcc generates the following code for these two functions: 
1 prod: 
2 pushl %ebp1 word_sum: 3 movl %esp, %ebp2 pushl %ebp 

4 subl $20, %esp 4 pushl %ebx 
3 movl %esp, %ebp 
5 leal 12(%ebp), %edx 5 movl 8(%ebp), %eax 
6 leal -8(%ebp), %ecx 6 movl 12(%ebp), %ebx 
7 movl 8(%ebp), %eax 7 movl 16(%ebp), %edx 
8 movl %eax, 4(%esp) 8 movl (%edx), %edx 
9 movl %edx, 8(%esp) 9 movl %ebx, %ecx 
10 movl %ecx, (%esp) 
11 call word_sum10 subl %edx, %ecx 12 subl $4, %esp11 movl %ecx, 4(%eax) 13 movl -4(%ebp), %eax12 addl %ebx, %edx 14 imull -8(%ebp), %eax13 movl %edx, (%eax) 15 leave14 popl %ebx 16 ret15 popl %ebp 

16 ret $4 

The instruction ret $4 is like a normal return instruction, but it increments the stack pointer by 8 (4 for the return address plus 4 additional), rather than 4. 
A. We can see in lines 5¨C7 of the code for word_sum that it appears as if three values are being retrieved from the stack, even though the function has only a single argument. Describe what these three values are. 
B. We can see in line 4 of the code for prod that 20 bytes are allocated in the stack frame. These get used as .ve .elds of 4 bytes each. Describe how each of these .elds gets used. 
C. How would you describe the general strategy for passing structures as argu-ments to a function? 
D. How would you describe the general strategy for handling a structure as a return value from a function? 
3.65 ¡ô¡ô¡ô 
In the following code, A and B are constants de.ned with #define: 
typedef struct { short x[A][B]; /* Unknown constants A and B */ int y; 
} str1; 
typedef struct { char array[B]; int t; short s[B]; int u; 
} str2; 
void setVal(str1 *p, str2 *q) { int v1 = q->t; int v2 = q->u; p->y = v1+v2; 
} 
gcc generates the following code for the body of setVal: 
1 movl 12(%ebp), %eax 2 movl 36(%eax), %edx 3 addl 12(%eax), %edx 4 movl 8(%ebp), %eax 5 movl %edx, 92(%eax) 
What are the values of A and B? (The solution is unique.) 
3.66 ¡ô¡ô¡ô 
You are charged with maintaining a large C program, and you come across the following code: 
1 typedef struct { 2 int left; 3 a_struct a[CNT]; 4 int right; 
5 } b_struct; 6 7 void test(int i, b_struct *bp) 8 { 9 int n = bp->left + bp->right; 
10 a_struct *ap = &bp->a[i]; 11 ap->x[ap->idx] = n; 
12 } 

1 00000000 <test>: 2 0: 55 push %ebp 3 1: 89 e5 mov %esp,%ebp 4 3: 8b 45 08 mov 0x8(%ebp),%eax 5 6: 8b 4d 0c mov 0xc(%ebp),%ecx 6 9: 8d 04 80 lea (%eax,%eax,4),%eax 7 c: 03 44 81 04 add 0x4(%ecx,%eax,4),%eax 
8 10: 8b 91 b8 00 00 00 mov 0xb8(%ecx),%edx 
9 16: 03 11 add (%ecx),%edx 10 18: 89 54 81 08 mov %edx,0x8(%ecx,%eax,4) 11 1c: 5d pop %ebp 12 1d: c3 ret 
Figure 3.45 Disassembled code for Problem 3.66. 
The declarations of the compile-time constant CNT and the structure a_struct are in a .le for which you do not have the necessary access privilege. Fortunately, you have a copy of the ¡®.o¡¯ version of code, which you are able to disassemble with the objdump program, yielding the disassembly shown in Figure 3.45. 
Using your reverse engineering skills, deduce the following. 
A. The value of CNT. 
B. A complete declaration of structure a_struct. Assume that the only .elds in this structure are idx and x. 
3.67 ¡ô¡ô¡ô 

Consider the following union declaration: 
union ele { 
struct { int *p; int y; 
} e1; 

struct { int x; union ele *next; 
} e2; }; 

This declaration illustrates that structures can be embedded within unions. The following procedure (with some expressions omitted) operates on a linked list having these unions as list elements: 
void proc (union ele *up) { up-> = *(up-> ) -up-> ; } 
A. What would be the offsets (in bytes) of the following .elds: e1.p: e1.y: e2.x: e2.next: 
B. How many total bytes would the structure require? 
C. The compiler generates the following assembly code for the body of proc: 
up at %ebp+8 1 movl 8(%ebp), %edx 2 movl 4(%edx), %ecx 3 movl (%ecx), %eax 4 movl (%eax), %eax 5 subl (%edx), %eax 6 movl %eax, 4(%ecx) 
On the basis of this information, .ll in the missing expressions in the code for proc. Hint: Some union references can have ambiguous interpretations. These ambiguities get resolved as you see where the references lead. There is only one answer that does not perform any casting and does not violate any type constraints. 
3.68 ¡ô 
Write a function good_echo that reads a line from standard input and writes it to standard output. Your implementation should work for an input line of arbitrary length. You may use the library function fgets, but you must make sure your function works correctly even when the input line requires more space than you have allocated for your buffer. Your code should also check for error conditions and return when one is encountered. Refer to the de.nitions of the standard I/O functions for documentation [48, 58]. 
3.69 ¡ô 
The following declaration de.nes a class of structures for use in constructing binary trees: 
1 typedef struct ELE *tree_ptr; 2 3 struct ELE { 4 long val; 5 tree_ptr left; 6 tree_ptr right; 7 }; 

For a function with the following prototype: 
long trace(tree_ptr tp); 
gcc generates the following x86-64 code: 
1 trace: 

tp in %rdi 2 movl $0, %eax 3 testq %rdi, %rdi 4 je .L3 5 .L5: 6 movq (%rdi), %rax 7 movq 16(%rdi), %rdi 8 testq %rdi, %rdi 9 jne .L5 
10 .L3: 11 rep 12 ret 

A. Generate a C version of the function, using a while loop. 
B. Explain in English what this function computes. 
3.70 ¡ô¡ô 

Using the same tree data structure we saw in Problem 3.69, and a function with the prototype 
long traverse(tree_ptr tp); 
gcc generates the following x86-64 code: 
1 traverse: 

tp in %rdi 2 movq %rbx, -24(%rsp) 3 movq %rbp, -16(%rsp) 4 movq %r12, -8(%rsp) 5 subq $24, %rsp 6 movq %rdi, %rbp 7 movabsq $-9223372036854775808, %rax 8 testq %rdi, %rdi 9 je .L9 
10 movq (%rdi), %rbx 11 movq 8(%rdi), %rdi 12 call traverse 13 movq %rax, %r12 14 movq 16(%rbp), %rdi 15 call traverse 16 cmpq %rax, %r12 17 cmovge %r12, %rax 18 cmpq %rbx, %rax 19 cmovl %rbx, %rax 20 .L9: 21 movq (%rsp), %rbx 22 movq 8(%rsp), %rbp 23 movq 16(%rsp), %r12 24 addq $24, %rsp 25 ret 
A. Generate a C version of the function. 
B. Explain in English what this function computes. 
Solutions 
to 
Practice 
Problems 

Solution to Problem 3.1 (page 170) 
This exercise gives you practice with the different operand forms. 
Operand Value Comment 
%eax  0x100  Register  
0x104  0xAB  Absolute address  
$0x108  0x108  Immediate  
(%eax)  0xFF  Address 0x100  
4(%eax)  0xAB  Address 0x104  
9(%eax,%edx)  0x11  Address 0x10C  
260(%ecx,%edx)  0x13  Address 0x108  
0xFC(,%ecx,4)  0xFF  Address 0x100  
(%eax,%edx,4)  0x11  Address 0x10C  

Solution to Problem 3.2 (page 174) 
As we have seen, the assembly code generated by gcc includes suf.xes on the instructions, while the disassembler does not. Being able to switch between these two forms is an important skill to learn. One important feature is that memory references in IA32 are always given with double-word registers, such as %eax, even if the operand is a byte or single word. 
Here is the code written with suf.xes: 
1 movl %eax, (%esp) 2 movw (%eax), %dx 3 movb $0xFF, %bl 4 movb (%esp,%edx,4), %dh 5 pushl $0xFF 6 movw %dx, (%eax) 7 popl %edi 

Solution to Problem 3.3 (page 174) 
Since we will rely on gcc to generate most of our assembly code, being able to write correct assembly code is not a critical skill. Nonetheless, this exercise will help you become more familiar with the different instruction and operand types. 
Here is the code with explanations of the errors: 
1 movb $0xF, (%bl) Cannot use %bl as address register 2 movl %ax, (%esp) Mismatch between instruction suffix and register ID 3 movw (%eax),4(%esp) Cannot have both source and destination be memory references 4 movb %ah,%sh No register named %sh 5 movl %eax,$0x123 Cannot have immediate as destination 6 movl %eax,%dx Destination operand incorrect size 7 movb %si, 8(%ebp) Mismatch between instruction suffix and register ID 
Solution to Problem 3.4 (page 176) 
This exercise gives you more experience with the different data movement instruc-tions and how they relate to the data types and conversion rules of C. 
src_t dest_t Instruction 
int int movl %eax, (%edx) char int movsbl %al, (%edx) char unsigned movsbl %al, (%edx) unsigned char int movzbl %al, (%edx) int char movb %al, (%edx) unsigned unsigned char movb %al, (%edx) unsigned int movl %eax, (%edx) 
Solution to Problem 3.5 (page 176) 
Reverse engineering is a good way to understand systems. In this case, we want to reverse the effect of the C compiler to determine what C code gave rise to this assembly code. The best way is to run a ¡°simulation,¡± starting with values x, y, and z at the locations designated by pointers xp, yp, and zp, respectively. We would then get the following behavior: From this, we can generate the following C code: 
xp  at  %ebp+8,  yp  at  %ebp+12,  zp  at  %ebp+16  
1  movl  8(%ebp),  %edi  Get  xp  
2  movl  12(%ebp),  %edx  Get  yp  
3  movl  16(%ebp),  %ecx  Get  zp  
4  movl  (%edx),  %ebx  Get  y  
5  movl  (%ecx),  %esi  Get  z  
6  movl  (%edi),  %eax  Get  x  
7  movl  %eax,  (%edx)  Store  x  at  yp  
8  movl  %ebx,  (%ecx)  Store  y  at  zp  
9  movl  %esi,  (%edi)  Store  z  at  xp  

void decode1(int *xp, int *yp, int *zp) 
{ int tx = *xp; int ty = *yp; int tz = *zp; 
*yp = tx; *zp = ty; *xp = tz; 
} 
Solution to Problem 3.6 (page 178) 
This exercise demonstrates the versatility of the leal instruction and gives you more practice in deciphering the different operand forms. Although the operand forms are classi.ed as type ¡°Memory¡± in Figure 3.3, no memory access occurs. 
Instruction Result 
leal 6(%eax), %edx 6 + x leal (%eax,%ecx), %edx x + y leal (%eax,%ecx,4), %edx x + 4y leal 7(%eax,%eax,8), %edx 7 + 9x leal 0xA(,%ecx,4), %edx 10 + 4y leal 9(%eax,%ecx,2), %edx 9 + x + 2y 
Solution to Problem 3.7 (page 179) 
This problem gives you a chance to test your understanding of operands and the arithmetic instructions. The instruction sequence is designed so that the result of each instruction does not affect the behavior of subsequent ones. 
Instruction Destination Value 
addl %ecx,(%eax)  0x100  0x100  
subl %edx,4(%eax)  0x104  0xA8  
imull $16,(%eax,%edx,4)  0x10C  0x110  
incl 8(%eax)  0x108  0x14  
decl %ecx  %ecx  0x0  
subl %edx,%eax  %eax  0xFD  

Solution to Problem 3.8 (page 180) 
This exercise gives you a chance to generate a little bit of assembly code. The solution code was generated by gcc. By loading parameter n in register %ecx,it can then use byte register %cl to specify the shift amount for the sarl instruction: 

1 movl 8(%ebp), %eax Get x 2 sall $2, %eax x <<= 2 3 movl 12(%ebp), %ecx Get n 4 sarl %cl, %eax x >>= n 
Solution to Problem 3.9 (page 181) 
This problem is fairly straightforward, since each of the expressions is imple-mented by a single instruction and there is no reordering of the expressions. 
5 int t1 = x^y; 6 int t2=t1>>3; 7 int t3 = ~t2; 8 int t4 = t3-z; 
Solution to Problem 3.10 (page 182) 
A. This instruction is used to set register %edx to zero, exploiting the property that x ^ x = 0 for any x. It corresponds to the C statement x=0. 
B. A more direct way of setting register %edx to zero is with the instruction movl $0,%edx. 
C. Assembling and disassembling this code, however, we .nd that the version with xorl requires only 2 bytes, while the version with movl requires 5. 
Solution to Problem 3.11 (page 184) 
We can simply replace the cltd instruction with one that sets register %edx to 0, and use divl rather than idivl as our division instruction, yielding the following code: 
xat %ebp+8,yat %ebp+12 movl 8(%ebp),%eax Load x into %eax movl $0,%edx Set high-order bits to 0 divl 12(%ebp) Unsigned divide by y movl %eax, 4(%esp) Storex/y movl %edx, (%esp) Storex%y 
Solution to Problem 3.12 (page 184) 
A. We can see that the program is performing multiprecision operations on 64-bit data. We can also see that the 64-bit multiply operation (line 4) uses unsigned arithmetic, and so we conclude that num_t is unsigned long long. 
B. Let x denote the value of variable x, and let y denote the value of y, which . 232 + yl
we can write as y = yh , where yh and yl are the values represented by the high-and low-order 32 bits, respectively. We can therefore compute 
x.y = x .yh . 232 + x .yl. The full representation of the product would be 96 bits long, but we require only the low-order 64 bits. We can therefore let s be the low-order 32 bits of x .yh and t be the full 64-bit product x .yl, which we can split into high-and low-order parts th and tl. The .nal result has tl as the low-order part, and s + th as the high-order part. 
Here is the annotated assembly code: 
dest at %ebp+8,xat %ebp+12, y at %ebp+16 1 movl 12(%ebp), %eax Get x 2 movl 20(%ebp), %ecx Get y_h 3 imull %eax, %ecx Compute s = x*y_h 4 mull 16(%ebp) Compute t = x*y_l 5 leal (%ecx,%edx), %edx Add s to t_h 6 movl 8(%ebp), %ecx Get dest 7 movl %eax, (%ecx) Store t_l 8 movl %edx, 4(%ecx) Store s+t_h 
Solution to Problem 3.13 (page 188) 
It is important to understand that assembly code does not keep track of the type of a program value. Instead, the different instructions determine the operand sizes and whether they are signed or unsigned. When mapping from instruction sequences back to C code, we must do a bit of detective work to infer the data types of the program values. 
A. The suf.x ¡®l¡¯ and the register identi.ers indicate 32-bit operands, while the comparison is for a two¡¯s complement ¡®<¡¯. We can infer that data_t must be int. 
B. The suf.x ¡®w¡¯ and the register identi.ers indicate 16-bit operands, while the comparison is for a two¡¯s-complement ¡®>=¡¯. We can infer that data_t must be short. 
C. The suf.x ¡®b¡¯ and the register identi.ers indicate 8-bit operands, while the comparison is for an unsigned ¡®<¡¯. We can infer that data_t must be un-signed char. 
D. The suf.x ¡®l¡¯ and the register identi.ers indicate 32-bit operands, while the comparison is for ¡®!=¡¯, which is the same whether the arguments are signed, unsigned, or pointers. We can infer that data_t could be either int, unsigned, or some form of pointer. For the .rst two cases, they could also have the long size designator. 
Solution to Problem 3.14 (page 189) 
This problem is similar to Problem 3.13, except that it involves test instructions rather than cmp instructions. 
A. The suf.x ¡®l¡¯ and the register identi.ers indicate 32-bit operands, while the comparison is for ¡®!=¡¯, which is the same for signed or unsigned. We can infer that data_t must be either int, unsigned, or some type of pointer. For the .rst two cases, they could also have the long size designator. 
B. The suf.x ¡®w¡¯ and the register identi.er indicate 16-bit operands, while the comparison is for ¡®==¡¯, which is the same for signed or unsigned. We can infer that data_t must be either short or unsigned short. 

C. The suf.x ¡®b¡¯ and the register identi.er indicate an 8-bit operand, while the comparison is for two¡¯s complement ¡®>¡¯. We can infer that data_t must be char. 
D. The suf.x ¡®w¡¯ and the register identi.er indicate 16-bit operands, while the comparison is for unsigned ¡®>¡¯. We can infer that data_t must be unsigned short. 
Solution to Problem 3.15 (page 192) 
This exercise requires you to examine disassembled code in detail and reason about the encodings for jump targets. It also gives you practice in hexadecimal arithmetic. 
A. The je instruction has as target 0x8048291 + 0x05. As the original disas-sembled code shows, this is 0x8048296: 
804828f: 74 05 je 8048296 8048291: e8 1e 00 00 00 call 80482b4 
B. The jb instruction has as target 0x8048359 . 25 (since 0xe7 is the 1-byte, two¡¯s-complement representation of .25). As the original disassembled code shows, this is 0x8048340: 
8048357: 72 e7 jb 8048340 8048359: c6 05 10 a0 04 08 01 movb $0x1,0x804a010 
C. According to the annotation produced by the disassembler, the jump target is at absolute address 0x8048391. According to the byte encoding, this must be at an address 0x12 bytes beyond that of the mov instruction. Subtracting these gives address 0x804837f, as con.rmed by the disassembled code: 
804837d: 74 12 je 8048391 804837f: b8 00 00 00 00 mov $0x0,%eax 
D. Reading the bytes in reverse order, we see that the target offset is 0xffffffe0, or decimal .32. Adding this to 0x80482c4 (the address of the nop instruction) gives address 0x80482a4: 
80482bf: e9 e0 ff ff ff jmp 80482a4 80482c4: 90 nop 
E. An indirect jump is denoted by instruction code ff 25. The address from which the jump target is to be read is encoded explicitly by the following 4 bytes. Since the machine is little endian, these are given in reverse order as fc9f 04 08. 
Solution to Problem 3.16 (page 195) 
Annotating assembly code and writing C code that mimics its control .ow are good .rst steps in understanding assembly-language programs. This problem gives you practice for an example with simple control .ow. It also gives you a chance to examine the implementation of logical operations. 
A. Here is the C code: 
1 void goto_cond(int a, int *p) { 
2 if (p==0) 
3 goto done; 
4 if (a<=0) 
5 goto done; 
6 *p +=a; 
7 done: 
8 return; 
9 } 
B. The .rst conditional branch is part of the implementation of the && ex-pression. If the test for p being non-null fails, the code will skip the test of a>0. 
Solution to Problem 3.17 (page 196) 
This is an exercise to help you think about the idea of a general translation rule and how to apply it. 
A. Converting to this alternate form involves only switching around a few lines of the code: 
1 int gotodiff_alt(int x, int y) { 
2 int result; 
3 if (x< y) 
4 goto true; 
5 result=x-y; 
6 goto done; 
7 true: 
8 result= y-x; 
9 done: 
10 return result; 
11 } 
B. In most respects, the choice is arbitrary. But the original rule works better for the common case where there is no else statement. For this case, we can simply modify the translation rule to be as follows: 
t= test-expr; if (!t) goto done; 
then-statement 
done: 
A translation based on the alternate rule is more cumbersome. 
Solution to Problem 3.18 (page 196) 
This problem requires that you work through a nested branch structure, where you will see how our rule for translating if statements has been applied. For the most part, the machine code is a straightforward translation of the C code. The only difference is that the initialization expression (line 2 in the C code) has been moved down (line 15 in the assembly code) so that it only gets computed when it is certain that this will be the returned value. 

1 int test(int x, int y) { 2 int val = x^y; 3 if (x < -3){ 4 if(y <x) 5 val = x*y; 6 else 7 val = x+y; 
8 } else if (x > 2) 
9 val = x-y; 10 return val; 
11 } 

Solution to Problem 3.19 (page 198) 
A. If we build up a table of factorials computed with data type int, we get the following: 
nn! OK? 
1  1  Y  
2  2  Y  
3  6  Y  
4  24  Y  
5  120  Y  
6  720  Y  
7  5,040  Y  
8  40,320  Y  
9  362,880  Y  
10  3,628,800  Y  
11  39,916,800  Y  
12  479,001,600  Y  
13  1,932,053,504  Y  
14  1,278,945,280  N  

We can see that 14! has over.owed, since the numbers stopped growing. As we learned in Problem 2.35, we can also test whether or not the computation of n!has over.owed by computing n!/n and seeing whether it equals (n . 1)!. 
B. Doing the computation with data type long long lets us go up to 20!, yielding 2,432,902,008,176,640,000. 
Solution to Problem 3.20 (page 199) 
The code generated when compiling loops can be tricky to analyze, because the compiler can perform many different optimizations on loop code, and because it can be dif.cult to match program variables with registers. We start practicing this skill with a fairly simple loop. 
A. The register usage can be determined by simply looking at how the argu-ments get fetched. 
Register usage 
Register Variable Initially 
%eax x x %ecx y y %edx n n 
B. The body-statement portion consists of lines 3 through 5 in the C code and lines 5 through 7 in the assembly code. The test-expr portion is on line 6 in the C code. In the assembly code, it is implemented by the instructions on lines 8 through 11. 
C. The annotated code is as follows: 
xat %ebp+8,yat %ebp+12, n at %ebp+16 
1 movl 8(%ebp), %eax Get x 
2 movl 12(%ebp), %ecx Get y 
3 movl 16(%ebp), %edx Get n 
4 .L2: loop: 
5 addl %edx, %eax x+=n 
6 imull %edx, %ecx y*=n 
7 subl $1, %edx n--
8 testl %edx, %edx Test n 
9 jle .L5 If <= 0, goto done 
10 cmpl %edx, %ecx Compare y:n 11 jl .L2 If <, goto loop 12 .L5: done: 
As with the code of Problem 3.16, two conditional branches are required to implement the && operation. 
Solution to Problem 3.21 (page 201) 
This problem demonstrates how the transformations made by the compiler can make it dif.cult to decipher the generated assembly code. 
A. We can see that the register is initialized to a + b and then incremented on each iteration. Similarly, the value of a (held in register %ecx) is incremented on each iteration. We can therefore see that the value in register %edx will always equal a + b. Let us call this apb (for ¡°a plus b¡±). 

B. Here is a table of register usage: Register Program value Initial value 
%ecx a a %ebx b b %eax result 1 %edx apb a + b 
C. The annotated code is as follows: 
Arguments: a at %ebp+8,bat %ebp+12 
Registers: a in %ecx, b in %ebx, result in %eax, %edx set to apb (a+b) 
1 movl 8(%ebp), %ecx Get a 
2 movl 12(%ebp), %ebx Get b 
3 movl $1, %eax Set result = 1 
4 cmpl %ebx, %ecx Compare a:b 
5 jge .L11 If >=, goto done 
6 leal (%ebx,%ecx), %edx Compute apb = a+b 
7 movl $1, %eax Set result = 1 
8 .L12: loop: 
9 imull %edx, %eax Compute result *= apb 
10 addl $1, %ecx Compute a++ 11 addl $1, %edx Compute apb++ 12 cmpl %ecx, %ebx Compare b:a 13 jg .L12 If >, goto loop 14 .L11: done: 
Return result 
D. The equivalent goto code is as follows: 
1 int loop_while_goto(int a, int b) 
2 { 

3 int result = 1; 
4 if (a >= b) 
5 goto done; 
6 /* apb has same value as a+b in original code */ 
7 int apb = a+b; 
8 loop: 

9 result *= apb; 
10 a++; 
11 apb++; 
12 if (b > a) 
13 goto loop; 
14 done: 

15 return result; 
16 } 

Solution to Problem 3.22 (page 202) 
Being able to work backward from assembly code to C code is a prime example of reverse engineering. 
A. Here is the original C code: 
int fun_a(unsigned x) { int val=0; while (x) { 
val ^= x; 
x >>= 1; } return val & 0x1; 
} 
B. This code computes the parity of argument x. That is, it returns 1 if there is an odd number of ones in x and 0 if there is an even number. 
Solution to Problem 3.23 (page 205) 
This problem is trickier than Problem 3.22, since the code within the loop is more complex and the overall operation is less familiar. 
A. Here is the original C code: 
int fun_b(unsigned x) { int val=0; int i; for (i= 0; i< 32;i++){ 
val=(val<< 1) | (x & 0x1); 
x >>= 1; } return val; 
} 
B. This code reverses the bits in x, creating a mirror image. It does this by shifting the bits of x from left to right, and then .lling these bits in as it shifts val from right to left. 
Solution to Problem 3.24 (page 206) 
Our stated rule for translating a for loop into a while loop is just a bit too simplistic¡ªthis is the only aspect that requires special consideration. 
A. Applying our translation rule would yield the following code: 
/* Naive translation of for loop into while loop */ /* WARNING: This is buggy code */ 
int sum=0; inti=0; while (i < 10) { 

if(i &1) 
/* This will cause an infinite loop */ 

continue; sum += i; i++; 
} 

This code has an in.nite loop, since the continue statement would prevent index variable i from being updated. 
B. The general solution is to replace the continue statement with a goto statement that skips the rest of the loop body and goes directly to the update portion: 
/* Correct translation of for loop into while loop */ 
intsum = 0; inti=0; while (i < 10) { 
if(i &1) goto update; sum += i; update: i++; } 
Solution to Problem 3.25 (page 209) 
This problem reinforces our method of computing the misprediction penalty. 
A. We can apply our formula directly to get TMP = 2(31 . 16) = 30. 
B. When misprediction occurs, the function will require around 16 + 30 = 46 cycles. 
Solution to Problem 3.26 (page 212) 
This problem provides a chance to study the use of conditional moves. 
A. The operator is ¡®/¡¯. We see this is an example of dividing by a power of 2 by right shifting (see Section 2.3.7). Before shifting by k = 2, we must add a bias of 2k . 1 = 3 when the dividend is negative. 
B. Here is an annotated version of the assembly code: 
Computation by function arith 
Register: x in %edx 1 leal 3(%edx), %eax temp = x+3 2 testl %edx, %edx Test x 3 cmovns %edx, %eax If >= 0, temp = x 4 sarl $2, %eax Return temp >> 2 (= x/4) 
The program creates a temporary value equal to x + 3, in anticipation of x being negative and therefore requiring biasing. The cmovns instruction conditionally changes this number to x when x ¡Ý 0, and then it is shifted by 2 to generate x/4. 
Solution to Problem 3.27 (page 212) 
This problem is similar to Problem 3.18, except that some of the conditionals have been implemented by conditional data transfers. Although it might seem daunting to .t this code into the framework of the original C code, you will .nd that it follows the translation rules fairly closely. 
1 int test(int x, int y) { 2 int val = 4*x; 3 if (y>0){ 4 if (x < y) 5 val = x-y; 6 else 7 val = x^y; 
8 } else if (y < -2) 
9 val = x+y; 10 return val; 
11 } 
Solution to Problem 3.28 (page 217) 
This problem gives you a chance to reason about the control .ow of a switch statement. Answering the questions requires you to combine information from several places in the assembly code. 
1. 
Line 2 of the assembly code adds 2 to x to set the lower range of the cases to zero. That means that the minimum case label is .2. 

2. 
Lines 3 and 4 cause the program to jump to the default case when the adjusted case value is greater than 6. This implies that the maximum case label is .2 + 6 = 4. 

3. 
In the jump table, we see that the entry on line 3 (case value .1) has the same destination (.L2) as the jump instruction on line 4, indicating the default case behavior. Thus, case label .1 is missing in the switch statement body. 

4. 
In the jump table, we see that the entries on lines 6 and 7 have the same destination. These correspond to case labels 2 and 3. 


From this reasoning, we draw the following two conclusions: 
A. The case labels in the switch statement body had values .2, 0, 1, 2, 3, and 4. 
B. The case with destination .L6 had labels 2 and 3. 
Solution to Problem 3.29 (page 218) 
The key to reverse engineering compiled switch statements is to combine the information from the assembly code and the jump table to sort out the different cases. We can see from the ja instruction (line 3) that the code for the default case has label .L2. We can see that the only other repeated label in the jump table is .L4, and so this must be the code for the cases C and D. We can see that the code falls through at line 14, and so label .L6 must match case A and label .L3 must match case B. That leaves only label .L2 to match case E. 

The original C code is as follows. Observe how the compiler optimized the case where a equals 4 by setting the return value to be 4, rather than a. 
1 int switcher(int a, int b, int c) 2 { 3 int answer; 4 switch(a) { 6 case 5: 7 c=b^15; 8 /* Fall through */ 9 case 0: 
10 answer=c+ 112; 11 break; 12 case 2: 13 case 7: 14 answer= (c + b) << 2; 15 break; 16 case 4: 17 answer = a; /* equivalently, answer=4*/ 18 break; 19 default: 20 answer = b; 
21 } 22 return answer; 
23 } 

Solution to Problem 3.30 (page 223) 
This is another example of an assembly-code idiom. At .rst it seems quite peculiar¡ªa call instruction with no matching ret. Then we realize that it is not really a procedure call after all. 
A. %eax is set to the address of the popl instruction. 
B. This is not a true procedure call, since the control follows the same ordering as the instructions and the return address is popped from the stack. 
C. This is the only way in IA32 to get the value of the program counter into an integer register. 
Solution to Problem 3.31 (page 224) 
This problem makes concrete the discussion of register usage conventions. Reg-isters %edi, %esi, and %ebx are callee-save. The procedure must save them on the stack before altering their values and restore them before returning. The other three registers are caller-save. They can be altered without affecting the behavior of the caller. 
Solution to Problem 3.32 (page 228) 
One step in learning to read IA32 code is to become very familiar with the way arguments are passed on the stack. The key to solving this problem is to note that the storage of d at p is implemented by the instruction at line 3 of the assembly code, from which you work backward to determine the types and positions of arguments d and p. Similarly, the subtraction is performed at line 6, and from this you can work backward to determine the types and positions of arguments x and c. 
The following is the function prototype: 
int fun(short c, char d, int *p, int x); 
As this example shows, reverse engineering is like solving a puzzle. It¡¯s important to identify the points where there is a unique choice, and then work around these points to .ll in the rest of the details. 
Solution to Problem 3.33 (page 228) 
Being able to reason about how functions use the stack is a critical part of under-standing compiler-generated code. As this example illustrates, the compiler may allocate a signi.cant amount of space that never gets used. 
A. We started with %esp having value 0x800040.The pushl instruction on line 2 decrements the stack pointer by 4, giving 0x80003C, and this becomes the new value of %ebp. 
B. Line 4 decrements the stack pointer by 40 (hex 0x28), yielding 0x800014. 
C. We can see how the two leal instructions (lines 5 and 7) compute the arguments to pass to scanf, while the two movl instructions (lines 6 and 8) store them on the stack. Since the function arguments appear on the stack at increasingly positive offsets from %esp, we can conclude that line 5 computes &x, while line 7 computes line &y. These have values 0x800038 and 0x800034, respectively. 
D. The stack frame has the following structure and contents: 
0x80003C 
0x800060  
0x53  
0x46  





0x800038  
0x800034  
0x300070  

%ebp 0x800038 

x 0x800034 
y 0x800030 0x80002C 0x800028 0x800024 0x800020 0x80001C 0x800018 0x800014 

%esp 
E. Byte addresses 0x800020 through 0x800033 are unused. 

Solution to Problem 3.34 (page 231) 
This problem provides a chance to examine the code for a recursive function. An important lesson to learn is that recursive code has the exact same structure as the other functions we have seen. The stack and register-saving disciplines suf.ce to make recursive functions operate correctly. 
A. Register %ebx holds the value of parameter x, so that it can be used to compute the result expression. 
B. The assembly code was generated from the following C code: 
int rfun(unsigned x) { if (x==0) 
return 0; unsigned nx = x>>1; int rv = rfun(nx); return (x & 0x1) + rv; 
} 

C. Like the code of Problem 3.49, this function computes the sum of the bits in argument x. It recursively computes the sum of all but the least signi.cant bit, and then it adds the least signi.cant bit to get the result. 
Solution to Problem 3.35 (page 233) 
This exercise tests your understanding of data sizes and array indexing. Observe that a pointer of any kind is 4 bytes long. For IA32, gcc allocates 12 bytes for data type long double, even though the actual format requires only 10 bytes. 
Array Element size Total size Start address Element i 
S 2 14 xS xS + 2i T 4 12 xT xT + 4i U 4 24 xU xU + 4i V 12 96 xV xV + 12i W 4 16 xW xW + 4i 
Solution to Problem 3.36 (page 234) 
This problem is a variant of the one shown for integer array E. It is important to understand the difference between a pointer and the object being pointed to. Since data type short requires 2 bytes, all of the array indices are scaled by a factor of 2. Rather than using movl, as before, we now use movw. 
Expression Type Value Assembly 
S+1 short * xS + 2 leal 2(%edx),%eax S[3] short M[xS + 6] movw 6(%edx),%ax &S[i] short * xS + 2i leal (%edx,%ecx,2),%eax S[4*i+1] short M[xS + 8i + 2] movw 2(%edx,%ecx,8),%ax S+i-5 short * xS + 2i . 10 leal -10(%edx,%ecx,2),%eax 
Solution to Problem 3.37 (page 236) 
This problem requires you to work through the scaling operations to determine the address computations, and to apply Equation 3.1 for row-major indexing. The .rst step is to annotate the assembly code to determine how the address references are computed: 
1 movl 8(%ebp), %ecx Get i 2 movl 12(%ebp), %edx Get j 3 leal 0(,%ecx,8), %eax 8*i 4 subl %ecx, %eax 8*i-i = 7*i 5 addl %edx, %eax 7*i+j 6 leal (%edx,%edx,4), %edx 5*j 7 addl %ecx, %edx 5*j+i 8 movl mat1(,%eax,4), %eax mat1[7*i+j] 9 addl mat2(,%edx,4), %eax mat2[5*j+i] 
We can see that the reference to matrix mat1 is at byte offset 4(7i + j), while the reference to matrix mat2 is at byte offset 4(5j + i). From this, we can determine that mat1 has 7 columns, while mat2 has 5, giving M = 5 and N = 7. 
Solution to Problem 3.38 (page 238) 
This exercise requires that you be able to study compiler-generated assembly code to understand what optimizations have been performed. In this case, the compiler was clever in its optimizations. 
Let us .rst study the following C code, and then see how it is derived from the assembly code generated for the original function. 
1 /* Set all diagonal elements to val */ 2 void fix_set_diag_opt(fix_matrix A, int val) { 3 int *Abase = &A[0][0]; 4 int index = 0; 5 do { 6 Abase[index] = val; 7 index += (N+1); 
8 } while (index != (N+1)*N); 
9 } 
This function introduces a variable Abase, of type int *, pointing to the start of array A. This pointer designates a sequence of 4-byte integers consisting of elements of A in row-major order. We introduce an integer variable index that steps through the diagonal elements of A, with the property that diagonal elements i and i + 1are spaced N + 1elements apart in the sequence, and that once we reach diagonal element N (index value N(N + 1)), we have gone beyond the end. 
The actual assembly code follows this general form, but now the pointer increments must be scaled by a factor of 4. We label register %eax as holding a value index4 equal to index in our C version, but scaled by a factor of 4. For N = 16, we can see that our stopping point for index4 will be 4 . 16(16 + 1) = 1088. 

Aat %ebp+8, val at %ebp+12 1 movl 8(%ebp), %ecx Get Abase = &A[0][0] 2 movl 12(%ebp), %edx Get val 3 movl $0, %eax Set index4 to 0 4 .L14: loop: 5 movl %edx, (%ecx,%eax) Set Abase[index4/4] to val 6 addl $68, %eax index4 += 4(N+1) 7 cmpl $1088, %eax Compare index4:4N(N+1) 8 jne .L14 If !=, goto loop 
Solution to Problem 3.39 (page 243) 
This problem gets you to think about structure layout and the code used to access structure .elds. The structure declaration is a variant of the example shown in the text. It shows that nested structures are allocated by embedding the inner structures within the outer ones. 
A. The layout of the structure is as follows: 
Offset 0 4 8 12 16 
Contents 

p  s.x  s.y  next  

B. It uses 16 bytes. 
C. As always, we start by annotating the assembly code: 
sp at %ebp+8 1 movl 8(%ebp), %eax Get sp 2 movl 8(%eax), %edx Get sp->s.y 3 movl %edx, 4(%eax) Store in sp->s.x 4 leal 4(%eax), %edx Compute &(sp->s.x) 5 movl %edx, (%eax) Store in sp->p 6 movl %eax, 12(%eax) Store sp in sp->next 
From this, we can generate C code as follows: 
void sp_init(struct prob *sp) 
{ 

sp->s.x = sp->s.y; 
sp->p = &(sp->s.x); 
sp->next = sp; 
} 

Solution to Problem 3.40 (page 247) 
Structures and unions involve a simple set of concepts, but it takes practice to be comfortable with the different referencing patterns and their implementations. 
EXPR TYPE Code 
up->t1.s int movl 4(%eax), %eax movl %eax, (%edx) 
up->t1.v short movw (%eax), %ax movw %ax, (%edx) 
&up->t1.d short * leal 2(%eax), %eax movl %eax, (%edx) 
up->t2.a int * movl %eax, (%edx) 
up->t2.a[up->t1.s] int movl 4(%eax), %ecx movl (%eax,%ecx,4), %eax movl %eax, (%edx) 
*up->t2.p char movl 8(%eax), %eax movb (%eax), %al movb %al, (%edx) 
Solution to Problem 3.41 (page 251) 
Understanding structure layout and alignment is very important for understand-ing how much storage different data structures require and for understanding the code generated by the compiler for accessing structures. This problem lets you work out the details of some example structures. 
A. struct P1 { int i; char c; int j; char d; }; 
icj d Total Alignment 04812 16 4 
B. struct P2 { int i; char c; char d; int j; }; 
icjd Total Alignment 0458 12 4 
C. struct P3 { short w[3]; char c[3] }; 
wc Total Alignment 0610 2 
D. struct P4 { short w[3]; char *c[3] }; 
wc Total Alignment 0820 4 
E. struct P3 { struct P1 a[2]; struct P2 *p }; 
ap Total Alignment 032 36 4 

Solution to Problem 3.42 (page 251) 
This is an exercise in understanding structure layout and alignment. 
A. Here are the object sizes and byte offsets: 
Field abcd e f g h 
Size 42814184 Offset048 1620243240 
B. The structure is a total of 48 bytes long. The end of the structure must be padded by 4 bytes to satisfy the 8-byte alignment requirement. 
C. One strategy that works, when all data elements have a length equal to a power of two, is to order the structure elements in descending order of size. This leads to a declaration, 
struct { double c; long long g; float e; char *a; void *h; short b; char d; char f; 
} foo; 

with the following offsets, for a total of 32 bytes: 
Field cgea h b d f 
Size 884 44211 Offset0 816 20 24 28 30 31 
Solution to Problem 3.43 (page 259) 
This problem covers a wide range of topics, such as stack frames, string represen-tations, ASCII code, and byte ordering. It demonstrates the dangers of out-of-bounds memory references and the basic ideas behind buffer over.ow. 
A. Stack after line 7: 

08 04 86 43  
bf ff fc 94  
00 00 00 03  
00 00 00 02  
00 00 00 01  




Return address %ebp 
Saved %ebp Saved %edi Saved %esi Saved %ebx buf[4-7] buf[0-3] 

B. Stack after line 10: 
%ebp 

08 04 86 00  
33 32 31 30  
39 38 37 36  
35 34 33 32  
31 30 39 38  
37 36 35 34  
33 32 31 30  


Return address Saved %ebp Saved %edi Saved %esi Saved %ebx buf[4-7] buf[0-3] 
C. The program is attempting to return to address 0x08048600. The low-order byte was overwritten by the terminating null character. 
D. The saved values of the following registers were altered: 
Register  Value  
%ebp  33323130  
%edi  39383736  
%esi  35343332  
%ebx  31303938  

These values will be loaded into the registers before getline returns. 
E. The call to malloc should have had strlen(buf)+1 as its argument, and the code should also check that the returned value is not equal to NULL. 
Solution to Problem 3.44 (page 262) 
A. This corresponds to a range of around 213 addresses. 
B. A 128-byte nop sled would cover 27 addresses with each test, and so we would only require 26 = 64 attempts. 
This example clearly shows that the degree of randomization in this version of Linux would provide only minimal deterrence against an over.ow attack. 
Solution to Problem 3.45 (page 264) 
This problem gives you another chance to see how IA32 code manages the stack, and to also better understand how to defend against buffer over.ow attacks. 
A. For the unprotected code, we can see that lines 4 and 6 compute the positions of v and buf to be at offsets .8 and .20 relative to %ebp. In the protected code, the canary is stored at offset .8 (line 3), while v and buf are at offsets .24 and .20 (lines 7 and 9). 
B. In the protected code, local variable v is positioned closer to the top of the stack than buf, and so an overrun of buf will not corrupt the value of v. 

In fact, buf is positioned so that any buffer overrun will corrupt the canary value. 
Solution to Problem 3.46 (page 271) 
Achieving a factor of 51 price improvement every 10 years over 3 decades has been truly remarkable, and it helps explain why computers have become so pervasive in our society. 
A. Assuming the baseline of 16.3 gigabytes in 2010, 256 terabytes represents an increase by a factor of 1.608 ¡Á 104, which would take around 25 years, giving us 2035. 
B. Sixteen exabytes is an increase of 1.054 ¡Á 109 over 16.3 gigabytes. This would take around 53 years, giving us 2063. 
C. Increasing the budget by a factor of 10 cuts about 6 years off our schedule, making it possible to meet the two memory-size goals in years 2029 and 2057, respectively. 
These numbers, of course, should not be taken too literally. It would require scaling memory technology well beyond what are believed to be fundamental physical limits of the current technology. Nonetheless, it indicates that, within the lifetimes of many readers of this book, there will be systems with exabyte-scale memory systems. 
Solution to Problem 3.47 (page 276) 
This problem illustrates some of the subtleties of type conversion and the different move instructions. In some cases, we make use of the property that the movl instruction will set the upper 32 bits of the destination register to zeros. Some of the problems have multiple solutions. 
src_t dest_t Instruction SD Explanation 
long  long  movq  %rdi  %rax  No conversion  
int  long  movslq  %edi  %rax  Sign extend  
char  long  movsbq  %dil  %rax  Sign extend  
unsigned int  unsigned long  movl  %edi  %eax  Zero extend to 64 bits  
unsigned char  unsigned long  movzbq  %dil  %rax  Zero extend to 64  
unsigned char  unsigned long  movzbl  %dil  %eax  Zero extend to 64 bits  
long  int  movslq  %edi  %rax  Sign extend to 64 bits  
long  int  movl  %edi  %eax  Zero extend to 64 bits  
unsigned long  unsigned  movl  %edi  %eax  Zero extend to 64 bits  

We show that the long to int conversion can use either movslq or movl, even though one will sign extend the upper 32 bits, while the other will zero extend it. This is because the values of the upper 32 bits are ignored for any subsequent instruction having %eax as an operand. 
Solution to Problem 3.48 (page 278) 
We can step through the code for arithprob and determine the following: 
1. 
The .rst movslq instruction sign extends d to a long integer prior to its multi-plication by c. This implies that d has type int and c has type long. 

2. 
The movsbl instruction (line 4) sign extends b to an integer prior to its multi-plication by a. This means that b has type char and a has type int. 

3. 
The sum is computed using a leaq instruction, indicating that the return value has type long. 


From this, we can determine that the unique prototype for arithprob is 
long arithprob(int a, char b, long c, int d); 
Solution to Problem 3.49 (page 281) 
This problem demonstrates a clever way to count the number of 1 bits in a word. It uses several tricks that look fairly obscure at the assembly-code level. 
A. Here is the original C code: 
long fun_c(unsigned long x) { long val = 0; int i; for (i=0;i<8; i++) { 
val += x & 0x0101010101010101L; 
x >>= 1; } val += (val >> 32); val += (val >> 16); val += (val >> 8); return val & 0xFF; 
} 
B. This code sums the bits in x by computing 8 single-byte sums in parallel, using all 8 bytes of val. It then sums the two halves of val, then the two low-order 16 bits, and then the 2 low-order bytes of this sum to get the .nal amount in the low-order byte. It masks off the high-order bits to get the .nal result. This approach has the advantage that it requires only 8 iterations, rather than the more typical 64. 
Solution to Problem 3.50 (page 284) 
We can step through the code for incrprob and determine the following: 
1. The addl instruction fetches a 32-bit integer from the location given by the third argument register and adds it to the 32-bit version of the .rst argument register. From this, we can infer that t is the third argument and x is the .rst argument. We can see that t must be a pointer to a signed or unsigned integer, but x could be either signed or unsigned, and it could either be 32 bits or 64 (since when adding it to *t, the code should truncate it to 32 bits). 

2. 
The movslq instruction sign extends the sum (a copy of *t) to a long integer. From this, we can infer that t must be a pointer to a signed integer. 

3. 
The addq instruction adds the sign-extended value of the previous sum to the location indicated by the second argument register. From this, we can infer that q is the second argument and that it is a pointer to a long integer. 


There are four valid prototypes for incrprob, depending on whether or not x is long, and whether it is signed or unsigned. We show these as four different prototypes: 
void incrprob_s(int x, long *q, int *t); void incrprob_u(unsigned x, long *q, int *t); void incrprob_sl(long x, long *q, int *t); void incrprob_ul(unsigned long x, long *q, int *t); 
Solution to Problem 3.51 (page 289) 
This function is an example of a leaf function that requires local storage. It can use space beyond the stack pointer for its local storage, never altering the stack pointer. 
A. Stack locations used: 
Stack pointer %rsp 
0 ¨C8 ¨C16 ¨C24 ¨C32 ¨C40 
Unused  
Unused  
a[3]  
a[2]  
a[1]  
a[0]  


B.  x86-64  implementation  of  local_array  
Argument  i  in  %edi  
1  local_array:  
2  movq  $2,  -40(%rsp)  Store  2  in  a[0]  
3  movq  $3,  -32(%rsp)  Store  3  in  a[1]  
4  movq  $5,  -24(%rsp)  Store  5  in  a[2]  
5  movq  $7,  -16(%rsp)  Store  7  in  a[3]  
6  andl  $3,  %edi  Compute  idx=i&3  
7  movq  -40(%rsp,%rdi,8),  %rax  Compute a[idx]  as  return  value  
8  ret  Return  

C. The function never changes the stack pointer. It stores all of its local values in the region beyond the stack pointer. 
Solution to Problem 3.52 (page 290) 
A. Register %rbx is used to hold the parameter x. 
B. Since %rbx is callee-saved, it must be stored on the stack. Since this is the only use of the stack for this function, the code uses push and pop instructions to save and restore the register. 
C. x86-64 implementation of recursive factorial function rfact Argument: x in %rdi 
1 rfact: 
2 pushq %rbx Save %rbx (callee save) 
3 movq %rdi, %rbx Copy x to %rbx 
4 movl $1, %eax result = 1 
5 testq %rdi, %rdi Test x 
6 jle .L11 If <=0, goto done 
7 leaq -1(%rdi), %rdi Compute xm1 = x-1 
8 call rfact Call rfact(xm1) 
9 imulq %rbx, %rax Compute result = x*rfact(xm1) 
10 .L11: done: 11 popq %rbx Restore %rbx 12 ret Return 
D. Instead of explicitly decrementing and incrementing the stack pointer, the code can use pushq and popq to both modify the stack pointer and to save and restore register state. 
Solution to Problem 3.53 (page 291) 
This problem is similar to Problem 3.41, but updated for x86-64. 
A. struct P1 { int i; char c; long j; char d; }; 
icj d Total Alignment 
04816 24 8 
B. struct P2 { long i; char c; char d; int j; }; 
icd j Total Alignment 
08912 16 8 
C. struct P3 { short w[3]; char c[3] }; wc Total Alignment 
0610 2 
D. struct P4 { short w[3]; char *c[3] }; wc Total Alignment 
0832 8 
E. struct P3 { struct P1 a[2]; struct P2 *p }; ap Total Alignment 
048 56 8 

CHAPTER 
4 
Processor 
Architecture 
4.1 
The 
Y86 
Instruction 
Set 
Architecture 
336 
4.2 
Logic 
Design 
and 
the 
Hardware 
Control 
Language 
HCL 
352 
4.3 
Sequential 
Y86 
Implementations 
364 
4.4 
General 
Principles 
of 
Pipelining 
391 
4.5 
Pipelined 
Y86 
Implementations 
400 
4.6 
Summary 
449 
Bibliographic 
Notes 
451 
Homework 
Problems 
451 
Solutions 
to 
Practice 
Problems 
457 
Modern microprocessors are among the most complex systems ever created by humans. A single silicon chip, roughly the size of a .ngernail, can contain a complete high-performance processor, large cache memories, and the logic re-quired to interface it to external devices. In terms of performance, the processors implemented on a single chip today dwarf the room-sized supercomputers that cost over $10 million just 20 years ago. Even the embedded processors found in everyday appliances such as cell phones, personal digital assistants, and handheld game systems are far more powerful than the early developers of computers ever envisioned. 
Thus far, we have only viewed computer systems down to the level of machine-language programs. We have seen that a processor must execute a sequence of instructions, where each instruction performs some primitive operation, such as adding two numbers. An instruction is encoded in binary form as a sequence of 1 or more bytes. The instructions supported by a particular processor and their byte-level encodings are known as its instruction-set architecture (ISA). Different ¡°families¡± of processors, such as Intel IA32, IBM/Freescale PowerPC, and the ARM processor family have different ISAs. A program compiled for one type of machine will not run on another. On the other hand, there are many different models of processors within a single family. Each manufacturer produces proces-sors of ever-growing performance and complexity, but the different models remain compatible at the ISA level. Popular families, such as IA32, have processors sup-plied by multiple manufacturers. Thus, the ISA provides a conceptual layer of abstraction between compiler writers, who need only know what instructions are permitted and how they are encoded, and processor designers, who must build machines that execute those instructions. 
In this chapter, we take a brief look at the design of processor hardware. We study the way a hardware system can execute the instructions of a particular ISA. This view will give you a better understanding of how computers work and the technological challenges faced by computer manufacturers. One important con-cept is that the actual way a modern processor operates can be quite different from the model of computation implied by the ISA. The ISA model would seem to imply sequential instruction execution, where each instruction is fetched and executed to completion before the next one begins. By executing different parts of multiple instructions simultaneously, the processor can achieve higher perfor-mance than if it executed just one instruction at a time. Special mechanisms are used to make sure the processor computes the same results as it would with se-quential execution. This idea of using clever tricks to improve performance while maintaining the functionality of a simpler and more abstract model is well known in computer science. Examples include the use of caching in Web browsers and information retrieval data structures such as balanced binary trees and hash tables. 
Chances are you will never design your own processor. This is a task for 
experts working at fewer than 100 companies worldwide. Why, then, should you 
learn about processor design? 
. It is intellectually interesting and important.There is an intrinsic value in learn-ing how things work. It is especially interesting to learn the inner workings of 

a system that is such a part of the daily lives of computer scientists and engi-neers and yet remains a mystery to many. Processor design embodies many of the principles of good engineering practice. It requires creating a simple and regular structure to perform a complex task. 
. Understanding how the processor works aids in understanding how the overall computer system works. In Chapter 6, we will look at the memory system and the techniques used to create an image of a very large memory with a very fast access time. Seeing the processor side of the processor-memory interface will make this presentation more complete. 
. Although few people design processors, many design hardware systems that contain processors. This has become commonplace as processors are embed-ded into real-world systems such as automobiles and appliances. Embedded-system designers must understand how processors work, because these sys-tems are generally designed and programmed at a lower level of abstraction than is the case for desktop systems. 
. You just might work on a processor design.Although the number of companies producing microprocessors is small, the design teams working on those pro-cessors are already large and growing. There can be over 1000 people involved in the different aspects of a major processor design. 
In this chapter, we start by de.ning a simple instruction set that we use as a running example for our processor implementations. We call this the ¡°Y86¡± instruction set, because it was inspired by the IA32 instruction set, which is colloquially referred to as ¡°x86.¡± Compared with IA32, the Y86 instruction set has fewer data types, instructions, and addressing modes. It also has a simpler byte-level encoding. Still, it is suf.ciently complete to allow us to write simple programs manipulating integer data. Designing a processor to implement Y86 requires us to face many of the challenges faced by processor designers. 
We then provide some background on digital hardware design. We describe the basic building blocks used in a processor and how they are connected together and operated. This presentation builds on our discussion of Boolean algebra and bit-level operations from Chapter 2. We also introduce a simple language, HCL (for ¡°Hardware Control Language¡±), to describe the control portions of hardware systems. We will later use this language to describe our processor designs. Even if you already have some background in logic design, read this section to understand our particular notation. 
As a .rst step in designing a processor, we present a functionally correct, but somewhat impractical, Y86 processor based on sequential operation. This processor executes a complete Y86 instruction on every clock cycle. The clock must run slowly enough to allow an entire series of actions to complete within one cycle. Such a processor could be implemented, but its performance would be well below what could be achieved for this much hardware. 
With the sequential design as a basis, we then apply a series of transforma-tions to create a pipelined processor. This processor breaks the execution of each instruction into .ve steps, each of which is handled by a separate section or stage of the hardware. Instructions progress through the stages of the pipeline, with one instruction entering the pipeline on each clock cycle. As a result, the processor can be executing the different steps of up to .ve instructions simultaneously. Making this processor preserve the sequential behavior of the Y86 ISA requires handling a variety of hazard conditions, where the location or operands of one instruction depend on those of other instructions that are still in the pipeline. 
We have devised a variety of tools for studying and experimenting with our processor designs. These include an assembler for Y86, a simulator for run-ning Y86 programs on your machine, and simulators for two sequential and one pipelined processor design. The control logic for these designs is described by .les in HCL notation. By editing these .les and recompiling the simulator, you can al-ter and extend the simulator¡¯s behavior. A number of exercises are provided that involve implementing new instructions and modifying how the machine processes instructions. Testing code is provided to help you evaluate the correctness of your modi.cations. These exercises will greatly aid your understanding of the material and will give you an appreciation for the many different design alternatives faced by processor designers. 
Web Aside arch:vlog presents a representation of our pipelined Y86 proces-sor in the Verilog hardware description language. This involves creating modules for the basic hardware building blocks and for the overall processor structure. We automatically translate the HCL description of the control logic into Verilog. By .rst debugging the HCL description with our simulators, we eliminate many of the tricky bugs that would otherwise show up in the hardware design. Given a Verilog description, there are commercial and open-source tools to support simulation and logic synthesis, generating actual circuit designs for the microprocessors. So, although much of the effort we expend here is to create pictorial and textual de-scriptions of a system, much as one would when writing software, the fact that these designs can be automatically synthesized demonstrates that we are indeed creating a system that can be realized as hardware. 
4.1 
The 
Y86 
Instruction 
Set 
Architecture 

De.ning an instruction set architecture, such as Y86, includes de.ning the differ-ent state elements, the set of instructions and their encodings, a set of programming conventions, and the handling of exceptional events. 
4.1.1 Programmer-Visible State 
As Figure 4.1 illustrates, each instruction in a Y86 program can read and modify some part of the processor state. This is referred to as the programmer-visible state, where the ¡°programmer¡± in this case is either someone writing programs in assembly code or a compiler generating machine-level code. We will see in our processor implementations that we do not need to represent and organize this state in exactly the manner implied by the ISA, as long as we can make sure that machine-level programs appear to have access to the programmer-visible state. The state for Y86 is similar to that for IA32. There are eight program registers: 

%eax  %esi  
%ecx  %edi  
%edx  %esp  
%ebx  %ebp  
Figure 4.1 Y86 programmer-visible state. As with IA32, programs for Y86 access and modify the program registers, the condition code, the program counter (PC), and the memory. The status code indicates whether the program is running normally, or some special event has occurred. 


CC:

RF: Program registers Stat: Program status
Condition codes 
ZF  SF  OF  

DMEM: Memory 
PC 

%eax, %ecx, %edx, %ebx, %esi, %edi, %esp, and %ebp. Each of these stores a word. Register %esp is used as a stack pointer by the push, pop, call, and return instructions. Otherwise, the registers have no .xed meanings or values. There are three single-bit condition codes, ZF, SF, and OF, storing information about the effect of the most recent arithmetic or logical instruction. The program counter (PC) holds the address of the instruction currently being executed. 
The memory is conceptually a large array of bytes, holding both program and data. Y86 programs reference memory locations using virtual addresses.A combination of hardware and operating system software translates these into the actual, or physical, addresses indicating where the values are actually stored in memory. We will study virtual memory in more detail in Chapter 9. For now, we can think of the virtual memory system as providing Y86 programs with an image of a monolithic byte array. 
A .nal part of the program state is a status code Stat, indicating the overall state of program execution. It will indicate either normal operation, or that some sort of exception has occurred, such as when an instruction attempts to read from an invalid memory address. The possible status codes and the handling of exceptions is described in Section 4.1.4. 
4.1.2 Y86 Instructions 
Figure 4.2 gives a concise description of the individual instructions in the Y86 ISA. We use this instruction set as a target for our processor implementations. The set of Y86 instructions is largely a subset of the IA32 instruction set. It includes only 4-byte integer operations, has fewer addressing modes, and includes a smaller set of operations. Since we only use 4-byte data, we can refer to these as ¡°words¡± without any ambiguity. In this .gure, we show the assembly-code representation of the instructions on the left and the byte encodings on the right. The assembly-code format is similar to the ATT format for IA32. 
Here are some further details about the different Y86 instructions. 
. The IA32 movl instruction is split into four different instructions: irmovl, rrmovl, mrmovl, and rmmovl, explicitly indicating the form of the source and destination. The source is either immediate (i), register (r), or memory (m). 
Figure 4.2 Byte 01234 
Y86 instruction set. 

Instruction encodings halt 
0  0  


range between 1 and 6 bytes. An instruction nop consists of a 1-byte 
1  0  

rrmovl rA, rB

instruction speci.er, possibly a 1-byte register 
2  0  rA  rB  

irmovl V, rB

speci.er, and possibly a 4-byte constant word. Field 
3  0  F  rB  V  

rmmovl rA, D(rB)

fn speci.es a particular integer operation (OPl), 
4  0  rA  rB  D  

mrmovl D(rB), rA

data movement condition (cmovXX), or branch 
5  0  rA  rB  D  

OPl rA, rB 

condition (jXX). All numeric values are shown 
6  fn  rA  rB  

jXX Dest 
in hexadecimal. 
7  fn  Dest  

cmovXX rA, rB 
2  fn  rA  rB  

call Dest 
8  0  Dest  

ret 
9  0  

pushl rA 
A  0  rA  F  

popl rA 
B  0  rA  F  

It is designated by the .rst character in the instruction name. The destination is either register (r) or memory (m). It is designated by the second character in the instruction name. Explicitly identifying the four types of data transfer will prove helpful when we decide how to implement them. 
The memory references for the two memory movement instructions have a simple base and displacement format. We do not support the second index register or any scaling of a register¡¯s value in the address computation. 
As with IA32, we do not allow direct transfers from one memory location to another. In addition, we do not allow a transfer of immediate data to memory. 
. There are four integer operation instructions, shown in Figure 4.2 as OPl. These are addl, subl, andl, and xorl. They operate only on register data, whereas IA32 also allows operations on memory data. These instructions set the three condition codes ZF, SF, and OF (zero, sign, and over.ow). 
. The seven jump instructions (shown in Figure 4.2 as jXX) are jmp, jle, jl, je, jne, jge, and jg. Branches are taken according to the type of branch and the settings of the condition codes. The branch conditions are the same as with IA32 (Figure 3.12). 

Section 4.1 The Y86 Instruction Set Architecture 339 

.  There are six conditional move instructions (shown in Figure 4.2 as cmovXX): cmovle, cmovl, cmove, cmovne, cmovge, and cmovg. These have the same format as the register-register move instruction rrmovl, but the destination register is updated only if the condition codes satisfy the required constraints.  
.  The call instruction pushes the return address on the stack and jumps to the destination address. The ret instruction returns from such a call.  
.  The pushl and popl instructions implement push and pop, just as they do in IA32.  
.  The halt instruction stops instruction execution. IA32 has a comparable instruction, called hlt. IA32 application programs are not permitted to use this instruction, since it causes the entire system to suspend operation. For Y86, executing the halt instruction causes the processor to stop, with the  

status code set to HLT. (See Section 4.1.4.) 
4.1.3 Instruction Encoding 
Figure 4.2 also shows the byte-level encoding of the instructions. Each instruction requires between 1 and 6 bytes, depending on which .elds are required. Every instruction has an initial byte identifying the instruction type. This byte is split into two 4-bit parts: the high-order, or code, part, and the low-order, or function, part. As you can see in Figure 4.2, code values range from 0 to 0xB. The function values are signi.cant only for the cases where a group of related instructions share a common code. These are given in Figure 4.3, showing the speci.c encodings of the integer operation, conditional move, and branch instructions. Observe that rrmovl has the same instruction code as the conditional moves. It can be viewed as an ¡°unconditional move¡± just as the jmp instruction is an unconditional jump, both having function code 0. 
As shown in Figure 4.4, each of the eight program registers has an associated register identi.er (ID) ranging from 0 to 7. The numbering of registers in Y86 matches what is used in IA32. The program registers are stored within the CPU in a register .le, a small random-access memory where the register IDs serve 
Operations Branches Moves 
6  0  

7  0  

7  4  

2  0  

2  4  

2  5  

2  6  

addl 

jmp 
jne 
rrmovl 
cmovne 
subl 

6  1  

jle 
7  1  

jge 
7  5  

cmovle 
2  1  

cmovge 
andl 

6  2  

jl 
7  2  

jg 
7  6  

cmovl 
2  2  

cmovg 
xorl 

6  3  

je 
7  3  

cmove 
2  3  


Figure 4.3 Function codes for Y86 instruction set. The code speci.es a particular integer operation, branch condition, or data transfer condition. These instructions are shown as OPl, jXX, and cmovXX in Figure 4.2. 
Number Register name 
0 %eax 1 %ecx 2 %edx 3 %ebx 4 %esp 5 %ebp 6 %esi 7 %edi F No register 
Figure 4.4 Y86 program register identi.ers. Each of the eight program registers has an associated identi.er (ID) ranging from 0 to 7.ID 0xF in a register .eld of an instruction indicates the absence of a register operand. 
as addresses. ID value 0xF is used in the instruction encodings and within our hardware designs when we need to indicate that no register should be accessed. 
Some instructions are just 1 byte long, but those that require operands have longer encodings. First, there can be an additional register speci.er byte, specifying either one or two registers. These register .elds are called rA and rB in Figure 4.2. As the assembly-code versions of the instructions show, they can specify the registers used for data sources and destinations, as well as the base register used in an address computation, depending on the instruction type. Instructions that have no register operands, such as branches and call, do not have a register speci.er byte. Those that require just one register operand (irmovl, pushl, and popl) have the other register speci.er set to value 0xF. This convention will prove useful in our processor implementation. 
Some instructions require an additional 4-byte constant word. This word can serve as the immediate data for irmovl, the displacement for rmmovl and mrmovl address speci.ers, and the destination of branches and calls. Note that branch and call destinations are given as absolute addresses, rather than using the PC-relative addressing seen in IA32. Processors use PC-relative addressing to give more compact encodings of branch instructions and to allow code to be copied from one part of memory to another without the need to update all of the branch target addresses. Since we are more concerned with simplicity in our presentation, we use absolute addressing. As with IA32, all integers have a little-endian encoding. When the instruction is written in disassembled form, these bytes appear in reverse order. 
As an example, let us generate the byte encoding of the instruction rmmovl %esp,0x12345(%edx) in hexadecimal. From Figure 4.2, we can see that rmmovl has initial byte 40. We can also see that source register %esp should be encoded in the rA .eld, and base register %edx should be encoded in the rB .eld. Using the register numbers in Figure 4.4, we get a register speci.er byte of 42. Finally, the displacement is encoded in the 4-byte constant word. We .rst pad 0x12345 with leading zeros to .ll out 4 bytes, giving a byte sequence of 00 01 23 45. We write this in byte-reversed order as 45 23 01 00. Combining these, we get an instruction encoding of 404245230100. 

One important property of any instruction set is that the byte encodings must have a unique interpretation. An arbitrary sequence of bytes either encodes a unique instruction sequence or is not a legal byte sequence. This property holds for Y86, because every instruction has a unique combination of code and function in its initial byte, and given this byte, we can determine the length and meaning of any additional bytes. This property ensures that a processor can execute an object-code program without any ambiguity about the meaning of the code. Even if the code is embedded within other bytes in the program, we can readily determine the instruction sequence as long as we start from the .rst byte in the sequence. On the other hand, if we do not know the starting position of a code sequence, we cannot reliably determine how to split the sequence into individual instructions. This causes problems for disassemblers and other tools that attempt to extract machine-level programs directly from object-code byte sequences. 
Practice Problem 4.1 
Determine the byte encoding of the Y86 instruction sequence that follows. The line ¡°.pos 0x100¡± indicates that the starting address of the object code should be 0x100. 
.pos 0x100 # Start code at address 0x100 irmovl $15,%ebx # Load 15 into %ebx rrmovl %ebx,%ecx # Copy 15 to %ecx 
loop: # loop: rmmovl %ecx,-3(%ebx) # Save %ecx at address 15-3 = 12 addl %ebx,%ecx # Increment %ecx by 15 jmp loop # Goto loop 
Practice Problem 4.2 
For each byte sequence listed, determine the Y86 instruction sequence it encodes. If there is some invalid byte in the sequence, show the instruction sequence up to that point and indicate where the invalid value occurs. For each sequence, we show the starting address, then a colon, and then the byte sequence. 
A. 0x100:30f3fcffffff40630008000000 B. 0x200:a06f80080200000030f30a00000090 C. 0x300:50540700000010f0b01f D. 0x400:6113730004000000 E. 0x500:6362a0f0 
Aside Comparing IA32 to Y86 instruction encodings 
Compared with the instruction encodings used in IA32, the encoding of Y86 is much simpler but also less compact. The register .elds occur only in .xed positions in all Y86 instructions, whereas they are packed into various positions in the different IA32 instructions. We use a 4-bit encoding of registers, even though there are only eight possible registers. IA32 uses just 3 bits. Thus, IA32 can pack a push or pop instruction into just 1 byte, with a 5-bit .eld indicating the instruction type and the remaining 3 bits for the register speci.er. IA32 can encode constant values in 1, 2, or 4 bytes, whereas Y86 always requires 4 bytes. 
Aside RISC and CISC instruction sets 
IA32 is sometimes labeled as a ¡°complex instruction set computer¡± (CISC¡ªpronounced ¡°sisk¡±), and is deemed to be the opposite of ISAs that are classi.ed as ¡°reduced instruction set computers¡± (RISC¡ªpronounced ¡°risk¡±). Historically, CISC machines came .rst, having evolved from the earliest computers. By the early 1980s, instruction sets for mainframe and minicomputers had grown quite large, as machine designers incorporated new instructions to support high-level tasks, such as manipulating circular buffers, performing decimal arithmetic, and evaluating polynomials. The .rst microprocessors appeared in the early 1970s and had limited instruction sets, because the integrated-circuit technology then posed severe constraints on what could be implemented on a single chip. Microprocessors evolved quickly and, by the early 1980s, were following the path of increasing instruction-set complexity set by mainframes and minicomputers. The x86 family took this path, evolving into IA32, and more recently into x86-64. Even the x86 line continues to evolve as new classes of instructions are added based on the needs of emerging applications. 
The RISC design philosophy developed in the early 1980s as an alternative to these trends. A group of hardware and compiler experts at IBM, strongly in.uenced by the ideas of IBM researcher John Cocke, recognized that they could generate ef.cient code for a much simpler form of instruction set. In fact, many of the high-level instructions that were being added to instruction sets were very dif.cult to generate with a compiler and were seldom used. A simpler instruction set could be implemented with much less hardware and could be organized in an ef.cient pipeline structure, similar to those described later in this chapter. IBM did not commercialize this idea until many years later, when it developed the Power and PowerPC ISAs. 
The RISC concept was further developed by Professors David Patterson, of the University of California at Berkeley, and John Hennessy, of Stanford University. Patterson gave the name RISC to this new class of machines, and CISC to the existing class, since there had previously been no need to have a special designation for a nearly universal form of instruction set. 
Comparing CISC with the original RISC instruction sets, we .nd the following general character-istics: 
CISC Early RISC 

A large number of instructions. The Intel document describing the complete set of instructions [28, 29] is over 1200 pages long. 
Some instructions with long execution times. These include instructions that copy an entire block from one part of memory to another and others that copy multiple registers to and from memory. 
Many fewer instructions. Typically less than 100. 
No instruction with a long execution time. Some early RISC machines did not even have an integer multiply instruction, requiring compilers to implement multiplication as a sequence of additions. 
CISC Early RISC 

Variable-length encodings. IA32 instructions Fixed-length encodings. Typically all instructions can range from 1 to 15 bytes. are encoded as 4 bytes. 
Multiple formats for specifying operands. In IA32, a memory operand speci.er can have many different combinations of displacement, base and index registers, and scale factors. 
Arithmetic and logical operations can be applied to both memory and register operands. 
Implementation artifacts hidden from machine-level programs. The ISA provides a clean abstraction between programs and how they get executed. 
Condition codes. Special .ags are set as a side effect of instructions and then used for conditional branch testing. 
Stack-intensive procedure linkage. The stack is used for procedure arguments and return addresses. 
Simple addressing formats. Typically just base and displacement addressing. 
Arithmetic and logical operations only use register operands. Memory referencing is only allowed by load instructions, reading from memory into a register, and store instructions, writing from a register to memory. This convention is referred to as a load/store architecture. 
Implementation artifacts exposed to machine-level programs. Some RISC machines prohibit particular instruction sequences and have jumps that do not take effect until the following instruction is executed. The compiler is given the task of optimizing performance within these constraints. 
No condition codes. Instead, explicit test instructions store the test results in normal registers for use in conditional evaluation. 
Register-intensive procedure linkage. Registers are used for procedure arguments and return addresses. Some procedures can thereby avoid any memory references. Typically, the processor has many more (up to 32) registers. 

The Y86 instruction set includes attributes of both CISC and RISC instruction sets. On the CISC side, it has condition codes, variable-length instructions, and stack-intensive procedure linkages. On the RISC side, it uses a load-store architecture and a regular encoding. It can be viewed as taking a CISC instruction set (IA32) and simplifying it by applying some of the principles of RISC. 
Aside The RISC versus CISC controversy 
Through the 1980s, battles raged in the computer architecture community regarding the merits of RISC versus CISC instruction sets. Proponents of RISC claimed they could get more computing power for a given amount of hardware through a combination of streamlined instruction set design, advanced compiler technology, and pipelined processor implementation. CISC proponents countered that fewer CISC instructions were required to perform a given task, and so their machines could achieve higher overall performance. 
Major companies introduced RISC processor lines, including Sun Microsystems (SPARC), IBM and Motorola (PowerPC), and Digital Equipment Corporation (Alpha). A British company, Acorn Computers Ltd., developed its own architecture, ARM (originally an acronym for ¡°Acorn RISC Machine¡±), which is widely used in embedded applications, such as cellphones. 
In the early 1990s, the debate diminished as it became clear that neither RISC nor CISC in their purest forms were better than designs that incorporated the best ideas of both. RISC machines evolved and introduced more instructions, many of which take multiple cycles to execute. RISC machines today have hundreds of instructions in their repertoire, hardly .tting the name ¡°reduced instruction set machine.¡± The idea of exposing implementation artifacts to machine-level programs proved to be short-sighted. As new processor models were developed using more advanced hardware structures, many of these artifacts became irrelevant, but they still remained part of the instruction set. Still, the core of RISC design is an instruction set that is well-suited to execution on a pipelined machine. 
More recent CISC machines also take advantage of high-performance pipeline structures. As we will discuss in Section 5.7, they fetch the CISC instructions and dynamically translate them into a sequence of simpler, RISC-like operations. For example, an instruction that adds a register to memory is translated into three operations: one to read the original memory value, one to perform the addition, and a third to write the sum to memory. Since the dynamic translation can generally be performed well in advance of the actual instruction execution, the processor can sustain a very high execution rate. 
Marketing issues, apart from technological ones, have also played a major role in determining the success of different instruction sets. By maintaining compatibility with its existing processors, Intel with x86 made it easy to keep moving from one generation of processor to the next. As integrated-circuit technology improved, Intel and other x86 processor manufacturers could overcome the inef.ciencies created by the original 8086 instruction set design, using RISC techniques to produce performance comparable to the best RISC machines. As we saw in Section 3.13, the evolution of IA32 into x86-64 provided an opportunity to incorporate several features of RISC into x86. In the areas of desktop and laptop computing, x86 has achieved total domination, and it is increasingly popular for high-end server machines. 
RISC processors have done very well in the market for embedded processors, controlling such systems as cellular telephones, automobile brakes, and Internet appliances. In these applications, saving on cost and power is more important than maintaining backward compatibility. In terms of the number of processors sold, this is a very large and growing market. 
4.1.4 Y86 Exceptions 
The programmer-visible state for Y86 (Figure 4.1) includes a status code Stat describing the overall state of the executing program. The possible values for this code are shown in Figure 4.5. Code value 1, named AOK, indicates that the program is executing normally, while the other codes indicate that some type of exception has occurred. Code 2, named HLT, indicates that the processor has executed a halt instruction. Code 3, named ADR, indicates that the processor attempted to read from or write to an invalid memory address, either while fetching an instruction or while reading or writing data. We limit the maximum address (the exact limit varies by implementation), and any access to an address beyond this limit will trigger an ADR exception. Code 4, named INS, indicates that an invalid instruction code has been encountered. 

Value  Name  Meaning  
1  AOK  Normal operation  
2  HLT  halt instruction encountered  
3  ADR  Invalid address encountered  
4  INS  Invalid instruction encountered  
Figure 4.5 Y86 status codes. In our design, the processor halts for any code other than AOK. 


For Y86, we will simply have the processor stop executing instructions when it encounters any of the exceptions listed. In a more complete design, the processor would typically invoke an exception handler, a procedure designated to handle the speci.c type of exception encountered. As described in Chapter 8, exception handlers can be con.gured to have different effects, such as aborting the program or invoking a user-de.ned signal handler. 
4.1.5 Y86 Programs 
Figure 4.6 shows IA32 and Y86 assembly code for the following C function: 
int Sum(int *Start, int Count) 
{ 
int sum=0; 

while (Count) { 
sum += *Start; 
Start++; 
Count--; 
} 
return sum; } 

The IA32 code was generated by the gcc compiler. The Y86 code is essentially the same, except that Y86 sometimes requires two instructions to accomplish what can be done with a single IA32 instruction. If we had written the program using array indexing, however, the conversion to Y86 code would be more dif.cult, since Y86 does not have scaled addressing modes. This code follows many of the programming conventions we have seen for IA32, including the use of the stack and frame pointers. For simplicity, it does not follow the IA32 convention of having some registers designated as callee-save registers. This is just a programming convention that we can either adopt or ignore as we please. 
Figure 4.7 shows an example of a complete program .le written in Y86 as-sembly code. The program contains both data and instructions. Directives indicate where to place code or data and how to align it. The program speci.es issues such 
IA32 code Y86 code 
int  Sum(int  *Start,  int  Count)  int  Sum(int  *Start,  int  Count)  
1  Sum:  1  Sum:  
2  pushl  %ebp  2  pushl  %ebp  
3  movl  %esp,%ebp  3  rrmovl  %esp,%ebp  
4  movl  8(%ebp),%ecx  ecx  =  Start  4  mrmovl  8(%ebp),%ecx  ecx  =  Start  
5  movl  12(%ebp),%edx  edx  =  Count  5  mrmovl  12(%ebp),%edx  edx  =  Count  
6  xorl  %eax,%eax  sum=0  6  xorl  %eax,%eax  sum=0  
7  testl  %edx,%edx  7  andl  %edx,%edx  Set  condition  codes  
8  je  .L34  8  je  End  
9  .L35:  9  Loop:  
10  addl  (%ecx),%eax  add  *Start  to  sum  10  mrmovl  (%ecx),%esi  get  *Start  
11  addl  %esi,%eax  add  to  sum  
11  addl  $4,%ecx  Start++  12  irmovl  $4,%ebx  
13  addl  %ebx,%ecx  Start++  
12  decl  %edx  Count-- 14  irmovl  $-1,%ebx  
15  addl  %ebx,%edx  Count-- 
13  jnz .L35  Stop when 0  16  jne  Loop  Stop when 0  
14  .L34:  17  End:  
15  movl  %ebp,%esp  18  rrmovl  %ebp,%esp  
16  popl  %ebp  19  popl  %ebp  
17  ret  20  ret  
Figure 4.6 Comparison of Y86 and IA32 assembly programs. The Sum function computes the sum of an integer array. The Y86 code differs from the IA32 mainly in that it may require multiple instructions to perform what can be done with a single IA32 instruction. 


as stack placement, data initialization, program initialization, and program termi-nation. 
In this program, words beginning with ¡°.¡± are assembler directives telling the assembler to adjust the address at which it is generating code or to insert some words of data. The directive .pos 0 (line 2) indicates that the assembler should begin generating code starting at address 0. This is the starting address for all Y86 programs. The next two instructions (lines 3 and 4) initialize the stack and frame pointers. We can see that the label Stack is declared at the end of the program (line 47), to indicate address 0x100 using a .pos directive (line 46). Our stack will therefore start at this address and grow toward lower addresses. We must ensure that the stack does not grow so large that it overwrites the code or other program data. 
Lines 9 to 13 of the program declare an array of four words, having values 0xd, 0xc0, 0xb00, and 0xa000. The label array denotes the start of this array, and is aligned on a 4-byte boundary (using the .align directive). Lines 17 to 6 show a ¡°main¡± procedure that calls the function Sum on the four-word array and then halts. 

2  .pos 0  
3  init:  irmovl Stack, %esp #  Set up stack pointer  
4  irmovl Stack, %ebp #  Set up base pointer  
call Main #  Execute main program  
6  halt #  Terminate program  
7  
8  # Array  of 4 elements  
9  .align 4  
array:  .long 0xd  
11  .long 0xc0  
12  .long 0xb00  
13  .long 0xa000  
14  
Main:  pushl %ebp  
16  rrmovl %esp,%ebp  
17  irmovl $4,%eax  
18  pushl %eax #  Push 4  
19  irmovl array,%edx  
pushl %edx #  Push array  
21  call Sum #  Sum(array, 4)  
22  rrmovl %ebp,%esp  
23  popl %ebp  
24  ret  
26  # int Sum(int *Start, int  Count)  
27  Sum:  pushl %ebp  
28  rrmovl %esp,%ebp  
29  mrmovl 8(%ebp),%ecx #  ecx = Start  
mrmovl 12(%ebp),%edx #  edx = Count  
31  xorl %eax,%eax #  sum = 0  
32  andl %edx,%edx #  Set condition codes  
33  je End  
34  Loop:  mrmovl (%ecx),%esi #  get *Start  
addl %esi,%eax #  add to sum  
36  irmovl $4,%ebx #  
37  addl %ebx,%ecx #  Start++  
38  irmovl $-1,%ebx #  
39  addl %ebx,%edx #  Count-- 
jne Loop #  Stop when 0  
41  End:  rrmovl %ebp,%esp  
42  popl %ebp  
43  ret  
44  
# The stack starts here and  grows  to lower addresses  
46  .pos 0x100  
47  Stack:  
Figure 4.7 Sample program written in Y86 assembly code. The Sum function is called to compute the sum of a four-element array. 


As this example shows, since our only tool for creating Y86 code is an assem-bler, the programmer must perform tasks we ordinarily delegate to the compiler, linker, and run-time system. Fortunately, we only do this for small programs, for which simple mechanisms suf.ce. 
Figure 4.8 shows the result of assembling the code shown in Figure 4.7 by an assembler we call yas. The assembler output is in ASCII format to make it more readable. On lines of the assembly .le that contain instructions or data, the object code contains an address, followed by the values of between 1 and 6 bytes. 
We have implemented an instruction set simulator we call yis, the purpose of which is to model the execution of a Y86 machine-code program, without attempting to model the behavior of any speci.c processor implementation. This form of simulation is useful for debugging programs before actual hardware is available, and for checking the result of either simulating the hardware or running the program on the hardware itself. Running on our sample object code, yis generates the following output: 
Stopped in 52 steps at PC = 0x11. Status ¡¯HLT¡¯, CC Z=1 S=0 O=0 Changes to registers: %eax: 0x00000000 0x0000abcd %ecx: 0x00000000 0x00000024 %ebx: 0x00000000 0xffffffff %esp: 0x00000000 0x00000100 %ebp: 0x00000000 0x00000100 %esi: 0x00000000 0x0000a000 
Changes to memory: 0x00e8: 0x00000000 0x000000f8 0x00ec: 0x00000000 0x0000003d 0x00f0: 0x00000000 0x00000014 0x00f4: 0x00000000 0x00000004 0x00f8: 0x00000000 0x00000100 0x00fc: 0x00000000 0x00000011 
The .rst line of the simulation output summarizes the execution and the resulting values of the PC and program status. In printing register and memory values, it only prints out words that change during simulation, either in registers or in memory. The original values (here they are all zero) are shown on the left, and the .nal values are shown on the right. We can see in this output that register %eax contains 0xabcd, the sum of the four-element array passed to subroutine Sum. In addition, we can see that the stack, which starts at address 0x100 and grows toward lower addresses, has been used, causing changes to words of memory at addresses 0xe8 through 0xfc. This is well away from 0x7c, the maximum address of the executable code. 

Section 4.1 The Y86 Instruction Set Architecture 349 

|  #  Execution  begins  at  address  0  
0x000:  |  .pos  0  
0x000:  30f400010000  |  init:  irmovl  Stack,  %esp  #  Set  up  stack  pointer  
0x006:  30f500010000  |  irmovl  Stack,  %ebp  #  Set  up  base  pointer  
0x00c:  8024000000  |  call  Main  #  Execute  main  program  
0x011:  00  |  halt  #  Terminate  program  
|  
|  #  Array  of  4  elements  
0x014:  |  .align  4  
0x014:  0d000000  |  array:  .long  0xd  
0x018:  c0000000  |  .long  0xc0  
0x01c:  000b0000  |  .long  0xb00  
0x020:  00a00000  |  .long  0xa000  
|  
0x024:  a05f  |  Main:  pushl  %ebp  
0x026:  2045  |  rrmovl  %esp,%ebp  
0x028:  30f004000000  |  irmovl  $4,%eax  
0x02e:  a00f  |  pushl  %eax  #  Push  4  
0x030:  30f214000000  |  irmovl  array,%edx  
0x036:  a02f  |  pushl  %edx  #  Push  array  
0x038:  8042000000  |  call  Sum  #  Sum(array,  4)  
0x03d:  2054  |  rrmovl  %ebp,%esp  
0x03f:  b05f  |  popl  %ebp  
0x041:  90  |  ret  
|  
|  #  int  Sum(int  *Start,  int  Count)  
0x042:  a05f  |  Sum:  pushl  %ebp  
0x044:  2045  |  rrmovl  %esp,%ebp  
0x046:  501508000000  |  mrmovl  8(%ebp),%ecx  #  ecx  =  Start  
0x04c:  50250c000000  |  mrmovl  12(%ebp),%edx  #  edx  =  Count  
0x052:  6300  |  xorl  %eax,%eax  #  sum  =  0  
0x054:  6222  |  andl  %edx,%edx  #  Set  condition  codes  
0x056:  7378000000  |  je  End  
0x05b:  506100000000  |  Loop:  mrmovl  (%ecx),%esi  #  get  *Start  
0x061:  6060  |  addl  %esi,%eax  #  add  to  sum  
0x063:  30f304000000  |  irmovl  $4,%ebx  #  
0x069:  6031  |  addl  %ebx,%ecx  #  Start++  
0x06b:  30f3ffffffff  |  irmovl  $-1,%ebx  #  
0x071:  6032  |  addl  %ebx,%edx  #  Count-- 
0x073:  745b000000  |  jne  Loop  #  Stop  when  0  
0x078:  2054  |  End:  rrmovl  %ebp,%esp  
0x07a:  b05f  |  popl  %ebp  
0x07c:  90  |  ret  
|  
|  #  The  stack  starts  here  and  grows  to  lower  addresses  
0x100:  |  .pos  0x100  
0x100:  |  Stack:  

Figure 4.8 Output of yas assembler. Each line includes a hexadecimal address and between 1 and 6 bytes of object code. 
Practice Problem 4.3 
Write Y86 code to implement a recursive sum function rSum, based on the follow-ing C code: 
int rSum(int *Start, int Count) { if (Count <= 0) return 0; return *Start + rSum(Start+1, Count-1); } 
You might .nd it helpful to compile the C code on an IA32 machine and then translate the instructions to Y86. 
Practice Problem 4.4 
Modify the Y86 code for the Sum function (Figure 4.6) to implement a function AbsSum that computes the sum of absolute values of an array. Use a conditional jump instruction within your inner loop. 
Practice Problem 4.5 
Modify the Y86 code for the Sum function (Figure 4.6) to implement a function AbsSum that computes the sum of absolute values of an array. Use a conditional move instruction within your inner loop. 
4.1.6 Some Y86 Instruction Details 
Most Y86 instructions transform the program state in a straightforward manner, and so de.ning the intended effect of each instruction is not dif.cult. Two unusual instruction combinations, however, require special attention. 
The pushl instruction both decrements the stack pointer by 4 and writes a register value to memory. It is therefore not totally clear what the processor should do when executing the instruction pushl %esp, since the register being pushed is being changed by the same instruction. Two different conventions are possible: 
(1) push the original value of %esp, or (2) push the decremented value of %esp. 
For the Y86 processor, let us adopt the same convention as is used with IA32, as determined in the following problem. 
Practice Problem 4.6 
Let us determine the behavior of the instruction pushl %esp for an IA32 proces-sor. We could try reading the Intel documentation on this instruction, but a simpler approach is to conduct an experiment on an actual machine. The C compiler would not normally generate this instruction, so we must use hand-generated assembly code for this task. Here is a test function we have written (Web Aside asm:easm describes how to write programs that combine C code with hand-written assembly code): 

1 .text 2 .globl pushtest 3 pushtest: 4 pushl %ebp 5 movl %esp, %ebp 6 movl %esp, %eax Copy stack pointer 7 pushl %esp Push stack pointer 8 popl %edx Pop it back 9 subl %edx,%eax Subtract new from old stack pointer 
10 leave Restore stack & frame pointers 11 ret 
In our experiments, we .nd that function pushtest always returns zero. What does this imply about the behavior of the instruction pushl %esp under IA32? 
A similar ambiguity occurs for the instruction popl %esp. It could either set %esp to the value read from memory or to the incremented stack pointer. As with Problem 4.6, let us run an experiment to determine how an IA32 machine would handle this instruction, and then design our Y86 machine to follow the same convention. 
Practice Problem 4.7 
The following assembly-code function lets us determine the behavior of the in-struction popl %esp for IA32: 
1  .text  
2  .globl  poptest  
3  poptest:  
4  pushl  %ebp  
5  movl  %esp,  %ebp  
6  pushl  $0xabcd  Push  test  value  
7  popl  %esp  Pop  to  stack  pointer  
8  movl  %esp,  %eax  Set  popped  value  as  return  value  
9  leave  Restore  stack  and  frame  pointers  
10  ret  

We .nd this function always returns 0xabcd. What does this imply about the behavior of popl %esp? What other Y86 instruction would have the exact same behavior? 
Aside Getting the details right: Inconsistencies across x86 models 
Problems 4.6 and 4.7 are designed to help us devise a consistent set of conventions for instructions that push or pop the stack pointer. There seems to be little reason why one would want to perform either of these operations, and so a natural question to ask is ¡°Why worry about such picky details?¡± 
Several useful lessons can be learned about the importance of consistency from the following excerpt from the Intel documentation of the pop instruction [29]: 
For IA-32 processors from the Intel 286 on, the PUSH ESP instruction pushes the value of the ESP register as it existed before the instruction was executed. (This is also true for Intel 64 architecture, real-address and virtual-8086 modes of IA-32 architecture.) For the Intel. 8086 processor, the PUSH SP instruction pushes the new value of the SP register (that is the value after it has been decremented by 2). 
What this note states is that different models of x86 processors do different things when instructed to push the stack pointer register. Some push the original value, while others push the decremented value. (Interestingly, there is no corresponding ambiguity about popping to the stack pointer register.) There are two drawbacks to this inconsistency: 
. It decreases code portability. Programs may have different behavior depending on the processor model. Although the particular instruction is not at all common, even the potential for incompat-ibility can have serious consequences. 
. It complicates the documentation. As we see here, a special note is required to try to clarify the differences. The documentation for x86 is already complex enough without special cases such as this one. 
We conclude, therefore, that working out details in advance and striving for complete consistency can save a lot of trouble in the long run. 
4.2 
Logic 
Design 
and 
the 
Hardware 
Control 
Language 
HCL 

In hardware design, electronic circuits are used to compute functions on bits and to store bits in different kinds of memory elements. Most contemporary circuit technology represents different bit values as high or low voltages on signal wires. In current technology, logic value 1 is represented by a high voltage of around 1.0 volt, while logic value 0 is represented by a low voltage of around 0.0 volts. Three major components are required to implement a digital system: combinational logic to compute functions on the bits, memory elements to store bits, and clock signals to regulate the updating of the memory elements. 
In this section, we provide a brief description of these different components. We also introduce HCL (for ¡°hardware control language¡±), the language that we use to describe the control logic of the different processor designs. We only describe HCL informally here. A complete reference for HCL can be found in Web Aside arch:hcl. 

Aside Modern logic design 
At one time, hardware designers created circuit designs by drawing schematic diagrams of logic circuits (.rst with paper and pencil, and later with computer graphics terminals). Nowadays, most designs are expressed in a hardware description language (HDL), a textual notation that looks similar to a programming language but that is used to describe hardware structures rather than program behaviors. The most commonly used languages are Verilog, having a syntax similar to C, and VHDL, having a syntax similar to the Ada programming language. These languages were originally designed for expressing simulation models of digital circuits. In the mid-1980s, researchers developed logic synthesis programs that could generate ef.cient circuit designs from HDL descriptions. There are now a number of commercial synthesis programs, and this has become the dominant technique for generating digital circuits. This shift from hand-designed circuits to synthesized ones can be likened to the shift from writing programs in assembly code to writing them in a high-level language and having a compiler generate the machine code. 
Our HCL language expresses only the control portions of a hardware design, with only a limited set of operations and with no modularity. As we will see, however, the control logic is the most dif.cult part of designing a microprocessor. We have developed tools that can directly translate HCL into Verilog, and by combining this code with Verilog code for the basic hardware units, we can generate HDL descriptions from which actual working microprocessors can be synthesized. By carefully separating out, designing, and testing the control logic, we can create a working microprocessor with reasonable effort. Web Aside arch:vlog describes how we can generate Verilog versions of a Y86 processor. 
4.2.1 Logic Gates 
Logic gates are the basic computing elements for digital circuits. They generate an output equal to some Boolean function of the bit values at their inputs. Figure 4.9 shows the standard symbols used for Boolean functions And, Or, and Not. HCL expressions are shown below the gates for the operators in C (Section 2.1.9): && for And, || for Or, and ! for Not. We use these instead of the bit-level C operators &, |, and ~, because logic gates operate on single-bit quantities, not entire words. Although the .gure illustrates only two-input versions of the And and Or gates, it is common to see these being used as n-way operations for n>2. We still write these in HCL using binary operators, though, so the operation of a three-input And gate with inputs a, b, and c is described with the HCL expression a&&b&&c. 
Logic gates are always active. If some input to a gate changes, then within some small amount of time, the output will change accordingly. 
Figure 4.9 And Or Not Logic gate types. Each 
a
a gate generates output out 
out a 
out 
bb

equal to some Boolean 
out a && b out a || b out !a
function of its inputs. 

Figure 4.10  
Combinational circuit to  a  
test for bit equality. The  
output will equal 1 when  
both inputs are 0, or both  
 
are 1.  
b  

4.2.2 Combinational Circuits and HCL Boolean Expressions 
By assembling a number of logic gates into a network, we can construct compu-tational blocks known as combinational circuits. Two restrictions are placed on how the networks are constructed: 
. The outputs of two or more logic gates cannot be connected together. Other-wise, the two could try to drive the wire in opposite directions, possibly causing an invalid voltage or a circuit malfunction. 
. The network must be acyclic. That is, there cannot be a path through a series of gates that forms a loop in the network. Such loops can cause ambiguity in the function computed by the network. 
Figure 4.10 shows an example of a simple combinational circuit that we will .nd useful. It has two inputs, a and b. It generates a single output eq, such that the output will equal 1 if either a and b are both 1 (detected by the upper And gate) or are both 0 (detected by the lower And gate). We write the function of this network in HCL as 
booleq =(a && b) || (!a&&!b); 
This code simply de.nes the bit-level (denoted by data type bool) signal eq as a function of inputs a and b. As this example shows, HCL uses C-style syntax, with ¡®=¡¯ associating a signal name with an expression. Unlike C, however, we do not view this as performing a computation and assigning the result to some memory location. Instead, it is simply a way to give a name to an expression. 

Practice Problem 4.8 
Write an HCL expression for a signal xor, equal to the Exclusive-Or of inputs a and b. What is the relation between the signals xor and eq de.ned above? 
Figure 4.11 shows another example of a simple but useful combinational circuit known as a multiplexor (commonly referred to as a ¡°MUX¡±). A multiplexor selects a value from among a set of different data signals, depending on the value of a control input signal. In this single-bit multiplexor, the two data signals are the input bits a and b, while the control signal is the input bit s. The output will equal a when s is 1, and it will equal b when s is 0. In this circuit, we can see that the two And gates determine whether to pass their respective data inputs to the Or gate. 

Single-bit multiplexor circuit. The output will equal input a if the control signal s is 1 and will equal b input b when s is 0. 
a 


The upper And gate passes signal b when s is 0 (since the other input to the gate is !s), while the lower And gate passes signal a when s is 1. Again, we can write an HCL expression for the output signal, using the same operations as are present in the combinational circuit: 
bool out=(s && a) || (!s && b); 
Our HCL expressions demonstrate a clear parallel between combinational logic circuits and logical expressions in C. They both use Boolean operations to compute functions over their inputs. Several differences between these two ways of expressing computation are worth noting: 
. Since a combinational circuit consists of a series of logic gates, it has the property that the outputs continually respond to changes in the inputs. If some input to the circuit changes, then after some delay, the outputs will change accordingly. In contrast, a C expression is only evaluated when it is encountered during the execution of a program. 
. Logical expressions in C allow arguments to be arbitrary integers, interpreting 0as false and anything else as true. In contrast, our logic gates only operate over the bit values 0 and 1. 
. Logical expressions in C have the property that they might only be partially evaluated. If the outcome of an And or Or operation can be determined by just evaluating the .rst argument, then the second argument will not be evaluated. For example, with the C expression 
(a && !a) && func(b,c) 
the function func will not be called, because the expression (a && !a) evalu-ates to 0. In contrast, combinational logic does not have any partial evaluation rules. The gates simply respond to changing inputs. 
4.2.3 Word-Level Combinational Circuits and HCL Integer Expressions 
By assembling large networks of logic gates, we can construct combinational circuits that compute much more complex functions. Typically, we design circuits that operate on data words. These are groups of bit-level signals that represent an integer or some control pattern. For example, our processor designs will contain numerous words, with word sizes ranging between 4 and 32 bits, representing integers, addresses, instruction codes, and register identi.ers. 
Figure 4.12 
b31 
eq31 

Word-level equality test Bit equal circuit. The output will a31 equal 1 when each bit b30 
eq30 

from word A equals its Bit equal counterpart from word B. a30 Word-level equality is one 
B 
. . .. . .

A = B 

of the operations in HCL. 
Eq 
A 
b1 eq1 
Bit equal 
a1 
b0 eq0 
Bit equal 
a0 
(a) Bit-level implementation (b) Word-level abstraction 
Combinational circuits to perform word-level computations are constructed using logic gates to compute the individual bits of the output word, based on the individual bits of the input words. For example, Figure 4.12 shows a combinational circuit that tests whether two 32-bit words A and B are equal. That is, the output will equal 1 if and only if each bit of A equals the corresponding bit of B. This circuit is implemented using 32 of the single-bit equality circuits shown in Figure 4.10. The outputs of these single-bit circuits are combined with an And gate to form the circuit output. 
In HCL, we will declare any word-level signal as an int, without specifying the word size. This is done for simplicity. In a full-featured hardware description language, every word can be declared to have a speci.c number of bits. HCL allows words to be compared for equality, and so the functionality of the circuit shown in Figure 4.12 can be expressed at the word level as 
boolEq =(A == B); 
where arguments A and B are of type int. Note that we use the same syntax conventions as in C, where ¡®=¡¯ denotes assignment, while ¡®==¡¯ denotes the equality operator. 
As is shown on the right side of Figure 4.12, we will draw word-level circuits using medium-thickness lines to represent the set of wires carrying the individual bits of the word, and we will show the resulting Boolean signal as a dashed line. 
Practice Problem 4.9 
Suppose you want to implement a word-level equality circuit using the Exclusive-Or circuits from Problem 4.8 rather than from bit-level equality circuits. Design such a circuit for a 32-bit word consisting of 32 bit-level Exclusive-Or circuits and two additional logic gates. 

Figure 4.13 s 
Word-level multiplexor circuit. The output will equal input word A when the control signal s is b31 

out31

1, and it will equal B otherwise. Multiplexors are 
a31 

described in HCL using case expressions. 
sb30 
out30 B 

Out a30 A 
int Out = [
 s : A;
 l : B; ]; 
b0 out0 
a0 (a) Bit-level implementation (b) Word-level abstraction 

Figure 4.13 shows the circuit for a word-level multiplexor. This circuit gener-ates a 32-bit word Out equal to one of the two input words, A or B, depending on the control input bit s. The circuit consists of 32 identical subcircuits, each having a structure similar to the bit-level multiplexor from Figure 4.11. Rather than simply replicating the bit-level multiplexor 32 times, the word-level version reduces the number of inverters by generating !s once and reusing it at each bit position. 
We will use many forms of multiplexors in our processor designs. They allow us to select a word from a number of sources depending on some control condi-tion. Multiplexing functions are described in HCL using case expressions. A case expression has the following general form: 
[ 

select 1 : expr 1 
select 2 : expr 2 
. 
. 

. select k : expr k ] 
The expression contains a series of cases, where each case i consists of a Boolean expression selecti, indicating when this case should be selected, and an integer expression expri, indicating the resulting value. 
Unlike the switch statement of C, we do not require the different selection expressions to be mutually exclusive. Logically, the selection expressions are eval-uated in sequence, and the case for the .rst one yielding 1 is selected. For example, the word-level multiplexor of Figure 4.13 can be described in HCL as 
intOut =[ 
s: A; 
1: B; ]; 
In this code, the second selection expression is simply 1, indicating that this case should be selected if no prior one has been. This is the way to specify a default case in HCL. Nearly all case expressions end in this manner. 
Allowing nonexclusive selection expressions makes the HCL code more read-able. An actual hardware multiplexor must have mutually exclusive signals con-trolling which input word should be passed to the output, such as the signals s and !s in Figure 4.13. To translate an HCL case expression into hardware, a logic syn-thesis program would need to analyze the set of selection expressions and resolve any possible con.icts by making sure that only the .rst matching case would be selected. 
The selection expressions can be arbitrary Boolean expressions, and there can be an arbitrary number of cases. This allows case expressions to describe blocks where there are many choices of input signals with complex selection criteria. For example, consider the diagram of a four-way multiplexor shown in Figure 4.14. This circuit selects from among the four input words A, B, C, and D based on the control signals s1 and s0, treating the controls as a 2-bit binary number. We can express this in HCL using Boolean expressions to describe the different combinations of control bit patterns: 
intOut4= [ !s1&&!s0: A; #00 !s1 :B;#01 !s0 :C;#10 1 :D;#11 
]; 
The comments on the right (any text starting with # and running for the rest of the line is a comment) show which combination of s1 and s0 will cause the case to 

The different combinations of control signals s1 and D s0 determine which data C input is transmitted to the B output. A 

be selected. Observe that the selection expressions can sometimes be simpli.ed, since only the .rst matching case is selected. For example, the second expression can be written !s1, rather than the more complete !s1 &&s0, since the only other possibility having s1 equal to 0 was given as the .rst selection expression. Similarly, the third expression can be written as !s0, while the fourth can simply be written as 1. 
As a .nal example, suppose we want to design a logic circuit that .nds the minimum value among a set of words A, B, and C, diagrammed as follows: 

We can express this using an HCL case expression as 
intMin3=[ A<=B&&A<=C:A; B<=A&&B<=C:B; 1 :C; 
]; 

Practice Problem 4.10 
Write HCL code describing a circuit that for word inputs A, B, and C selects the median of the three values. That is, the output equals the word lying between the minimum and maximum of the three inputs. 
Combinational logic circuits can be designed to perform many different types of operations on word-level data. The detailed design of these is beyond the scope of our presentation. One important combinational circuit, known as an arithmetic/logic unit (ALU), is diagrammed at an abstract level in Figure 4.15. This circuit has three inputs: two data inputs labeled A and B, and a control input. Depending on the setting of the control input, the circuit will perform different arithmetic or logical operations on the data inputs. Observe that the four 
0123 

operations diagrammed for this ALU correspond to the four different integer operations supported by the Y86 instruction set, and the control values match the function codes for these instructions (Figure 4.3). Note also the ordering of operands for subtraction, where the A input is subtracted from the B input. This ordering is chosen in anticipation of the ordering of arguments in the subl instruction. 
4.2.4 Set Membership 
In our processor designs, we will .nd many examples where we want to compare one signal against a number of possible matching signals, such as to test whether the code for some instruction being processed matches some category of instruc-tion codes. As a simple example, suppose we want to generate the signals s1 and s0 for the four-way multiplexor of Figure 4.14 by selecting the high-and low-order bits from a 2-bit signal code, as follows: 

In this circuit, the 2-bit signal code would then control the selection among the four data words A, B, C, and D. We can express the generation of signals s1 and s0 using equality tests based on the possible values of code: 
bools1 =code== 2|| code ==3; bools0 =code== 1|| code ==3; 
A more concise expression can be written that expresses the property that s1 is 1 when code is in the set {2,3}, and s0 is 1 when code is in the set {1,3}: 
bools1 =codein {2, 3 }; bools0 =codein {1, 3 }; 
The general form of a set membership test is iexpr in {iexpr1,iexpr2,...,iexprk} 

where the value being tested, iexpr, and the candidate matches, iexpr1 through iexprk, are all integer expressions. 
4.2.5 Memory and Clocking 
Combinational circuits, by their very nature, do not store any information. Instead, they simply react to the signals at their inputs, generating outputs equal to some function of the inputs. To create sequential circuits, that is, systems that have state and perform computations on that state, we must introduce devices that store information represented as bits. Our storage devices are all controlled by a single clock, a periodic signal that determines when new values are to be loaded into the devices. We consider two classes of memory devices: 
Clocked registers (or simply registers) store individual bits or words. The clock signal controls the loading of the register with the value at its input. 
Random-access memories (or simply memories) store multiple words, using an address to select which word should be read or written. Examples of random-access memories include (1) the virtual memory system of a processor, where a combination of hardware and operating system software make it appear to a processor that it can access any word within a large address space; and (2) the register .le, where register identi.ers serve as the addresses. In an IA32 or Y86 processor, the register .le holds the eight program registers (%eax, %ecx, etc.). 
As we can see, the word ¡°register¡± means two slightly different things when speaking of hardware versus machine-language programming. In hardware, a register is directly connected to the rest of the circuit by its input and output wires. In machine-level programming, the registers represent a small collection of addressable words in the CPU, where the addresses consist of register IDs. These words are generally stored in the register .le, although we will see that the hardware can sometimes pass a word directly from one instruction to another to avoid the delay of .rst writing and then reading the register .le. When necessary to avoid ambiguity, we will call the two classes of registers ¡°hardware registers¡± and ¡°program registers,¡± respectively. 
Figure 4.16 gives a more detailed view of a hardware register and how it operates. For most of the time, the register remains in a .xed state (shown as x), generating an output equal to its current state. Signals propagate through the combinational logic preceding the register, creating a new value for the register input (shown as y), but the register output remains .xed as long as the clock is low. As the clock rises, the input signals are loaded into the register as its next state (y), and this becomes the new register output until the next rising clock edge. A key point is that the registers serve as barriers between the combinational logic in different parts of the circuit. Values only propagate from a register input to its output once every clock cycle at the rising clock edge. Our Y86 processors will 
State = x State = y Input = y Rising
Output = x 

Output = y

clock 




Figure 4.16 Register operation. The register outputs remain held at the current register state until the clock signal rises. When the clock rises, the values at the register inputs are captured to become the new register state. 
use clocked registers to hold the program counter (PC), the condition codes (CC), and the program status (Stat). 
The following diagram shows a typical register .le: 
Read ports 
dstW Write port 

clock 
This register .le has two read ports, named A and B, and one write port, named 
W. Such a multiported random-access memory allows multiple read and write operations to take place simultaneously. In the register .le diagrammed, the circuit can read the values of two program registers and update the state of a third. Each port has an address input, indicating which program register should be selected, and a data output or input giving a value for that program register. The addresses are register identi.ers, using the encoding shown in Figure 4.4. The two read ports have address inputs srcA and srcB (short for ¡°source A¡± and ¡°source B¡±) and data outputs valA and valB (short for ¡°value A¡± and ¡°value B¡±). The write port has address input dstW (short for ¡°destination W¡±) and data input valW (short for ¡°value W¡±). 
The register .le is not a combinational circuit, since it has internal storage. In our implementation, however, data can be read from the register .le as if it were a block of combinational logic having addresses as inputs and the data as outputs. When either srcA or srcB is set to some register ID, then, after some delay, the value stored in the corresponding program register will appear on either valA or valB. For example, setting srcA to 3 will cause the value of program register %ebx to be read, and this value will appear on output valA. 
The writing of words to the register .le is controlled by the clock signal in a manner similar to the loading of values into a clocked register. Every time the clock rises, the value on input valW is written to the program register indicated by the register ID on input dstW. When dstW is set to the special ID value 0xF,no program register is written. Since the register .le can be both read and written, a natural question to ask is ¡°What happens if we attempt to read and write the same register simultaneously?¡± The answer is straightforward: if we update a register while using the same register ID on the read port, we would observe a transition from the old value to the new. When we incorporate the register .le into our processor design, we will make sure that we take this property into consideration. 

Our processor has a random-access memory for storing program data, as illustrated below: 
data out 
error read write 


clock 


address data in 
This memory has a single address input, a data input for writing, and a data output for reading. Like the register .le, reading from our memory operates in a manner similar to combinational logic: If we provide an address on the address input and set the write control signal to 0, then after some delay, the value stored at that address will appear on data out.The error signal will be set to 1 if the address is out of range and to 0 otherwise. Writing to the memory is controlled by the clock: we set address to the desired address, data in to the desired value, and write to 
1. When we then operate the clock, the speci.ed location in the memory will be updated, as long as the address is valid. As with the read operation, the error signal will be set to 1 if the address is invalid. This signal is generated by combinational logic, since the required bounds checking is purely a function of the address input and does not involve saving any state. 
Aside Real-life memory design 
The memory system in a full-scale microprocessor is far more complex than the simple one we assume in our design. It consists of several forms of hardware memories, including several random-access memories plus magnetic disk, as well as a variety of hardware and software mechanisms for managing these devices. The design and characteristics of the memory system are described in Chapter 6. 
Nonetheless, our simple memory design can be used for smaller systems, and it provides us with an abstraction of the interface between the processor and memory for more complex systems. 
Our processor includes an additional read-only memory for reading instruc-tions. In most actual systems, these memories are merged into a single memory with two ports: one for reading instructions and the other for reading or writing data. 
4.3 
Sequential 
Y86 
Implementations 

Now we have the components required to implement a Y86 processor. As a .rst step, we describe a processor called SEQ (for ¡°sequential¡± processor). On each clock cycle, SEQ performs all the steps required to process a complete instruction. This would require a very long cycle time, however, and so the clock rate would be unacceptably low. Our purpose in developing SEQ is to provide a .rst step toward our ultimate goal of implementing an ef.cient, pipelined processor. 
4.3.1 Organizing Processing into Stages 
In general, processing an instruction involves a number of operations. We organize them in a particular sequence of stages, attempting to make all instructions follow a uniform sequence, even though the instructions differ greatly in their actions. The detailed processing at each step depends on the particular instruction being executed. Creating this framework will allow us to design a processor that makes best use of the hardware. The following is an informal description of the stages and the operations performed within them: 
Fetch: The fetch stage reads the bytes of an instruction from memory, using the program counter (PC) as the memory address. From the instruction it extracts the two 4-bit portions of the instruction speci.er byte, referred to as icode (the instruction code) and ifun (the instruction function). It possibly fetches a register speci.er byte, giving one or both of the register operand speci.ers rA and rB. It also possibly fetches a 4-byte constant word valC. It computes valP to be the address of the instruction following the current one in sequential order. That is, valP equals the value of the PC plus the length of the fetched instruction. 
Decode: The decode stage reads up to two operands from the register .le, giving values valA and/or valB. Typically, it reads the registers designated by instruction .elds rA and rB, but for some instructions it reads register %esp. 
Execute: In the execute stage, the arithmetic/logic unit (ALU) either performs the operation speci.ed by the instruction (according to the value of ifun), computes the effective address of a memory reference, or increments or decrements the stack pointer. We refer to the resulting value as valE.The condition codes are possibly set. For a jump instruction, the stage tests the condition codes and branch condition (given by ifun) to see whether or not the branch should be taken. 
Memory: The memory stage may write data to memory, or it may read data from memory. We refer to the value read as valM. 
Write back: The write-back stage writes up to two results to the register .le. 
PC update: The PC is set to the address of the next instruction. 
The processor loops inde.nitely, performing these stages. In our simpli.ed im-plementation, the processor will stop when any exception occurs: it executes a 

Section 4.3 Sequential Y86 Implementations 365 

1  0x000:  30f209000000  |  irmovl  $9,  %edx  
2  0x006:  30f315000000  |  irmovl  $21,  %ebx  
3  0x00c:  6123  |  subl  %edx,  %ebx  #  subtract  
4  0x00e:  30f480000000  |  irmovl  $128,%esp  #  Problem  4.11  
5  0x014:  404364000000  |  rmmovl  %esp,  100(%ebx)  #  store  
6  0x01a:  a02f  |  pushl  %edx  #  push  
7  0x01c:  b00f  |  popl  %eax  #  Problem  4.12  
8  0x01e:  7328000000  |  je  done  #  Not  taken  
9  0x023:  8029000000  |  call  proc  #  Problem  4.16  
10  0x028:  |  done:  
11  0x028:  00  |  halt  
12  0x029:  |  proc:  
13  0x029:  90  |  ret  #  Return  

Figure 4.17 Sample Y86 instruction sequence. We will trace the processing of these instructions through the different stages. 
halt or invalid instruction, or it attempts to read or write an invalid address. In a more complete design, the processor would enter an exception-handling mode and begin executing special code determined by the type of exception. 
As can be seen by the preceding description, there is a surprising amount of processing required to execute a single instruction. Not only must we perform the stated operation of the instruction, we must also compute addresses, update stack pointers, and determine the next instruction address. Fortunately, the overall .ow can be similar for every instruction. Using a very simple and uniform structure is important when designing hardware, since we want to minimize the total amount of hardware, and we must ultimately map it onto the two-dimensional surface of an integrated-circuit chip. One way to minimize the complexity is to have the different instructions share as much of the hardware as possible. For example, each of our processor designs contains a single arithmetic/logic unit that is used in different ways depending on the type of instruction being executed. The cost of duplicating blocks of logic in hardware is much higher than the cost of having multiple copies of code in software. It is also more dif.cult to deal with many special cases and idiosyncrasies in a hardware system than with software. 
Our challenge is to arrange the computing required for each of the different instructions to .t within this general framework. We will use the code shown in Figure 4.17 to illustrate the processing of different Y86 instructions. Figures 4.18 through 4.21 contain tables describing how the different Y86 instructions proceed through the stages. It is worth the effort to study these tables carefully. They are in a form that enables a straightforward mapping into the hardware. Each line in these tables describes an assignment to some signal or stored state (indicated by the assignment operation ¡û). These should be read as if they were evaluated in sequence from top to bottom. When we later map the computations to hardware, we will .nd that we do not need to perform these evaluations in strict sequential order. 
366  Chapter 4  Processor Architecture  
Stage Fetch Decode Execute Memory Write back  OPl rA, rB icode:ifun ¡û M1[PC] rA :rB ¡û M1[PC + 1] valP ¡û PC + 2 valA ¡û R[rA] valB ¡û R[rB] valE ¡û valB OP valA Set CC R[rB] ¡û valE  rrmovl rA, rB icode:ifun ¡û M1[PC] rA :rB ¡û M1[PC + 1] valP ¡û PC + 2 valA ¡û R[rA] valE ¡û 0 + valA R[rB] ¡û valE  irmovl V, rB icode:ifun ¡û M1[PC] rA :rB ¡û M1[PC + 1] valC ¡û M4[PC + 2] valP ¡û PC + 6 valE ¡û 0 + valC R[rB] ¡û valE  
PC update  PC ¡û valP  PC ¡û valP  PC ¡û valP  

Figure 4.18 Computations in sequential implementation of Y86 instructions OPl, rrmovl, and irmovl. These instructions compute a value and store the result in a register. The notation icode : ifun indicates the two components of the instruction byte, while rA : rB indicates the two components of the register speci.er byte. The notation M1[x] indicates accessing (either reading or writing) 1 byte at memory location x, while M4[x] indicates accessing 4 bytes. 
Figure 4.18 shows the processing required for instruction types OPl (integer and logical operations), rrmovl (register-register move), and irmovl (immediate-register move). Let us .rst consider the integer operations. Examining Figure 4.2, we can see that we have carefully chosen an encoding of instructions so that the four integer operations (addl, subl, andl, and xorl) all have the same value of icode. We can handle them all by an identical sequence of steps, except that the ALU computation must be set according to the particular instruction operation, encoded in ifun. 
The processing of an integer-operation instruction follows the general pattern listed above. In the fetch stage, we do not require a constant word, and so valP is computed as PC + 2. During the decode stage, we read both operands. These are supplied to the ALU in the execute stage, along with the function speci.er ifun, so that valE becomes the instruction result. This computation is shown as the expression valB OP valA, where OP indicates the operation speci.ed by ifun. Note the ordering of the two arguments¡ªthis order is consistent with the conventions of Y86 (and IA32). For example, the instruction subl %eax,%edx is supposed to compute the value R[%edx] . R[%eax]. Nothing happens in the memory stage for these instructions, but valE is written to register rB in the write-back stage, and the PC is set to valP to complete the instruction execution. 

Aside Tracing the execution of a subl instruction 
As an example, let us follow the processing of the subl instruction on line 3 of the object code shown in Figure 4.17. We can see that the previous two instructions initialize registers %edx and %ebx to 9 and 21, respectively. We can also see that the instruction is located at address 0x00c and consists of 2 bytes, having values 0x61 and 0x23. The stages would proceed as shown in the following table, which lists the generic rule for processing an OPl instruction (Figure 4.18) on the left, and the computations for this speci.c instruction on the right. 
Generic  Speci.c  
Stage  OPl rA, rB  subl %edx, %ebx  
Fetch  icode:ifun ¡û M1[PC]  icode: ifun ¡û M1[0x00c] = 6 : 1  
rA :rB ¡û M1[PC + 1]  rA : rB ¡û M1[0x00d] = 2 : 3  
valP ¡û PC + 2  valP ¡û 0x00c + 2 = 0x00e  
Decode  valA ¡û R[rA]  valA ¡û R[%edx] = 9  
valB ¡û R[rB]  valB ¡û R[%ebx] = 21  
Execute  valE ¡û valB OP valA  valE ¡û 21 . 9 = 12  
Set CC  ZF ¡û 0, SF ¡û 0, OF ¡û 0  
Memory  
Write back  R[rB] ¡û valE  R[%ebx] ¡û valE = 12  
PC update  PC ¡û valP  PC ¡û valP = 0x00e  

As this trace shows, we achieve the desired effect of setting register %ebx to 12, setting all three condition codes to zero, and incrementing the PC by 2. 
Executing an rrmovl instruction proceeds much like an arithmetic operation. We do not need to fetch the second register operand, however. Instead, we set the second ALU input to zero and add this to the .rst, giving valE = valA, which is then written to the register .le. Similar processing occurs for irmovl, except that we use constant value valC for the .rst ALU input. In addition, we must increment the program counter by 6 for irmovl due to the long instruction format. Neither of these instructions changes the condition codes. 
Practice Problem 4.11 
Fill in the right-hand column of the following table to describe the processing of the irmovl instruction on line 4 of the object code in Figure 4.17: 
368  Chapter 4  Processor Architecture  
Generic  Speci.c  
Stage  irmovl V, rB  irmovl $128, %esp  

Fetch Decode  icode:ifun ¡û M1[PC] rA :rB ¡û M1[PC + 1] valC ¡û M4[PC + 2] valP ¡û PC + 6  
Execute  valE ¡û 0 + valC  
Memory Write back  R[rB] ¡û valE  
PC update  PC ¡û valP  
Figure 4.19 shows the processing required for the memory write and read in-structions rmmovl and mrmovl. We see the same basic .ow as before, but using the ALU to add valC to valB, giving the effective address (the sum of the displacement 


How does this instruction execution modify the registers and the PC? 
Stage Fetch  rmmovl rA, D(rB) icode:ifun ¡û M1[PC] rA :rB ¡û M1[PC + 1] valC ¡û M4[PC + 2] valP ¡û PC + 6  mrmovl D(rB), rA icode:ifun ¡û M1[PC] rA :rB ¡û M1[PC + 1] valC ¡û M4[PC + 2] valP ¡û PC + 6  
Decode  valA ¡û R[rA] valB ¡û R[rB]  valB ¡û R[rB]  
Execute  valE ¡û valB + valC  valE ¡û valB + valC  
Memory  M4[valE] ¡û valA  valM ¡û M4[valE]  
Write back  R[rA] ¡û valM  
PC update  PC ¡û valP  PC ¡û valP  
Figure 4.19 Computations in sequential implementation of Y86 instructions rmmovl and mrmovl. These instructions read or write memory. 




and the base register value) for the memory operation. In the memory stage we either write the register value valA to memory, or we read valM from memory. 
Aside Tracing the execution of an rmmovl instruction 
Let us trace the processing of the rmmovl instruction on line 5 of the object code shown in Figure 4.17. We can see that the previous instruction initialized register %esp to 128, while %ebx still holds 12, as computed by the subl instruction (line 3). We can also see that the instruction is located at address 0x014 and consists of 6 bytes. The .rst 2 have values 0x40 and 0x43, while the .nal 4 are a byte-reversed version of the number 0x00000064 (decimal 100). The stages would proceed as follows: 
Generic Speci.c Stage rmmovl rA, D(rB) rmmovl %esp, 100(%ebx) 
Fetch icode:ifun ¡û M1[PC] icode: ifun ¡û M1[0x014] = 4 : 0 rA :rB ¡û M1[PC + 1] rA : rB ¡û M1[0x015] = 4 : 3 valC ¡û M4[PC + 2] valC ¡û M4[0x016] = 100 valP ¡û PC + 6 valP ¡û 0x014 + 6 = 0x01a 
Decode valA ¡û R[rA] valA ¡û R[%esp] = 128 valB ¡û R[rB] valB ¡û R[%ebx]= 12 
Execute valE ¡û valB + valC valE ¡û 12 + 100 = 112 
Memory M4[valE] ¡û valA M4[112] ¡û 128 
Write back 

PC update PC ¡û valP PC ¡û 0x01a 
As this trace shows, the instruction has the effect of writing 128 to memory address 112 and incrementing thePC by 6. 
Figure 4.20 shows the steps required to process pushl and popl instructions. These are among the most dif.cult Y86 instructions to implement, because they in-volve both accessing memory and incrementing or decrementing the stack pointer. Although the two instructions have similar .ows, they have important differences. 
The pushl instruction starts much like our previous instructions, but in the decode stage we use %esp as the identi.er for the second register operand, giving the stack pointer as value valB. In the execute stage, we use the ALU to decrement the stack pointer by 4. This decremented value is used for the memory write address and is also stored back to %esp in the write-back stage. By using valE as the address for the write operation, we adhere to the Y86 (and IA32) convention that pushl should decrement the stack pointer before writing, even though the actual updating of the stack pointer does not occur until after the memory operation has completed. 
370  Chapter 4  Processor Architecture  
Stage Fetch  pushl rA icode:ifun ¡û M1[PC] rA :rB ¡û M1[PC + 1]  popl rA icode:ifun ¡û M1[PC] rA :rB ¡û M1[PC + 1]  
Decode Execute  valP ¡û PC + 2 valA ¡û R[rA] valB ¡û R[%esp] valE ¡û valB + (.4)  valP ¡û PC + 2 valA ¡û R[%esp] valB ¡û R[%esp] valE ¡û valB + 4  
Memory Write back PC update  M4[valE] ¡û valA R[%esp] ¡û valE PC ¡û valP  valM ¡û M4[valA] R[%esp] ¡û valE R[rA] ¡û valM PC ¡û valP  
Figure 4.20 Computations in sequential implementation of Y86 instructions pushl and popl. These instructions push and pop the stack. 


Aside Tracing the execution of a pushl instruction 
Let us trace the processing of the pushl instruction on line 6 of the object code shown in Figure 4.17. At this point, we have 9 in register %edx and 128 in register %esp. We can also see that the instruction is located at address 0x01a and consists of 2 bytes having values 0xa0 and 0x28. The stages would proceed as follows: 
Generic  Speci.c  
Stage  pushl rA  pushl %edx  
Fetch  icode:ifun ¡û M1[PC]  icode:ifun ¡û M1[0x01a] = a : 0  
rA :rB ¡û M1[PC + 1]  rA :rB ¡û M1[0x01b] = 2 : 8  
valP ¡û PC + 2  valP ¡û 0x01a + 2 = 0x01c  
Decode  valA ¡û R[rA]  valA ¡û R[%edx] = 9  
valB ¡û R[%esp]  valB ¡û R[%esp] = 128  
Execute  valE ¡û valB + (.4)  valE ¡û 128 + (.4) = 124  
Memory  M4[valE] ¡û valA  M4[124] ¡û 9  
Write back  R[%esp] ¡û valE  R[%esp] ¡û 124  
PC update  PC ¡û valP  PC ¡û 0x01c  

As this trace shows, the instruction has the effect of setting %esp to 124, writing 9 to address 124, and incrementing the PC by 2. 
The popl instruction proceeds much like pushl, except that we read two copies of the stack pointer in the decode stage. This is clearly redundant, but we will see that having the stack pointer as both valA and valB makes the subsequent .ow more similar to that of other instructions, enhancing the overall uniformity of the design. We use the ALU to increment the stack pointer by 4 in the execute stage, but use the unincremented value as the address for the memory operation. In the write-back stage, we update both the stack pointer register with the incre-mented stack pointer, and register rA with the value read from memory. Using the unincremented stack pointer as the memory read address preserves the Y86 (and IA32) convention that popl should .rst read memory and then increment the stack pointer. 
Practice Problem 4.12 
Fill in the right-hand column of the following table to describe the processing of the popl instruction on line 7 of the object code in Figure 4.17: 
Generic Speci.c Stage popl rA popl %eax 
Fetch  icode:ifun ¡û M1[PC] rA :rB ¡û M1[PC + 1]  
valP ¡û PC + 2  
Decode  valA ¡û R[%esp] valB ¡û R[%esp]  
Execute  valE ¡û valB + 4  
Memory  valM ¡û M4[valA]  
Write back  R[%esp] ¡û valE R[rA] ¡û valM  
PC update  PC ¡û valP  

What effect does this instruction execution have on the registers and the PC? 
Practice Problem 4.13 
What would be the effect of the instruction pushl %esp according to the steps listed in Figure 4.20? Does this conform to the desired behavior for Y86, as determined in Problem 4.6? 
Practice Problem 4.14 
Assume the two register writes in the write-back stage for popl occur in the order listed in Figure 4.20. What would be the effect of executing popl %esp? Does this conform to the desired behavior for Y86, as determined in Problem 4.7? 
Figure 4.21 indicates the processing of our three control transfer instructions: the different jumps, call, and ret. We see that we can implement these instruc-tions with the same overall .ow as the preceding ones. 
As with integer operations, we can process all of the jumps in a uniform manner, since they differ only when determining whether or not to take the branch. A jump instruction proceeds through fetch and decode much like the previous instructions, except that it does not require a register speci.er byte. In the execute stage, we check the condition codes and the jump condition to deter-mine whether or not to take the branch, yielding a 1-bit signal Cnd. During the PC update stage, we test this .ag, and set the PC to valC (the jump target) if the .ag is 1, and to valP (the address of the following instruction) if the .ag is 0. Our notation x ? a : b is similar to the conditional expression in C¡ªit yields a when x is nonzero and b when x is zero. 

Stage Fetch  jXX Dest icode: ifun ¡û M1[PC]  call Dest icode: ifun ¡û M1[PC]  ret icode: ifun ¡û M1[PC]  
valC ¡û M4[PC + 1] valP ¡û PC + 5  valC ¡û M4[PC + 1] valP ¡û PC + 5  valP ¡û PC + 1  
Decode Execute Memory Write back  Cnd ¡û Cond(CC, ifun)  valB ¡û R[%esp] valE ¡û valB + (.4) M4[valE] ¡û valP R[%esp] ¡û valE  valA ¡û R[%esp] valB ¡û R[%esp] valE ¡û valB + 4 valM ¡û M4[valA] R[%esp] ¡û valE  
PC update  PC ¡û Cnd ? valC : valP  PC ¡û valC  PC ¡û valM  
Figure 4.21 Computations in sequential implementation of Y86 instructions jXX, call, and ret. These instructions cause control transfers. 


Aside Tracing the execution of a je instruction 
Let us trace the processing of the je instruction on line 8 of the object code shown in Figure 4.17. The condition codes were all set to zero by the subl instruction (line 3), and so the branch will not be taken. The instruction is located at address 0x01e and consists of 5 bytes. The .rst has value 0x73, while the remaining 4 are a byte-reversed version of the number 0x00000028, the jump target. The stages would proceed as follows: 
Generic Speci.c Stage jXX Dest je 0x028 
Fetch icode:ifun ¡û M1[PC] icode:ifun ¡û M1[0x01e] = 7 : 3 
valC ¡û M4[PC + 1] valC ¡û M4[0x01f] = 0x028 valP ¡û PC + 5 valP ¡û 0x01e + 5 = 0x023 
Decode 
Execute 

Cnd ¡û Cond(CC, ifun) Cnd ¡û Cond( 0, 0, 0 , 3) = 0 Memory Write back 
PC update PC ¡û Cnd ? valC : valP PC ¡û 0? 0x028 : 0x023 = 0x023 
As this trace shows, the instruction has the effect of incrementing the PC by 5. 
Practice Problem 4.15 
We can see by the instruction encodings (Figures 4.2 and 4.3) that the rmmovl instruction is the unconditional version of a more general class of instructions that include the conditional moves. Show how you would modify the steps for the rrmovl instruction below to also handle the six conditional move instructions. You may .nd it useful to see how the implementation of the jXX instructions (Figure 4.21) handles conditional behavior. 
Stage  cmovXX rA, rB  
Fetch  icode:ifun ¡û M1[PC]  
rA :rB ¡û M1[PC + 1]  
valP ¡û PC + 2  
Decode  valA ¡û R[rA]  
Execute  valE ¡û 0 + valA  
Memory  
Write back  
R[rB] ¡û valE  
PC update  PC ¡û valP  

Instructions call and ret bear some similarity to instructions pushl and popl, except that we push and pop program counter values. With instruction call,we push valP, the address of the instruction that follows the call instruction. During the PC update stage, we set the PC to valC, the call destination. With instruction ret, we assign valM, the value popped from the stack, to the PC in the PC update stage. 
Practice Problem 4.16 
Fill in the right-hand column of the following table to describe the processing of the call instruction on line 9 of the object code in Figure 4.17: 
Generic Speci.c Stage call Dest call 0x029 
Fetch  icode:ifun ¡û M1[PC]  
Decode Execute  valC ¡û M4[PC + 1] valP ¡û PC + 5 valB ¡û R[%esp] valE ¡û valB + (.4)  
Memory Write back  M4[valE] ¡û valP R[%esp] ¡û valE  
PC update  PC ¡û valC  

What effect would this instruction execution have on the registers, the PC, and the memory? 
We have created a uniform framework that handles all of the different types of Y86 instructions. Even though the instructions have widely varying behavior, we can organize the processing into six stages. Our task now is to create a hardware design that implements the stages and connects them together. 

Aside Tracing the execution of a ret instruction 
Let us trace the processing of the ret instruction on line 13 of the object code shown in Figure 4.17. The instruction address is 0x029 and is encoded by a single byte 0x90. The previous call instruction set %esp to 124 and stored the return address 0x028 at memory address 124. The stages would proceed as follows: 
Section 4.3  Sequential Y86 Implementations  375  
Generic  Speci.c  
Stage  ret  ret  

Fetch icode:ifun ¡û M1[PC] icode:ifun ¡û M1[0x029] = 9 : 0 
valP ¡û PC + 1 valP ¡û 0x029 + 1 = 0x02a 
Decode valA ¡û R[%esp] valA ¡û R[%esp] = 124 valB ¡û R[%esp] valB ¡û R[%esp] = 124 
Execute valE ¡û valB + 4 valE ¡û 124 + 4 = 128 
Memory valM ¡û M4[valA] valM ¡û M4[124] = 0x028 
Write back R[%esp] ¡û valE R[%esp] ¡û 128 
PC update PC ¡û valM PC ¡û 0x028 
As this trace shows, the instruction has the effect of setting the PC to 0x028, the address of the halt instruction. It also sets %esp to 128. 
4.3.2 SEQ Hardware Structure 
The computations required to implement all of the Y86 instructions can be orga-nized as a series of six basic stages: fetch, decode, execute, memory, write back, and PC update. Figure 4.22 shows an abstract view of a hardware structure that can perform these computations. The program counter is stored in a register, shown in the lower left-hand corner (labeled ¡°PC¡±). Information then .ows along wires (shown grouped together as a heavy black line), .rst upward and then around to the right. Processing is performed by hardware units associated with the different stages. The feedback paths coming back down on the right-hand side contain the updated values to write to the register .le and the updated program counter. In SEQ, all of the processing by the hardware units occurs within a single clock cycle, as is discussed in Section 4.3.3. This diagram omits some small blocks of combi-national logic as well as all of the control logic needed to operate the different hardware units and to route the appropriate values to the units. We will add this detail later. Our method of drawing processors with the .ow going from bottom to top is unconventional. We will explain the reason for our convention when we start designing pipelined processors. 
The hardware units are associated with the different processing stages: 
Fetch: Using the program counter register as an address, the instruction 
memory reads the bytes of an instruction. The PC incrementer computes 
valP, the incremented program counter. 
Figure 4.22 

Abstract view of SEQ, a sequential implemen-tation. The information processed during exe-cution of an instruction follows a clockwise .ow starting with an instruction fetch using the program counter (PC), shown in the lower left-hand corner of the .gure. 
PC newPC 
Write back 
Memory 
Execute 
Decode 
icode, ifun rA, rB valC 
Fetch 



Section 4.3 Sequential Y86 Implementations 377 

Decode:  The register .le has two read ports, A and B, via which register values valA and valB are read simultaneously.  
Execute:  The execute stage uses the arithmetic/logic (ALU) unit for different purposes according to the instruction type. For integer operations, it performs the speci.ed operation. For other instructions, it serves as an adder to compute an incremented or decremented stack pointer, to compute an effective address, or simply to pass one of its inputs to its outputs by adding zero.  
The condition code register (CC) holds the three condition-code bits. New values for the condition codes are computed by the ALU. When executing a jump instruction, the branch signal Cnd is computed based on the condition codes and the jump type.  
Memory:  The data memory reads or writes a word of memory when executing a memory instruction. The instruction and data memories access the same memory locations, but for different purposes.  
Write back:  The register .le has two write ports. Port E is used to write values computed by the ALU, while port M is used to write values read from the data memory.  

Figure 4.23 gives a more detailed view of the hardware required to implement SEQ (although we will not see the complete details until we examine the individual stages). We see the same set of hardware units as earlier, but now the wires are shown explicitly. In this .gure, as well as in our other hardware diagrams, we use the following drawing conventions: 
. Hardware units are shown as light blue boxes. These include the memories, the ALU, and so forth. We will use the same basic set of units for all of our processor implementations. We will treat these units as ¡°black boxes¡± and not go into their detailed designs. 
. Control logic blocks are drawn as gray rounded rectangles. These blocks serve to select from among a set of signal sources, or to compute some Boolean func-tion. We will examine these blocks in complete detail, including developing HCL descriptions. 
. Wire names are indicated in white round boxes. These are simply labels on the wires, not any kind of hardware element. 
. Word-wide data connections are shown as medium lines. Each of these lines actually represents a bundle of 32 wires, connected in parallel, for transferring a word from one part of the hardware to another. 
. Byte and narrower data connections are shown as thin lines.Each of these lines actually represents a bundle of four or eight wires, depending on what type of values must be carried on the wires. 
. Single-bit connections are shown as dotted lines.These represent control values passed between the units and blocks on the chip. 
All of the computations we have shown in Figures 4.18 through 4.21 have the property that each line represents either the computation of a speci.c value, such 
PC 
Memory 
Execute 
Decode 
instr_valid imem_error 
Fetch 


Section 4.3  Sequential Y86 Implementations  379  
Stage Fetch Decode Execute Memory Write back PC update  Computation icode, ifun rA, rB valC valP valA, srcA valB, srcB valE Cond. codes read/write E port, dstE M port, dstM PC  OPl rA, rB icode:ifun ¡û M1[PC] rA :rB ¡û M1[PC + 1] valP ¡û PC + 2 valA ¡û R[rA] valB ¡û R[rB] valE ¡û valB OP valA Set CC R[rB] ¡û valE PC ¡û valP  mrmovl D(rB), rA icode:ifun ¡û M1[PC] rA :rB ¡û M1[PC + 1] valC ¡û M4[PC + 2] valP ¡û PC + 6 valB ¡û R[rB] valE ¡û valB + valC valM ¡û M4[valE] R[rA] ¡û valM PC ¡û valP  
Figure 4.24 Identifying the different computation steps in the sequential imple-mentation. The second column identi.es the value being computed or the operation being performed in the stages of SEQ. The computations for instructions OPl and mrmovl are shown as examples of the computations. 


as valP, or the activation of some hardware unit, such as the memory. These com-putations and actions are listed in the second column of Figure 4.24. In addition to the signals we have already described, this list includes four register ID signals: srcA, the source of valA; srcB, the source of valB; dstE, the register to which valE gets written; and dstM, the register to which valM gets written. 
The two right-hand columns of this .gure show the computations for the OPl and mrmovl instructions to illustrate the values being computed. To map the computations into hardware, we want to implement control logic that will transfer the data between the different hardware units and operate these units in such a way that the speci.ed operations are performed for each of the different instruction types. That is the purpose of the control logic blocks, shown as gray rounded boxes in Figure 4.23. Our task is to proceed through the individual stages and create detailed designs for these blocks. 
4.3.3 SEQ Timing 
In introducing the tables of Figures 4.18 through 4.21, we stated that they should be read as if they were written in a programming notation, with the assignments performed in sequence from top to bottom. On the other hand, the hardware structure of Figure 4.23 operates in a fundamentally different way, with a single clock transition triggering a .ow through combinational logic to execute an entire instruction. Let us see how the hardware can implement the behavior listed in these tables. 
Our implementation of SEQ consists of combinational logic and two forms of memory devices: clocked registers (the program counter and condition code register) and random-access memories (the register .le, the instruction memory, and the data memory). Combinational logic does not require any sequencing or control¡ªvalues propagate through a network of logic gates whenever the inputs change. As we have described, we also assume that reading from a random-access memory operates much like combinational logic, with the output word generated based on the address input. This is a reasonable assumption for smaller memories (such as the register .le), and we can mimic this effect for larger circuits using special clock circuits. Since our instruction memory is only used to read instructions, we can therefore treat this unit as if it were combinational logic. 
We are left with just four hardware units that require an explicit control over their sequencing¡ªthe program counter, the condition code register, the data memory, and the register .le. These are controlled via a single clock signal that triggers the loading of new values into the registers and the writing of values to the random-access memories. The program counter is loaded with a new instruction address every clock cycle. The condition code register is loaded only when an integer operation instruction is executed. The data memory is written only when an rmmovl, pushl,or call instruction is executed. The two write ports of the register .le allow two program registers to be updated on every cycle, but we can use the special register ID 0xF as a port address to indicate that no write should be performed for this port. 
This clocking of the registers and memories is all that is required to control the sequencing of activities in our processor. Our hardware achieves the same effect as would a sequential execution of the assignments shown in the tables of Figures 4.18 through 4.21, even though all of the state updates actually occur simultaneously and only as the clock rises to start the next cycle. This equivalence holds because of the nature of the Y86 instruction set, and because we have organized the computations in such a way that our design obeys the following principle: 
The processor never needs to read back the state updated by an instruction in order to complete the processing of this instruction. 
This principle is crucial to the success of our implementation. As an illustration, suppose we implemented the pushl instruction by .rst decrementing %esp by 4 and then using the updated value of %esp as the address of a write operation. This approach would violate the principle stated above. It would require reading the updated stack pointer from the register .le in order to perform the memory operation. Instead, our implementation (Figure 4.20) generates the decremented value of the stack pointer as the signal valE and then uses this signal both as the data for the register write and the address for the memory write. As a result, it can perform the register and memory writes simultaneously as the clock rises to begin the next clock cycle. 

As another illustration of this principle, we can see that some instructions (the integer operations) set the condition codes, and some instructions (the jump instructions) read these condition codes, but no instruction must both set and then read the condition codes. Even though the condition codes are not set until the clock rises to begin the next clock cycle, they will be updated before any instruction attempts to read them. 
Figure 4.25 shows how the SEQ hardware would process the instructions at lines 3 and 4 in the following code sequence, shown in assembly code with the instruction addresses listed on the left: 
1 0x000: irmovl $0x100,%ebx # %ebx <--0x100 
2 0x006: irmovl $0x200,%edx # %edx <--0x200 
3 0x00c: addl %edx,%ebx # %ebx <--0x300 CC <--000 
4 0x00e: je dest # Not taken 
5 0x013: rmmovl %ebx,0(%edx) # M[0x200] <--0x300 
6 0x019: dest: halt 
Each of the diagrams labeled 1 through 4 shows the four state elements plus the combinational logic and the connections among the state elements. We show the combinational logic as being wrapped around the condition code register, because some of the combinational logic (such as the ALU) generates the input to the condition code register, while other parts (such as the branch computation and the PC selection logic) have the condition code register as input. We show the register .le and the data memory as having separate connections for reading and writing, since the read operations propagate through these units as if they were combinational logic, while the write operations are controlled by the clock. 
The color coding in Figure 4.25 indicates how the circuit signals relate to the different instructions being executed. We assume the processing starts with the condition codes, listed in the order ZF, SF, and OF, set to 100. At the beginning of clock cycle 3 (point 1), the state elements hold the state as updated by the second irmovl instruction (line 2 of the listing), shown in light gray. The combinational logic is shown in white, indicating that it has not yet had time to react to the changed state. The clock cycle begins with address 0x00c loaded into the program counter. This causes the addl instruction (line 3 of the listing), shown in blue, to be fetched and processed. Values .ow through the combinational logic, including the reading of the random-access memories. By the end of the cycle (point 2), the combinational logic has generated new values (000) for the condition codes, an update for program register %ebx, and a new value (0x00e) for the program counter. At this point, the combinational logic has been updated according to the addl instruction (shown in blue), but the state still holds the values set by the second irmovl instruction (shown in light gray). 
As the clock rises to begin cycle 4 (point 3), the updates to the program counter, the register .le, and the condition code register occur, and so we show these in blue, but the combinational logic has not yet reacted to these changes, and so we show this in white. In this cycle, the je instruction (line 4 in the listing), shown in dark gray, is fetched and executed. Since condition code ZF is 0, the branch is not 

Cycle 1: Cycle 2: Cycle 3: Cycle 4: Cycle 5: 
0x000:  irmovl $0x100,%ebx # %ebx <-- 0x100  
0x006:  irmovl $0x200,%edx # %edx <-- 0x200  
0x00c:  addl %edx,%ebx # %ebx <-- 0x300 CC <-- 000  
0x00e:  je dest # Not taken  
0x013:  rmmov1 %ebx,0(%edx) # M[0x200] <-- 0x300  


Beginning of cycle 3 
End of cycle 3 
Read Write 

%ebx 0x300 



Beginning of cycle 4 
End of cycle 4 


taken. By the end of the cycle (point 4), a new value of 0x013 has been generated for the program counter. The combinational logic has been updated according to the je instruction (shown in dark gray), but the state still holds the values set by the addl instruction (shown in blue) until the next cycle begins. 
As this example illustrates, the use of a clock to control the updating of the state elements, combined with the propagation of values through combinational logic, suf.ces to control the computations performed for each instruction in our implementation of SEQ. Every time the clock transitions from low to high, the processor begins executing a new instruction. 
4.3.4 SEQ Stage Implementations 
In this section, we devise HCL descriptions for the control logic blocks required to implement SEQ. A complete HCL description for SEQ is given in Web Aside arch:hcl. We show some example blocks here, while others are given as practice problems. We recommend that you work these practice problems as a way to check your understanding of how the blocks relate to the computational requirements of the different instructions. 
Part of the HCL description of SEQ that we do not include here is a de.nition of the different integer and Boolean signals that can be used as arguments to the HCL operations. These include the names of the different hardware signals, as well as constant values for the different instruction codes, function codes, register names, ALU operations, and status codes. Only those that must be explicitly referenced in the control logic are shown. The constants we use are documented in Figure 4.26. By convention, we use uppercase names for constant values. 
In addition to the instructions shown in Figures 4.18 to 4.21, we include the processing for the nop and halt instructions. The nop instruction simply .ows through stages without much processing, except to increment the PC by 1. The halt instruction causes the processor status to be set to HLT, causing it to halt operation. 
Fetch Stage 

As shown in Figure 4.27, the fetch stage includes the instruction memory hardware unit. This unit reads 6 bytes from memory at a time, using the PC as the address of the .rst byte (byte 0). This byte is interpreted as the instruction byte and is split (by the unit labeled ¡°Split¡±) into two 4-bit quantities. The control logic blocks labeled ¡°icode¡± and ¡°ifun¡± then compute the instruction and function codes as equaling either the values read from memory or, in the event that the instruction address is not valid (as indicated by the signal imem_error), the values corresponding to a nop instruction. Based on the value of icode, we can compute three 1-bit signals (shown as dashed lines): 
instr_valid: Does this byte correspond to a legal Y86 instruction? This signal is used to detect an illegal instruction. need_regids: Does this instruction include a register speci.er byte? need_valC: Does this instruction include a constant word? 
384  Chapter 4  Processor Architecture  
Name INOP IHALT IRRMOVL IIRMOVL IRMMOVL IMRMOVL IOPL IJXX ICALL IRET IPUSHL IPOPL FNONE RESP RNONE ALUADD SAOK SADR SINS SHLT  Value (Hex) 0 1 2 3 4 5 6 7 8 9 A B 0 4 F 0 1 2 3 4  Meaning Code for nop instruction Code for halt instruction Code for rrmovl instruction Code for irmovl instruction Code for rmmovl instruction Code for mrmovl instruction Code for integer operation instructions Code for jump instructions Code for call instruction Code for ret instruction Code for pushl instruction Code for popl instruction Default function code Register ID for %esp Indicates no register .le access Function for addition operation Status code for normal operation Status code for address exception Status code for illegal instruction exception Status code for halt  
Figure 4.26 Constant values used in HCL descriptions. These values represent the encodings of the instructions, function codes, register IDs, ALU operations, and status codes. 


The signals instr_valid and imem_error (generated when the instruction address is out of bounds) are used to generate the status code in the memory stage. 
As an example, the HCL description for need_regids simply determines whether the value of icode is one of the instructions that has a register speci.er byte: 
bool need_regids = icode in { IRRMOVL, IOPL, IPUSHL, IPOPL, IIRMOVL, IRMMOVL, IMRMOVL }; 
Practice Problem 4.17 
Write HCL code for the signal need_valC in the SEQ implementation. 
icode ifun rA rB valC valP
Figure 4.27 

SEQ fetch stage. Six bytes are read from the instruction memory using the PC as the starting address. From these bytes, we generate the different instruction .elds. The PC increment block computes signal valP. 


As Figure 4.27 shows, the remaining 5 bytes read from the instruction memory encode some combination of the register speci.er byte and the constant word. These bytes are processed by the hardware unit labeled ¡°Align¡± into the register .elds and the constant word. When the computed signal need_regids is 1, then byte 1 is split into register speci.ers rA and rB. Otherwise, these two .elds are set to 0xF (RNONE), indicating there are no registers speci.ed by this instruction. Recall also (Figure 4.2) that for any instruction having only one register operand, the other .eld of the register speci.er byte will be 0xF (RNONE). Thus, we can assume that the signals rA and rB either encode registers we want to access or indicate that register access is not required. The unit labeled ¡°Align¡± also generates the constant word valC. This will either be bytes 1 to 4 or bytes 2 to 5, depending on the value of signal need_regids. 
The PC incrementer hardware unit generates the signal valP, based on the current value of the PC, and the two signals need_regids and need_valC. For PC value p, need_regids value r, and need_valC value i, the incrementer generates the value p + 1 + r + 4i. 
Decode and Write-Back Stages 
Figure 4.28 provides a detailed view of logic that implements both the decode and write-back stages in SEQ. These two stages are combined because they both access the register .le. 
The register .le has four ports. It supports up to two simultaneous reads (on ports A and B) and two simultaneous writes (on ports E and M). Each port has both an address connection and a data connection, where the address connection is a register ID, and the data connection is a set of 32 wires serving as either an output word (for a read port) or an input word (for a write port) of the register .le. The two read ports have address inputs srcA and srcB, while the two write 
Cnd valA valB valM valE 

ports have address inputs dstE and dstM. The special identi.er 0xF (RNONE)onan address port indicates that no register should be accessed. 
The four blocks at the bottom of Figure 4.28 generate the four different register IDs for the register .le, based on the instruction code icode, the register speci.ers rA and rB, and possibly the condition signal Cnd computed in the execute stage. Register ID srcA indicates which register should be read to generate valA. The desired value depends on the instruction type, as shown in the .rst row for the decode stage in Figures 4.18 to 4.21. Combining all of these entries into a single computation gives the following HCL description of srcA (recall that RESP is the register ID of %esp): 
#  Code  from  SEQ  
int  srcA  =  [  
icode  in  {  IRRMOVL,  IRMMOVL,  IOPL, IPUSHL } : rA;  
icode  in  {  IPOPL, IRET } :  RESP;  
1  :  RNONE;  #  Don¡¯t  need  regis ter  
];  

Practice Problem 4.18 
The register signal srcB indicates which register should be read to generate the signal valB. The desired value is shown as the second step in the decode stage in Figures 4.18 to 4.21. Write HCL code for srcB. 
Register ID dstE indicates the destination register for write port E, where the computed value valE is stored. This is shown in Figures 4.18 to 4.21 as the .rst step in the write-back stage. If we ignore for the moment the conditional move instructions, then we can combine the destination registers for all of the different instructions to give the following HCL description of dstE: 

# WARNING: Conditional move not implemented correctly here 
int  dstE  =  [  
icode  in  {  IRRMOVL}:rB;  
icode  in  {  IIRMOVL,  IOPL}  :  rB;  
icode  in  {  IPUSHL,  IPOPL,  ICALL,  IRET  }  :  RESP;  
1  :  RNONE;  #  Don¡¯t  write  any  register  

]; 

We will revisit this signal and how to implement conditional moves when we examine the execute stage. 
Practice Problem 4.19 
Register ID dstM indicates the destination register for write port M, where valM, the value read from memory, is stored. This is shown in Figures 4.18 to 4.21 as the second step in the write-back stage. Write HCL code for dstM. 
Practice Problem 4.20 
Only the popl instruction uses both register .le write ports simultaneously. For the instruction popl %esp, the same address will be used for both the E and M write ports, but with different data. To handle this con.ict, we must establish a priority among the two write ports so that when both attempt to write the same register on the same cycle, only the write from the higher-priority port takes place. Which of the two ports should be given priority in order to implement the desired behavior, as determined in Problem 4.7? 
Execute Stage 

The execute stage includes the arithmetic/logic unit (ALU). This unit performs the operation add, subtract, and,or Exclusive-Or on inputs aluA and aluB based on the setting of the alufun signal. These data and control signals are generated by three control blocks, as diagrammed in Figure 4.29. The ALU output becomes the signal valE. 
Figure 4.29 Cnd valE SEQ execute stage. The ALU either performs the operation for an integer operation instruction or it acts as an adder. The condition code registers are set according to the ALU value. The condition code values are tested to determine whether or not 
icode 
ifun valC valA valB 

a branch should be taken. 
In Figures 4.18 to 4.21, the ALU computation for each instruction is shown as the .rst step in the execute stage. The operands are listed with aluB .rst, followed by aluA to make sure the subl instruction subtracts valA from valB. We can see that the value of aluA can be valA, valC, or either .4or +4, depending on the instruction type. We can therefore express the behavior of the control block that generates aluA as follows: 
intaluA= [ icode in { IRRMOVL, IOPL } : valA; icode in { IIRMOVL, IRMMOVL, IMRMOVL } : valC; icode in { ICALL, IPUSHL}:-4; icode in { IRET, IPOPL}:4; # Other instructions don¡¯t need ALU 
]; 
Practice Problem 4.21 
Based on the .rst operand of the .rst step of the execute stage in Figures 4.18 to 4.21, write an HCL description for the signal aluB in SEQ. 
Looking at the operations performed by the ALU in the execute stage, we can see that it is mostly used as an adder. For the OPl instructions, however, we want it to use the operation encoded in the ifun .eld of the instruction. We can therefore write the HCL description for the ALU control as follows: 
int alufun = [ icode == IOPL : ifun; 
1 : ALUADD; ]; 
The execute stage also includes the condition code register. Our ALU gen-erates the three signals on which the condition codes are based¡ªzero, sign, and over.ow¡ªevery time it operates. However, we only want to set the condition codes when an OPl instruction is executed. We therefore generate a signal set_cc that controls whether or not the condition code register should be updated: 
bool set_cc = icode in { IOPL }; 
The hardware unit labeled ¡°cond¡± uses a combination of the condition codes and the function code to determine whether a conditional branch or data transfer should take place (Figure 4.3). It generates the Cnd signal used both for the setting of dstE with conditional moves, and in the next PC logic for conditional branches. For other instructions, the Cnd signal may be set to either 1 or 0, depending on the instruction¡¯s function code and the setting of the condition codes, but it will be ignored by the control logic. We omit the detailed design of this unit. 
Figure 4.30 

SEQ memory stage. The data memory can either write or read memory values. The value read from memory forms the signal valM. 


Practice Problem 4.22 
The conditional move instructions, abbreviated cmovXX, have instruction code IRRMOVL. As Figure 4.28 shows, we can implement these instructions by making use of the Cnd signal, generated in the execute stage. Modify the HCL code for dstE to implement these instructions. 
Memory Stage 

The memory stage has the task of either reading or writing program data. As shown in Figure 4.30, two control blocks generate the values for the memory address and the memory input data (for write operations). Two other blocks generate the control signals indicating whether to perform a read or a write operation. When a read operation is performed, the data memory generates the value valM. 
The desired memory operation for each instruction type is shown in the memory stage of Figures 4.18 to 4.21. Observe that the address for memory reads and writes is always valE or valA. We can describe this block in HCL as follows: 
int  mem_addr  =  [  
icode  in  {  IRMMOVL,  IPUSHL,  ICALL,  IMRMOVL  }  :  valE;  
icode  in  {  IPOPL,  IRET  }  :  valA;  
#  Other  ins tructions  don¡¯t  need  address  
];  

Practice Problem 4.23 
Looking at the memory operations for the different instructions shown in Fig-ures 4.18 to 4.21, we can see that the data for memory writes is always either valA or valP. Write HCL code for the signal mem_data in SEQ. 
We want to set the control signal mem_read only for instructions that read data from memory, as expressed by the following HCL code: 
bool mem_read = icode in { IMRMOVL, IPOPL, IRET }; 
Practice Problem 4.24 
We want to set the control signal mem_write only for instructions that write data to memory. Write HCL code for the signal mem_write in SEQ. 
A .nal function for the memory stage is to compute the status code Stat result-ing from the instruction execution, according to the values of icode, imem_error, instr_valid generated in the fetch stage, and the signal dmem_error generated by the data memory. 
Practice Problem 4.25 
Write HCL code for Stat, generating the four status codes SAOK, SADR, SINS, and SHLT (see Figure 4.26). 
PC Update Stage 
The .nal stage in SEQ generates the new value of the program counter. (See Figure 4.31.) As the .nal steps in Figures 4.18 to 4.21 show, the new PC will be valC, valM,or valP, depending on the instruction type and whether or not a branch should be taken. This selection can be described in HCL as follows: 
int new_pc = [ # Call. Use instruction constant icode == ICALL : valC; # Taken branch. Use instruction constant icode == IJXX && Cnd : valC; # Completion of RET instruction. Use value from stack icode == IRET : valM; # Default: Use incremented PC 
1 : valP; ]; 
Figure 4.31 
PC 

SEQ PC update stage. 
The next value of the PC is selected from among the signals valC, valM, and valP, depending on the instruction code and the 

icode Cnd valC valM valP 
branch .ag. 
Surveying SEQ 

We have now stepped through a complete design for a Y86 processor. We have seen that by organizing the steps required to execute each of the different in-structions into a uniform .ow, we can implement the entire processor with a small number of different hardware units and with a single clock to control the sequenc-ing of computations. The control logic must then route the signals between these units and generate the proper control signals based on the instruction types and the branch conditions. 
The only problem with SEQ is that it is too slow. The clock must run slowly enough so that signals can propagate through all of the stages within a single cycle. As an example, consider the processing of a ret instruction. Starting with an updated program counter at the beginning of the clock cycle, the instruction must be read from the instruction memory, the stack pointer must be read from the register .le, the ALU must decrement the stack pointer, and the return address must be read from the memory in order to determine the next value for the program counter. All of this must be completed by the end of the clock cycle. 
This style of implementation does not make very good use of our hardware units, since each unit is only active for a fraction of the total clock cycle. We will see that we can achieve much better performance by introducing pipelining. 
4.4 
General 
Principles 
of 
Pipelining 

Before attempting to design a pipelined Y86 processor, let us consider some general properties and principles of pipelined systems. Such systems are familiar to anyone who has been through the serving line at a cafeteria or run a car through an automated car wash. In a pipelined system, the task to be performed is divided into a series of discrete stages. In a cafeteria, this involves supplying salad, a main dish, dessert, and beverage. In a car wash, this involves spraying water and soap, scrubbing, applying wax, and drying. Rather than having one customer run through the entire sequence from beginning to end before the next can begin, we allow multiple customers to proceed through the system at once. In a typical cafeteria line, the customers maintain the same order in the pipeline and pass through all stages, even if they do not want some of the courses. In the case of the car wash, a new car is allowed to enter the spraying stage as the preceding car moves from the spraying stage to the scrubbing stage. In general, the cars must move through the system at the same rate to avoid having one car crash into the next. 
A key feature of pipelining is that it increases the throughput of the system, that is, the number of customers served per unit time, but it may also slightly increase the latency, that is, the time required to service an individual customer. For example, a customer in a cafeteria who only wants a salad could pass through a nonpipelined system very quickly, stopping only at the salad stage. A customer in a pipelined system who attempts to go directly to the salad stage risks incurring the wrath of other customers. 
Figure 4.32 

Unpipelined computation hardware. On each 320 
Delay 320 ps 
ps cycle, the system 
Throughput 3.12 GIPS 

spends 300 ps evaluating a combinational logic function and 20 ps storing 
Clock

the results in an output 
(a) Hardware: Unpipelined
register. 


(b) Pipeline diagram 
4.4.1 Computational Pipelines 
Shifting our focus to computational pipelines, the ¡°customers¡± are instructions and the stages perform some portion of the instruction execution. Figure 4.32 shows an example of a simple nonpipelined hardware system. It consists of some logic that performs a computation, followed by a register to hold the results of this computation. A clock signal controls the loading of the register at some regular time interval. An example of such a system is the decoder in a compact disk (CD) player. The incoming signals are the bits read from the surface of the CD, and the logic decodes these to generate audio signals. The computational block in the .gure is implemented as combinational logic, meaning that the signals will pass through a series of logic gates, with the outputs becoming some function of the inputs after some time delay. 
In contemporary logic design, we measure circuit delays in units of picosec-onds (abbreviated ¡°ps¡±), or 10.12 seconds. In this example, we assume the combi-national logic requires 300 picoseconds, while the loading of the register requires 20 ps. Figure 4.32 also shows a form of timing diagram known as a pipeline dia-gram. In this diagram, time .ows from left to right. A series of instructions (here named I1, I2, and I3) are written from top to bottom. The solid rectangles indicate the times during which these instructions are executed. In this implementation, we must complete one instruction before beginning the next. Hence, the boxes do not overlap one another vertically. The following formula gives the maximum rate at which we could operate the system: 
1 instruction 1000 picosecond 
. (20 + 300) picosecond 1 nanosecond 
Throughput =¡Ö 3.12 GIPS 
We express throughput in units of giga-instructions per second (abbreviated GIPS), or billions of instructions per second. The total time required to perform a single instruction from beginning to end is known as the latency. In this system, the latency is 320 ps, the reciprocal of the throughput. 

100 ps 20 ps 100 ps 20 ps 100 ps 20 ps 



Delay 360 ps Throughput 8.33 GIPS 

(a) Hardware: Three-stage pipeline 

Time 

(b) Pipeline diagram 

Figure 4.33 Three-stage pipelined computation hardware. The computation is split into stages A, B, and C. On each 120-ps cycle, each instruction progresses through one stage. 
Suppose we could divide the computation performed by our system into three stages, A, B, and C, where each requires 100 ps, as illustrated in Figure 4.33. Then we could put pipeline registers between the stages so that each instruction moves through the system in three steps, requiring three complete clock cycles from beginning to end. As the pipeline diagram in Figure 4.33 illustrates, we could allow I2 to enter stage A as soon as I1 moves from A to B, and so on. In steady state, all three stages would be active, with one instruction leaving and a new one entering the system every clock cycle. We can see this during the third clock cycle in the pipeline diagram where I1 is in stage C, I2 is in stage B, and I3 is in stage A. In this system, we could cycle the clocks every 100 + 20 = 120 picoseconds, giving a throughput of around 8.33 GIPS. Since processing a single instruction requires 3 clock cycles, the latency of this pipeline is 3 ¡Á 120 = 360 ps. We have increased the throughput of the system by a factor of 8.33/3.12 = 2.67 at the expense of some added hardware and a slight increase in the latency (360/320 = 1.12). The increased latency is due to the time overhead of the added pipeline registers. 
4.4.2 A Detailed Look at Pipeline Operation 
To better understand how pipelining works, let us look in some detail at the timing and operation of pipeline computations. Figure 4.34 shows the pipeline diagram for the three-stage pipeline we have already looked at (Figure 4.33). The transfer of the instructions between pipeline stages is controlled by a clock signal, as shown above the pipeline diagram. Every 120 ps, this signal rises from 0 to 1, initiating the next set of pipeline stage evaluations. 
Figure 4.34 
Clock 

Three-stage pipeline timing. The rising edge of I1 the clock signal controls the I2 movement of instructions I3 from one pipeline stage to 

0 120 240 360 480 600
the next. 
Time 
Figure 4.35 traces the circuit activity between times 240 and 360, as instruction I1 (shown in dark gray) propagates through stage C, I2 (shown in blue) propa-gates through stage B, and I3 (shown in light gray) propagates through stage A. Just before the rising clock at time 240 (point 1), the values computed in stage A for instruction I2 have reached the input of the .rst pipeline register, but its state and output remain set to those computed during stage A for instruction I1.The values computed in stage B for instruction I1 have reached the input of the second pipeline register. As the clock rises, these inputs are loaded into the pipeline reg-isters, becoming the register outputs (point 2). In addition, the input to stage A is set to initiate the computation of instruction I3. The signals then propagate through the combinational logic for the different stages (point 3). As the curved wavefronts in the diagram at point 3 suggest, signals can propagate through differ-ent sections at different rates. Before time 360, the result values reach the inputs of the pipeline registers (point 4). When the clock rises at time 360, each of the instructions will have progressed through one pipeline stage. 
We can see from this detailed view of pipeline operation that slowing down the clock would not change the pipeline behavior. The signals propagate to the pipeline register inputs, but no change in the register states will occur until the clock rises. On the other hand, we could have disastrous effects if the clock were run too fast. The values would not have time to propagate through the combinational logic, and so the register inputs would not yet be valid when the clock rises. 
As with our discussion of the timing for the SEQ processor (Section 4.3.3), we see that the simple mechanism of having clocked registers between blocks of combinational logic suf.ces to control the .ow of instructions in the pipeline. As the clock rises and falls repeatedly, the different instructions .ow through the stages of the pipeline without interfering with one another. 
4.4.3 Limitations of Pipelining 
The example of Figure 4.33 shows an ideal pipelined system in which we are able to divide the computation into three independent stages, each requiring one-third of the time required by the original logic. Unfortunately, other factors often arise that diminish the effectiveness of pipelining. 

Figure 4.35 Clock 


Oneclock cycle of pipeline 
I1 

operation. Just before the 
I2 

clock rises at time 240 
I3

(point 1), instructions I1 (shown in dark gray) and I2 (shown in blue) have Time 120 
240 
B  C  
A  B  
A  

360 

completed stages B and 
A. After the clock rises, 
Time 239 these instructions begin 
100 ps 20 ps 100 ps 20 ps 100 ps 20 ps

propagating through 
stages C and B, while 
Comb.

instruction I3 (shown 

logic
in light gray) begins 
A 


propagating through stage A (points 2 and 3). Just before the clock 

Time 241

rises again, the results for the instructions have 

100 ps 20 ps 100 ps 20 ps 100 ps 20 ps 

propagated to the inputs of the pipeline registers 
Comb. (point 4). logic 
A 

Time 300 

100 ps 20 ps 100 ps 20 ps 100 ps 20 ps 


Time 359 100 ps 20 ps 100 ps 20 ps 100 ps 20 ps 

Comb. logic A 




Nonuniform Partitioning 
Figure 4.36 shows a system in which we divide the computation into three stages as before, but the delays through the stages range from 50 to 150 ps. The sum of the delays through all of the stages remains 300 ps. However, the rate at which we 
50 ps 20 ps 150 ps 20 ps 100 ps 20 ps 

Delay 510 ps Throughput 5.88 GIPS 
(a)
 Hardware: Three-stage pipeline, nonuniform stage delays 

Time 

(b)
 Pipeline diagram 




Figure 4.36 Limitations of pipelining due to nonuniform stage delays. The system throughput is limited by the speed of the slowest stage. 
can operate the clock is limited by the delay of the slowest stage. As the pipeline diagram in this .gure shows, stage A will be idle (shown as a white box) for 100 ps every clock cycle, while stage C will be idle for 50 ps every clock cycle. Only stage B will be continuously active. We must set the clock cycle to 150 + 20 = 170 picoseconds, giving a throughput of 5.88 GIPS. In addition, the latency would increase to 510 ps due to the slower clock rate. 
Devising a partitioning of the system computation into a series of stages having uniform delays can be a major challenge for hardware designers. Often, some of the hardware units in a processor, such as the ALU and the memories, cannot be subdivided into multiple units with shorter delay. This makes it dif.cult to create a set of balanced stages. We will not concern ourselves with this level of detail in designing our pipelined Y86 processor, but it is important to appreciate the importance of timing optimization in actual system design. 
Practice Problem 4.26 
Suppose we analyze the combinational logic of Figure 4.32 and determine that it can be separated into a sequence of six blocks, named A to F, having delays of 80, 30, 60, 50, 70, and 10 ps, respectively, illustrated as follows: 

Clock 

We can create pipelined versions of this design by inserting pipeline registers between pairs of these blocks. Different combinations of pipeline depth (how many stages) and maximum throughput arise, depending on where we insert the pipeline registers. Assume that a pipeline register has a delay of 20 ps. 
A. Inserting a single register gives a two-stage pipeline. Where should the register be inserted to maximize throughput? What would be the throughput and latency? 
B. Where should two registers be inserted to maximize the throughput of a three-stage pipeline? What would be the throughput and latency? 
C. Where should three registers be inserted to maximize the throughput of a four-stage pipeline? What would be the throughput and latency? 
D. What is the minimum number of stages that would yield a design with the maximum achievable throughput? Describe this design, its throughput, and its latency. 
Diminishing Returns of Deep Pipelining 
Figure 4.37 illustrates another limitation of pipelining. In this example, we have divided the computation into six stages, each requiring 50 ps. Inserting a pipeline register between each pair of stages yields a six-stage pipeline. The minimum clock period for this system is 50 + 20 = 70 picoseconds, giving a throughput of 
14.29 GIPS. Thus, in doubling the number of pipeline stages, we improve the performance by a factor of 14.29/8.33 = 1.71. Even though we have cut the time required for each computation block by a factor of 2, we do not get a doubling of the throughput, due to the delay through the pipeline registers. This delay becomes a limiting factor in the throughput of the pipeline. In our new design, this delay consumes 28.6% of the total clock period. 
Modern processors employ very deep (15 or more stages) pipelines in an attempt to maximize the processor clock rate. The processor architects divide the instruction execution into a large number of very simple steps so that each stage can have a very small delay. The circuit designers carefully design the pipeline registers to minimize their delay. The chip designers must also carefully design the 
50 ps 20 ps 50 ps 20 ps 50 ps 20 ps 50 ps 20 ps 50 ps 20 ps 50 ps 20 ps 

Clock Delay = 420 ps, Throughput = 14.29 GIPS 
Figure 4.37 Limitations of pipelining due to overhead. As the combinational logic is split into shorter blocks, the delay due to register updating becomes a limiting factor. 
clock distribution network to ensure that the clock changes at the exact same time across the entire chip. All of these factors contribute to the challenge of designing high-speed microprocessors. 
Practice Problem 4.27 
Suppose we could take the system of Figure 4.32 and divide it into an arbitrary number of pipeline stages k, each having a delay of 300/k, and with each pipeline register having a delay of 20 ps. 
A. What would be the latency and the throughput of the system, as functions of k? 
B. What would be the ultimate limit on the throughput? 
4.4.4 Pipelining a System with Feedback 
Up to this point, we have considered only systems in which the objects passing through the pipeline¡ªwhether cars, people, or instructions¡ªare completely in-dependent of one another. For a system that executes machine programs such as IA32 or Y86, however, there are potential dependencies between successive instructions. For example, consider the following Y86 instruction sequence: 
1 2 3 

1  irmovl  $50,%eax  
2  addl %eax,%ebx  
3  mrmovl  100(%ebx),%edx  

In this three-instruction sequence, there is a data dependency between each succes-sive pair of instructions, as indicated by the circled register names and the arrows between them. The irmovl instruction (line 1) stores its result in %eax, which then must be read by the addl instruction (line 2); and this instruction stores its result in %ebx, which must then be read by the mrmovl instruction (line 3). 
Another source of sequential dependencies occurs due to the instruction control .ow. Consider the following Y86 instruction sequence: 
1 loop: 
2 subl %edx,%ebx 
3 jne targ 
4 irmovl $10,%edx 
5 jmp loop 
6 targ: 
7 halt 
Figure 4.38 

Limitations of pipelining due to logical depen-dencies. In going from an unpipelined system with feedback (a) to a pipelined one (c), we change its computational behavior, as can be seen by the two pipeline diagrams (b and d). 
Section 4.4 General Principles of Pipelining 399 

(a) Hardware: Unpipelined with feedback 
I1 I2 I3 

Time 

(b) Pipeline diagram 

Clock 
(c) Hardware: Three-stage pipeline with feedback 
I1 I2 I3 I4 

Time 

(d) Pipeline diagram 

The jne instruction (line 3) creates a control dependency since the outcome of the conditional test determines whether the next instruction to execute will be the irmovl instruction (line 4) or the halt instruction (line 7). In our design for SEQ, these dependencies were handled by the feedback paths shown on the right-hand side of Figure 4.22. This feedback brings the updated register values down to the register .le and the new PC value down to the PC register. 
Figure 4.38 illustrates the perils of introducing pipelining into a system con-taining feedback paths. In the original system (Figure 4.38(a)), the result of each instruction is fed back around to the next instruction. This is illustrated by the pipeline diagram (Figure 4.38(b)), where the result of I1 becomes an input to I2, and so on. If we attempt to convert this to a three-stage pipeline in the most straightforward manner (Figure 4.38(c)), we change the behavior of the system. As Figure 4.38(c) shows, the result of I1 becomes an input to I4. In attempting to speed up the system via pipelining, we have changed the system behavior. 
When we introduce pipelining into a Y86 processor, we must deal with feed-back effects properly. Clearly, it would be unacceptable to alter the system be-havior as occurred in the example of Figure 4.38. Somehow we must deal with the data and control dependencies between instructions so that the resulting behavior matches the model de.ned by the ISA. 
4.5 
Pipelined 
Y86 
Implementations 

We are .nally ready for the major task of this chapter¡ªdesigning a pipelined Y86 processor. We start by making a small adaptation of the sequential processor SEQ to shift the computation of the PC into the fetch stage. We then add pipeline registers between the stages. Our .rst attempt at this does not handle the dif-ferent data and control dependencies properly. By making some modi.cations, however, we achieve our goal of an ef.cient pipelined processor that implements the Y86 ISA. 
4.5.1 SEQ+: Rearranging the Computation Stages 
As a transitional step toward a pipelined design, we must slightly rearrange the order of the .ve stages in SEQ so that the PC update stage comes at the beginning of the clock cycle, rather than at the end. This transformation requires only minimal change to the overall hardware structure, and it will work better with the sequencing of activities within the pipeline stages. We refer to this modi.ed design as ¡°SEQ+.¡± 
We can move the PC update stage so that its logic is active at the beginning of the clock cycle by making it compute the PC value for the current instruction. Figure 4.39 shows how SEQ and SEQ+ differ in their PC computation. With SEQ (Figure 4.39(a)), the PC computation takes place at the end of the clock cycle, computing the new value for the PC register based on the values of signals 
PC 
PC 


icode Cnd valC valM valP 
(a) SEQ new PC computation (b) SEQ PC selection 
Figure 4.39 Shifting the timing of the PC computation. With SEQ+, we compute the value of the program counter for the current state as the .rst step in instruction execution. 

computed during the current clock cycle. With SEQ+ (Figure 4.39(b)), we create state registers to hold the signals computed during an instruction. Then, as a new clock cycle begins, the values propagate through the exact same logic to compute the PC for the now-current instruction. We label the registers ¡°pIcode,¡± ¡°pCnd,¡± and so on, to indicate that on any given cycle, they hold the control signals generated during the previous cycle. 
Figure 4.40 shows a more detailed view of the SEQ+ hardware. We can see that it contains the exact same hardware units and control blocks that we had in SEQ (Figure 4.23), but with the PC logic shifted from the top, where it was active at the end of the clock cycle, to the bottom, where it is active at the beginning. 
Aside Where is the PC in SEQ+? 
One curious feature of SEQ+ is that there is no hardware register storing the program counter. Instead, the PC is computed dynamically based on some state information stored from the previous instruction. This is a small illustration of the fact that we can implement a processor in a way that differs from the conceptual model implied by the ISA, as long as the processor correctly executes arbitrary machine-language programs. We need not encode the state in the form indicated by the programmer-visible state, as long as the processor can generate correct values for any part of the programmer-visible state (such as the program counter). We will exploit this principle even more in creating a pipelined design. Out-of-order processing techniques, as described in Section 5.7, take this idea to an extreme by executing instructions in a completely different order than they occur in the machine-level program. 
The shift of state elements from SEQ to SEQ+ is an example of a general transformation known as circuit retiming [65]. Retiming changes the state repre-sentation for a system without changing its logical behavior. It is often used to balance the delays between different stages of a system. 
4.5.2 Inserting Pipeline Registers 
In our .rst attempt at creating a pipelined Y86 processor, we insert pipeline registers between the stages of SEQ+ and rearrange signals somewhat, yielding the PIPE¨C processor, where the ¡°¨C¡± in the name signi.es that this processor has somewhat less performance than our ultimate processor design. The structure of PIPE¨C is illustrated in Figure 4.41. The pipeline registers are shown in this .gure as black boxes, each containing different .elds that are shown as white boxes. As indicated by the multiple .elds, each pipeline register holds multiple bytes and words. Unlike the labels shown in rounded boxes in the hardware structure of the two sequential processors (Figures 4.23 and 4.40), these white boxes represent actual hardware components. 
Observe that PIPE¨C uses nearly the same set of hardware units as our sequen-tial design SEQ (Figure 4.40), but with the pipeline registers separating the stages. The differences between the signals in the two systems is discussed in Section 4.5.3. 

Memory 
Execute 
Decode 
Fetch 
PC 
SEQ+ hardware structure. Shifting the PC computation from the end of the clock cycle to the beginning makes it more suitable for pipelining. 


The pipeline registers are labeled as follows: 
F holds a predicted value of the program counter, as will be discussed shortly. D sits between the fetch and decode stages. It holds information about the most recently fetched instruction for processing by the decode stage. E sits between the decode and execute stages. It holds information about the most recently decoded instruction and the values read from the register .le for processing by the execute stage. M sits between the execute and memory stages. It holds the results of the most recently executed instruction for processing by the memory stage. It also holds information about branch conditions and branch targets for processing conditional jumps. W sits between the memory stage and the feedback paths that supply the computed results to the register .le for writing and the return address to the PC selection logic when completing a ret instruction. 
Figure 4.42 shows how the following code sequence would .ow through our 
.ve-stage pipeline, where the comments identify the instructions as I1 to I5 for 
reference: 
1 irmovl $1,%eax # I1 2 irmovl $2,%ebx # I2 3 irmovl $3,%ecx # I3 4 irmovl $4,%edx # I4 5 halt # I5 
The right side of the .gure shows a pipeline diagram for this instruction sequence. As with the pipeline diagrams for the simple pipelined computation units of Section 4.4, this diagram shows the progression of each instruction through the pipeline stages, with time increasing from left to right. The numbers along the top identify the clock cycles at which the different stages occur. For example, in cycle 1, instruction I1 is fetched, and it then proceeds through the pipeline stages, with its result being written to the register .le after the end of cycle 5. Instruction I2 is fetched in cycle 2, and its result is written back after the end of cycle 6, and so on. At the bottom, we show an expanded view of the pipeline for cycle 5. At this point, there is an instruction in each of the pipeline stages. 
From Figure 4.42, we can also justify our convention of drawing processors so that the instructions .ow from bottom to top. The expanded view for cycle 5 shows the pipeline stages with the fetch stage on the bottom and the write-back stage on the top, just as do our diagrams of the pipeline hardware (Figure 4.41). If we look at the ordering of instructions in the pipeline stages, we see that they appear in the same order as they do in the program listing. Since normal program .ow goes from top to bottom of a listing, we preserve this ordering by having the pipeline .ow go from bottom to top. This convention is particularly useful when working with the simulators that accompany this text. 
123456789 

irmovl $1,%eax #Il irmovl $2,%ebx #I2 irmovl $3,%ecx #I3 irmovl $4,%edx #I4 halt #I5 



Figure 4.42 Example of instruction .ow through pipeline. 
4.5.3 Rearranging and Relabeling Signals 
Our sequential implementations SEQ and SEQ+ only process one instruction at a time, and so there are unique values for signals such as valC, srcA, and valE.In our pipelined design, there will be multiple versions of these values associated with the different instructions .owing through the system. For example, in the detailed structure of PIPE¨C, there are four white boxes labeled ¡°stat¡± that hold the status codes for four different instructions. (See Figure 4.41.) We need to take great care to make sure we use the proper version of a signal, or else we could have serious errors, such as storing the result computed for one instruction at the destination register speci.ed by another instruction. We adopt a naming scheme where a signal stored in a pipeline register can be uniquely identi.ed by pre.xing its name with that of the pipe register written in uppercase. For example, the four status codes are named D_stat, E_stat, M_stat, and W_stat. We also need to refer to some signals that have just been computed within a stage. These are labeled by pre.xing the signal name with the .rst character of the stage name, written in lowercase. Using the status codes as examples, we can see control logic blocks labeled ¡°stat¡± in the fetch and memory stages. The outputs of these blocks are therefore named f_stat and m_stat. We can also see that the actual status of the overall processor Stat is computed by a block in the write-back stage, based on the status value in pipeline register W. 
Aside What is the difference between signals M_stat and m_stat? 
With our naming system, the uppercase pre.xes ¡°D,¡± ¡°E,¡± ¡°M,¡± and ¡°W¡± refer to pipeline registers, and so M_stat refers to the status code .eld of pipeline register M. The lowercase pre.xes ¡°f,¡± ¡°d,¡± ¡°e,¡± ¡°m,¡± and ¡°w¡± refer to the pipeline stages, and so m_stat refers to the status signal generated in the memory stage by a control logic block. 
Understanding this naming convention is critical to understanding the operation of our pipelined processors. 
The decode stages of SEQ+ and PIPE¨C both generate signals dstE and dstM indicating the destination register for values valE and valM. In SEQ+, we could connect these signals directly to the address inputs of the register .le write ports. With PIPE¨C, these signals are carried along in the pipeline through the execute and memory stages, and are directed to the register .le only once they reach the write-back stage (shown in the more detailed views of the stages). We do this to make sure the write port address and data inputs hold values from the same instruction. Otherwise, the write back would be writing the values for the instruction in the write-back stage, but with register IDs from the instruction in the decode stage. As a general principle, we want to keep all of the information about a particular instruction contained within a single pipeline stage. 
One block of PIPE¨C that is not present in SEQ+ in the exact same form is the block labeled ¡°Select A¡± in the decode stage. We can see that this block generates the value valA for the pipeline register E by choosing either valP from pipeline register D or the value read from the A port of the register .le. This block is included to reduce the amount of state that must be carried forward to pipeline registers E and M. Of all the different instructions, only the call requires valP in the memory stage. Only the jump instructions require the value of valP in the execute stage (in the event the jump is not taken). None of these instructions requires a value read from the register .le. Therefore, we can reduce the amount of pipeline register state by merging these two signals and carrying them through the pipeline as a single signal valA. This eliminates the need for the block labeled ¡°Data¡± in SEQ (Figure 4.23) and SEQ+ (Figure 4.40), which served a similar purpose. In hardware design, it is common to carefully identify how signals get used and then reduce the amount of register state and wiring by merging signals such as these. 
As shown in Figure 4.41, our pipeline registers include a .eld for the status code Stat, initially computed during the fetch stage and possibly modi.ed during the memory stage. We will discuss how to implement the processing of exceptional events in Section 4.5.9, after we have covered the implementation of normal in-struction execution. Suf.ce it to say at this point that the most systematic approach is to associate a status code with each instruction as it passes through the pipeline, as we have indicated in the .gure. 
4.5.4 Next PC Prediction 
We have taken some measures in the design of PIPE¨C to properly handle control dependencies. Our goal in the pipelined design is to issue a new instruction on 

every clock cycle, meaning that on each clock cycle, a new instruction proceeds into the execute stage and will ultimately be completed. Achieving this goal would yield a throughput of one instruction per cycle. To do this, we must determine the location of the next instruction right after fetching the current instruction. Unfortunately, if the fetched instruction is a conditional branch, we will not know whether or not the branch should be taken until several cycles later, after the instruction has passed through the execute stage. Similarly, if the fetched instruction is a ret, we cannot determine the return location until the instruction has passed through the memory stage. 
With the exception of conditional jump instructions and ret, we can deter-mine the address of the next instruction based on information computed during the fetch stage. For call and jmp (unconditional jump), it will be valC, the con-stant word in the instruction, while for all others it will be valP, the address of the next instruction. We can therefore achieve our goal of issuing a new instruction every clock cycle in most cases by predicting the next value of the PC. For most in-struction types, our prediction will be completely reliable. For conditional jumps, we can predict either that a jump will be taken, so that the new PC value would be valC, or we can predict that it will not be taken, so that the new PC value would be valP. In either case, we must somehow deal with the case where our prediction was incorrect and therefore we have fetched and partially executed the wrong instructions. We will return to this matter in Section 4.5.11. 
This technique of guessing the branch direction and then initiating the fetching of instructions according to our guess is known as branch prediction. It is used in some form by virtually all processors. Extensive experiments have been conducted on effective strategies for predicting whether or not branches will be taken [49, Section 2.3]. Some systems devote large amounts of hardware to this task. In our design, we will use the simple strategy of predicting that conditional branches are always taken, and so we predict the new value of the PC to be valC. 
Aside Other branch prediction strategies 
Our design uses an always taken branch prediction strategy. Studies show this strategy has around a 60% success rate [47, 120]. Conversely, a never taken (NT) strategy has around a 40% success rate. A slightly more sophisticated strategy, known as backward taken, forward not-taken (BTFNT), predicts that branches to lower addresses than the next instruction will be taken, while those to higher addresses will not be taken. This strategy has a success rate of around 65%. This improvement stems from the fact that loops are closed by backward branches, and loops are generally executed multiple times. Forward branches are used for conditional operations, and these are less likely to be taken. In Problems 4.54 and 4.55, you can modify the Y86 pipeline processor to implement the NT and BTFNT branch prediction strategies. 
As we saw in Section 3.6.6, mispredicted branches can degrade the performance of a program considerably, thus motivating the use of conditional data transfer rather than conditional control transfer when possible. 
We are still left with predicting the new PC value resulting from a ret in-struction. Unlike conditional jumps, we have a nearly unbounded set of possible 
results, since the return address will be whatever word is on the top of the stack. In our design, we will not attempt to predict any value for the return address. Instead, we will simply hold off processing any more instructions until the ret instruction passes through the write-back stage. We will return to this part of the implementation in Section 4.5.11. 

Aside Return address prediction with a stack 
With most programs, it is very easy to predict return addresses, since procedure calls and returns occur in matched pairs. Most of the time that a procedure is called, it returns to the instruction following the call. This property is exploited in high-performance processors by including a hardware stack within the instruction fetch unit that holds the return address generated by procedure call instructions. Every time a procedure call instruction is executed, its return address is pushed onto the stack. When a return instruction is fetched, the top value is popped from this stack and used as the predicted return address. Like branch prediction, a mechanism must be provided to recover when the prediction was incorrect, since there are times when calls and returns do not match. In general, the prediction is highly reliable. This hardware stack is not part of the programmer-visible state. 
The PIPE¨C fetch stage, diagrammed at the bottom of Figure 4.41, is responsi-ble for both predicting the next value of the PC and for selecting the actual PC for the instruction fetch. We can see the block labeled ¡°Predict PC¡± can choose either valP, as computed by the PC incrementer or valC, from the fetched instruction. This value is stored in pipeline register F as the predicted value of the program counter. The block labeled ¡°Select PC¡± is similar to the block labeled ¡°PC¡± in the SEQ+ PC selection stage (Figure 4.40). It chooses one of three values to serve as the address for the instruction memory: the predicted PC, the value of valP for a not-taken branch instruction that reaches pipeline register M (stored in regis-ter M_valA), or the value of the return address when a ret instruction reaches pipeline register W (stored in W_valM). 
We will return to the handling of jump and return instructions when we complete the pipeline control logic in Section 4.5.11. 
4.5.5 Pipeline Hazards 
Our structure PIPE¨C is a good start at creating a pipelined Y86 processor. Recall from our discussion in Section 4.4.4, however, that introducing pipelining into a system with feedback can lead to problems when there are dependencies between successive instructions. We must resolve this issue before we can complete our design. These dependencies can take two forms: (1) data dependencies, where the results computed by one instruction are used as the data for a following instruction, and (2) control dependencies, where one instruction determines the location of the following instruction, such as when executing a jump, call, or return. When such dependencies have the potential to cause an erroneous computation by the pipeline, they are called hazards. Like dependencies, hazards can be classi.ed as either data hazards or control hazards. In this section, we concern ourselves 

# progl# progl 1 2 3 4 5 6 7 8 910 11 
0x000: irmovl $10,%edx 0x006: irmovl $3,%eax 0x00c: nop 0x00d: nop 0x00e: nop 0x00f: addl %edx,%eax 0x011: halt 


Figure 4.43 Pipelined execution of prog1 without special pipeline control. In cycle 6, the second irmovl writes its result to program register %eax. The addl instruction reads its source operands in cycle 7, so it gets correct values for both %edx and %eax. 
with data hazards. Control hazards will be discussed as part of the overall pipeline control (Section 4.5.11). 
Figure 4.43 illustrates the processing of a sequence of instructions we refer to as prog1 by the PIPE¨C processor. Let us assume in this example and successive ones that the program registers initially all have value 0. The code loads values 10 and 3 into program registers %edx and %eax, executes three nop instructions, and then adds register %edx to %eax. We focus our attention on the potential data hazards resulting from the data dependencies between the two irmovl instructions and the addl instruction. On the right-hand side of the .gure, we show a pipeline diagram for the instruction sequence. The pipeline stages for cycles 6 and 7 are shown highlighted in the pipeline diagram. Below this, we show an expanded view of the write-back activity in cycle 6 and the decode activity during cycle 7. After the start of cycle 7, both of the irmovl instructions have passed through the write-back stage, and so the register .le holds the updated values of %edx and %eax. As the addl instruction passes through the decode stage during cycle 7, it will therefore read the correct values for its source operands. The data dependencies between the two irmovl instructions and the addl instruction have not created data hazards in this example. 
# prog2 1 2 3 4 5 6 7 8 910 
# prog2 
0x000: irmovl $10,%edx 0x006: irmovl $3,%eax 0x00c: nop 0x00d: nop 0x00e: addl %edx,%eax 0x010: halt 


Cycle 6 
. . . 

We saw that prog1 will .ow through our pipeline and get the correct results, because the three nop instructions create a delay between instructions with data dependencies. Let us see what happens as these nop instructions are removed. Figure 4.44 illustrates the pipeline .ow of a program, named prog2, containing two nop instructions between the two irmovl instructions generating values for registers %edx and %eax, and the addl instruction having these two registers as operands. In this case, the crucial step occurs in cycle 6, when the addl instruc-tion reads its operands from the register .le. An expanded view of the pipeline activities during this cycle is shown at the bottom of the .gure. The .rst irmovl instruction has passed through the write-back stage, and so program register %edx has been updated in the register .le. The second irmovl instruction is in the write-back stage during this cycle, and so the write to program register %eax only occurs at the start of cycle 7 as the clock rises. As a result, the incorrect value zero would be read for register %eax (recall that we assume all registers are initially 0), since the pending write for this register has not yet occurred. Clearly we will have to adapt our pipeline to handle this hazard properly. 
Figure 4.45 shows what happens when we have only one nop instruction between the irmovl instructions and the addl instruction, yielding a program 

# prog3 123456789
# prog3 

0x000: irmovl $10,%edx 0x006: irmovl $3,%eax 0x00c: nop 0x00d: addl %edx,%eax 0x00f: halt 

. . . 


prog3. Now we must examine the behavior of the pipeline during cycle 5 as the addl instruction passes through the decode stage. Unfortunately, the pending write to register %edx is still in the write-back stage, and the pending write to %eax is still in the memory stage. Therefore, the addl instruction would get the incorrect values for both operands. 
Figure 4.46 shows what happens when we remove all of the nop instructions between the irmovl instructions and the addl instruction, yielding a program prog4. Now we must examine the behavior of the pipeline during cycle 4 as the addl instruction passes through the decode stage. Unfortunately, the pending write to register %edx is still in the memory stage, and the new value for %eax is just being computed in the execute stage. Therefore, the addl instruction would get the incorrect values for both operands. 
These examples illustrate that a data hazard can arise for an instruction when one of its operands is updated by any of the three preceding instructions. These hazards occur because our pipelined processor reads the operands for an 
# prog4 12345678
# prog4 
0x000: irmovl $10,%edx 0x006: irmovl $3,%eax 0x00c: addl %edx,%eax 0x00e: halt 

instruction from the register .le in the decode stage but does not write the results for the instruction to the register .le until three cycles later, after the instruction passes through the write-back stage. 

Aside Enumerating classes of data hazards 
Hazards can potentially occur when one instruction updates part of the program state that will be read by a later instruction. For Y86, the program state includes the program registers, the program counter, the memory, the condition code register, and the status register. Let us look at the hazard possibilities in our proposed design for each of these forms of state. 
Program registers: These are the hazards we have already identi.ed. They arise because the register .le is read in one stage and written in another, leading to possible unintended interactions between different instructions. 
Program counter: Con.icts between updating and reading the program counter give rise to control hazards. No hazard arises when our fetch-stage logic correctly predicts the new value of the program counter before fetching the next instruction. Mispredicted branches and ret instructions require special handling, as will be discussed in Section 4.5.11. 
Memory: Writes and reads of the data memory both occur in the memory stage. By the time an instruction reading memory reaches this stage, any preceding instructions writing memory will have already done so. On the other hand, there can be interference between instructions writing data in the memory stage and the reading of instructions in the fetch stage, since the instruction and data memories reference a single address space. This can only happen with programs containing self-modifying code, where instructions write to a portion of memory from which instructions are later fetched. Some systems have complex mechanisms to detect and avoid such hazards, while others simply mandate that programs should not use self-modifying code. We will assume for simplicity that programs do not modify themselves, and therefore we do not need to take special measures to update the instruction memory based on updates to the data memory during program execution. 
Condition code register: These are written by integer operations in the execute stage. They are read by conditional moves in the execute stage and by conditional jumps in the memory stage. By the time a conditional move or jump reaches the execute stage, any preceding integer operation will have already completed this stage. No hazards can arise. 
Status register: The program status can be affected by instructions as they .ow through the pipeline. Our mechanism of associating a status code with each instruction in the pipeline enables the processor to come to an orderly halt when an exception occurs, as will be discussed in Section 4.5.9. 
This analysis shows that we only need to deal with register data hazards, control hazards, and making sure exceptions are handled properly. A systematic analysis of this form is important when designing a complex system. It can identify the potential dif.culties in implementing the system, and it can guide the generation of test programs to be used in checking the correctness of the system. 
4.5.6 Avoiding Data Hazards by Stalling 
One very general technique for avoiding hazards involves stalling, where the processor holds back one or more instructions in the pipeline until the hazard condition no longer holds. Our processor can avoid data hazards by holding back an instruction in the decode stage until the instructions generating its source operands have passed through the write-back stage. The details of this mechanism will be discussed in Section 4.5.11. It involves simple enhancements to the pipeline control logic. The effect of stalling is diagrammed in Figures 4.47 (prog2) and 4.48 (prog4). (We omit prog3 from this discussion, since it operates similarly to the other two examples.) When the addl instruction is in the decode stage, the pipeline control logic detects that at least one of the instructions in the execute, memory, or write-back stage will update either register %edx or register %eax. Rather than letting the addl instruction pass through the stage with the incorrect results, it stalls the instruction, holding it back in the decode stage for either one (for prog2)or three (for prog4) extra cycles. For all three programs, the addl instruction .nally gets correct values for its two source operands in cycle 7 and then proceeds down the pipeline. 
# prog2 1 2 3 4 5 6 7 8 910 11
# prog2 

0x000: irmovl $10,%edx 0x006: irmovl $3,%eax 0x00c: nop 0x00d: nop 
bubble 

0x00e: addl %edx,%eax 0x010: halt 

# prog4 1 345 789 11
# prog4 2 610 
0x000: irmovl $10,%edx 0x006: irmovl $3,%eax 
bubble 
bubble 

bubble 0x00c: addl %edx,%eax 0x00e: halt 


Figure 4.48 Pipelined execution of prog4 using stalls. After decoding the addl instruction in cycle 4, the stall control logic detects data hazards for both source registers. It injects a bubble into the execute stage and repeats the decoding of the addl instruction on cycle 5. It again detects hazards for both source registers, injects a bubble into the execute stage, and repeats the decoding of the addl instruction on cycle 6. Still, it detects a hazard for source register %eax, injects a bubble into the execute stage, and repeats the decoding of the addl instruction on cycle 7. In effect, the machine has dynamically inserted three nop instructions, giving a .ow similar to that shown for prog1 (Figure 4.43). 
In holding back the addl instruction in the decode stage, we must also hold back the halt instruction following it in the fetch stage. We can do this by keeping the program counter at a .xed value, so that the halt instruction will be fetched repeatedly until the stall has completed. 
Stalling involves holding back one group of instructions in their stages while allowing other instructions to continue .owing through the pipeline. What then should we do in the stages that would normally be processing the addl instruction? We handle these by injecting a bubble into the execute stage each time we hold an instruction back in the decode stage. A bubble is like a dynamically generated nop instruction¡ªit does not cause any changes to the registers, the memory, the condition codes, or the program status. These are shown as white boxes in the pipeline diagrams of Figures 4.47 and 4.48. In these .gures, the arrow between the box labeled ¡°D¡± for the addl instruction and the box labeled ¡°E¡± for one of the pipeline bubbles indicates that a bubble was injected into the execute stage in place of the addl instruction that would normally have passed from the decode to the execute stage. We will look at the detailed mechanisms for making the pipeline stall and for injecting bubbles in Section 4.5.11. 

In using stalling to handle data hazards, we effectively execute programs prog2 and prog4 by dynamically generating the pipeline .ow seen for prog1 (Fig-ure 4.43). Injecting one bubble for prog2 and three for prog4 has the same effect as having three nop instructions between the second irmovl instruction and the addl instruction. This mechanism can be implemented fairly easily (see Problem 4.51), but the resulting performance is not very good. There are numerous cases in which one instruction updates a register and a closely following instruction uses the same register. This will cause the pipeline to stall for up to three cycles, reducing the overall throughput signi.cantly. 
4.5.7 Avoiding Data Hazards by Forwarding 
Our design for PIPE¨C reads source operands from the register .le in the decode stage, but there can also be a pending write to one of these source registers in the write-back stage. Rather than stalling until the write has completed, it can simply pass the value that is about to be written to pipeline register E as the source operand. Figure 4.49 shows this strategy with an expanded view of the 
# prog2# prog2 12345678 910 
0x000: irmovl $10,%edx 0x006: irmovl $3,%eax 0x00c: nop 0x00d: nop 0x00e: addl %edx,%eax 0x010: halt 

. . . 

D  
 
srcA  %edx  valA  R[%edx]  10  
srcB  %eax  valB  W_valE  3  
Figure 4.49 Pipelined execution of prog2 using forwarding. In cycle 6, the decode-stage logic detects the presence of a pending write to register %eax in the write-back stage. It uses this value for source operand valB rather than the value read from the register .le. 


# prog3 123456789
# prog3 
0x000: irmovl $10,%edx 0x006: irmovl $3,%eax 0x00c: nop 0x00d: addl %edx,%eax 0x00f: halt 

. . . 
pipeline diagram for cycle 6 of prog2. The decode-stage logic detects that register %eax is the source register for operand valB, and that there is also a pending write to %eax on write port E. It can therefore avoid stalling by simply using the data word supplied to port E (signal W_valE) as the value for operand valB. This technique of passing a result value directly from one pipeline stage to an earlier one is commonly known as data forwarding (or simply forwarding, and sometimes bypassing). It allows the instructions of prog2 to proceed through the pipeline without any stalling. Data forwarding requires adding additional data connections and control logic to the basic hardware structure. 
As Figure 4.50 illustrates, data forwarding can also be used when there is a pending write to a register in the memory stage, avoiding the need to stall for program prog3. In cycle 5, the decode-stage logic detects a pending write to register %edx on port E in the write-back stage, as well as a pending write to register %eax that is on its way to port E but is still in the memory stage. Rather than stalling until the writes have occurred, it can use the value in the write-back stage (signal W_valE) for operand valA and the value in the memory stage (signal M_valE) for operand valB. 

# prog4 12345678
# prog4 

0x000: irmovl $10,%edx 0x006: irmovl $3,%eax 0x00c: addl %edx,%eax 0x00e: halt 



To exploit data forwarding to its full extent, we can also pass newly computed values from the execute stage to the decode stage, avoiding the need to stall for program prog4, as illustrated in Figure 4.51. In cycle 4, the decode-stage logic detects a pending write to register %edx in the memory stage, and also that the value being computed by the ALU in the execute stage will later be written to register %eax. It can use the value in the memory stage (signal M_valE) for operand valA. It can also use the ALU output (signal e_valE) for operand valB. Note that using the ALU output does not introduce any timing problems. The decode stage only needs to generate signals valA and valB by the end of the clock cycle so that pipeline register E can be loaded with the results from the decode stage as the clock rises to start the next cycle. The ALU output will be valid before this point. 
The uses of forwarding illustrated in programs prog2 to prog4 all involve the forwarding of values generated by the ALU and destined for write port E. Forwarding can also be used with values read from the memory and destined for write port M. From the memory stage, we can forward the value that has just been read from the data memory (signal m_valM). From the write-back stage, we can forward the pending write to port M (signal W_valM). This gives a total of .ve different forwarding sources (e_valE, m_valM, M_valE, W_valM, and W_valE) and two different forwarding destinations (valA and valB). 
The expanded diagrams of Figures 4.49 to 4.51 also show how the decode-stage logic can determine whether to use a value from the register .le or to use a forwarded value. Associated with every value that will be written back to the register .le is the destination register ID. The logic can compare these IDs with the source register IDs srcA and srcB to detect a case for forwarding. It is possible to have multiple destination register IDs match one of the source IDs. We must establish a priority among the different forwarding sources to handle such cases. This will be discussed when we look at the detailed design of the forwarding logic. 
Figure 4.52 shows the structure of PIPE, an extension of PIPE¨C that can handle data hazards by forwarding. Comparing this to the structure of PIPE¨C (Figure 4.41), we can see that the values from the .ve forwarding sources are fed back to the two blocks labeled ¡°Sel+Fwd A¡± and ¡°Fwd B¡± in the decode stage. The block labeled ¡°Sel+Fwd A¡± combines the role of the block labeled ¡°Select A¡± in PIPE¨C with the forwarding logic. It allows valA for pipeline register E to be either the incremented program counter valP, the value read from the A port of the register .le, or one of the forwarded values. The block labeled ¡°Fwd B¡± implements the forwarding logic for source operand valB. 
4.5.8 Load/Use Data Hazards 
One class of data hazards cannot be handled purely by forwarding, because mem-ory reads occur late in the pipeline. Figure 4.53 illustrates an example of a load/use hazard, where one instruction (the mrmovl at address 0x018) reads a value from memory for register %eax while the next instruction (the addl at address 0x01e) needs this value as a source operand. Expanded views of cycles 7 and 8 are shown in the lower part of the .gure, where we assume all program registers initially have value 0. The addl instruction requires the value of the register in cycle 7, but it is not generated by the mrmovl instruction until cycle 8. In order to ¡°forward¡± from the mrmovl to the addl, the forwarding logic would have to make the value go backward in time! Since this is clearly impossible, we must .nd some other mech-anism for handling this form of data hazard. (The data hazard for register %ebx, with the value being generated by the irmovl instruction at address 0x012 and used by the addl instruction at address 0x01e, can be handled by forwarding.) 
As Figure 4.54 demonstrates, we can avoid a load/use data hazard with a combination of stalling and forwarding. This requires modi.cations of the con-trol logic, but it can use existing bypass paths. As the mrmovl instruction passes through the execute stage, the pipeline control logic detects that the instruction in the decode stage (the addl) requires the result read from memory. It stalls the instruction in the decode stage for one cycle, causing a bubble to be injected into the execute stage. As the expanded view of cycle 8 shows, the value read from memory can then be forwarded from the memory stage to the addl instruction in the decode stage. The value for register %ebx is also forwarded from the write-back to the memory stage. As indicated in the pipeline diagram by the arrow from the box labeled ¡°D¡± in cycle 7 to the box labeled ¡°E¡± in cycle 8, the injected bub-ble replaces the addl instruction that would normally continue .owing through the pipeline. 


# prog5 123456 8910 11 
# prog5 7 
0x000: irmovl $128,%edx 0x006: irmovl $3,%ecx 0x00c: rmmovl %ecx, 0(%edx) 0x012: irmovl $10,%ebx 0x018: mrmovl 0(%edx),%eax # Load %eax 0x01e: addl %ebx,%eax # Use %eax 0x020: halt 

Error 
This use of a stall to handle a load/use hazard is called a load interlock. Load interlocks combined with forwarding suf.ce to handle all possible forms of data hazards. Since only load interlocks reduce the pipeline throughput, we can nearly achieve our throughput goal of issuing one new instruction on every clock cycle. 
4.5.9 Exception Handling 
As we will discuss in Chapter 8, a variety of activities in a processor can lead to exceptional control .ow, where the normal chain of program execution gets broken. Exceptions can be generated either internally, by the executing program, or externally, by some outside signal. Our instruction set architecture includes three different internally generated exceptions, caused by (1) a halt instruction, 
(2) an instruction with an invalid combination of instruction and function code, and (3) an attempt to access an invalid address, either for instruction fetch or data read or write. A more complete processor design would also handle external exceptions, such as when the processor receives a signal that the network interface has received a new packet, or the user has clicked a mouse button. Handling exceptions correctly is a challenging aspect of any microprocessor design. They can 

# prog5 1 2 3 4 5 6 7 8 910 11 12
# prog5 

0x000: irmovl $128,%edx 0x006: irmovl $3,%ecx 0x00c: rmmovl %ecx, 0(%edx) 0x012: irmovl $10,%ebx 0x018: mrmovl 0(%edx),%eax # Load %eax 
bubble 

0x01e: addl %ebx,%eax # Use %eax 0x020: halt 



Cycle 8 
. . . 

occur at unpredictable times, and they require creating a clean break in the .ow of instructions through the processor pipeline. Our handling of the three internal exceptions gives just a glimpse of the true complexity of correctly detecting and handling exceptions. 
Let us refer to the instruction causing the exception as the excepting instruc-tion. In the case of an invalid instruction address, there is no actual excepting instruction, but it is useful to think of there being a sort of ¡°virtual instruction¡± at the invalid address. In our simpli.ed ISA model, we want the processor to halt when it reaches an exception and to set the appropriate status code, as listed in Fig-ure 4.5. It should appear that all instructions up to the excepting instruction have completed, but none of the following instructions should have any effect on the programmer-visible state. In a more complete design, the processor would con-tinue by invoking an exception handler, a procedure that is part of the operating 
422  Chapter 4  Processor Architecture  
system, but implementing this part of exception handling is beyond the scope of  
our presentation.  
In a pipelined system, exception handling involves several subtleties. First, it is  
possible to have exceptions triggered by multiple instructions simultaneously. For  
example, during one cycle of pipeline operation, we could have a halt instruction  
in the fetch stage, and the data memory could report an out-of-bounds data  
address for the instruction in the memory stage. We must determine which of these  
exceptions the processor should report to the operating system. The basic rule is to  
put priority on the exception triggered by the instruction that is furthest along the  
pipeline. In the example above, this would be the out-of-bounds address attempted  
by the instruction in the memory stage. In terms of the machine-language program,  
the instruction in the memory stage should appear to execute before one in the  
fetch stage, and therefore only this exception should be reported to the operating  
system.  
A second subtlety occurs when an instruction is .rst fetched and begins  
execution, causes an exception, and later is canceled due to a mispredicted branch.  
The following is an example of such a program in its object code form:  
0x000: 6300 | xorl %eax,%eax  
0x002: 740e000000 | jne Target # Not taken  
0x007: 30f001000000 | irmovl $1, %eax # Fall through  
0x00d: 00 | halt  
0x00e: | Target:  
0x00e: ff | .byte 0xFF # Invalid instruction code  
In this program, the pipeline will predict that the branch should be taken,  
and so it will fetch and attempt to use a byte with value 0xFF as an instruction  
(generated in the assembly code using the .byte directive). The decode stage will  
therefore detect an invalid instruction exception. Later, the pipeline will discover  
that the branch should not be taken, and so the instruction at address 0x00e  
should never even have been fetched. The pipeline control logic will cancel this  
instruction, but we want to avoid raising an exception.  
A third subtlety arises because a pipelined processor updates different parts  
of the system state in different stages. It is possible for an instruction following  
one causing an exception to alter some part of the state before the excepting  
instruction completes. For example, consider the following code sequence, in  
which we assume that user programs are not allowed to access addresses greater  
than 0xc0000000 (as is the case for 32-bit versions of Linux):  
1  irmovl $1,%eax  
2  xorl %esp,%esp # Set stack pointer to 0 and CC to 100  
3  pushl %eax # Attempt to write to 0xfffffffc  
4  addl %eax,%eax # (Should not be executed) Would set CC to 000  
The pushl instruction causes an address exception, because decrementing the  
stack pointer causes it to wrap around to 0xfffffffc. This exception is detected in  
the memory stage. On the same cycle, the addl instruction is in the execute stage,  

and it will cause the condition codes to be set to new values. This would violate our requirement that none of the instructions following the excepting instruction should have had any effect on the system state. 
In general, we can both correctly choose among the different exceptions and avoid raising exceptions for instructions that are fetched due to mispredicted branches by merging the exception-handling logic into the pipeline structure. That is the motivation for us to include a status code Stat in each of our pipeline registers (Figures 4.41 and 4.52). If an instruction generates an exception at some stage in its processing, the status .eld is set to indicate the nature of the exception. The exception status propagates through the pipeline with the rest of the information for that instruction, until it reaches the write-back stage. At this point, the pipeline control logic detects the occurrence of the exception and stops execution. 
To avoid having any updating of the programmer-visible state by instructions beyond the excepting instruction, the pipeline control logic must disable any updating of the condition code register or the data memory when an instruction in the memory or write-back stages has caused an exception. In the example program above, the control logic would detect that the pushl in the memory stage has caused an exception, and therefore the updating of the condition code register by the addl instruction would be disabled. 
Let us consider how this method of handling exceptions deals with the sub-tleties we have mentioned. When an exception occurs in one or more stages of a pipeline, the information is simply stored in the status .elds of the pipeline reg-isters. The event has no effect on the .ow of instructions in the pipeline until an excepting instruction reaches the .nal pipeline stage, except to disable any updat-ing of the programmer-visible state (the condition code register and the memory) by later instructions in the pipeline. Since instructions reach the write-back stage in the same order as they would be executed in a nonpipelined processor, we are guaranteed that the .rst instruction encountering an exception will arrive .rst in the write-back stage, at which point program execution can stop and the status code in pipeline register W can be recorded as the program status. If some in-struction is fetched but later canceled, any exception status information about the instruction gets canceled as well. No instruction following one that causes an ex-ception can alter the programmer-visible state. The simple rule of carrying the exception status together with all other information about an instruction through the pipeline provides a simple and reliable mechanism for handling exceptions. 
4.5.10 PIPE Stage Implementations 
We have now created an overall structure for PIPE, our pipelined Y86 processor with forwarding. It uses the same set of hardware units as the earlier sequential designs, with the addition of pipeline registers, some recon.gured logic blocks, and additional pipeline control logic. In this section, we go through the design of the different logic blocks, deferring the design of the pipeline control logic to the next section. Many of the logic blocks are identical to their counterparts in SEQ and SEQ+, except that we must choose proper versions of the different signals from the pipeline registers (written with the pipeline register name, written in uppercase, 
as a pre.x) or from the stage computations (written with the .rst character of the stage name, written in lowercase, as a pre.x). As an example, compare the HCL code for the logic that generates the srcA signal in SEQ to the corresponding code in PIPE: 
# Code from SEQ 
intsrcA= [ icode in { IRRMOVL, IRMMOVL, IOPL, IPUSHL } : rA; icode in { IPOPL, IRET } : RESP; 
1 : RNONE; # Don¡¯t need register ]; 
# Code from PIPE 
int d_srcA = [ D_icode in { IRRMOVL, IRMMOVL, IOPL, IPUSHL } : D_rA; D_icode in { IPOPL, IRET } : RESP; 
1 : RNONE; # Don¡¯t need register ]; 
They differ only in the pre.xes added to the PIPE signals: ¡°D_¡± for the source values, to indicate that the signals come from pipeline register D, and ¡°d_¡± for the result value, to indicate that it is generated in the decode stage. To avoid repetition, we will not show the HCL code here for blocks that only differ from those in SEQ because of the pre.xes on names. As a reference, the complete HCL code for PIPE is given in Web Aside arch:hcl. 
PC Selection and Fetch Stage 
Figure 4.55 provides a detailed view of the PIPE fetch stage logic. As discussed earlier, this stage must also select a current value for the program counter and predict the next PC value. The hardware units for reading the instruction from memory and for extracting the different instruction .elds are the same as those we considered for SEQ (see the fetch stage in Section 4.3.4). 
The PC selection logic chooses between three program counter sources. As a mispredicted branch enters the memory stage, the value of valP for this instruction (indicating the address of the following instruction) is read from pipeline register M (signal M_valA). When a ret instruction enters the write-back stage, the return address is read from pipeline register W (signal W_valM). All other cases use the predicted value of the PC, stored in pipeline register F (signal F_predPC): 
intf_pc= [ # Mispredicted branch. Fetch at incremented PC M_icode == IJXX && !M_Cnd : M_valA; # Completion of RET instruction. W_icode == IRET : W_valM; # Default: Use predicted value of PC 
1 : F_predPC; ]; 
M_icode 
M_Bch M_valA 


W_icode W_valM 

Instr valid 
imem_error 
f_pc 


Figure 4.55 PIPE PC selection and fetch logic. Within the one cycle time limit, the processor can only predict the address of the next instruction. 
The PC prediction logic chooses valC for the fetched instruction when it is either a call or a jump, and valP otherwise: 
int  f_predPC  =  [  
f_icode  in  {  IJXX,  ICALL  }  :  f_valC;  
1  :  f_valP;  
];  

The logic blocks labeled ¡°Instr valid,¡± ¡°Need regids,¡± and ¡°Need valC¡± are the same as for SEQ, with appropriately named source signals. 
Unlike in SEQ, we must split the computation of the instruction status into two parts. In the fetch stage, we can test for a memory error due to an out-of-range instruction address, and we can detect an illegal instruction or a halt instruction. Detecting an invalid data address must be deferred to the memory stage. 
Practice Problem 4.28 
Write HCL code for the signal f_stat, providing the provisional status for the fetched instruction. 
e_dstE 


Figure 4.56 PIPE decode and write-back stage logic. No instruction requires both valP and the value read from register port A, and so these two can be merged to form the signal valA for later stages. The block labeled ¡°Sel+Fwd A¡± performs this task and also implements the forwarding logic for source operand valA. The block labeled ¡°Fwd B¡± implements the forwarding logic for source operand valB. The register write locations are speci.ed by the dstE and dstM signals from the write-back stage rather than from the decode stage, since it is writing the results of the instruction currently in the write-back stage. 
Decode and Write-Back Stages 
Figure 4.56 gives a detailed view of the decode and write-back logic for PIPE. The blocks labeled ¡°dstE¡±, ¡°dstM¡±, ¡°srcA¡±, and ¡°srcB¡± are very similar to their coun-terparts in the implementation of SEQ. Observe that the register IDs supplied to the write ports come from the write-back stage (signals W_dstE and W_dstM), rather than from the decode stage. This is because we want the writes to occur to the destination registers speci.ed by the instruction in the write-back stage. 
Practice Problem 4.29 
The block labeled ¡°dstE¡± in the decode stage generates the register ID for the E port of the register .le, based on .elds from the fetched instruction in pipeline 

register D. The resulting signal is named d_dstE in the HCL description of PIPE. Write HCL code for this signal, based on the HCL description of the SEQ signal dstE. (See the decode stage for SEQ in Section 4.3.4.) Do not concern yourself with the logic to implement conditional moves yet. 
Most of the complexity of this stage is associated with the forwarding logic. As mentioned earlier, the block labeled ¡°Sel+Fwd A¡± serves two roles. It merges the valP signal into the valA signal for later stages in order to reduce the amount of state in the pipeline register. It also implements the forwarding logic for source operand valA. 
The merging of signals valA and valP exploits the fact that only the call and jump instructions need the value of valP in later stages, and these instructions do not need the value read from the A port of the register .le. This selection is controlled by the icode signal for this stage. When signal D_icode matches the instruction code for either call or jXX, this block should select D_valP as its output. 
As mentioned in Section 4.5.7, there are .ve different forwarding sources, each with a data word and a destination register ID: 
Data word Register ID Source description 
e_valE  e_dstE  ALU output  
m_valM  M_dstM  Memory output  
M_valE  M_dstE  Pending write to port E in memory stage  
W_valM  W_dstM  Pending write to port M in write-back stage  
W_valE  W_dstE  Pending write to port E in write-back stage  

If none of the forwarding conditions hold, the block should select d_rvalA, the value read from register port A as its output. Putting all of this together, we get the following HCL description for the new value of valA for pipeline register E: 
int d_valA = [ D_icode in { ICALL, IJXX } : D_valP; # Use incremented PC d_srcA == e_dstE : e_valE; # Forward valE from execute d_srcA == M_dstM : m_valM; # Forward valM from memory d_srcA == M_dstE : M_valE; # Forward valE from memory d_srcA == W_dstM : W_valM; # Forward valM from write back d_srcA == W_dstE : W_valE; # Forward valE from write back 
1 : d_rvalA; # Use value read from register file ]; 
The priority given to the .ve forwarding sources in the above HCL code is very important. This priority is determined in the HCL code by the order in which 
# prog6 12345678
# prog6 
0x000: irmovl $10,%edx 0x006: irmovl $3,%edx 0x00c: rrmovl %edx,%eax 0x00e: halt 


the .ve destination register IDs are tested. If any order other than the one shown were chosen, the pipeline would behave incorrectly for some programs. Figure 
4.57 shows an example of a program that requires a correct setting of priority among the forwarding sources in the execute and memory stages. In this program, the .rst two instructions write to register %edx, while the third uses this register as its source operand. When the rrmovl instruction reaches the decode stage in cycle 4, the forwarding logic must choose between two values destined for its source register. Which one should it choose? To set the priority, we must consider the behavior of the machine-language program when it is executed one instruction at a time. The .rst irmovl instruction would set register %edx to 10, the second would set the register to 3, and then the rrmovl instruction would read 3 from %edx. To imitate this behavior, our pipelined implementation should always give priority to the forwarding source in the earliest pipeline stage, since it holds the latest instruction in the program sequence setting the register. Thus, the logic in the HCL code above .rst tests the forwarding source in the execute stage, then those in the memory stage, and .nally the sources in the write-back stage. 
The forwarding priority between the two sources in either the memory or the write-back stages are only a concern for the instruction popl %esp, since only this instruction can write two registers simultaneously. 

Practice Problem 4.30 
Suppose the order of the third and fourth cases (the two forwarding sources from the memory stage) in the HCL code for d_valA were reversed. Describe the resulting behavior of the rrmovl instruction (line 5) for the following program: 
1  irmovl  $5, %edx  
2  irmovl  $0x100,%esp  
3  rmmovl  %edx,0(%esp)  
4  popl %esp  
5  rrmovl  %esp,%eax  

Practice Problem 4.31 
Suppose the order of the .fth and sixth cases (the two forwarding sources from the write-back stage) in the HCL code for d_valA were reversed. Write a Y86 program that would be executed incorrectly. Describe how the error would occur and its effect on the program behavior. 
Practice Problem 4.32 
Write HCL code for the signal d_valB, giving the value for source operand valB supplied to pipeline register E. 
One small part of the write-back stage remains. As shown in Figure 4.52, the overall processor status Stat is computed by a block based on the status value in pipeline register W. Recall from Section 4.1.1 that the code should indicate either normal operation (AOK) or one of the three exception conditions. Since pipeline register W holds the state of the most recently completed instruction, it is natural to use this value as an indication of the overall processor status. The only special case to consider is when there is a bubble in the write-back stage. This is part of normal operation, and so we want the status code to be AOK for this case as well: 
int Stat = [ W_stat == SBUB : SAOK; 
1 : W_stat; ]; 
Execute Stage 

Figure 4.58 shows the execute stage logic for PIPE. The hardware units and the logic blocks are identical to those in SEQ, with an appropriate renaming of signals. We can see the signals e_valEand e_dstE directed toward the decode stage as one of the forwarding sources. One difference is that the logic labeled ¡°Set CC,¡± which determines whether or not update the condition codes, has signals m_stat and 
e_valE e_dstE 

W_stat as inputs. These signals are used to detect cases where an instruction causing an exception is passing through later pipeline stages, and therefore any updating of the condition codes should be suppressed. This aspect of the design is discussed in Section 4.5.11. 
Practice Problem 4.33 
Our second case in the HCL code for d_valA uses signal e_dstE to see whether to select the ALU output e_valE as the forwarding source. Suppose instead that we use signal E_dstE, the destination register ID in pipeline register E for this selection. Write a Y86 program that would give an incorrect result with this modi.ed forwarding logic. 
Memory Stage 
Figure 4.59 shows the memory stage logic for PIPE. Comparing this to the memory stage for SEQ (Figure 4.30), we see that, as noted before, the block labeled ¡°Data¡± in SEQ is not present in PIPE. This block served to select between data sources valP (for call instructions) and valA, but this selection is now performed by the block labeled ¡°Sel+Fwd A¡± in the decode stage. Most other blocks in this stage are identical to their counterparts in SEQ, with an appropriate renaming of the signals. In this .gure, you can also see that many of the values in pipeline registers and M and W are supplied to other parts of the circuit as part of the forwarding and pipeline control logic. 


4.5.11 Pipeline Control Logic 
We are now ready to complete our design for PIPE by creating the pipeline control logic. This logic must handle the following four control cases for which other mechanisms, such as data forwarding and branch prediction, do not suf.ce: 
Processing ret: The pipeline must stall until the ret instruction reaches the write-back stage. Load/use hazards: The pipeline must stall for one cycle between an instruction that reads a value from memory and an instruction that uses this value. 
Mispredicted branches: By the time the branch logic detects that a jump should not have been taken, several instructions at the branch target will have started down the pipeline. These instructions must be removed from the pipeline. 
Exceptions: When an instruction causes an exception, we want to disable the updating of the programmer-visible state by later instructions and halt execution once the excepting instruction reaches the write-back stage. 
We will go through the desired actions for each of these cases and then develop control logic to handle all of them. 
Desired Handling of Special Control Cases 
For the ret instruction, consider the following example program. This program is shown in assembly code, but with the addresses of the different instructions on the left for reference: 

0x000:  irmovl  Stack,%esp  #  Initialize  stack  pointer  
0x006:  call  Proc  #  procedure  call  
0x00b:  irmovl  $10,%edx  #  return  point  
0x011:  halt  
0x020:  .pos  0x20  
0x020:  Proc:  #  Proc:  
0x020:  ret  #  return  immediately  
0x021:  rrmovl  %edx,%ebx  #  not  executed  
0x030:  .pos  0x30  
0x030:  Stack:  #  Stack:  Stack  pointer  

Figure 4.60 shows how we want the pipeline to process the ret instruction. As with our earlier pipeline diagrams, this .gure shows the pipeline activity with time growing to the right. Unlike before, the instructions are not listed in the same order they occur in the program, since this program involves a control .ow where instructions are not executed in a linear sequence. Look at the instruction addresses to see from where the different instructions come in the program. 
As this diagram shows, the ret instruction is fetched during cycle 3 and proceeds down the pipeline, reaching the write-back stage in cycle 7. While it passes through the decode, execute, and memory stages, the pipeline cannot do any useful activity. Instead, we want to inject three bubbles into the pipeline. Once the ret instruction reaches the write-back stage, the PC selection logic will set the program counter to the return address, and therefore the fetch stage will fetch the irmovl instruction at the return point (address 0x00b). 

# prog7# prog7 1 2 3 4 5 6 7 8 910 11 
0x000: irmovl Stack,%edx 0x006: call proc 0x020: ret 
bubble 
bubble 
bubble 

0x00b: irmovl $10,%edx # Return point 

# prog7# prog7 1 2 3 4 5 6 7 8 910 11 
0x000: irmovl Stack,%edx 0x006: call proc 0x020: ret 0x021: rrmovl %edx,%ebx # Not executed 
bubble 

0x021: rrmovl %edx,%ebx # Not executed 
bubble 

0x021: rrmovl %edx,%ebx # Not executed 
bubble 

0x00b: irmovl $10,%edx # Return point 


Figure 4.61 Actual processing of the ret instruction. The fetch stage repeatedly fetches the rrmovl instruction following the ret instruction, but then the pipeline control logic injects a bubble into the decode stage rather than allowing the rrmovl instruction to proceed. The resulting behavior is equivalent to that shown in Figure 4.60. 
Figure 4.61 shows the actual processing of the ret instruction for the example program. The key observation here is that there is no way to inject a bubble into the fetch stage of our pipeline. On every cycle, the fetch stage reads some instruction from the instruction memory. Looking at the HCL code for implementing the PC prediction logic in Section 4.5.10, we can see that for the ret instruction the new value of the PC is predicted to be valP, the address of the following instruction. In our example program, this would be 0x021, the address of the rrmovl instruction following the ret. This prediction is not correct for this example, nor would it be for most cases, but we are not attempting to predict return addresses correctly in our design. For three clock cycles, the fetch stage stalls, causing the rrmovl instruction to be fetched but then replaced by a bubble in the decode stage. This process is illustrated in Figure 4.61 by the three fetches, with an arrow leading down to the bubbles passing through the remaining pipeline stages. Finally, the irmovl instruction is fetched on cycle 7. Comparing Figure 4.61 with Figure 4.60, we see that our implementation achieves the desired effect, but with a slightly peculiar fetching of an incorrect instruction for 3 consecutive cycles. 
For a load/use hazard, we have already described the desired pipeline opera-tion in Section 4.5.8, as illustrated by the example of Figure 4.54. Only the mrmovl and popl instructions read data from memory. When either of these is in the ex-ecute stage, and an instruction requiring the destination register is in the decode stage, we want to hold back the second instruction in the decode stage and inject a bubble into the execute stage on the next cycle. After this, the forwarding logic will resolve the data hazard. The pipeline can hold back an instruction in the de-code stage by keeping pipeline register D in a .xed state. In doing so, it should also keep pipeline register F in a .xed state, so that the next instruction will be fetched a second time. In summary, implementing this pipeline .ow requires detecting the 
# prog8 1 2 3 4 5 6 7 8 910 
0x000: xorl %eax,%eax 0x002: jne target # Not taken 0x00e: irmovl $2,%edx # Target 
bubble 
0x014: irmovl $3,%ebx # Target1 
bubble 
0x007: irmovl $1,%eax # Fall through 0x00d: halt 

Figure 4.62 Processing mispredicted branch instructions. The pipeline predicts branches will be taken and so starts fetching instructions at the jump target. Two instructions are fetched before the misprediction is detected in cycle 4 when the jump instruction .ows through the execute stage. In cycle 5, the pipeline cancels the two target instructions by injecting bubbles into the decode and execute stages, and it also fetches the instruction following the jump. 
hazard condition, keeping pipeline register F and D .xed, and injecting a bubble into the execute stage. 
To handle a mispredicted branch, consider the following program, shown in assembly code, but with the instruction addresses shown on the left for reference: 
0x000:  xorl  %eax,%eax  
0x002:  jne  target  #  Not taken  
0x007:  irmovl $1, %eax  #  Fall  through  
0x00d:  halt  

0x00e: target: 
0x00e: irmovl $2, %edx # Target 
0x014: irmovl $3, %ebx # Target+1 
0x01a: halt 
Figure 4.62 shows how these instructions are processed. As before, the instruc-tions are listed in the order they enter the pipeline, rather than the order they occur in the program. Since the jump instruction is predicted as being taken, the instruc-tion at the jump target will be fetched in cycle 3, and the instruction following this one will be fetched in cycle 4. By the time the branch logic detects that the jump should not be taken during cycle 4, two instructions have been fetched that should not continue being executed. Fortunately, neither of these instructions has caused a change in the programmer-visible state. That can only occur when an instruction reaches the execute stage, where it can cause the condition codes to change. We can simply cancel (sometimes called instruction squashing) the two misfetched in-structions by injecting bubbles into the decode and execute instructions on the following cycle while also fetching the instruction following the jump instruction. The two misfetched instructions will then simply disappear from the pipeline. As we will discuss in Section 4.5.11, a simple extension to the basic clocked register design will enable us to inject bubbles into pipeline registers as part of the pipeline control logic. 

For an instruction that causes an exception, we must make the pipelined im-plementation match the desired ISA behavior, with all prior instructions complet-ing and with none of the following instructions having any effect on the program state. Achieving these effects is complicated by the facts that (1) exceptions are detected during two different stages (fetch and memory) of program execution, and (2) the program state is updated in three different stages (execute, memory, and write-back). 
Our stage designs include a status code stat in each pipeline register to track the status of each instruction as it passes through the pipeline stages. When an exception occurs, we record that information as part of the instruction¡¯s status and continue fetching, decoding, and executing instructions as if nothing were amiss. As the excepting instruction reaches the memory stage, we take steps to pre-vent later instructions from modifying programmer-visible state by (1) disabling the setting of condition codes by instructions in the execute stage, (2) injecting bubbles into the memory stage to disable any writing to the data memory, and 
(3) stalling the write-back stage when it has an excepting instruction, thus bringing the pipeline to a halt. 
The pipeline diagram in Figure 4.63 illustrates how our pipeline control han-dles the situation where an instruction causing an exception is followed by one that would change the condition codes. On cycle 6, the pushl instruction reaches the memory stage and generates a memory error. On the same cycle, the addl instruc-tion in the execute stage generates new values for the condition codes. We disable 
# prog10 1 345 8910 0x000: irmovl $1,%eax 
# prog10 2 67 
. . . 


W 

Condition Trigger 
Processing ret Load/use hazard Mispredicted branch  IRET ¡Ê{D icode, E icode, M icode}E icode ¡Ê{IMRMOVL, IPOPL}&& E dstM ¡Ê{d srcA, d srcB}E icode =IJXX && !e Cnd  
Exception  m stat ¡Ê{SADR, SINS, SHLT}|| W stat ¡Ê{SADR, SINS, SHLT}  
Figure 4.64 Detection conditions for pipeline control logic. Four different conditions require altering the pipeline .ow by either stalling the pipeline or canceling partially executed instructions. 


the setting of condition codes when an excepting instruction is in the memory or write-back stage (by examining the signals m_stat and W_stat and then setting the signal set_cc to zero). We can also see the combination of injecting bubbles into the memory stage and stalling the excepting instruction in the write-back stage in the example of Figure 4.63¡ªthe pushl instruction remains stalled in the write-back stage, and none of the subsequent instructions get past the execute stage. 
By this combination of pipelining the status signals, controlling the setting of condition codes, and controlling the pipeline stages, we achieve the desired behav-ior for exceptions: all instructions prior to the excepting instruction are completed, while none of the following instructions has any effect on the programmer-visible state. 
Detecting Special Control Conditions 
Figure 4.64 summarizes the conditions requiring special pipeline control. It gives expressions describing the conditions under which the three special cases arise. These expressions are implemented by simple blocks of combinational logic that must generate their results before the end of the clock cycle in order to control the action of the pipeline registers as the clock rises to start the next cycle. During a clock cycle, pipeline registers D, E, and M hold the states of the instructions that are in the decode, execute, and memory pipeline stages, respectively. As we approach the end of the clock cycle, signals d_srcA and d_srcB will be set to the register IDs of the source operands for the instruction in the decode stage. Detecting a ret instruction as it passes through the pipeline simply involves checking the instruction codes of the instructions in the decode, execute, and memory stages. Detecting a load/use hazard involves checking the instruction type (mrmovl or popl) of the instruction in the execute stage and comparing its destination register with the source registers of the instruction in the decode stage. The pipeline control logic should detect a mispredicted branch while the jump instruction is in the execute stage, so that it can set up the conditions required to recover from the misprediction as the instruction enters the memory stage. When a jump instruction is in the execute stage, the signal e_Cnd indicates whether or not the jump should be taken. We detect an excepting instruction by examining the instruction status values in the memory and write-back stages. For the memory stage, we use the signal m_stat, computed within the stage, rather than M_stat 

State x State y 
Rising
Input y Output x 



Output yclock 


Output x clock 


Output nopclock 
x 
stall 
bubble 0

 0 
(a) Normal 

State x State x 
Rising
Input y Output x 

x 

stall 
bubble 1

 0 
(b) Stall 

State x State nop 
Rising
Input y Output x 

n x 

o p 

stall 
bubble 0

 1 
(c) Bubble 

Figure 4.65 Additional pipeline register operations. (a) Under normal conditions, the state and output of the register are set to the value at the input when the clock rises. 
(b) When operated in stall mode, the state is held .xed at its previous value. (c) When operated in bubble mode, the state is overwritten with that of a nop operation. 
from the pipeline register. This internal signal incorporates the possibility of a data memory address error. 
Pipeline Control Mechanisms 
Figure 4.65 shows low-level mechanisms that allow the pipeline control logic to hold back an instruction in a pipeline register or to inject a bubble into the pipeline. These mechanisms involve small extensions to the basic clocked register described in Section 4.2.5. Suppose that each pipeline register has two control inputs stall and bubble. The settings of these signals determine how the pipeline register is updated as the clock rises. Under normal operation (Figure 4.65(a)), both of these inputs are set to 0, causing the register to load its input as its new state. When the stall signal is set to 1 (Figure 4.65(b)), the updating of the state is disabled. Instead, the register will remain in its previous state. This makes it possible to hold back an instruction in some pipeline stage. When the bubble signal is set to 1 (Figure 4.65(c)), the state of the register will be set to some .xed reset con.guration giving a state equivalent to that of a nop instruction. The particular pattern of ones and zeros for a pipeline register¡¯s reset con.guration depends on the set of .elds in the pipeline register. For example, to inject a bubble into pipeline register D, we want the icode .eld to be set to the constant value INOP (Figure 4.26). To inject a bubble into pipeline register E, we want the icode .eld to be set to INOP and the dstE, dstM, srcA, and srcB .elds to be set to the constant RNONE. Determining the reset con.guration is one of the tasks for the hardware designer in designing a pipeline register. We will not concern ourselves with the details here. We will consider it an error to set both the bubble and the stall signals to 1. 
438  Chapter 4  Processor Architecture  
Condition Processing ret Load/use hazard Mispredicted branch  F stall stall normal  Pipeline register D E M bubble normal normal stall bubble normal bubble bubble normal  W normal normal normal  
Figure 4.66 Actions for pipeline control logic. The different conditions require altering the pipeline .ow by either stalling the pipeline or by canceling partially executed instructions. 


The table in Figure 4.66 shows the actions the different pipeline stages should take for each of the three special conditions. Each involves some combination of normal, stall, and bubble operations for the pipeline registers. 
In terms of timing, the stall and bubble control signals for the pipeline registers are generated by blocks of combinational logic. These values must be valid as the clock rises, causing each of the pipeline registers to either load, stall, or bubble as the next clock cycle begins. With this small extension to the pipeline register designs, we can implement a complete pipeline, including all of its control, using the basic building blocks of combinational logic, clocked registers, and random-access memories. 
Combinations of Control Conditions 
In our discussion of the special pipeline control conditions so far, we assumed that at most one special case could arise during any single clock cycle. A common bug in designing a system is to fail to handle instances where multiple special conditions arise simultaneously. Let us analyze such possibilities. We need not worry about combinations involving program exceptions, since we have carefully designed our exception-handling mechanism to consider other instructions in the pipeline. Figure 4.67 diagrams the pipeline states that cause the other three special control conditions. These diagrams show blocks for the decode, execute, and memory stages. The shaded boxes represent particular constraints that must be satis.ed for the condition to arise. A load/use hazard requires that the instruction in the 

ret  
bubble  





Figure 4.67 Load/use Mispredict ret 1 ret 2 ret 3 Pipeline states for special M 


Load  
Use  

M 
M M M 

control conditions. The 
E 
E 
E E E 

two pairs indicated can 
D 
D 
D D D 
arise simultaneously. 
ret  
bubble  
bubble  


execute stage reads a value from memory into a register, and that the instruction in the decode stage has this register as a source operand. A mispredicted branch requires the instruction in the execute stage to have a jump instruction. There are three possible cases for ret¡ªthe instruction can be in either the decode, execute, or memory stage. As the ret instruction moves through the pipeline, the earlier pipeline stages will have bubbles. 
We can see by these diagrams that most of the control conditions are mutually exclusive. For example, it is not possible to have a load/use hazard and a mispre-dicted branch simultaneously, since one requires a load instruction (mrmovl or popl) in the execute stage, while the other requires a jump. Similarly, the second and third ret combinations cannot occur at the same time as a load/use hazard or a mispredicted branch. Only the two combinations indicated by arrows can arise simultaneously. 
Combination A involves a not-taken jump instruction in the execute stage and a ret instruction in the decode stage. Setting up this combination requires the ret to be at the target of a not-taken branch. The pipeline control logic should detect that the branch was mispredicted and therefore cancel the ret instruction. 
Practice Problem 4.35 
Write a Y86 assembly-language program that causes combination A to arise and determines whether the control logic handles it correctly. 
Combining the control actions for the combination A conditions (Figure 4.66), we get the following pipeline control actions (assuming that either a bubble or a stall overrides the normal case): 
Pipeline register 

Condition F D E MW 
Processing ret  stall  bubble  normal  normal  normal  
Mispredicted branch  normal  bubble  bubble  normal  normal  
Combination  stall  bubble  bubble  normal  normal  

That is, it would be handled like a mispredicted branch, but with a stall in the fetch stage. Fortunately, on the next cycle, the PC selection logic will choose the address of the instruction following the jump, rather than the predicted program counter, and so it does not matter what happens with the pipeline register F. We conclude that the pipeline will correctly handle this combination. 
Combination B involves a load/use hazard, where the loading instruction sets register %esp, and the ret instruction then uses this register as a source operand, since it must pop the return address from the stack. The pipeline control logic should hold back the ret instruction in the decode stage. 
Practice Problem 4.36 
Write a Y86 assembly-language program that causes combination B to arise and completes with a halt instruction if the pipeline operates correctly. 
Combining the control actions for the combination B conditions (Figure 4.66), we get the following pipeline control actions: 
Pipeline register 
Condition F D E MW 
Processing ret  stall  bubble  normal  normal  normal  
Load/use hazard  stall  stall  bubble  normal  normal  
Combination  stall  bubble+stall  bubble  normal  normal  
Desired  stall  stall  bubble  normal  normal  

If both sets of actions were triggered, the control logic would try to stall the ret instruction to avoid the load/use hazard but also inject a bubble into the decode stage due to the ret instruction. Clearly, we do not want the pipeline to perform both sets of actions. Instead, we want it to just take the actions for the load/use hazard. The actions for processing the ret instruction should be delayed for one cycle. 
This analysis shows that combination B requires special handling. In fact, our original implementation of the PIPE control logic did not handle this combination correctly. Even though the design had passed many simulation tests, it had a subtle bug that was uncovered only by the analysis we have just shown. When a program having combination B was executed, the control logic would set both the bubble and the stall signals for pipeline register D to 1. This example shows the importance of systematic analysis. It would be unlikely to uncover this bug by just running normal programs. If left undetected, the pipeline would not faithfully implement the ISA behavior. 
Control Logic Implementation 
Figure 4.68 shows the overall structure of the pipeline control logic. Based on signals from the pipeline registers and pipeline stages, the control logic generates stall and bubble control signals for the pipeline registers, and also determines whether the condition code registers should be updated. We can combine the detection conditions of Figure 4.64 with the actions of Figure 4.66 to create HCL descriptions for the different pipeline control signals. 


Figure 4.68 PIPE pipeline control logic. This logic overrides the normal .ow of instructions through the pipeline to handle special conditions such as procedure returns, mispredicted branches, load/use hazards, and program exceptions. 
Pipeline register F must be stalled for either a load/use hazard or a ret instruction: 
bool F_stall = 

# Conditions for a load/use hazard 
E_icode in { IMRMOVL, IPOPL } && 
E_dstM in { d_srcA, d_srcB } || 
# Stalling at fetch while ret passes through pipeline 
IRET in { D_icode, E_icode, M_icode }; 
Practice Problem 4.37 
Write HCL code for the signal D_stall in the PIPE implementation. 
Pipeline register D must be set to bubble for a mispredicted branch or a ret instruction. As the analysis in the preceding section shows, however, it should not inject a bubble when there is a load/use hazard in combination with a ret instruction: 
bool D_bubble = # Mispredicted branch (E_icode == IJXX && !e_Cnd) || # Stalling at fetch while ret passes through pipeline # but not condition for a load/use hazard !(E_icode in { IMRMOVL, IPOPL } 
&& E_dstM in { d_srcA, d_srcB }) && IRET in { D_icode, E_icode, M_icode }; 
Practice Problem 4.38 
Write HCL code for the signal E_bubble in the PIPE implementation. 
Practice Problem 4.39 
Write HCL code for the signal set_cc in the PIPE implementation. This should only occur for OPl instructions, and should consider the effects of program excep-tions. 
Practice Problem 4.40 
Write HCL code for the signals M_bubble and W_stall in the PIPE implemen-tation. The latter signal requires modifying the exception condition listed in Fig-ure 4.64. 
This covers all of the special pipeline control signal values. In the complete HCL code for PIPE, all other pipeline control signals are set to zero. 

Aside Testing the design 
As we have seen, there are many ways to introduce bugs into a design even for a simple microprocessor. With pipelining, there are many subtle interactions between the instructions at different pipeline stages. We have seen that many of the design challenges involve unusual instructions (such as popping to the stack pointer) or unusual instruction combinations (such as a not-taken jump followed by a ret). We also see that exception handling adds an entirely new dimension to the possible pipeline behaviors. How then can we be sure that our design is correct? For hardware manufacturers, this is a dominant concern, since they cannot simply report an error and have users download code patches over the Internet. Even a simple logic design error can have serious consequences, especially as microprocessors are increasingly used to operate systems that are critical to our lives and health, such as automotive antilock braking systems, heart pacemakers, and aircraft control systems. 
Simply simulating a design while running a number of ¡°typical¡± programs is not a suf.cient means of testing a system. Instead, thorough testing requires devising ways of systematically generating many tests that will exercise as many different instructions and instruction combinations as possible. In creating our Y86 processor designs, we also devised a number of testing scripts, each of which generates many different tests, runs simulations of the processor, and compares the resulting register and memory values to those produced by our yis instruction set simulator. Here is a brief description of the scripts: 
optest: Runs 49 tests of different Y86 instructions with different source and destination registers jtest: Runs 64 tests of the different jump and call instructions, with different combinations of whether or not the branches are taken cmtest: Runs 28 tests of the different conditional move instructions, with different control combi-nations htest: Runs 600 tests of different data hazard possibilities, with different combinations of source and destination instructions, and with different numbers of nop instructions between the instruction pairs ctest: Tests 22 different control combinations, based on an analysis similar to what we did in Sec-tion 4.5.11 etest: Tests 12 different combinations of instructions causing exceptions and instructions following it that could alter the programmer-visible state 
The key idea of this testing method is that we want to be as systematic as possible, generating tests that create the different conditions that are likely to cause pipeline errors. 
Aside Formally verifying our design 
Even when a design passes an extensive set of tests, we cannot be certain that it will operate correctly for all possible programs. The number of possible programs we could test is unimaginably large, even if we only consider tests consisting of short code segments. Newer methods of formal veri.cation, however, hold the promise that we can have tools that rigorously consider all possible behaviors of a system and determine whether or not there are any design errors. 
We were able to apply formal veri.cation to an earlier version of our Y86 processors [13]. We set up a framework to compare the behavior of the pipelined design PIPE to the unpipelined version SEQ. That is, it was able to prove that for an arbitrary Y86 program, the two processors would have identical effects on the programmer-visible state. Of course, our veri.er cannot actually run all possible programs, since there are an in.nite number of them. Instead, it uses a form of proof by induction, showing a consistency between the two processors on a cycle-by-cycle basis. Carrying out this analysis requires reasoning about the hardware using symbolic methods in which we consider all program values to be arbitrary integers, and we abstract the ALU as a sort of ¡°black box,¡± computing some unspeci.ed function over its arguments. We assume only that the ALUs for SEQ and PIPE compute identical functions. 
We used the HCL descriptions of the control logic to generate the control logic for our symbolic processor models, and so we could catch any bugs in the HCL code. Being able to show that SEQ and PIPE are identical does not guarantee that either of them faithfully implements the Y86 instruction set architecture. However, it would uncover any bug due to an incorrect pipeline design, and this is the major source of design errors. 
In our experiments, we veri.ed not only the version of PIPE we have considered in this chapter but also several variants that we give as homework problems, in which we add more instructions, modify the hardware capabilities, or use different branch prediction strategies. Interestingly, we found only one bug in all of our designs, involving control combination B (described in Section 4.5.11) for our solution to the variant described in Problem 4.57. This exposed a weakness in our testing regime that caused us to add additional cases to the ctest testing script. 
Formal veri.cation is still in an early stage of development. The tools are often dif.cult to use, and they do not have the capacity to verify large-scale designs. We were able to verify our Y86 processors in part because of their relative simplicity. Even then, it required several weeks of effort and multiple runs of the tools, each requiring up to eight hours of computer time. This is an active area of research, with some tools becoming commercially available, and some in use at companies such as Intel, AMD, and IBM. 
Web Aside ARCH:VLOG Verilog implementation of a pipelined Y86 processor 
As we have mentioned, modern logic design involves writing textual representations of hardware designs in a hardware description language. The design can then be tested by both simulation and by a variety of formal veri.cation tools. Once we have con.dence in the design, we can use logic synthesis tools to translate the design into actual logic circuits. 
We have developed models of our Y86 processor designs in the Verilog hardware description language. These designs combine modules implementing the basic building blocks of the processor, along with control logic generated directly from the HCL descriptions. We have been able to synthesize some of these designs, download the logic circuit descriptions onto .eld-programmable gate array (FPGA) hardware, and run the processors on actual Y86 programs. 
4.5.12 Performance Analysis 
We can see that the conditions requiring special action by the pipeline control logic all cause our pipeline to fall short of the goal of issuing a new instruction on every clock cycle. We can measure this inef.ciency by determining how often a bubble gets injected into the pipeline, since these cause unused pipeline cycles. A return instruction generates three bubbles, a load/use hazard generates one, and a mispredicted branch generates two. We can quantify the effect these penalties have on the overall performance by computing an estimate of the average number of clock cycles PIPE would require per instruction it executes, a measure known as the CPI (for ¡°cycles per instruction¡±). This measure is the reciprocal of the average throughput of the pipeline, but with time measured in clock cycles rather than picoseconds. It is a useful measure of the architectural ef.ciency of a design. 
If we ignore the performance implications of exceptions (which, by de.nition, will only occur rarely), another way to think about CPI is to imagine we run the processor on some benchmark program and observe the operation of the execute stage. On each cycle, the execute stage would either process an instruction, and this instruction would then continue through the remaining stages to completion, or it would process a bubble, injected due to one of the three special cases. If the stage processes a total of Ci instructions and Cb bubbles, then the processor has required around Ci + Cb total clock cycles to execute Ci instructions. We say ¡°around¡± because we ignore the cycles required to start the instructions .owing through the pipeline. We can then compute the CPI for this benchmark as follows: 

Ci + Cb Cb
CPI == 1.0 + Ci Ci 

That is, the CPI equals 1.0 plus a penalty term Cb/Ci indicating the average number of bubbles injected per instruction executed. Since only three different instruction types can cause a bubble to be injected, we can break this penalty term into three components: 
CPI = 1.0 + lp + mp + rp 

where lp (for ¡°load penalty¡±) is the average frequency with which bubbles are in-jected while stalling for load/use hazards, mp (for ¡°mispredicted branch penalty¡±) is the average frequency with which bubbles are injected when canceling instruc-tions due to mispredicted branches, and rp (for ¡°return penalty¡±) is the average frequency with which bubbles are injected while stalling for ret instructions. Each of these penalties indicates the total number of bubbles injected for the stated reason (some portion of Cb) divided by the total number of instructions that were executed (Ci). 
To estimate each of these penalties, we need to know how frequently the relevant instructions (load, conditional branch, and return) occur, and for each of these how frequently the particular condition arises. Let us pick the following set of frequencies for our CPI computation (these are comparable to measurements reported in [47] and [49]): 
. Load instructions (mrmovl and popl) account for 25% of all instructions executed. Of these, 20% cause load/use hazards. 
. Conditional branches account for 20% of all instructions executed. Of these, 60% are taken and 40% are not taken. 
. Return instructions account for 2% of all instructions executed. 
We can therefore estimate each of our penalties as the product of the fre-quency of the instruction type, the frequency the condition arises, and the number of bubbles that get injected when the condition occurs: 
Instruction Condition Cause Name frequency frequency Bubbles Product 
Load/Use lp 0.25 0.20 10.05 Mispredict mp 0.20 0.40 20.16 Return rp 0.02 1.00 30.06 
Total Penalty 0.27 
The sum of the three penalties is 0.27, giving a CPI of 1.27. 
Our goal was to design a pipeline that can issue one instruction per cycle, giving a CPI of 1.0. We did not quite meet this goal, but the overall performance is still quite good. We can also see that any effort to reduce the CPI further should focus on mispredicted branches. They account for 0.16 of our total penalty of 0.27, because conditional branches are common, our prediction strategy often fails, and we cancel two instructions for every misprediction. 
Practice Problem 4.41 
Suppose we use a branch prediction strategy that achieves a success rate of 65%, such as backward taken, forward not-taken, as described in Section 4.5.4. What would be the impact on CPI, assuming all of the other frequencies are not affected? 
Practice Problem 4.42 
Let us analyze the relative performance of using conditional data transfers versus conditional control transfers for the programs you wrote for Problems 4.4 and 4.5. Assume we are using these programs to compute the sum of the absolute values of a very long array, and so the overall performance is determined largely by the number of cycles required by the inner loop. Assume our jump instructions are predicted as being taken, and that around 50% of the array values are positive. 
A. On average, how many instructions are executed in the inner loops of the two programs? 
B. On average, how many bubbles would be injected into the inner loop of the two programs? 
C. What is the average number of clock cycles required per array element for the two programs? 
4.5.13 Un.nished Business 
We have created a structure for the PIPE pipelined microprocessor, designed the control logic blocks, and implemented pipeline control logic to handle special cases where normal pipeline .ow does not suf.ce. Still, PIPE lacks several key features that would be required in an actual microprocessor design. We highlight a few of these and discuss what would be required to add them. 
Multicycle Instructions 
All of the instructions in the Y86 instruction set involve simple operations such as adding numbers. These can be processed in a single clock cycle within the execute stage. In a more complete instruction set, we would also need to implement instructions requiring more complex operations such as integer multiplication and division, and .oating-point operations. In a medium-performance processor such as PIPE, typical execution times for these operations range from 3 or 4 cycles for .oating-point addition up to 32 for integer division. To implement these instructions, we require both additional hardware to perform the computations and a mechanism to coordinate the processing of these instructions with the rest of the pipeline. 

One simple approach to implementing multicycle instructions is to simply expand the capabilities of the execute stage logic with integer and .oating-point arithmetic units. An instruction remains in the execute stage for as many clock cycles as it requires, causing the fetch and decode stages to stall. This approach is simple to implement, but the resulting performance is not very good. 
Better performance can be achieved by handling the more complex opera-tions with special hardware functional units that operate independently of the main pipeline. Typically, there is one functional unit for performing integer mul-tiplication and division, and another for performing .oating-point operations. As an instruction enters the decode stage, it can be issued to the special unit. While the unit performs the operation, the pipeline continues processing other instructions. Typically, the .oating-point unit is itself pipelined, and thus multiple operations can execute concurrently in the main pipeline and in the different units. 
The operations of the different units must be synchronized to avoid incorrect behavior. For example, if there are data dependencies between the different operations being handled by different units, the control logic may need to stall one part of the system until the results from an operation handled by some other part of the system have been completed. Often, different forms of forwarding are used to convey results from one part of the system to other parts, just as we saw between the different stages of PIPE. The overall design becomes more complex than we have seen with PIPE, but the same techniques of stalling, forwarding, and pipeline control can be used to make the overall behavior match the sequential ISA model. 
Interfacing with the Memory System 
In our presentation of PIPE, we assumed that both the instruction fetch unit and the data memory could read or write any memory location in one clock cycle. We also ignored the possible hazards caused by self-modifying code where one instruction writes to the region of memory from which later instructions are fetched. Furthermore, we reference memory locations according to their virtual addresses, and these require a translation into physical addresses before the actual read or write operation can be performed. Clearly, it is unrealistic to do all of this processing in a single clock cycle. Even worse, the memory values being accessed may reside on disk, requiring millions of clock cycles to read into the processor memory. 
As will be discussed in Chapters 6 and 9, the memory system of a processor uses a combination of multiple hardware memories and operating system soft-ware to manage the virtual memory system. The memory system is organized as a hierarchy, with faster but smaller memories holding a subset of the memory being backed up by slower and larger memories. At the level closest to the processor, the cache memories provide fast access to the most heavily referenced memory locations. A typical processor has two .rst-level caches¡ªone for reading instruc-tions and one for reading and writing data. Another type of cache memory, known as a translation look-aside buffer, or TLB, provides a fast translation from virtual to physical addresses. Using a combination of TLBs and caches, it is indeed pos-sible to read instructions and read or write data in a single clock cycle most of the time. Thus, our simpli.ed view of memory referencing by our processors is actually quite reasonable. 
Although the caches hold the most heavily referenced memory locations, there will be times when a cache miss occurs, where some reference is made to a location that is not held in the cache. In the best case, the missing data can be retrieved from a higher-level cache or from the main memory of the processor, requiring 3 to 20 clock cycles. Meanwhile, the pipeline simply stalls, holding the instruction in the fetch or memory stage until the cache can perform the read or write operation. In terms of our pipeline design, this can be implemented by adding more stall conditions to the pipeline control logic. A cache miss and the consequent synchronization with the pipeline is handled completely by hardware, keeping the time required down to a small number of clock cycles. 
In some cases, the memory location being referenced is actually stored in the disk memory. When this occurs, the hardware signals a page fault exception. Like other exceptions, this will cause the processor to invoke the operating system¡¯s exception handler code. This code will then set up a transfer from the disk to the main memory. Once this completes, the operating system will return back to the original program, where the instruction causing the page fault will be re-executed. This time, the memory reference will succeed, although it might cause a cache miss. Having the hardware invoke an operating system routine, which then returns control back to the hardware, allows the hardware and system software to cooperate in the handling of page faults. Since accessing a disk can require millions of clock cycles, the several thousand cycles of processing performed by the OS page fault handler has little impact on performance. 
From the perspective of the processor, the combination of stalling to han-dle short-duration cache misses and exception handling to handle long-duration page faults takes care of any unpredictability in memory access times due to the structure of the memory hierarchy. 

Aside State-of-the-art microprocessor design 
A .ve-stage pipeline, such as we have shown with the PIPE processor, represented the state of the art in processor design in the mid-1980s. The prototype RISC processor developed by Patterson¡¯s research group at Berkeley formed the basis for the .rst SPARC processor, developed by Sun Microsystems in 1987. The processor developed by Hennessy¡¯s research group at Stanford was commercialized by MIPS Technologies (a company founded by Hennessy) in 1986. Both of these used .ve-stage pipelines. The Intel i486 processor also uses a .ve-stage pipeline, although with a different partitioning of responsibilities among the stages, with two decode stages and a combined execute/memory stage [33]. These pipelined designs are limited to a throughput of at most one instruction per clock cycle. The 
CPI (for ¡°cycles per instruction¡±) measure described in Section 4.5.12 can never be less than 1.0. The 
different stages can only process one instruction at a time. More recent processors support superscalar 
operation, meaning that they can achieve a CPI less than 1.0 by fetching, decoding, and executing multiple instructions in parallel. As superscalar processors have become widespread, the accepted performance measure has shifted from CPI to its reciprocal¡ªthe average number of instructions executed per cycle, or IPC. It can exceed 1.0 for superscalar processors. The most advanced designs use a technique known as out-of-order execution to execute multiple instructions in parallel, possibly in a totally different order than they occur in the program, while preserving the overall behavior implied by the sequential ISA model. This form of execution is described in Chapter 5 as part of our discussion of program optimization. 
Pipelined processors are not just historical artifacts, however. The majority of processors sold are used in embedded systems, controlling automotive functions, consumer products, and other devices where the processor is not directly visible to the system user. In these applications, the simplicity of a pipelined processor, such as the one we have explored in this chapter, reduces its cost and power requirements compared to higher-performance models. 
More recently, as multicore processors have gained a following, some have argued that we could get more overall computing power by integrating many simple processors on a single chip rather than a smaller number of more complex ones. This strategy is sometimes referred to as ¡°many-core¡± processors [10]. 
4.6 
Summary 


We have seen that the instruction set architecture, or ISA, provides a layer of abstraction between the behavior of a processor¡ªin terms of the set of instructions and their encodings¡ªand how the processor is implemented. The ISA provides a very sequential view of program execution, with one instruction executed to completion before the next one begins. 
We de.ned the Y86 instruction set by starting with the IA32 instructions and simplifying the data types, address modes, and instruction encoding considerably. The resulting ISA has attributes of both RISC and CISC instruction sets. We then organized the processing required for the different instructions into a series of .ve stages, where the operations at each stage vary according to the instruction being executed. From this, we constructed the SEQ processor, in which an en-tire instruction is executed every clock cycle by having it .ow through all .ve stages. 
Pipelining improves the throughput performance of a system by letting the different stages operate concurrently. At any given time, multiple operations are being processed by the different stages. In introducing this concurrency, we must be careful to provide the same program-level behavior as would a sequential execution of the program. We introduced pipelining by reordering parts of SEQ to get SEQ+, and then adding pipeline registers to create the PIPE¨C pipeline. We enhanced the pipeline performance by adding forwarding logic to speed the sending of a result from one instruction to another. Several special cases require additional pipeline control logic to stall or cancel some of the pipeline stages. 
Our design included rudimentary mechanisms to handle exceptions, where we make sure that only instructions up to the excepting instruction affect the programmer-visible state. Implementing a complete handling of exceptions would be signi.cantly more challenging. Properly handling exceptions gets even more 
complex in systems that employ greater degrees of pipelining and parallelism. In this chapter, we have learned several important lessons about processor 
design: 
. Managing complexity is a top priority. We want to make optimum use of the hardware resources to get maximum performance at minimum cost. We did this by creating a very simple and uniform framework for processing all of the different instruction types. With this framework, we could share the hardware units among the logic for processing the different instruction types. 
. We do not need to implement the ISA directly. A direct implementation of the ISA would imply a very sequential design. To achieve higher performance, we want to exploit the ability in hardware to perform many operations si-multaneously. This led to the use of a pipelined design. By careful design and analysis, we can handle the various pipeline hazards, so that the overall effect of running a program exactly matches what would be obtained with the ISA model. 
. Hardware designers must be meticulous. Once a chip has been fabricated, it is nearly impossible to correct any errors. It is very important to get the design right on the .rst try. This means carefully analyzing different instruction types and combinations, even ones that do not seem to make sense, such as popping to the stack pointer. Designs must be thoroughly tested with systematic simulation test programs. In developing the control logic for PIPE, our design had a subtle bug that was uncovered only after a careful and systematic analysis of control combinations. 

Web Aside ARCH:HCL HCL descriptions of Y86 processors 
In this chapter, we have looked at portions of the HCL code for several simple logic designs, and for the control logic for Y86 processors SEQ and PIPE. For reference, we provide documentation of the HCL language and complete HCL descriptions for the control logic of the two processors. Each of these descriptions requires only 5¨C7 pages of HCL code, and it is worthwhile to study them in their entirety. 
4.6.1 Y86 Simulators 
The lab materials for this chapter include simulators for the SEQ and PIPE processors. Each simulator has two versions: 
. The GUI (graphic user interface) version displays the memory, program code, and processor state in graphic windows. This provides a way to readily see how the instructions .ow through the processors. The control panel also allows you to reset, single-step, or run the simulator interactively. 
. The text version runs the same simulator, but it only displays information by printing to the terminal. This version is not as useful for debugging, but it allows automated testing of the processor. 

The control logic for the simulators is generated by translating the HCL declarations of the logic blocks into C code. This code is then compiled and linked with the rest of the simulation code. This combination makes it possible for you to test out variants of the original designs using the simulators. Testing scripts are also available that thoroughly exercise the different instructions and the different hazard possibilities. 
Bibliographic 
Notes 

For those interested in learning more about logic design, Katz¡¯s logic design textbook [56] is a standard introductory text, emphasizing the use of hardware description languages. 
Hennessy and Patterson¡¯s computer architecture textbook [49] provides ex-tensive coverage of processor design, including both simple pipelines, such as the one we have presented here, and more advanced processors that execute more instructions in parallel. Shriver and Smith [97] give a very thorough presentation of an Intel-compatible IA32 processor manufactured by AMD. 
Homework 
Problems 

4.43 ¡ô 

In Section 3.4.2, the IA32 pushl instruction was described as decrementing the stack pointer and then storing the register at the stack pointer location. So, if we had an instruction of the form pushl REG, for some register REG, it would be equivalent to the code sequence: 
subl $4,%esp Decrement stack pointer movl REG,(%esp) Store REG on stack 
A. In light of analysis done in Problem 4.6, does this code sequence correctly describe the behavior of the instruction pushl %esp? Explain. 
B. How could you rewrite the code sequence so that it correctly describes both the cases where REG is %esp as well as any other register? 
4.44 ¡ô 

In Section 3.4.2, the IA32 popl instruction was described as copying the result from the top of the stack to the destination register and then incrementing the stack pointer. So, if we had an instruction of the form popl REG, it would be equivalent to the code sequence: 
movl (%esp),REG Read REG from stack addl $4,%esp Increment stack pointer 
A. In light of analysis done in Problem 4.7, does this code sequence correctly describe the behavior of the instruction popl %esp? Explain. 
B. How could you rewrite the code sequence so that it correctly describes both the cases where REG is %esp as well as any other register? 
4.45 ¡ô¡ô¡ô 
Your assignment will be to write a Y86 program to perform bubblesort. For ref-erence, the following C function implements bubblesort using array referencing: 
/* Bubble sort: Array version */ 
void bubble_a(int *data, int count) { int i, last; for (last = count-1; last > 0; last--) { 
for(i=0; i<last; i++) if (data[i+1] < data[i]) { 
/* Swap adjacent elements */ 
int t = data[i+1]; data[i+1] = data[i]; data[i] = t; 
} } } 
A. Write and test a C version that references the array elements with pointers, rather than using array indexing. 
B. Write and test a Y86 program consisting of the function and test code. You may .nd it useful to pattern your implementation after IA32 code generated by compiling your C code. Although pointer comparisons are normally done using unsigned arithmetic, you can use signed arithmetic for this exercise. 
4.46 ¡ô¡ô 
Modify the code you wrote for Problem 4.46 to implement the test and swap in the inner loop of the bubblesort function using conditional moves. 
4.47 ¡ô 
In our example Y86 programs, such as the Sum function shown in Figure 4.6, we encounter many cases (e.g., lines 12 and 13 and lines 14 and 15) in which we want to add a constant value to a register. This requires .rst using an irmovl instruction to set a register to the constant, and then an addl instruction to add this value to the destination register. Suppose we want to add a new instruction iaddl with the following format: 
Byte 012345 
iaddl V, rB 
C  0  F  rB  V  

This instruction adds the constant value V to register rB. Describe the computa-tions performed to implement this instruction. Use the computations for irmovl and OPl (Figure 4.18) as a guide. 
4.48 ¡ô 

As described in Section 3.7.2, the IA32 instruction leave can be used to prepare the stack for returning. It is equivalent to the following Y86 code sequence: 
1 rrmovl %ebp, %esp Set stack pointer to beginning of frame 2 popl %ebp Restore saved %ebp and set stack ptr to end of caller¡¯s 
Suppose we add this instruction to the Y86 instruction set, using the following encoding: 
Byte 012345 
leave 

D  0  

Describe the computations performed to implement this instruction. Use the computations for popl (Figure 4.20) as a guide. 
4.49 ¡ô¡ô 

The .le seq-full.hcl contains the HCL description for SEQ, along with the dec-laration of a constant IIADDL having hexadecimal value C, the instruction code for iaddl. Modify the HCL descriptions of the control logic blocks to implement the iaddl instruction, as described in Homework Problem 4.47. See the lab material for directions on how to generate a simulator for your solution and how to test it. 
4.50 ¡ô¡ô 

The .le seq-full.hcl also contains the declaration of a constant ILEAVE having hexadecimal value D, the instruction code for leave, as well as the declaration of a constant REBP having value 7, the register ID for %ebp. Modify the HCL descriptions of the control logic blocks to implement the leave instruction, as described in Homework Problem 4.48. See the lab material for directions on how to generate a simulator for your solution and how to test it. 
4.51 ¡ô¡ô¡ô 

Suppose we wanted to create a lower-cost pipelined processor based on the struc-ture we devised for PIPE¨C (Figure 4.41), without any bypassing. This design would handle all data dependencies by stalling until the instruction generating a needed value has passed through the write-back stage. 
The .le pipe-stall.hcl contains a modi.ed version of the HCL code for PIPE in which the bypassing logic has been disabled. That is, the signals e_valA and e_valB are simply declared as follows: 
## DO NOT MODIFY THE FOLLOWING CODE. ## No forwarding. valA is either valP or value from register file int d_valA = [ 
D_icode in { ICALL, IJXX } : D_valP; # Use incremented PC 
1 : d_rvalA; # Use value read from register file ]; 
## No forwarding. valB is value from register file int d_valB = d_rvalB; 
frame 

Modify the pipeline control logic at the end of this .le so that it correctly handles all possible control and data hazards. As part of your design effort, you should analyze the different combinations of control cases, as we did in the design of the pipeline control logic for PIPE. You will .nd that many different combinations can occur, since many more conditions require the pipeline to stall. Make sure your control logic handles each combination correctly. See the lab material for directions on how to generate a simulator for your solution and how to test it. 
4.52 ¡ô¡ô 
The .le pipe-full.hcl contains a copy of the PIPE HCL description, along with a declaration of the constant value IIADDL. Modify this .le to implement the iaddl instruction, as described in Homework Problem 4.47. See the lab material for directions on how to generate a simulator for your solution and how to test it. 
4.53 ¡ô¡ô¡ô 
The .le pipe-full.hcl also contains declarations of constants ILEAVE and REBP. Modify this .le to implement the leave instruction, as described in Homework Problem 4.48. See the lab material for directions on how to generate a simulator for your solution and how to test it. 
4.54 ¡ô¡ô¡ô 
The .le pipe-nt.hcl contains a copy of the HCL code for PIPE, plus a declaration of the constant J_YES with value 0, the function code for an unconditional jump instruction. Modify the branch prediction logic so that it predicts conditional jumps as being not-taken while continuing to predict unconditional jumps and call as being taken. You will need to devise a way to get valC, the jump target address, to pipeline register M to recover from mispredicted branches. See the lab material for directions on how to generate a simulator for your solution and how to test it. 
4.55 ¡ô¡ô¡ô 
The .le pipe-btfnt.hcl contains a copy of the HCL code for PIPE, plus a decla-ration of the constant J_YES with value 0, the function code for an unconditional jump instruction. Modify the branch prediction logic so that it predicts conditional jumps as being taken when valC < valP (backward branch) and as being not-taken when valC ¡Ý valP (forward branch). (Since Y86 does not support unsigned arith-metic, you should implement this test using a signed comparison.) Continue to predict unconditional jumps and call as being taken. You will need to devise a way to get both valC and valP to pipeline register M to recover from mispredicted branches. See the lab material for directions on how to generate a simulator for your solution and how to test it. 
4.56 ¡ô¡ô¡ô 
In our design of PIPE, we generate a stall whenever one instruction performs a load, reading a value from memory into a register, and the next instruction has this register as a source operand. When the source gets used in the execute stage, this stalling is the only way to avoid a hazard. 

For cases where the second instruction stores the source operand to memory, such as with an rmmovl or pushl instruction, this stalling is not necessary. Consider the following code examples: 
1  mrmovl  0(%ecx),%edx  #  Load  1  
2  pushl  %edx  #  Store  1  
3  nop  
4  popl  %edx  #  Load  2  
5  rmmovl  %eax,0(%edx)  #  Store  2  
In lines 1 and 2, the mrmovl  instruction reads a value from memory into  

%edx, and the pushl instruction then pushes this value onto the stack. Our design for PIPE would stall the pushl instruction to avoid a load/use hazard. Observe, however, that the value of %edx is not required by the pushl instruction until it reaches the memory stage. We can add an additional bypass path, as diagrammed in Figure 4.69, to forward the memory output (signal m_valM)tothe valA .eld in pipeline register M. On the next clock cycle, this forwarded value can then be written to memory. This technique is known as load forwarding. 

Note that the second example (lines 4 and 5) in the code sequence above cannot make use of load forwarding. The value loaded by the popl instruction is used as part of the address computation by the next instruction, and this value is required in the execute stage rather than the memory stage. 
A. Write a logic formula describing the detection condition for a load/use haz-ard, similar to the one given in Figure 4.64, except that it will not cause a stall in cases where load forwarding can be used. 
B. The .le pipe-lf.hcl contains a modi.ed version of the control logic for PIPE. It contains the de.nition of a signal e_valA to implement the block labeled ¡°Fwd A¡± in Figure 4.69. It also has the conditions for a load/use haz-ard in the pipeline control logic set to zero, and so the pipeline control logic will not detect any forms of load/use hazards. Modify this HCL description to implement load forwarding. See the lab material for directions on how to generate a simulator for your solution and how to test it. 
4.57 ¡ô¡ô¡ô 
Our pipelined design is a bit unrealistic in that we have two write ports for the register .le, but only the popl instruction requires two simultaneous writes to the register .le. The other instructions could therefore use a single write port, sharing this for writing valE and valM. The following .gure shows a modi.ed version of the write-back logic, in which we merge the write-back register IDs (W_dstE and W_ dstM) into a single signal w_dstE and the write-back values (W_valE and W_valM) into a single signal w_valE: 
w_valE w_dstE
Stat 

The logic for performing the merges is written in HCL as follows: 
## Set E port register ID 
int w_dstE = [ ## writing from valM W_dstM != RNONE : W_dstM; 
1: 
W_dstE; ]; 

## Set E port value int w_valE = [ W_dstM != RNONE : W_valM; 

1: 
W_valE; ]; 



The control for these multiplexors is determined by dstE¡ªwhen it indicates there is some register, then it selects the value for port E, and otherwise it selects the value for port M. 
In the simulation model, we can then disable register port M, as shown by the following HCL code: 
## Disable register port M ## Set M port register ID int w_dstM = RNONE; 
## Set M port value int w_valM = 0; 
The challenge then becomes to devise a way to handle popl. One method is to use the control logic to dynamically process the instruction popl rA so that it has the same effect as the two-instruction sequence 
iaddl $4, %esp mrmovl -4(%esp), rA 
(See Homework Problem 4.47 for a description of the iaddl instruction.) Note the ordering of the two instructions to make sure popl %esp works properly. You can do this by having the logic in the decode stage treat popl the same as it would the iaddl listed above, except that it predicts the next PC to be equal to the current PC. On the next cycle, the popl instruction is refetched, but the instruction code is converted to a special value IPOP2. This is treated as a special instruction that has the same behavior as the mrmovl instruction listed above. 
The .le pipe-1w.hcl contains the modi.ed write-port logic described above. It contains a declaration of the constant IPOP2 having hexadecimal value E.It also contains the de.nition of a signal f_icode that generates the icode .eld for pipeline register D. This de.nition can be modi.ed to insert the instruction code IPOP2 the second time the popl instruction is fetched. The HCL .le also contains a declaration of the signal f_pc, the value of the program counter generated in the fetch stage by the block labeled ¡°Select PC¡± (Figure 4.55). 
Modify the control logic in this .le to process popl instructions in the manner we have described. See the lab material for directions on how to generate a simulator for your solution and how to test it. 
4.58 ¡ô¡ô 

Compare the performance of the two versions of bubblesort (Problems 4.45 and 4.46). Explain why one version performs better than the other. 
Solutions 
to 
Practice 
Problems 

Solution to Problem 4.1 (page 341) 
Encoding instructions by hand is rather tedious, but it will solidify your under-standing of the idea that assembly code gets turned into byte sequences by the assembler. In the following output from our Y86 assembler, each line shows an address and a byte sequence that starts at that address: 
1  0x100:  |  .pos  0x100  #  Start  code  at  address  0x100  
2  0x100:  30f30f000000  |  irmovl  $15,%ebx  #  Load  15  into  %ebx  
3  0x106:  2031  |  rrmovl  %ebx,%ecx  #  Copy  15  to  %ecx  
4  0x108:  |  loop:  #  loop:  
5  0x108:  4013fdffffff  |  rmmovl  %ecx,-3(%ebx)  #  Save  %ecx  at  address  15-3  =  12  
6  0x10e:  6031  |  addl  %ebx,%ecx  #  Increment  %ecx  by  15  
7  0x110:  7008010000  |  jmp  loop  #  Goto  loop  

Several features of this encoding are worth noting: 
. Decimal 15 (line 2) has hex representation 0x0000000f. Writing the bytes in reverse order gives 0f 0000 00. . Decimal .3 (line 5) has hex representation 0xfffffffd. Writing the bytes in reverse order gives fd ffff ff. 
. The code starts at address 0x100. The .rst instruction requires 6 bytes, while the second requires 2. Thus, the loop target will be 0x00000108. Writing these bytes in reverse order gives 08 01 0000. 
Solution to Problem 4.2 (page 341) 
Decoding a byte sequence by hand helps you understand the task faced by a processor. It must read byte sequences and determine what instructions are to be executed. In the following, we show the assembly code used to generate each of the byte sequences. To the left of the assembly code, you can see the address and byte sequence for each instruction. 
A. Some operations with immediate data and address displacements: 
0x100: 30f3fcffffff | irmovl $-4,%ebx 0x106: 406300080000 | rmmovl %esi,0x800(%ebx) 0x10c: 00 | halt 
B. Code including a function call: 
0x200: a06f | pushl %esi 0x202: 8008020000 | call proc 0x207: 00 | halt 0x208: | proc: 0x208: 30f30a000000 | irmovl $10,%ebx 0x20e: 90 | ret 
C. Code containing illegal instruction speci.er byte 0xf0: 
0x300: 505407000000 | mrmovl 7(%esp),%ebp 0x306: 10 | nop 0x307: f0 | .byte 0xf0 # invalid instruction code 0x308: b01f | popl %ecx 

D. Code containing a jump operation: 
0x400: | loop: 0x400: 6113 | subl %ecx, %ebx 0x402: 7300040000 | je loop 0x407: 00 | halt 
E. Code containing an invalid second byte in a pushl instruction: 
0x500: 6362 | xorl %esi,%edx 0x502: a0 | .byte 0xa0 # pushl instruction code 0x503: f0 | .byte 0xf0 # Invalid register specifier byte 
Solution to Problem 4.3 (page 350) 
As suggested in the problem, we adapted the code generated by gcc for an IA32 machine: 
# int Sum(int *Start, int Count) 
rSum: pushl %ebp rrmovl %esp,%ebp pushl %ebx # Save value of %ebx mrmovl 8(%ebp),%ebx # Get Start mrmovl 12(%ebp),%eax # Get Count andl %eax,%eax # Test value of Count jle L38 # If <= 0, goto zreturn irmovl $-1,%edx addl %edx,%eax # Count--pushl %eax # Push Count irmovl $4,%edx rrmovl %ebx,%eax addl %edx,%eax pushl %eax # Push Start+1 call rSum # Sum(Start+1, Count-1) mrmovl (%ebx),%edx addl %edx,%eax # Add *Start jmp L39 # goto done 
L38: xorl %eax,%eax # zreturn: 
L39: mrmovl -4(%ebp),%ebx # done: Restore %ebx rrmovl %ebp,%esp # Deallocate stack frame popl %ebp # Restore %ebp ret 
Solution to Problem 4.4 (page 350) 
This problem gives you a chance to try your hand at writing assembly code. 
int AbsSum(int *Start, int Count) 
1 AbsSum: 2 pushl %ebp 

3 rrmovl %esp,%ebp 4 mrmovl 8(%ebp),%ecx ecx = Start 5 mrmovl 12(%ebp),%edx edx = Count 6 irmovl $0, %eax sum=0 7 andl %edx,%edx 8 je End 9 Loop: 
10 mrmovl (%ecx),%esi get x = *Start 11 irmovl $0,%edi 0 12 subl %esi,%edi -x 13 jle Pos Skip if -x <= 0 14 rrmovl %edi,%esi x=-x 15 Pos: 16 addl %esi,%eax add x to sum 17 irmovl $4,%ebx 18 addl %ebx,%ecx Start++ 19 irmovl $-1,%ebx 20 addl %ebx,%edx Count--21 jne Loop Stop when 0 22 End: 23 popl %ebp 24 ret 
Solution to Problem 4.5 (page 350) 
This problem gives you a chance to try your hand at writing assembly code with conditional moves. We show only the code for the loop. The rest is the same as for Problem 4.4: 
9 Loop: 10 mrmovl (%ecx),%esi get x = *Start 11 irmovl $0,%edi 0 12 subl %esi,%edi -x 13 cmovg %edi,%esi if -x>0thenx=-x 14 addl %esi,%eax add x to sum 15 irmovl $4,%ebx 16 addl %ebx,%ecx Start++ 17 irmovl $-1,%ebx 18 addl %ebx,%edx Count--19 jne Loop Stop when 0 
Solution to Problem 4.6 (page 350) 
Although it is hard to imagine any practical use for this particular instruction, it is important when designing a system to avoid any ambiguities in the speci.cation. We want to determine a reasonable convention for the instruction¡¯s behavior and make sure each of our implementations adheres to this convention. 
The subl instruction in this test compares the starting value of %esp to the value pushed onto the stack. The fact that the result of this subtraction is zero implies that the old value of %esp gets pushed. 

Solution to Problem 4.7 (page 351) 
It is even more dif.cult to imagine why anyone would want to pop to the stack pointer. Still, we should decide on a convention and stick with it. This code sequence pushes 0xabcd onto the stack, pops to %esp, and returns the popped value. Since the result equals 0xabcd, we can deduce that popl %esp sets the stack pointer to the value read from memory. It is therefore equivalent to the instruction mrmovl (%esp),%esp. 
Solution to Problem 4.8 (page 354) 
The Exclusive-Or function requires that the 2 bits have opposite values: 
bool xor = (!a && b) || (a && !b); 
In general, the signals eq and xor will be complements of each other. That is, one will equal 1 whenever the other is 0. 
Solution to Problem 4.9 (page 356) 
The outputs of the Exclusive-Or circuits will be the complements of the bit equal-ity values. Using DeMorgan¡¯s laws (Web Aside data:bool), we can implement And using Or and Not, yielding the following circuit: 
b31 
a31 b30 a30 
b1 
a1 b0 a0 


Eq 

Solution to Problem 4.10 (page 359) 
This design is a simple variant of the one to .nd the minimum of the three inputs: 
int Med3 = [ A<=B&&B<=C:B; C<=B&&B<=A:B; B<=A&&A<=C:A; C<=A&&A<=B:A; 
1 :C; ]; 
Solution to Problem 4.11 (page 367) 
These exercises help make the stage computations more concrete. We can see from the object code that this instruction is located at address 0x00e. It consists of 6 bytes, with the .rst two being 0x30 and 0x84. The last 4 bytes are a byte-reversed version of 0x00000080 (decimal 128). 
Generic Speci.c Stage irmovl V, rB irmovl $128, %esp 
Fetch icode:ifun ¡û M1[PC] icode:ifun ¡û M1[0x00e] = 3 : 0 rA :rB ¡û M1[PC + 1] rA :rB ¡û M1[0x00f] = 8 : 4 valC ¡û M4[PC + 2] valC ¡û M4[0x010] = 128 valP ¡û PC + 6 valP ¡û 0x00e + 6 = 0x014 
Decode 
Execute valE ¡û 0 + valC valE ¡û 0 + 128 = 128 
Memory 
Write back R[rB] ¡û valE R[%esp] ¡û valE = 128 
PC update PC ¡û valP PC ¡û valP = 0x014 
This instruction sets register %esp to 128 and increments the PC by 6. 
Solution to Problem 4.12 (page 371) 
We can see that the instruction is located at address 0x01c and consists of 2 bytes with values 0xb0 and 0x08. Register %esp was set to 124 by the pushl instruction (line 6), which also stored 9 at this memory location. 
Generic  Speci.c  
Stage  popl rA  popl %eax  
Fetch  icode:ifun ¡û M1[PC]  icode:ifun ¡û M1[0x01c] = b : 0  
rA : rB ¡û M1[PC + 1]  rA :rB ¡û M1[0x01d] = 0 : 8  
valP ¡û PC + 2  valP ¡û 0x01c + 2 = 0x01e  
Decode  valA ¡û R[%esp]  valA ¡û R[%esp] = 124  
valB ¡û R[%esp]  valB ¡û R[%esp] = 124  
Execute  valE ¡û valB + 4  valE ¡û 124 + 4 = 128  


Memory  valM ¡û M4[valA]  valM ¡û M4[124] = 9  
Write back  R[%esp] ¡û valE  R[%esp] ¡û 128  
R[rA] ¡û valM  R[%eax] ¡û 9  
PC update  PC ¡û valP  PC ¡û 0x01e  

The instruction sets %eax to 9, sets %esp to 128, and increments the PC by 2. 
Solution to Problem 4.13 (page 372) 
Tracing the steps listed in Figure 4.20 with rA equal to %esp, we can see that in the memory stage, the instruction will store valA, the original value of the stack pointer, to memory, just as we found for IA32. 
Solution to Problem 4.14 (page 372) 
Tracing the steps listed in Figure 4.20 with rA equal to %esp, we can see that both of the write-back operations will update %esp. Since the one writing valM would occur last, the net effect of the instruction will be to write the value read from memory to %esp, just as we saw for IA32. 
Solution to Problem 4.15 (page 373) 
Implementing conditional moves requires only minor changes from register-to-register moves. We simply condition the write-back step on the outcome of the conditional test: 
Stage cmovXX rA, rB Fetch icode:ifun ¡û M1[PC] 
rA :rB ¡û M1[PC + 1] 
valP ¡û PC + 2 
Decode valA ¡û R[rA] 
Execute valE ¡û 0 + valA Cnd ¡û Cond(CC, ifun) 
Memory 

Write back if (Cnd) R[rB] ¡û valE 
PC update PC ¡û valP 
Solution to Problem 4.16 (page 374) 
We can see that this instruction is located at address 0x023 and is 5 bytes long. The .rst byte has value 0x80, while the last four are a byte-reversed version of 0x00000029, the call target. The stack pointer was set to 128 by the popl instruction (line 7). 
464  Chapter 4  Processor Architecture  
Generic  Speci.c  
Stage  call Dest  call 0x029  

Fetch icode:ifun ¡ûM1[PC] icode:ifun ¡ûM1[0x023] =8 : 0 
valC ¡ûM4[PC +1] valC ¡ûM4[0x024] =0x029 valP ¡ûPC +5 valP ¡û0x023 +5 =0x028 
Decode valB ¡ûR[%esp] valB ¡ûR[%esp] =128 
Execute valE ¡ûvalB +.4 valE ¡û128 +.4 =124 
Memory M4[valE] ¡ûvalP M4[124] ¡û0x028 
Write back R[%esp] ¡ûvalE R[%esp] ¡û124 
PC update PC ¡ûvalC PC ¡û0x029 
The effect of this instruction is to set %esp to 124, to store 0x028 (the return address) at this memory address, and to set the PC to 0x029 (the call target). 
Solution to Problem 4.17 (page 384) 
All of the HCL code in this and other practice problems is straightforward, but trying to generate it yourself will help you think about the different instructions and how they are processed. For this problem, we can simply look at the set of Y86 instructions (Figure 4.2) and determine which have a constant .eld. 
bool need_valC = icode in { IIRMOVL, IRMMOVL, IMRMOVL, IJXX, ICALL }; 
Solution to Problem 4.18 (page 386) 
This code is similar to the code for srcA. 
intsrcB= [ icode in { IOPL, IRMMOVL, IMRMOVL } : rB; icode in { IPUSHL, IPOPL, ICALL, IRET } : RESP; 
1 : RNONE; # Don¡¯t need register ]; 
Solution to Problem 4.19 (page 387) 
This code is similar to the code for dstE. 
intdstM= [ icode in { IMRMOVL, IPOPL}:rA; 
1 : RNONE; # Don¡¯t write any register ]; 

Solution to Problem 4.20 (page 387) 
As we found in Practice Problem 4.14, we want the write via the M port to take priority over the write via the E port in order to store the value read from memory into %esp. 
Solution to Problem 4.21 (page 388) 
This code is similar to the code for aluA. 
int aluB = [ 

icode in { IRMMOVL, IMRMOVL, IOPL, ICALL, 
IPUSHL, IRET, IPOPL } : valB; 

icode in { IRRMOVL, IIRMOVL}:0; 
# Other instructions don¡¯t need ALU ]; 
Solution to Problem 4.22 (page 389) 
Implementing conditional moves is surprisingly simple: we disable writing to the register .le by setting the destination register to RNONE when the condition does not hold. 
int dstE = [ 

icode in { IRRMOVL } && Cnd : rB; 
icode in { IIRMOVL, IOPL} : rB; 
icode in { IPUSHL, IPOPL, ICALL, IRET } : RESP; 
1 : RNONE; # Don¡¯t write any register ]; 
Solution to Problem 4.23 (page 389) 
This code is similar to the code for mem_addr. 
int mem_data = [ 

# Value from register 
icode in { IRMMOVL, IPUSHL } : valA; 
# Return PC 
icode == ICALL : valP; 
# Default: Don¡¯t write anything ]; 
Solution to Problem 4.24 (page 390) 
This code is similar to the code for mem_read. 
bool mem_write = icode in { IRMMOVL, IPUSHL, ICALL }; 
Solution to Problem 4.25 (page 390) 
Computing the Stat.eld requires collecting status information from several stages: 
## Determine instruction status 
intStat= [ imem_error || dmem_error : SADR; !instr_valid: SINS; icode == IHALT : SHLT; 
1 : SAOK; ]; 
Solution to Problem 4.26 (page 396) 
This problem is an interesting exercise in trying to .nd the optimal balance among a set of partitions. It provides a number of opportunities to compute throughputs and latencies in pipelines. 
A. For a two-stage pipeline, the best partition would be to have blocks A, B, and C in the .rst stage and D, E, and F in the second. The .rst stage has a delay 170 ps, giving a total cycle time of 170 + 20 = 190 picoseconds. We therefore have a throughput of 5.26 GOPS and a latency of 380 ps. 
B. For a three-stage pipeline, we should have blocks A and B in the .rst stage, blocks C and D in the second, and blocks E and F in the third. The .rst two stages have a delay of 110 ps, giving a total cycle time of 130 ps and a throughput of 7.69 GOPS. The latency is 390 ps. 
C. For a four-stage pipeline, we should have block A in the .rst stage, blocks B and C in the second, block D in the third, and blocks E and F in the fourth. The second stage requires 90 ps, giving a total cycle time of 110 ps and a throughput of 9.09 GOPS. The latency is 440 ps. 
D. The optimal design would be a .ve-stage pipeline, with each block in its own stage, except that the .fth stage has blocks E and F. The cycle time is 80 + 20 = 100 picoseconds, for a throughput of around 10.00 GOPS and a latency of 500 ps. Adding more stages would not help, since we cannot run the pipeline any faster than one cycle every 100 ps. 
Solution to Problem 4.27 (page 398) 
Each stage would have combinational logic requiring 300/k ps, and a pipeline register requiring 20 ps. 
A. The total latency would be 300 + 20k ps, while the throughput (in GIPS) would be 
1000 1000k 
= 
300 
+ 20 300 + 20K 
k 
B. Aswelet k go to in.nity, the throughput becomes 1000/20 = 50 GIPS. Of course, this would give us an in.nite latency, as well. 
This exercise quanti.es the diminishing returns of deep pipelining. As we try to subdivide the logic into many stages, the latency of the pipeline registers becomes a limiting factor. 

Solution to Problem 4.28 (page 425) 
This code is very similar to the corresponding code for SEQ, except that we cannot yet determine whether the data memory will generate an error signal for this instruction. 
# Determine status code for fetched instruction 
int f_stat = [ 

imem_error: SADR; 
!instr_valid : SINS; 
f_icode == IHALT : SHLT; 
1 : SAOK; ]; 

Solution to Problem 4.29 (page 426) 
This code simply involves pre.xing the signal names in the code for SEQ with ¡°d_¡± and ¡°D_¡±. 
int d_dstE = [ 

D_icode in { IRRMOVL, IIRMOVL, IOPL} : D_rB; 
D_icode in { IPUSHL, IPOPL, ICALL, IRET } : RESP; 
1 : RNONE; # Don¡¯t write any register ]; 
Solution to Problem 4.30 (page 429) 
The rrmovl instruction (line 5) would stall for one cycle due to a load-use hazard caused by the popl instruction (line 4). As it enters the decode stage, the popl instruction would be in the memory stage, giving both M_dstE and M_dstM equal to %esp. If the two cases were reversed, then the write back from M_valE would take priority, causing the incremented stack pointer to be passed as the argument to the rrmovl instruction. This would not be consistent with the convention for handling popl %esp determined in Practice Problem 4.7. 
Solution to Problem 4.31 (page 429) 
This problem lets you experience one of the important tasks in processor design¡ª devising test programs for a new processor. In general, we should have test pro-grams that will exercise all of the different hazard possibilities and will generate incorrect results if some dependency is not handled properly. 
For this example, we can use a slightly modi.ed version of the program shown in Practice Problem 4.30: 
1 irmovl $5, %edx 
2 irmovl $0x100,%esp 
3 rmmovl %edx,0(%esp) 
4 popl %esp 
5 nop 
6 nop 

7 rrmovl %esp,%eax 
The two nop instructions will cause the popl instruction to be in the write-back stage when the rrmovl instruction is in the decode stage. If the two forwarding sources in the write-back stage are given the wrong priority, then register %eax will be set to the incremented program counter rather than the value read from memory. 
Solution to Problem 4.32 (page 429) 
This logic only needs to check the .ve forwarding sources: 
int d_valB = [ d_srcB == e_dstE : e_valE; # Forward valE from execute d_srcB == M_dstM : m_valM; # Forward valM from memory d_srcB == M_dstE : M_valE; # Forward valE from memory d_srcB == W_dstM : W_valM; # Forward valM from write back d_srcB == W_dstE : W_valE; # Forward valE from write back 
1 : d_rvalB; # Use value read from register file ]; 
Solution to Problem 4.33 (page 430) 
This change would not handle the case where a conditional move fails to satisfy the condition, and therefore sets the dstE value to RNONE. The resulting value could get forwarded to the next instruction, even though the conditional transfer does not occur. 
1 irmovl $0x123,%eax 2 irmovl $0x321,%edx 3 xorl %ecx,%ecx # CC = 100 4 cmovne %eax,%edx # Not transferred 5 addl %edx,%edx # Should be 0x642 6 halt 
This code initializes register %edx to 0x321. The conditional data transfer does not take place, and so the .nal addl instruction should double the value in %edx to 0x642. With the altered design, however, the conditional move source value 0x321 gets forwarded into ALU input valA, while input valB correctly gets operand value 0x123. These inputs get added to produce result 0x444. 
Solution to Problem 4.34 (page 431) 
This code completes the computation of the status code for this instruction. 
## Update the status 
int m_stat = [ dmem_error : SADR; 
1 : M_stat; ]; 
Solution to Problem 4.35 (page 439) 
The following test program is designed to set up control combination A (Fig-ure 4.67) and detect whether something goes wrong: This program is designed so that if something goes wrong (for example, if the ret instruction is actually executed), then the program will execute one of the extra irmovl instructions and then halt. Thus, an error in the pipeline would cause some register to be updated incorrectly. This code illustrates the care required to implement a test program. It must set up a potential error condition and then detect whether or not an error occurs. 

1  #  Code  to  generate  a  combination  of  not-taken  branch  and  ret  
2  irmovl  Stack,  %esp  
3  irmovl  rtnp,%eax  
4  pushl  %eax  #  Set  up  return  pointer  
5  xorl  %eax,%eax  #  Set  Z  condition  code  
6  jne  target  #  Not  taken  (First  part  of  combination)  
7  irmovl  $1,%eax  #  Should  execute  this  
8  halt  
9  target:  ret  #  Second  part  of  combination  
10  irmovl  $2,%ebx  #  Should  not  execute  this  
11  halt  
12  rtnp:  irmovl  $3,%edx  #  Should  not  execute  this  
13  halt  
14  .pos  0x40  
15  Stack:  

Solution to Problem 4.36 (page 440) 
The following test program is designed to set up control combination B (Fig-ure 4.67). The simulator will detect a case where the bubble and stall control signals for a pipeline register are both set to zero, and so our test program need only set up the combination for it to be detected. The biggest challenge is to make the program do something sensible when handled correctly. 
1 # Test instruction that modifies %esp followed by ret 2 irmovl mem,%ebx 3 mrmovl 0(%ebx),%esp # Sets %esp to point to return point 4 ret # Returns to return point 5 halt # 6 rtnpt: irmovl $5,%esi # Return point 7 halt 8 .pos 0x40 9 mem: .long stack # Holds desired stack pointer 
10 .pos 0x50 11 stack: .long rtnpt # Top of stack: Holds return point 
This program uses two initialized word in memory. The .rst word (mem) holds the address of the second (stack¡ªthe desired stack pointer). The second word holds the address of the desired return point for the ret instruction. The program loads the stack pointer into %esp and executes the ret instruction. 
Solution to Problem 4.37 (page 441) 
From Figure 4.66, we can see that pipeline register D must be stalled for a load/use hazard. 
bool D_stall = # Conditions for a load/use hazard E_icode in { IMRMOVL, IPOPL } && 
E_dstM in { d_srcA, d_srcB }; 
Solution to Problem 4.38 (page 442) 
From Figure 4.66, we can see that pipeline register E must be set to bubble for a load/use hazard or for a mispredicted branch: 
bool E_bubble = # Mispredicted branch (E_icode == IJXX && !e_Cnd) || # Conditions for a load/use hazard E_icode in { IMRMOVL, IPOPL } && 
E_dstM in { d_srcA, d_srcB}; 
Solution to Problem 4.39 (page 442) 
This control requires examining the code of the executing instruction and checking for exceptions further down the pipeline. 
## Should the condition codes be updated? 
bool set_cc = E_icode == IOPL && # State changes only during normal operation !m_stat in { SADR, SINS, SHLT } && !W_stat in { SADR, SINS, SHLT }; 
Solution to Problem 4.40 (page 442) 
Injecting a bubble into the memory stage on the next cycle involves checking for an exception in either the memory or the write-back stage during the current cycle. 
# Start injecting bubbles as soon as exception passes through memory stage bool M_bubble= m_stat in{ SADR, SINS, SHLT} || W_stat in{ SADR, SINS, SHLT }; 
For stalling the write-back stage, we check only the status of the instruction in this stage. If we also stalled when an excepting instruction was in the memory stage, then this instruction would not be able to enter the write-back stage. 
bool W_stall = W_stat in { SADR, SINS, SHLT }; 
Solution to Problem 4.41 (page 446) 
We would then have a misprediction frequency of 0.35, giving mp = 0.20 ¡Á 0.35 ¡Á 2 = 0.14, giving an overall CPI of 1.25. This seems like a fairly marginal gain, but it would be worthwhile if the cost of implementing the new branch prediction strategy were not too high. 

Solution to Problem 4.42 (page 446) 
This simpli.ed analysis, where we focus on the inner loop, is a useful way to estimate program performance. As long as the array is suf.ciently large, the time spent in other parts of the code will be negligible. 
A. The inner loop of the code using the conditional jump has 11 instructions, all of which are executed when the array element is zero or negative, and 10 of which are executed when the array element is positive. The average is 10.5. The inner loop of the code using the conditional move has 10 instructions, all of which are executed every time. 
B. The loop-closing jump will be predicted correctly, except when the loop terminates. For a very long array, this one misprediction will have negligible effect on the performance. The only other source of bubbles for the jump-based code is the conditional jump depending on whether or not the array element is positive. This will cause two bubbles, but it only occurs 50% of the time, so the average is 1.0. There are no bubbles in the conditional move code. 
C. Our conditional jump code requires an average of 10.5 + 1.0 = 11.5 cycles per array element (11 cycles in the best case and 12 cycles in the worst), while our conditional move code requires 10.0 cycles in all cases. 
Our pipeline has a branch misprediction penalty of only two cycles¡ªfar better than those for the deep pipelines of higher-performance processors. As a result, using conditional moves does not affect program performance very much. 
This page intentionally left blank 
CHAPTER 
5 

Optimizing 
Program 
Performance 

5.1 Capabilities and Limitations of Optimizing Compilers 476 
5.2 Expressing Program Performance 480 
5.3 Program Example 482 
5.4 Eliminating Loop Inef.ciencies 486 
5.5 Reducing Procedure Calls 490 
5.6 Eliminating Unneeded Memory References 491 
5.7 Understanding Modern Processors 496 
5.8 Loop Unrolling 509 
5.9 Enhancing Parallelism 513 
5.10 Summary of Results for Optimizing Combining Code 524 
5.11 Some Limiting Factors 525 
5.12 Understanding Memory Performance 531 
5.13 Life in the Real World: Performance Improvement Techniques 539 
5.14 Identifying and Eliminating Performance Bottlenecks 540 5.15 Summary 547 Bibliographic Notes 548 Homework Problems 549 Solutions to Practice Problems 552 
The biggest speedup you¡¯ll ever get with a program will be when you .rst get it working. 
¡ªJohn K. Ousterhout 
The primary objective in writing a program must be to make it work correctly under all possible conditions. A program that runs fast but gives incorrect results serves no useful purpose. Programmers must write clear and concise code, not only so that they can make sense of it, but also so that others can read and understand the code during code reviews and when modi.cations are required later. 
On the other hand, there are many occasions when making a program run fast is also an important consideration. If a program must process video frames or network packets in real time, then a slow-running program will not provide the needed functionality. When a computation task is so demanding that it requires days or weeks to execute, then making it run just 20% faster can have signi.cant impact. In this chapter, we will explore how to make programs run faster via several different types of program optimization. 
Writing an ef.cient program requires several types of activities. First, we must select an appropriate set of algorithms and data structures. Second, we must write source code that the compiler can effectively optimize to turn into ef.cient executable code. For this second part, it is important to understand the capabilities and limitations of optimizing compilers. Seemingly minor changes in how a program is written can make large differences in how well a compiler can optimize it. Some programming languages are more easily optimized than others. Some features of C, such as the ability to perform pointer arithmetic and casting, make it challenging for a compiler to optimize. Programmers can often write their programs in ways that make it easier for compilers to generate ef.cient code. A third technique for dealing with especially demanding computations is to divide a task into portions that can be computed in parallel, on some combination of multiple cores and multiple processors. We will defer this aspect of performance enhancement to Chapter 12. Even when exploiting parallelism, it is important that each parallel thread execute with maximum performance, and so the material of this chapter remains relevant in any case. 
In approaching program development and optimization, we must consider how the code will be used and what critical factors affect it. In general, program-mers must make a trade-off between how easy a program is to implement and maintain, and how fast it runs. At an algorithmic level, a simple insertion sort can be programmed in a matter of minutes, whereas a highly ef.cient sort routine may take a day or more to implement and optimize. At the coding level, many low-level optimizations tend to reduce code readability and modularity, making the programs more susceptible to bugs and more dif.cult to modify or extend. For code that will be executed repeatedly in a performance-critical environment, extensive optimization may be appropriate. One challenge is to maintain some degree of elegance and readability in the code despite extensive transformations. 
We describe a number of techniques for improving code performance. Ideally, a compiler would be able to take whatever code we write and generate the most ef.cient possible machine-level program having the speci.ed behavior. Modern compilers employ sophisticated forms of analysis and optimization, and they keep getting better. Even the best compilers, however, can be thwarted by optimization blockers¡ªaspects of the program¡¯s behavior that depend strongly on the execu-tion environment. Programmers must assist the compiler by writing code that can be optimized readily. 

The .rst step in optimizing a program is to eliminate unnecessary work, mak-ing the code perform its intended task as ef.ciently as possible. This includes eliminating unnecessary function calls, conditional tests, and memory references. These optimizations do not depend on any speci.c properties of the target ma-chine. 
To maximize the performance of a program, both the programmer and the compiler require a model of the target machine, specifying how instructions are processed and the timing characteristics of the different operations. For example, the compiler must know timing information to be able to decide whether it should use a multiply instruction or some combination of shifts and adds. Modern com-puters use sophisticated techniques to process a machine-level program, executing many instructions in parallel and possibly in a different order than they appear in the program. Programmers must understand how these processors work to be able to tune their programs for maximum speed. We present a high-level model of such a machine based on recent designs of Intel and AMD processors. We also devise a graphical data-.ow notation to visualize the execution of instructions by the processor, with which we can predict program performance. 
With this understanding of processor operation, we can take a second step in program optimization, exploiting the capability of processors to provide instruction-level parallelism, executing multiple instructions simultaneously. We cover several program transformations that reduce the data dependencies be-tween different parts of a computation, increasing the degree of parallelism with which they can be executed. 
We conclude the chapter by discussing issues related to optimizing large pro-grams. We describe the use of code pro.lers¡ªtools that measure the performance of different parts of a program. This analysis can help .nd inef.ciencies in the code and identify the parts of the program on which we should focus our optimization efforts. Finally, we present an important observation, known as Amdahl¡¯s law, which quanti.es the overall effect of optimizing some portion of a system. 
In this presentation, we make code optimization look like a simple linear process of applying a series of transformations to the code in a particular order. In fact, the task is not nearly so straightforward. A fair amount of trial-and-error experimentation is required. This is especially true as we approach the later optimization stages, where seemingly small changes can cause major changes in performance, while some very promising techniques prove ineffective. As we will see in the examples that follow, it can be dif.cult to explain exactly why a particular code sequence has a particular execution time. Performance can depend on many detailed features of the processor design for which we have relatively little documentation or understanding. This is another reason to try a number of different variations and combinations of techniques. 
Studying the assembly-code representation of a program is one of the most effective means for gaining an understanding of the compiler and how the gen-erated code will run. A good strategy is to start by looking carefully at the code for the inner loops, identifying performance-reducing attributes such as excessive memory references and poor use of registers. Starting with the assembly code, we can also predict what operations will be performed in parallel and how well they will use the processor resources. As we will see, we can often determine the time (or at least a lower bound on the time) required to execute a loop by identifying critical paths, chains of data dependencies that form during repeated executions of a loop. We can then go back and modify the source code to try to steer the compiler toward more ef.cient implementations. 
Most major compilers, including gcc, are continually being updated and im-proved, especially in terms of their optimization abilities. One useful strategy is to do only as much rewriting of a program as is required to get it to the point where the compiler can then generate ef.cient code. By this means, we avoid compro-mising the readability, modularity, and portability of the code as much as if we had to work with a compiler of only minimal capabilities. Again, it helps to iteratively modify the code and analyze its performance both through measurements and by examining the generated assembly code. 
To novice programmers, it might seem strange to keep modifying the source code in an attempt to coax the compiler into generating ef.cient code, but this is indeed how many high-performance programs are written. Compared to the alternative of writing code in assembly language, this indirect approach has the advantage that the resulting code will still run on other machines, although per-haps not with peak performance. 
5.1 
Capabilities 
and 
Limitations 
of 
Optimizing 
Compilers 

Modern compilers employ sophisticated algorithms to determine what values are computed in a program and how they are used. They can then exploit opportuni-ties to simplify expressions, to use a single computation in several different places, and to reduce the number of times a given computation must be performed. Most compilers, including gcc, provide users with some control over which optimiza-tions they apply. As discussed in Chapter 3, the simplest control is to specify the optimization level. For example, invoking gcc with the command-line .ag ¡®-O1¡¯ will cause it to apply a basic set of optimizations. As discussed in Web Aside asm:opt, invoking gcc with .ag ¡®-O2¡¯or¡®-O3¡¯ will cause it to apply more extensive optimizations. These can further improve program performance, but they may ex-pand the program size and they may make the program more dif.cult to debug using standard debugging tools. For our presentation, we will mostly consider code compiled with optimization level 1, even though optimization level 2 has become the accepted standard for most gcc users. We purposely limit the level of opti-mization to demonstrate how different ways of writing a function in C can affect the ef.ciency of the code generated by a compiler. We will .nd that we can write C code that, when compiled just with optimization level 1, vastly outperforms a more naive version compiled with the highest possible optimization levels. 

Compilers must be careful to apply only safe optimizations to a program, meaning that the resulting program will have the exact same behavior as would an unoptimized version for all possible cases the program may encounter, up to the limits of the guarantees provided by the C language standards. Constraining the compiler to perform only safe optimizations eliminates possible sources of undesired run-time behavior, but it also means that the programmer must make more of an effort to write programs in a way that the compiler can then transform into ef.cient machine-level code. To appreciate the challenges of deciding which program transformations are safe or not, consider the following two procedures: 
1 void twiddle1(int *xp, int *yp) 
2 { 

3 *xp += *yp; 
4 *xp += *yp; 
5 } 6 7 void twiddle2(int *xp, int *yp) 8 { 9 *xp += 2* *yp; 
10 } 

At .rst glance, both procedures seem to have identical behavior. They both add twice the value stored at the location designated by pointer yp to that desig-nated by pointer xp. On the other hand, function twiddle2 is more ef.cient. It requires only three memory references (read *xp, read *yp, write *xp), whereas twiddle1 requires six (two reads of *xp, two reads of *yp, and two writes of *xp). Hence, if a compiler is given procedure twiddle1 to compile, one might think it could generate more ef.cient code based on the computations performed by twiddle2. 
Consider, however, the case in which xp and yp are equal. Then function twiddle1 will perform the following computations: 
3 *xp += *xp; /* Double value at xp */ 
4 *xp += *xp; /* Double value at xp */ 
The result will be that the value at xp will be increased by a factor of 4. On the other hand, function twiddle2 will perform the following computation: 
9 *xp += 2* *xp; /* Triple value at xp */ 
The result will be that the value at xp will be increased by a factor of 3. The compiler knows nothing about how twiddle1 will be called, and so it must assume that arguments xp and yp can be equal. It therefore cannot generate code in the style of twiddle2 as an optimized version of twiddle1. 
The case where two pointers may designate the same memory location is known as memory aliasing. In performing only safe optimizations, the compiler must assume that different pointers may be aliased. As another example, for a program with pointer variables p and q, consider the following code sequence: 
x = 1000; y = 3000; *q=y; /*3000*/ *p=x; /*1000*/ t1 = *q; /* 1000 or 3000 */ 
The value computed for t1 depends on whether or not pointers p and q are aliased¡ªif not, it will equal 3000, but if so it will equal 1000. This leads to one of the major optimization blockers, aspects of programs that can severely limit the opportunities for a compiler to generate optimized code. If a compiler cannot determine whether or not two pointers may be aliased, it must assume that either case is possible, limiting the set of possible optimizations. 
Practice Problem 5.1 
The following problem illustrates the way memory aliasing can cause unexpected program behavior. Consider the following procedure to swap two values: 
1 /* Swap value x at xp with value y at yp */ 2 void swap(int *xp, int *yp) 3 { 4 *xp=*xp+*yp;/*x+y */ 5 *yp = *xp-*yp;/*x+y-y=x*/ 6 *xp = *xp-*yp;/*x+y-x=y*/ 
7 } 
If this procedure is called with xp equal to yp, what effect will it have? 
A second optimization blocker is due to function calls. As an example, con-sider the following two procedures: 
1  int  f();  
2  
3  int  func1()  {  
4  return  f()  + f()  + f()  + f();  
5  }  
6  
7  int  func2()  {  
8  return  4*f();  
9  }  

It might seem at .rst that both compute the same result, but with func2 calling f only once, whereas func1 calls it four times. It is tempting to generate code in the style of func2 when given func1 as the source. 

Consider, however, the following code for f: 
1 int counter = 0; 2 3 int f() { 4 return counter++; 
5 } 

This function has a side effect¡ªit modi.es some part of the global program state. Changing the number of times it gets called changes the program behavior. In particular, a call to func1 would return 0 + 1 + 2 + 3 = 6, whereas a call to func2 would return 4 . 0 = 0, assuming both started with global variable counter set to 0. 
Most compilers do not try to determine whether a function is free of side ef-fects and hence is a candidate for optimizations such as those attempted in func2. Instead, the compiler assumes the worst case and leaves function calls intact. 
Aside Optimizing function calls by inline substitution 
As described in Web Aside asm:opt, code involving function calls can be optimized by a process known as inline substitution (or simply ¡°inlining¡±), where the function call is replaced by the code for the body of the function. For example, we can expand the code for func1 by substituting four instantiations of function f: 
1 /* Result of inlining f in func1 */ 2 int func1in() { 3 int t = counter++; /* +0 */ 4 t += counter++; /* +1 */ 5 t += counter++; /* +2 */ 6 t += counter++; /* +3 */ 7 return t; 
8 } 

This transformation both reduces the overhead of the function calls and allows further optimization of the expanded code. For example, the compiler can consolidate the updates of global variable counter in func1in to generate an optimized version of the function: 
1 /* Optimization of inlined code */ 2 int func1opt() { 3 intt=4* counter+ 6; 4 counter=t+4; 5 return t; 
6 } 

This code faithfully reproduces the behavior of func1 for this particular de.nition of function f. 
Recent versions of gcc attempt this form of optimization, either when directed to with the command-line option ¡®-finline¡¯ or for optimization levels 2 or higher. Since we are considering optimization level 1 in our presentation, we will assume that the compiler does not perform inline substitution. 
Among compilers, gcc is considered adequate, but not exceptional, in terms of its optimization capabilities. It performs basic optimizations, but it does not per-form the radical transformations on programs that more ¡°aggressive¡± compilers do. As a consequence, programmers using gcc must put more effort into writing programs in a way that simpli.es the compiler¡¯s task of generating ef.cient code. 
5.2 
Expressing 
Program 
Performance 

We introduce the metric cycles per element, abbreviated ¡°CPE,¡± as a way to express program performance in a way that can guide us in improving the code. CPE measurements help us understand the loop performance of an iterative program at a detailed level. It is appropriate for programs that perform a repetitive computation, such as processing the pixels in an image or computing the elements in a matrix product. 
The sequencing of activities by a processor is controlled by a clock providing a regular signal of some frequency, usually expressed in gigahertz (GHz), billions of cycles per second. For example, when product literature characterizes a system as a ¡°4 GHz¡± processor, it means that the processor clock runs at 4.0 ¡Á 109 cycles per second. The time required for each clock cycle is given by the reciprocal of the clock frequency. These typically are expressed in nanoseconds (1 nanosecond is 10.9 seconds), or picoseconds (1 picosecond is 10.12 seconds). For example, the period of a 4 GHz clock can be expressed as either 0.25 nanoseconds or 250 picoseconds. From a programmer¡¯s perspective, it is more instructive to express measurements in clock cycles rather than nanoseconds or picoseconds. That way, the measurements express how many instructions are being executed rather than how fast the clock runs. 
Many procedures contain a loop that iterates over a set of elements. For 
example, functions psum1 and psum2 in Figure 5.1 both compute the pre.x sum 
of a vector of length n. For a vector a= a0,a1,...,a .1 , the pre.x sum p= 
n 
p0,p1,...,pn.1 is de.ned as 
p0 = a
0 
(5.1) 
pi = pi.1 + ai,1 ¡Ü i<n 
Function psum1 computes one element of the result vector per iteration. The second uses a technique known as loop unrolling to compute two elements per iteration. We will explore the bene.ts of loop unrolling later in this chapter. See Problems 5.11, 5.12, and 5.21 for more about analyzing and optimizing the pre.x-sum computation. 
The time required by such a procedure can be characterized as a constant plus a factor proportional to the number of elements processed. For example, Figure 5.2 shows a plot of the number of clock cycles required by the two functions for a range of values of n. Using a least squares .t, we .nd that the run times (in clock cycles) for psum1 and psum2 can be approximated by the equations 496 + 10.0n and 500 + 6.5n, respectively. These equations indicate an overhead of 496 to 500 

1 /* Compute prefix sum of vector a */ 2 void psum1(float a[], float p[], long int n) 3 { 4 long int i; 5 p[0] = a[0]; 6 for (i=1;i<n; i++) 7 p[i] = p[i-1] + a[i]; 
8 } 

9 10 void psum2(float a[], float p[], long int n) 11 { 12 long int i; 13 p[0] = a[0]; 14 for (i = 1; i < n-1; i+=2) { 15 float mid_val = p[i-1] + a[i]; 16 p[i] = mid_val; 17 p[i+1] = mid_val + a[i+1]; 
18 } 19 /* For odd n, finish remaining element */ 20 if (i < n) 21 p[i] = p[i-1] + a[i]; 
22 } 

Figure 5.1 Pre.x-sum functions. These provide examples for how we express program performance. 
Cycles 
3000 
2500 
2000 
1500 
1000 
500 
0 



 
psum1  
 
Slope = 10.0  
 

 psum2  

 Slope = 6.5  

 


Figure 5.2 Performance of pre.x-sum functions. The slope of the lines indicates the number of clock cycles per element (CPE). 


0 50 100 150 200 
Elements 

cycles due to the timing code and to initiate the procedure, set up the loop, and complete the procedure, plus a linear factor of 6.5 or 10.0 cycles per element. For large values of n (say, greater than 200), the run times will be dominated by the linear factors. We refer to the coef.cients in these terms as the effective number of cycles per element, abbreviated ¡°CPE.¡± We prefer measuring the number of cycles per element rather than the number of cycles per iteration, because techniques such as loop unrolling allow us to use fewer iterations to complete the computation, but our ultimate concern is how fast the procedure will run for a given vector length. We focus our efforts on minimizing the CPE for our computations. By this measure, psum2, with a CPE of 6.50, is superior to psum1, with a CPE of 10.0. 

Aside What is a least squares .t? 
For a set of data points (x1,y1),...(x ,y ), we often try to draw a line that best approximates the X-Y 
nn 

trend represented by this data. With a least squares .t, we look for a line of the form y = mx + b that 
minimizes the following error measure: 
 
E(m, b) = (mxi + b . yi)2 i=1,n 

An algorithm for computing m and b can be derived by .nding the derivatives of E(m, b) with respect to m and b and setting them to 0. 
Practice Problem 5.2 
Later in this chapter, we will start with a single function and generate many differ-ent variants that preserve the function¡¯s behavior, but with different performance characteristics. For three of these variants, we found that the run times (in clock cycles) can be approximated by the following functions: 
Version 1: 60 + 35n Version 2: 136 + 4n Version 3: 157 + 1.25n 
For what values of n would each version be the fastest of the three? Remember that n will always be an integer. 
5.3 
Program 
Example 

To demonstrate how an abstract program can be systematically transformed into more ef.cient code, we will use a running example based on the vector data structure shown in Figure 5.3. A vector is represented with two blocks of memory: the header and the data array. The header is a structure declared as follows: 


Figure 5.3 Vector abstract data type. A vector is represented by header information plus array of designated length. 
code/opt/vec.h  
1  /*  Create  abstract  data  type  for  vector  */  
2  typedef  struct  {  
3  long  int  len;  
4  data_t  *data;  
5  }  vec_rec,  *vec_ptr;  
code/opt/vec.h  

The declaration uses data type data_t to designate the data type of the un-derlying elements. In our evaluation, we measure the performance of our code for integer (C int), single-precision .oating-point (C float), and double-precision .oating-point (C double) data. We do this by compiling and running the program separately for different type declarations, such as the following for data type int: 
typedef int data_t; 
We allocate the data array block to store the vector elements as an array of len objects of type data_t. 
Figure 5.4 shows some basic procedures for generating vectors, accessing vec-tor elements, and determining the length of a vector. An important feature to note is that get_vec_element, the vector access routine, performs bounds checking for every vector reference. This code is similar to the array representations used in many other languages, including Java. Bounds checking reduces the chances of program error, but it can also slow down program execution. 
As an optimization example, consider the code shown in Figure 5.5, which combines all of the elements in a vector into a single value according to some operation. By using different de.nitions of compile-time constants IDENT and OP, the code can be recompiled to perform different operations on the data. In particular, using the declarations 
#define IDENT 0 #define OP + 

it sums the elements of the vector. Using the declarations 
#define IDENT 1 #define OP * 

it computes the product of the vector elements. 
In our presentation, we will proceed through a series of transformations of the code, writing different versions of the combining function. To gauge progress, 
code/opt/vec.c 
1 /* Create vector of specified length */ 2 vec_ptr new_vec(long int len) 3 { 4 /* Allocate header structure */ 5 vec_ptr result = (vec_ptr) malloc(sizeof(vec_rec)); 6 if (!result) 7 return NULL; /* Couldn¡¯t allocate storage */ 8 result->len = len; 9 /* Allocate array */ 
10 if(len>0) { 
11 data_t *data = (data_t *)calloc(len, sizeof(data_t)); 
12 if (!data) { 
13 free((void *) result); 
14 return NULL; /* Couldn¡¯t allocate storage */ 
15 
} 16 result->data = data; 

17 
} 18 else 19 result->data = NULL; 20 return result; 

21 
} 22 23 /* 24 * Retrieve vector element and store at dest. 25 * Return 0 (out of bounds) or 1 (successful) 26 */ 27 int get_vec_element(vec_ptr v, long int index, data_t *dest) 28 { 29 if (index<0|| index >= v->len) 30 return 0; 31 *dest = v->data[index]; 32 return 1; 

33 
} 34 35 /* Return length of vector */ 36 long int vec_length(vec_ptr v) 37 { 38 return v->len; 


39 } 
code/opt/vec.c 
Figure 5.4 Implementation of vector abstract data type. In the actual program, data type data_t is declared to be int, float,or double. 

Section 5.3 Program Example 485 

1  /*  Implementation  with  maximum  use  of  data  abstraction  */  
2  void  combine1(vec_ptr  v,  data_t  *dest)  
3  {  
4  long  int  i;  
5  
6  *dest  =  IDENT;  
7  for  (i  =  0;  i  <  vec_l ength( v);  i++)  {  
8  data_t  val;  
9  get_vec_elemen t(v,  i,  &val);  
10  *dest  =  *dest  OP  val;  
11  }  
12  }  

Figure 5.5 Initial implementation of combining operation. Using different declara-tions of identity element IDENT and combining operation OP, we can measure the routine for different operations. 
we will measure the CPE performance of the functions on a machine with an Intel Core i7 processor, which we will refer to as our reference machine. Some characteristics of this processor were given in Section 3.1. These measurements characterize performance in terms of how the programs run on just one particular machine, and so there is no guarantee of comparable performance on other combinations of machine and compiler. However, we have compared the results with those for a number of different compiler/processor combinations and found them quite comparable. 
As we proceed through a set of transformations, we will .nd that many lead to only minimal performance gains, while others have more dramatic effects. Determining which combinations of transformations to apply is indeed part of the ¡°black art¡± of writing fast code. Some combinations that do not provide measurable bene.ts are indeed ineffective, while others are important as ways to enable further optimizations by the compiler. In our experience, the best approach involves a combination of experimentation and analysis: repeatedly attempting different approaches, performing measurements, and examining the assembly-code representations to identify underlying performance bottlenecks. 
As a starting point, the following are CPE measurements for combine1 run-ning on our reference machine, trying all combinations of data type and combining operation. For single-precision and double-precision .oating-point data, our ex-periments on this machine gave identical performance for addition, but differing performance for multiplication. We therefore report .ve CPE values: integer ad-dition and multiplication, .oating-point addition, single-precision multiplication (labeled ¡°F *¡±), and double-precision multiplication (labeled ¡°D *¡±). 
Integer Floating point 

Function Page Method +*+ F * D * 
combine1 485 Abstract unoptimized 29.02 29.21 27.40 27.90 27.36 combine1 485 Abstract -O1 12.00 12.00 12.00 12.01 13.00 
We can see that our measurements are somewhat imprecise. The more likely CPE number for integer sum and product is 29.00, rather than 29.02 or 29.21. Rather than ¡°fudging¡± our numbers to make them look good, we will present the measurements we actually obtained. There are many factors that complicate the task of reliably measuring the precise number of clock cycles required by some code sequence. It helps when examining these numbers to mentally round the results up or down by a few hundredths of a clock cycle. 
The unoptimized code provides a direct translation of the C code into machine code, often with obvious inef.ciencies. By simply giving the command-line option ¡®-O1¡¯, we enable a basic set of optimizations. As can be seen, this signi.cantly improves the program performance¡ªmore than a factor of two¡ªwith no effort on behalf of the programmer. In general, it is good to get into the habit of enabling at least this level of optimization. For the remainder of our measurements, we use optimization levels 1 and higher in generating and measuring our programs. 
5.4 
Eliminating 
Loop 
Inef.ciencies 

Observe that procedure combine1, as shown in Figure 5.5, calls function vec_ length as the test condition of the for loop. Recall from our discussion of how to translate code containing loops into machine-level programs (Section 3.6.5) that the test condition must be evaluated on every iteration of the loop. On the other hand, the length of the vector does not change as the loop proceeds. We could therefore compute the vector length only once and use this value in our test condition. 
Figure 5.6 shows a modi.ed version called combine2, which calls vec_length 
at the beginning and assigns the result to a local variable length. This transfor-
mation has noticeable effect on the overall performance for some data types and 
1 /* Move call to vec_length out of loop */ 2 void combine2(vec_ptr v, data_t *dest) 3 { 4 long int i; 5 long int length = vec_length(v); 6 7 *dest = IDENT; 8 for (i = 0; i < length; i++) { 9 data_t val; 
10 get_vec_element(v, i, &val); 11 *dest = *dest OP val; 
12 
} 

13 
} 


Figure 5.6 Improving the ef.ciency of the loop test. By moving the call to vec_ length out of the loop test, we eliminate the need to execute it on every iteration. 

operations, and minimal or even none for others. In any case, this transformation is required to eliminate inef.ciencies that would become bottlenecks as we attempt further optimizations. 
Integer Floating point Function Page Method + *+ F * D * 
combine1 485 Abstract -O1 12.00 12.00 12.00 12.01 13.00 combine2 486 Move vec_length 8.03 8.09 10.09 11.09 12.08 
This optimization is an instance of a general class of optimizations known as code motion. They involve identifying a computation that is performed multiple times (e.g., within a loop), but such that the result of the computation will not change. We can therefore move the computation to an earlier section of the code that does not get evaluated as often. In this case, we moved the call to vec_length from within the loop to just before the loop. 
Optimizing compilers attempt to perform code motion. Unfortunately, as dis-cussed previously, they are typically very cautious about making transformations that change where or how many times a procedure is called. They cannot reliably detect whether or not a function will have side effects, and so they assume that it might. For example, if vec_length had some side effect, then combine1 and combine2 could have different behaviors. To improve the code, the programmer must often help the compiler by explicitly performing code motion. 
As an extreme example of the loop inef.ciency seen in combine1, consider the procedure lower1 shown in Figure 5.7. This procedure is styled after routines sub-mitted by several students as part of a network programming project. Its purpose is to convert all of the uppercase letters in a string to lowercase. The procedure steps through the string, converting each uppercase character to lowercase. The case conversion involves shifting characters in the range ¡®A¡¯to¡®Z¡¯ to the range ¡®a¡¯ to ¡®z.¡¯ 
The library function strlen is called as part of the loop test of lower1. Al-though strlen is typically implemented with special x86 string-processing instruc-tions, its overall execution is similar to the simple version that is also shown in Figure 5.7. Since strings in C are null-terminated character sequences, strlen can only determine the length of a string by stepping through the sequence until it hits a null character. For a string of length n, strlen takes time proportional to n. Since strlen is called in each of the n iterations of lower1, the overall run time of lower1 is quadratic in the string length, proportional to n 2. 
This analysis is con.rmed by actual measurements of the functions for differ-ent length strings, as shown in Figure 5.8 (and using the library version of strlen). The graph of the run time for lower1 rises steeply as the string length increases (Figure 5.8(a)). Figure 5.8(b) shows the run times for seven different lengths (not the same as shown in the graph), each of which is a power of 2. Observe that for lower1 each doubling of the string length causes a quadrupling of the run time. This is a clear indicator of a quadratic run time. For a string of length 1,048,576, lower1 requires over 13 minutes of CPU time. 
1 /* Convert string to lowercase: slow */ 2 void lower1(char *s) 3 { 4 int i; 5 6 for (i = 0; i < strlen(s); i++) 7 if (s[i] >= ¡¯A¡¯ && s[i] <= ¡¯Z¡¯) 8 s[i] -= (¡¯A¡¯ -¡¯a¡¯); 
9 
} 10 11 /* Convert string to lowercase: faster */ 12 void lower2(char *s) 13 { 14 int i; 15 int len = strlen(s); 16 17 for (i= 0; i< len; i++) 18 if (s[i] >= ¡¯A¡¯ && s[i] <= ¡¯Z¡¯) 19 s[i] -= (¡¯A¡¯ -¡¯a¡¯); 

20 
} 21 22 /* Sample implementation of library function strlen */ 23 /* Compute length of string */ 24 size_t strlen(const char *s) 25 { 26 int length = 0; 27 while (*s != ¡¯\0¡¯) { 28 s++; 29 length++; 

30 
} 31 return length; 


32 } 
Figure 5.7 Lowercase conversion routines. The two procedures have radically different performance. 
Function lower2 shown in Figure 5.7 is identical to that of lower1, except that we have moved the call to strlen out of the loop. The performance im-proves dramatically. For a string length of 1,048,576, the function requires just 1.5 milliseconds¡ªover 500,000 times faster than lower1. Each doubling of the string length causes a doubling of the run time¡ªa clear indicator of linear run time. For longer strings, the run-time improvement will be even greater. 
In an ideal world, a compiler would recognize that each call to strlen in 
the loop test will return the same result, and thus the call could be moved out of 
the loop. This would require a very sophisticated analysis, since strlen checks 
200 180 160 140 120 100 80 60 40 20 0 

0 100,000 200,000 300,000 

String length 
(a) 
CPU seconds 

String length  
Function  16,384  32,768  65,536  131,072  262,144  524,288  1,048,576  
lower1  0.19  0.77  3.08  12.34  49.39  198.42  791.22  
lower2  0.0000  0.0000  0.0001  0.0002  0.0004  0.0008  0.0015  

(b) 

Figure 5.8 Comparative performance of lowercase conversion routines. The original code lower1 has a quadratic run time due to an inef.cient loop structure. The modi.ed code lower2 has a linear run time. 
the elements of the string and these values are changing as lower1 proceeds. The compiler would need to detect that even though the characters within the string are changing, none are being set from nonzero to zero, or vice versa. Such an analysis is well beyond the ability of even the most sophisticated compilers, even if they employ inlining, and so programmers must do such transformations themselves. 
This example illustrates a common problem in writing programs, in which a seemingly trivial piece of code has a hidden asymptotic inef.ciency. One would not expect a lowercase conversion routine to be a limiting factor in a program¡¯s performance. Typically, programs are tested and analyzed on small data sets, for which the performance of lower1 is adequate. When the program is ultimately deployed, however, it is entirely possible that the procedure could be applied to strings of over one million characters. All of a sudden this benign piece of code has become a major performance bottleneck. By contrast, the performance of lower2 will be adequate for strings of arbitrary length. Stories abound of major programming projects in which problems of this sort occur. Part of the job of a competent programmer is to avoid ever introducing such asymptotic inef.ciency. 
Practice Problem 5.3 
Consider the following functions: 
int min(int x, int y) { returnx<y?x:y;} int max(int x, int y) { returnx<y?y:x;} void incr(int *xp, int v) { *xp += v; } int square(int x) { return x*x; } 
The following three code fragments call these functions: 
A. for (i = min(x, y); i < max(x, y); incr(&i, 1)) t += square(i); 
B. for (i = max(x, y) -1; i >= min(x, y); incr(&i, -1)) t += square(i); 
C. int low = min(x, y); int high = max(x, y); 
for (i = low; i < high; incr(&i, 1)) t += square(i); 
Assume x equals 10 and y equals 100. Fill in the following table indicating the number of times each of the four functions is called in code fragments A¨CC: 
Code min max incr square 
A. 
B. C. 
5.5 
Reducing 
Procedure 
Calls 

As we have seen, procedure calls can incur overhead and also block most forms of program optimization. We can see in the code for combine2 (Figure 5.6) that get_ vec_element is called on every loop iteration to retrieve the next vector element. This function checks the vector index i against the loop bounds with every vector reference, a clear source of inef.ciency. Bounds checking might be a useful feature when dealing with arbitrary array accesses, but a simple analysis of the code for combine2 shows that all references will be valid. 
Suppose instead that we add a function get_vec_start to our abstract data type. This function returns the starting address of the data array, as shown in Figure 5.9. We could then write the procedure shown as combine3 in this .gure, having no function calls in the inner loop. Rather than making a function call to retrieve each vector element, it accesses the array directly. A purist might say that this transformation seriously impairs the program modularity. In principle, the user of the vector abstract data type should not even need to know that the vector 
code/opt/vec.c 

1 data_t *get_vec_start(vec_ptr v) 2 { 3 return v->data; 
4 } 
code/opt/vec.c 

1 /* Direct access to vector data */ 2 void combine3(vec_ptr v, data_t *dest) 3 { 4 long int i; 5 long int length = vec_length(v); 6 data_t *data = get_vec_start(v); 7 8 *dest = IDENT; 9 for (i = 0; i < length; i++) { 
10 *dest = *dest OP data[i]; 
11 
} 

12 
} 



Figure 5.9 Eliminating function calls within the loop. The resulting code runs much faster, at some cost in program modularity. 
contents are stored as an array, rather than as some other data structure such as a linked list. A more pragmatic programmer would argue that this transformation is a necessary step toward achieving high-performance results. 
Integer Floating point 

Function Page Method +* + F * D * 
combine2 486 Move vec_length 8.03 8.09 10.09 11.09 12.08 combine3 491 Direct data access 6.01 8.01 10.01 11.01 12.02 
The resulting improvement is surprisingly modest, only improving the per-formance for integer sum. Again, however, this inef.ciency would become a bot-tleneck as we attempt further optimizations. We will return to this function later (Section 5.11.2) and see why the repeated bounds checking by combine2 does not make its performance much worse. For applications in which performance is a sig-ni.cant issue, one must often compromise modularity and abstraction for speed. It is wise to include documentation on the transformations applied, as well as the assumptions that led to them, in case the code needs to be modi.ed later. 
5.6 
Eliminating 
Unneeded 
Memory 
References 

The code for combine3 accumulates the value being computed by the combining operation at the location designated by the pointer dest. This attribute can be seen by examining the assembly code generated for the compiled loop. We show here the x86-64 code generated for data type float and with multiplication as the combining operation: 
combine3:  data_t  =  float,  OP  =  *  
iin  %rdx,  data  in  %rax,  dest  in  %rbp  
1  .L498:  loop:  
2  movss  (%rbp),  %xmm0  Read  product  from  dest  
3  mulss  (%rax,%rdx,4),  %xmm0  Multiply product by data[i]  
4  movss  %xmm0,  (%rbp)  Store  product  at  dest  
5  addq  $1,  %rdx  Increment  i  
6  cmpq  %rdx,  %r12  Compare  i:limit  
7  jg  .L498  If  >,  goto  loop  

Aside Understanding x86-64 .oating-point code 
We cover .oating-point code for x86-64, the 64-bit version of the Intel instruction set in Web Aside asm:sse, but the program examples we show in this chapter can readily be understood by anyone familiar with IA32 code. Here, we brie.y review the relevant aspects of x86-64 and its .oating-point instructions. 
The x86-64 instruction set extends the 32-bit registers of IA32, such as %eax, %edi, and %esp,to 64-bit versions, with ¡®r¡¯ replacing ¡®e¡¯, e.g., %rax, %rdi, and %rsp. Eight more registers are available, named %r8¨C%r15, greatly improving the ability to hold temporary values in registers. Suf.x ¡®q¡¯ is used on integer instructions (e.g., addq, cmpq) to indicate 64-bit operations. 
Floating-point data are held in a set of XMM registers, named %xmm0¨C%xmm15. Each of these registers is 128 bits long, able to hold four single-precision (float) or two double-precision (double) .oating-point numbers. For our initial presentation, we will only make use of instructions that operate on single values held in SSE registers. 
The movss instruction copies one single-precision number. Like the various mov instructions of IA32, both the source and the destination can be memory locations or registers, but it uses XMM registers, rather than general-purpose registers. The mulss instruction multiplies single-precision num-bers, updating its second operand with the product. Again, the source and destination operands can be memory locations or XMM registers. 
We see in this loop code that the address corresponding to pointer dest is held in register %rbp (unlike in IA32, where %ebp has special use as a frame pointer, its 64-bit counterpart %rbp can be used to hold arbitrary data). On iteration i, the program reads the value at this location, multiplies it by data[i], and stores the result back at dest. This reading and writing is wasteful, since the value read from dest at the beginning of each iteration should simply be the value written at the end of the previous iteration. 
We can eliminate this needless reading and writing of memory by rewriting the code in the style of combine4 in Figure 5.10. We introduce a temporary variable acc that is used in the loop to accumulate the computed value. The result is stored at dest only after the loop has been completed. As the assembly code that follows shows, the compiler can now use register %xmm0 to hold the accumulated value. 

1 /* Accumulate result in local variable */ 
2 void combine4(vec_ptr v, data_t *dest) 
3 { 

4 long int i; 
5 long int length = vec_length(v); 
6 data_t *data = get_vec_start(v); 
7 data_t acc = IDENT; 
8 

9 for (i = 0; i < length; i++) { 
10 acc = acc OP data[i]; 
11 } 12 *dest = acc; 
13 } 

Figure 5.10 Accumulating result in temporary. Holding the accumulated value in local variable acc (short for ¡°accumulator¡±) eliminates the need to retrieve it from memory and write back the updated value on every loop iteration. 
Compared to the loop in combine3, we have reduced the memory operations per iteration from two reads and one write to just a single read. 
combine4: data_t = float, OP = * iin %rdx, data in %rax, limit in %rbp, acc in %xmm0 
1  .L488:  loop:  
2  mulss  (%rax,%rdx,4),  %xmm0  Multiply acc by data[i]  
3  addq  $1,  %rdx  Increment i  
4  cmpq  %rdx,  %rbp  Compare limit:i  
5  jg  .L488  If >, goto loop  

We see a signi.cant improvement in program performance, as shown in the following table: 
Integer Floating point Function Page Method +* + F * D * 
combine3 491 Direct data access 6.01 8.01 10.01 11.01 12.02 combine4 493 Accumulate in temporary 2.00 3.00 3.00 4.00 5.00 
All of our times improve by at least a factor of 2.4¡Á, with the integer addition case dropping to just two clock cycles per element. 
Aside Expressing relative performance 
The best way to express a performance improvement is as a ratio of the form Told/Tnew, where Told is the time required for the original version and Tnew is the time required by the modi.ed version. This will be a number greater than 1.0 if any real improvement occurred. We use the suf.x ¡®¡Á¡¯ to indicate such a ratio, where the factor ¡°2.4¡Á¡± is expressed verbally as ¡°2.4 times.¡± 
The more traditional way of expressing relative change as a percentage works well when the change is small, but its de.nition is ambiguous. Should it be 100 . (Told . Tnew)/Tnew or possibly 100 . (Told . Tnew)/Told, or something else? In addition, it is less instructive for large changes. Saying that ¡°perfor-mance improved by 140%¡± is more dif.cult to comprehend than simply saying that the performance improved by a factor of 2.4. 
Again, one might think that a compiler should be able to automatically trans-form the combine3 code shown in Figure 5.9 to accumulate the value in a register, as it does with the code for combine4 shown in Figure 5.10. In fact, however, the two functions can have different behaviors due to memory aliasing. Consider, for example, the case of integer data with multiplication as the operation and 1 as the identity element. Let v = [2, 3, 5] be a vector of three elements and consider the following two function calls: 
combine3(v, get_vec_start(v) + 2); 
combine4(v, get_vec_start(v) + 2); 
That is, we create an alias between the last element of the vector and the destina-tion for storing the result. The two functions would then execute as follows: 
Function Initial Before loop i=0 i=1 i=2 Final 
combine3 [2, 3, 5] [2, 3, 1] [2, 3, 2] [2, 3, 6] [2, 3, 36] [2, 3, 36] combine4 [2, 3, 5] [2, 3, 5] [2, 3, 5] [2, 3, 5] [2, 3, 5] [2, 3, 30] 
As shown previously, combine3 accumulates its result at the destination, which in this case is the .nal vector element. This value is therefore set .rst to 1, then to 2 . 1 = 2, and then to 3 . 2 = 6. On the .nal iteration, this value is then multiplied by itself to yield a .nal value of 36. For the case of combine4, the vector remains unchanged until the end, when the .nal element is set to the computed result 1 . 2 . 3 . 5 = 30. 
Of course, our example showing the distinction between combine3 and combine4 is highly contrived. One could argue that the behavior of combine4 more closely matches the intention of the function description. Unfortunately, a compiler cannot make a judgment about the conditions under which a function might be used and what the programmer¡¯s intentions might be. Instead, when given combine3 to compile, the conservative approach is to keep reading and writing memory, even though this is less ef.cient. 
Practice Problem 5.4 
When we use gcc to compile combine3 with command-line option ¡®-O2¡¯, we get code with substantially better CPE performance than with -O1: 

Section 5.6  Eliminating Unneeded Memory References  495  
Integer  Floating point  
Function  Page  Method  +  *  +  F *  D *  
combine3 491 Compiled -O1 6.01 8.01 10.01 11.01 12.02 combine3 491 Compiled -O2 3.00 3.00 3.00 4.02 5.03 combine4 493 Accumulate in temporary 2.00 3.00 3.00 4.00 5.00 


We achieve performance comparable to that for combine4, except for the case of integer sum, but even it improves signi.cantly. On examining the assembly code generated by the compiler, we .nd an interesting variant for the inner loop: 
combine3: data_t = float, OP = *, compiled -O2 iin %rdx, data in %rax, limit in %rbp, dest at %rx12 Product in %xmm0 
1 .L560: loop: 2 mulss (%rax,%rdx,4), %xmm0 Multiply product by data[i] 3 addq $1, %rdx Increment i 4 cmpq %rdx, %rbp Compare limit:i 5 movss %xmm0, (%r12) Store product at dest 6 jg .L560 If >, goto loop 
We can compare this to the version created with optimization level 1: 
combine3: data_t = float, OP = *, compiled -O1 
iin %rdx, data in %rax, dest in %rbp 1 .L498: loop: 2 movss (%rbp), %xmm0 Read product from dest 3 mulss (%rax,%rdx,4), %xmm0 Multiply product by data[i] 4 movss %xmm0, (%rbp) Store product at dest 5 addq $1, %rdx Increment i 6 cmpq %rdx, %r12 Compare i:limit 7 jg .L498 If >, goto loop 
We see that, besides some reordering of instructions, the only difference is that the more optimized version does not contain the movss implementing the read from the location designated by dest (line 2). 
A. How does the role of register %xmm0 differ in these two loops? 
B. Will the more optimized version faithfully implement the C code of com-bine3, including when there is memory aliasing between dest and the vector data? 
C. Explain either why this optimization preserves the desired behavior, or give an example where it would produce different results than the less optimized code. 
With this .nal transformation, we reached a point where we require just 2¨C5 clock cycles for each element to be computed. This is a considerable improvement over the original 11¨C13 cycles when we .rst enabled optimization. We would now like to see just what factors are constraining the performance of our code and how we can improve things even further. 
5.7 
Understanding 
Modern 
Processors 

Up to this point, we have applied optimizations that did not rely on any features of the target machine. They simply reduced the overhead of procedure calls and eliminated some of the critical ¡°optimization blockers¡± that cause dif.culties for optimizing compilers. As we seek to push the performance further, we must consider optimizations that exploit the microarchitecture of the processor, that is, the underlying system design by which a processor executes instructions. Getting every last bit of performance requires a detailed analysis of the program as well as code generation tuned for the target processor. Nonetheless, we can apply some basic optimizations that will yield an overall performance improvement on a large class of processors. The detailed performance results we report here may not hold for other machines, but the general principles of operation and optimization apply to a wide variety of machines. 
To understand ways to improve performance, we require a basic understand-ing of the microarchitectures of modern processors. Due to the large number of transistors that can be integrated onto a single chip, modern microprocessors em-ploy complex hardware that attempts to maximize program performance. One result is that their actual operation is far different from the view that is perceived by looking at machine-level programs. At the code level, it appears as if instruc-tions are executed one at a time, where each instruction involves fetching values from registers or memory, performing an operation, and storing results back to a register or memory location. In the actual processor, a number of instructions are evaluated simultaneously, a phenomenon referred to as instruction-level paral-lelism. In some designs, there can be 100 or more instructions ¡°in .ight.¡± Elaborate mechanisms are employed to make sure the behavior of this parallel execution exactly captures the sequential semantic model required by the machine-level program. This is one of the remarkable feats of modern microprocessors: they employ complex and exotic microarchitectures, in which multiple instructions can be executed in parallel, while presenting an operational view of simple sequential instruction execution. 
Although the detailed design of a modern microprocessor is well beyond the scope of this book, having a general idea of the principles by which they operate suf.ces to understand how they achieve instruction-level parallelism. We will .nd that two different lower bounds characterize the maximum performance of a program. The latency bound is encountered when a series of operations must be performed in strict sequence, because the result of one operation is required before the next one can begin. This bound can limit program performance when the data dependencies in the code limit the ability of the processor to 
Figure 5.11 

Block diagram of a modern processor. The instruction control unit is responsible for reading instructions from memory and generating a sequence of primitive operations. The execution unit then performs the operations and indicates whether the branches were correctly predicted. 


exploit instruction-level parallelism. The throughput bound characterizes the raw computing capacity of the processor¡¯s functional units. This bound becomes the ultimate limit on program performance. 
5.7.1 Overall Operation 
Figure 5.11 shows a very simpli.ed view of a modern microprocessor. Our hy-pothetical processor design is based loosely on the structure of the Intel Core i7 processor design, which is often referred to by its project code name ¡°Nehalem¡± [99]. The Nehalem microarchitecture typi.es the high-end processors produced by a number of manufacturers since the late 1990s. It is described in the industry as being superscalar, which means it can perform multiple operations on every clock cycle, and out-of-order, meaning that the order in which instructions execute need not correspond to their ordering in the machine-level program. The overall design has two main parts: the instruction control unit (ICU), which is responsible for reading a sequence of instructions from memory and generating from these a set of primitive operations to perform on program data, and the execution unit (EU), which then executes these operations. Compared to the simple in-order pipeline we studied in Chapter 4, out-of-order processors require far greater and more complex hardware, but they are better at achieving higher degrees of instruction-level parallelism. 
The ICU reads the instructions from an instruction cache¡ªa special high-speed memory containing the most recently accessed instructions. In general, the ICU fetches well ahead of the currently executing instructions, so that it has enough time to decode these and send operations down to the EU. One problem, however, is that when a program hits a branch,1 there are two possible directions the program might go. The branch can be taken, with control passing to the branch target. Alternatively, the branch can be not taken, with control passing to the next instruction in the instruction sequence. Modern processors employ a technique known as branch prediction, in which they guess whether or not a branch will be taken and also predict the target address for the branch. Using a technique known as speculative execution, the processor begins fetching and decoding instructions at where it predicts the branch will go, and even begins executing these operations before it has been determined whether or not the branch prediction was correct. If it later determines that the branch was predicted incorrectly, it resets the state to that at the branch point and begins fetching and executing instructions in the other direction. The block labeled ¡°Fetch control¡± incorporates branch prediction to perform the task of determining which instructions to fetch. 
The instruction decoding logic takes the actual program instructions and con-verts them into a set of primitive operations (sometimes referred to as micro-operations). Each of these operations performs some simple computational task such as adding two numbers, reading data from memory, or writing data to mem-ory. For machines with complex instructions, such as x86 processors, an instruction can be decoded into a variable number of operations. The details of how instruc-tions are decoded into sequences of more primitive operations varies between machines, and this information is considered highly proprietary. Fortunately, we can optimize our programs without knowing the low-level details of a particular machine implementation. 
In a typical x86 implementation, an instruction that only operates on registers, such as 
addl %eax,%edx 
is converted into a single operation. On the other hand, an instruction involving one or more memory references, such as 
addl %eax,4(%edx) 
yields multiple operations, separating the memory references from the arithmetic operations. This particular instruction would be decoded as three operations: one to load a value from memory into the processor, one to add the loaded value to the 
1. We use the term ¡°branch¡± speci.cally to refer to conditional jump instructions. Other instructions that can transfer control to multiple destinations, such as procedure return and indirect jumps, provide similar challenges for the processor. 

value in register %eax, and one to store the result back to memory. This decoding splits instructions to allow a division of labor among a set of dedicated hardware units. These units can then execute the different parts of multiple instructions in parallel. 
The EU receives operations from the instruction fetch unit. Typically, it can receive a number of them on each clock cycle. These operations are dispatched to a set of functional units that perform the actual operations. These functional units are specialized to handle speci.c types of operations. Our .gure illustrates a typical set of functional units, based on those of the Intel Core i7. We can see that three functional units are dedicated to computation, while the remaining two are for reading (load) and writing (store) memory. Each computational unit can perform multiple different operations: all can perform at least basic integer operations, such as addition and bit-wise logical operations. Floating-point operations and integer multiplication require more complex hardware, and so these can only be handled by speci.c functional units. 
Reading and writing memory is implemented by the load and store units. The load unit handles operations that read data from the memory into the processor. This unit has an adder to perform address computations. Similarly, the store unit handles operations that write data from the processor to the memory. It also has an adder to perform address computations. As shown in the .gure, the load and store units access memory via a data cache, a high-speed memory containing the most recently accessed data values. 
With speculative execution, the operations are evaluated, but the .nal results are not stored in the program registers or data memory until the processor can be certain that these instructions should actually have been executed. Branch operations are sent to the EU, not to determine where the branch should go, but rather to determine whether or not they were predicted correctly. If the prediction was incorrect, the EU will discard the results that have been computed beyond the branch point. It will also signal the branch unit that the prediction was incorrect and indicate the correct branch destination. In this case, the branch unit begins fetching at the new location. As we saw in Section 3.6.6, such a misprediction incurs a signi.cant cost in performance. It takes a while before the new instructions can be fetched, decoded, and sent to the execution units. 
Within the ICU, the retirement unit keeps track of the ongoing processing and makes sure that it obeys the sequential semantics of the machine-level program. Our .gure shows a register .le containing the integer, .oating-point, and more recently SSE registers as part of the retirement unit, because this unit controls the updating of these registers. As an instruction is decoded, information about it is placed into a .rst-in, .rst-out queue. This information remains in the queue until one of two outcomes occurs. First, once the operations for the instruction have completed and any branch points leading to this instruction are con.rmed as having been correctly predicted, the instruction can be retired, with any updates to the program registers being made. If some branch point leading to this instruction was mispredicted, on the other hand, the instruction will be .ushed, discarding any results that may have been computed. By this means, mispredictions will not alter the program state. 
As we have described, any updates to the program registers occur only as instructions are being retired, and this takes place only after the processor can be certain that any branches leading to this instruction have been correctly predicted. To expedite the communication of results from one instruction to another, much of this information is exchanged among the execution units, shown in the .gure as ¡°Operation results.¡± As the arrows in the .gure show, the execution units can send results directly to each other. This is a more elaborate form of the data forwarding techniques we incorporated into our simple processor design in Section 4.5.7. 
The most common mechanism for controlling the communication of operands among the execution units is called register renaming. When an instruction that up-dates register r is decoded, a tag t is generated giving a unique identi.er to the re-sult of the operation. An entry (r, t) is added to a table maintaining the association between program register r and tag t for an operation that will update this register. When a subsequent instruction using register r as an operand is decoded, the oper-ation sent to the execution unit will contain t as the source for the operand value. When some execution unit completes the .rst operation, it generates a result (v, t) indicating that the operation with tag t produced value v. Any operation waiting for t as a source will then use v as the source value, a form of data forwarding. By this mechanism, values can be forwarded directly from one operation to another, rather than being written to and read from the register .le, enabling the second operation to begin as soon as the .rst has completed. The renaming table only contains entries for registers having pending write operations. When a decoded instruction requires a register r, and there is no tag associated with this register, the operand is retrieved directly from the register .le. With register renaming, an entire sequence of operations can be performed speculatively, even though the registers are updated only after the processor is certain of the branch outcomes. 

Aside The history of out-of-order processing 
Out-of-order processing was .rst implemented in the Control Data Corporation 6600 processor in 1964. Instructions were processed by ten different functional units, each of which could be operated independently. In its day, this machine, with a clock rate of 10 Mhz, was considered the premium machine for scienti.c computing. 
IBM .rst implemented out-of-order processing with the IBM 360/91 processor in 1966, but just to execute the .oating-point instructions. For around 25 years, out-of-order processing was considered an exotic technology, found only in machines striving for the highest possible performance, until IBM reintroduced it in the RS/6000 line of workstations in 1990. This design became the basis for the IBM/Motorola PowerPC line, with the model 601, introduced in 1993, becoming the .rst single-chip microprocessor to use out-of-order processing. Intel introduced out-of-order processing with its PentiumPro model in 1995, with an underlying microarchitecture similar to that of the Core i7. 
5.7.2 Functional Unit Performance 
Figure 5.12 documents the performance of some of the arithmetic operations for an Intel Core i7, determined by both measurements and by reference to Intel liter-

Integer Single-precision Double-precision Operation Latency Issue Latency Issue Latency Issue 
Addition 1 0.33 3 1 3 1 Multiplication 3 1 4 1 5 1 Division 11¨C21 5¨C13 10¨C15 6¨C11 10¨C23 6¨C19 
Figure 5.12 Latency and issue time characteristics of Intel Core i7 arithmetic operations. Latency indicates the total number of clock cycles required to perform the actual operations, while issue time indicates the minimum number of cycles between two operations. The times for division depend on the data values. 
ature [26]. These timings are typical for other processors as well. Each operation is characterized by its latency, meaning the total time required to perform the op-eration, and the issue time, meaning the minimum number of clock cycles between two successive operations of the same type. 
We see that the latencies increase as the word sizes increase (e.g., from single to double precision), for more complex data types (e.g., from integer to .oating point), and for more complex operations (e.g., from addition to multiplication). 
We see also that most forms of addition and multiplication operations have issue times of 1, meaning that on each clock cycle, the processor can start a new one of these operations. This short issue time is achieved through the use of pipelining. A pipelined function unit is implemented as a series of stages, each of which performs part of the operation. For example, a typical .oating-point adder contains three stages (and hence the three-cycle latency): one to process the exponent values, one to add the fractions, and one to round the result. The arithmetic operations can proceed through the stages in close succession rather than waiting for one operation to complete before the next begins. This capability can be exploited only if there are successive, logically independent operations to be performed. Functional units with issue times of 1 cycle are said to be fully pipelined: they can start a new operation every clock cycle. The issue time of 
0.33 given for integer addition is due to the fact that the hardware has three fully pipelined functional units capable of performing integer addition. The processor has the potential to perform three additions every clock cycle. We see also that the divider (used for integer and .oating-point division, as well as .oating-point square root) is not fully pipelined¡ªits issue time is just a few cycles less than its latency. What this means is that the divider must complete all but the last few steps of a division before it can begin a new one. We also see the latencies and issue times for division are given as ranges, because some combinations of dividend and divisor require more steps than others. The long latency and issue times of division make it a comparatively costly operation. 
A more common way of expressing issue time is to specify the maximum throughput of the unit, de.ned as the reciprocal of the issue time. A fully pipelined functional unit has a maximum throughput of one operation per clock cycle, while units with higher issue times have lower maximum throughput. 
Circuit designers can create functional units with wide ranges of performance characteristics. Creating a unit with short latency or with pipelining requires more hardware, especially for more complex functions such as multiplication and .oating-point operations. Since there is only a limited amount of space for these units on the microprocessor chip, CPU designers must carefully balance the num-ber of functional units and their individual performance to achieve optimal overall performance. They evaluate many different benchmark programs and dedicate the most resources to the most critical operations. As Figure 5.12 indicates, inte-ger multiplication and .oating-point multiplication and addition were considered important operations in design of the Core i7, even though a signi.cant amount of hardware is required to achieve the low latencies and high degree of pipelin-ing shown. On the other hand, division is relatively infrequent and dif.cult to implement with either short latency or full pipelining. 
Both the latencies and the issue times (or equivalently, the maximum through-put) of these arithmetic operations can affect the performance of our combining functions. We can express these effects in terms of two fundamental bounds on the CPE values: 
Integer Floating point Bound +*+ F * D * 
Latency 1.00 3.00 3.00 4.00 5.00 Throughput 1.00 1.00 1.00 1.00 1.00 
The latency bound gives a minimum value for the CPE for any function that must perform the combining operation in a strict sequence. The throughput bound gives a minimum bound for the CPE based on the maximum rate at which the functional units can produce results. For example, since there is only one multiplier, and it has an issue time of 1 clock cycle, the processor cannot possibly sustain a rate of more than one multiplication per clock cycle. We noted earlier that the processor has three functional units capable of performing integer addition, and so we listed the issue time for this operation as 0.33. Unfortunately, the need to read elements from memory creates an additional throughput bound for the CPE of 1.00 for the combining functions. We will demonstrate the effect of both of the latency and throughput bounds with different versions of the combining functions. 
5.7.3 An Abstract Model of Processor Operation 
As a tool for analyzing the performance of a machine-level program executing on a modern processor, we will use a data-.ow representation of programs, a graphical notation showing how the data dependencies between the different operations constrain the order in which they are executed. These constraints then lead to critical paths in the graph, putting a lower bound on the number of clock cycles required to execute a set of machine instructions. 

Before proceeding with the technical details, it is instructive to examine the CPE measurements obtained for function combine4, our fastest code up to this point: 
Integer Floating point Function Page Method +*+ F * D * 
combine4  493  Accumulate in temporary  2.00  3.00  3.00  4.00  5.00  
Latency bound  1.00  3.00  3.00  4.00  5.00  
Throughput bound  1.00  1.00  1.00  1.00  1.00  

We can see that these measurements match the latency bound for the processor, except for the case of integer addition. This is not a coincidence¡ªit indicates that the performance of these functions is dictated by the latency of the sum or product computation being performed. Computing the product or sum of n elements requires around L.n + K clock cycles, where L is the latency of the combining operation and K represents the overhead of calling the function and initiating and terminating the loop. The CPE is therefore equal to the latency bound L. 
From Machine-Level Code to Data-Flow Graphs 
Our data-.ow representation of programs is informal. We only want to use it as a way to visualize how the data dependencies in a program dictate its perfor-mance. We present the data-.ow notation by working with combine4 (Figure 5.10, page 493) as an example. We focus just on the computation performed by the loop, since this is the dominating factor in performance for large vectors. We consider the case of .oating-point data with multiplication as the combining operation, although other combinations of data type and operation have nearly identical structure. The compiled code for this loop consists of four instructions, with reg-isters %rdx holding loop index i, %rax holding array address data, %rcx holding loop bound limit, and %xmm0 holding accumulator value acc. 
combine4: data_t = float, OP = * 
iin %rdx, data in %rax, limit in %rbp, acc in %xmm0 1 .L488: loop: 2 mulss (%rax,%rdx,4), %xmm0 Multiply acc by data[i] 3 addq $1, %rdx Increment i 4 cmpq %rdx, %rbp Compare limit:i 5 jg .L488 If >, goto loop 
As Figure 5.13 indicates, with our hypothetical processor design, the four in-structions are expanded by the instruction decoder into a series of .ve operations, with the initial multiplication instruction being expanded into a load operation to read the source operand from memory, and a mul operation to perform the multiplication. 
Figure 5.13 Graphical representation of inner-loop code for combine4. Instructions mulss (%rax,%rdx,4), %xmm0are dynamically translated into one or two operations, each of which receives addq $1,%rdx values from other opera-tions or from registers and cmpq %rdx,%rbp produces values for other jg loopoperations and for regis-ters. We show the target of the .nal instruction as the label loop. It jumps to the .rst instruction shown.  


As a step toward generating a data-.ow graph representation of the program, the boxes and lines along the left-hand side of Figure 5.13 show how the registers are used and updated by the different operations, with the boxes along the top representing the register values at the beginning of the loop, and those along the bottom representing the values at the end. For example, register %rax is only used as a source value by the load operation in performing its address calculation, and so the register has the same value at the end of the loop as at the beginning. Similarly, register %rcx is only used by the cmp operation. Register %rdx, on the other hand, is both used and updated within the loop. Its initial value is used by the load and add operations; its new value is generated by the add operation, which is then used by the cmp operation. Register %xmm0 is also updated within the loop by the mul operation, which .rst uses the initial value as a source value. 
Some of the operations in Figure 5.13 produce values that do not correspond to registers. We show these as arcs between operations on the right-hand side. The load operation reads a value from memory and passes it directly to the mul operation. Since these two operations arise from decoding a single mulss instruction, there is no register associated with the intermediate value passing between them. The cmp operation updates the condition codes, and these are then tested by the jg operation. 
For a code segment forming a loop, we can classify the registers that are accessed into four categories: 
Read-only: These are used as source values, either as data or to compute memory addresses, but they are not modi.ed within the loop. The read-only registers for the loop combine4 are %rax and %rcx. 
Write-only: These are used as the destinations of data-movement operations. There are no such registers in this loop. Local: These are updated and used within the loop, but there is no dependency from one iteration to another. The condition code registers are examples 
Figure 5.14 

Abstracting combine4 operations as data-.ow graph. (a) We rearrange the operators of Figure 5.13 to more clearly show the data dependencies, and then (b) show only those operations that use values from one iteration to produce new values for the next. 




(a) (b) 

for this loop: they are updated by the cmp operation and used by the jl operation, but this dependency is contained within individual iterations. 
Loop: These are both used as source values and as destinations for the loop, with the value generated in one iteration being used in another. We can see that %rdx and %xmm0 are loop registers for combine4, corresponding to program values i and acc. 
As we will see, the chains of operations between loop registers determine the performance-limiting data dependencies. 
Figure 5.14 shows further re.nements of the graphical representation of Fig-ure 5.13, with a goal of showing only those operations and data dependencies that affect the program execution time. We see in Figure 5.14(a) that we rearranged the operators to show more clearly the .ow of data from the source registers at the top (both read-only and loop registers), and to the destination registers at the bottom (both write-only and loop registers). 
In Figure 5.14(a), we also color operators white if they are not part of some chain of dependencies between loop registers. For this example, the compare (cmp) and branch (jl) operations do not directly affect the .ow of data in the program. We assume that the Instruction Control Unit predicts that branch will be taken, and hence the program will continue looping. The purpose of the compare and branch operations is to test the branch condition and notify the ICU if it is not. We assume this checking can be done quickly enough that it does not slow down the processor. 
In Figure 5.14(b), we have eliminated the operators that were colored white on the left, and we have retained only the loop registers. What we have left is an abstract template showing the data dependencies that form among loop registers due to one iteration of the loop. We can see in this diagram that there are two data dependencies from one iteration to the next. Along one side, we see the dependencies between successive values of program value acc, stored in register %xmm0. The loop computes a new value for acc by multiplying the old value by 
Data-.ow representation of computation by n iterations by the inner loop of combine4. The sequence of multiplication operations forms a critical path that limits program performance. 

a data element, generated by the load operation. Along the other side, we see the dependencies between successive values of loop index i. On each iteration, the old value is used to compute the address for the load operation, and it is also incremented by the add operation to compute the new value. 
Figure 5.15 shows the data-.ow representation of n iterations by the inner loop of function combine4. We can see that this graph was obtained by simply replicating the template shown on the right-hand side of Figure 5.14 n times. We can see that the program has two chains of data dependencies, corresponding to the updating of program values acc and i with operations mul and add, respec-tively. Given that single-precision multiplication has a latency of 4 cycles, while integer addition has latency 1, we can see that the chain on the left will form a critical path, requiring 4n cycles to execute. The chain on the left would require only n cycles to execute, and so it does not limit the program performance. 
Figure 5.15 demonstrates why we achieved a CPE equal to the latency bound of 4 cycles for combine4, when performing single-precision .oating-point multi-plication. When executing the function, the .oating-point multiplier becomes the limiting resource. The other operations required during the loop¡ªmanipulating and testing loop index i, computing the address of the next data elements, and reading data from memory¡ªproceed in parallel with the multiplier. As each suc-cessive value of acc is computed, it is fed back around to compute the next value, but this will not be completed until four cycles later. 

The .ow for other combinations of data type and operation are identical to those shown in Figure 5.15, but with a different data operation forming the chain of data dependencies shown on the left. For all of the cases where the operation has a latency Lgreater than 1, we see that the measured CPE is simply L, indicating that this chain forms the performance-limiting critical path. 
Other Performance Factors 
For the case of integer addition, on the other hand, our measurements of combine4 show a CPE of 2.00, slower than the CPE of 1.00 we would predict based on the chains of dependencies formed along either the left-or the right-hand side of the graph of Figure 5.15. This illustrates the principle that the critical paths in a data-.ow representation provide only a lower bound on how many cycles a program will require. Other factors can also limit performance, including the total number of functional units available and the number of data values that can be passed among the functional units on any given step. For the case of integer addition as the combining operation, the data operation is suf.ciently fast that the rest of the operations cannot supply data fast enough. Determining exactly why the program requires 2.00 cycles per element would require a much more detailed knowledge of the hardware design than is publicly available. 
To summarize our performance analysis of combine4: our abstract data-.ow representation of program operation showed that combine4 has a critical path of length L.ncaused by the successive updating of program value acc, and this path limits the CPE to at least L. This is indeed the CPE we measure for all cases except integer addition, which has a measured CPE of 2.00 rather than the CPE of 1.00 we would expect from the critical path length. 
It may seem that the latency bound forms a fundamental limit on how fast our combining operations can be performed. Our next task will be to restructure the operations to enhance instruction-level parallelism. We want to transform the program in such a way that our only limitation becomes the throughput bound, yielding CPEs close to 1.00. 
Practice Problem 5.5 
Suppose we wish to write a function to evaluate a polynomial, where a polynomial of degree nis de.ned to have a set of coef.cients a0,a1,a2,...,a . For a value x,
n 

we evaluate the polynomial by computing 
n 
a0 + a1x+ a2x 2 + ...+ anx (5.2) 

This evaluation can be implemented by the following function, having as argu-ments an array of coef.cients a, a value x, and the polynomial degree, degree 
(the value n in Equation 5.2). In this function, we compute both the successive terms of the equation and the successive powers of x within a single loop: 
1 double poly(double a[], double x, int degree) 
2 { 
3 long int i; 
4 double result = a[0]; 
5 double xpwr = x; /* Equals x^i at start of loop */ 
6 for (i = 1; i <= degree; i++) { 
7 result += a[i] * xpwr; 
8 xpwr=x* xpwr; 
9 } 10 return result; 11 
12 } 
A. For degree n, how many additions and how many multiplications does this code perform? 
B. On our reference machine, with arithmetic operations having the latencies shown in Figure 5.12, we measure the CPE for this function to be 5.00. Ex-plain how this CPE arises based on the data dependencies formed between iterations due to the operations implementing lines 7¨C8 of the function. 
Practice Problem 5.6 
Let us continue exploring ways to evaluate polynomials, as described in Prob-lem 5.5. We can reduce the number of multiplications in evaluating a polyno-mial by applying Horner¡¯s method, named after British mathematician William 
G. Horner (1786¨C1837). The idea is to repeatedly factor out the powers of x to get the following evaluation: 
a0 + x(a1 + x(a2 + ... + x(an.1 + xa )...)) (5.3)
n 
Using Horner¡¯s method, we can implement polynomial evaluation using the fol-lowing code: 
1 /* Apply Horner¡¯s method */ 2 double polyh(double a[], double x, int degree) 3 { 4 long int i; 5 double result = a[degree]; 6 for (i = degree-1; i >= 0; i--) 7 result = a[i] + x*result; 8 return result; 
9 } 

A. For degree n, how many additions and how many multiplications does this code perform? 
B. On our reference machine, with the arithmetic operations having the laten-cies shown in Figure 5.12, we measure the CPE for this function to be 8.00. Explain how this CPE arises based on the data dependencies formed be-tween iterations due to the operations implementing line 7 of the function. 
C. Explain how the function shown in Problem 5.5 can run faster, even though it requires more operations. 
5.8 
Loop 
Unrolling 

Loop unrolling is a program transformation that reduces the number of iterations for a loop by increasing the number of elements computed on each iteration. We saw an example of this with the function psum2 (Figure 5.1), where each iteration computes two elements of the pre.x sum, thereby halving the total number of iterations required. Loop unrolling can improve performance in two ways. First, it reduces the number of operations that do not contribute directly to the program result, such as loop indexing and conditional branching. Second, it exposes ways in which we can further transform the code to reduce the number of operations in the critical paths of the overall computation. In this section, we will examine simple loop unrolling, without any further transformations. 
Figure 5.16 shows a version of our combining code using two-way loop un-rolling. The .rst loop steps through the array two elements at a time. That is, the loop index i is incremented by 2 on each iteration, and the combining operation is applied to array elements iand i+ 1 in a single iteration. 
In general, the vector length will not be a multiple of 2. We want our code to work correctly for arbitrary vector lengths. We account for this requirement in two ways. First, we make sure the .rst loop does not overrun the array bounds. For a vector of length n, we set the loop limit to be n. 1. We are then assured that the loop will only be executed when the loop index isatis.es i<n. 1, and hence the maximum array index i+ 1 will satisfy i+ 1 <(n. 1)+ 1 = n. 
We can generalize this idea to unroll a loop by any factor k.Todoso, we set the upper limit to be n. k+ 1, and within the loop apply the combining operation to elements ithrough i+ k. 1. Loop index i is incremented by kin each iteration. The maximum array index i+ k. 1 will then be less than n. We include the second loop to step through the .nal few elements of the vector one at a time. The body of this loop will be executed between 0 and k. 1 times. For k= 2, we could use a simple conditional statement to optionally add a .nal iteration, as we did with the function psum2 (Figure 5.1). For k>2, the .nishing cases are better expressed with a loop, and so we adopt this programming convention for k= 2 as well. 
1 /* Unroll loop by 2 */ 2 void combine5(vec_ptr v, data_t *dest) 3 { 4 long int i; 5 long int length = vec_length(v); 6 long int limit = length-1; 7 data_t *data = get_vec_start(v); 8 data_t acc = IDENT; 9 
10 /* Combine 2 elements at a time */ 
11 for (i = 0; i < limit; i+=2) { 
12 acc = (acc OP data[i]) OP data[i+1]; 
13 
} 14 15 /* Finish any remaining elements */ 16 for (; i < length; i++) { 17 acc = acc OP data[i]; 

18 
} 19 *dest = acc; 


20 } 
Figure 5.16 Unrolling loop by factor k = 2. Loop unrolling can reduce the effect of loop overhead. 
Practice Problem 5.7 
Modify the code for combine5 to unroll the loop by a factor k = 5. 
When we measure the performance of unrolled code for unrolling factors k = 2(combine5) and k = 3, we get the following results: 
Integer Floating point 
Function Page Method +*+ F * D * 
combine4 493 No unrolling 2.00 3.00 3.00 4.00 5.00 combine5 510 Unroll by ¡Á2 2.00 1.50 3.00 4.00 5.00 Unroll by ¡Á3 1.00 1.00 3.00 4.00 5.00 
Latency bound 1.00 3.00 3.00 4.00 5.00 Throughput bound 1.00 1.00 1.00 1.00 1.00 
We see that CPEs for both integer addition and multiplication improve, while those for the .oating-point operations do not. Figure 5.17 shows CPE measure-ments when unrolling the loop by up to a factor of 6. We see that the trends we 
Figure 5.17 

CPE performance for different degrees of loop unrolling. Only integer addition and multiplication improve by loop unrolling. 
CPE 
6.00 5.00 

double *
4.00 
float * 
float + int * 
3.00 
2.00 
int + 
1.00 0.00 123456 
Unrolling factor K 

observed for unrolling by 2 and 3 continue¡ªit does not help the .oating-point operations, while both integer addition and multiplication drop down to CPEs of 
1.00. Several phenomena contribute to these measured values of CPE. For the case of integer addition, we see that unrolling by a factor of 2 makes no difference, but unrolling by a factor of 3 drops the CPE to 1.00, achieving both the latency and the throughput bounds for this operation. This result can be attributed to the bene.ts of reducing loop overhead operations. By reducing the number of overhead op-erations relative to the number of additions required to compute the vector sum, we can reach the point where the one-cycle latency of integer addition becomes the performance-limiting factor. 
The improving CPE for integer multiplication is surprising. We see that for un-rolling factor k between 1 and 3, the CPE is 3.00/k. It turns out that the compiler is making an optimization based on a reassociation transformation, altering the order in which values are combined. We will cover this transformation in Section 5.9.2. The fact that gcc applies this transformation to integer multiplication but not to .oating-point addition or multiplication is due to the associativity properties of the different operations and data types, as will also be discussed later. 
To understand why the three .oating-point cases do not improve by loop unrolling, consider the graphical representation for the inner loop, shown in Figure 5.18 for the case of single-precision multiplication. We see here that the mulss instructions each get translated into two operations: one to load an array element from memory, and one to multiply this value by the accumulated value. We see here that register %xmm0 gets read and written twice in each execution of the loop. We can rearrange, simplify, and abstract this graph, following the process shown in Figure 5.19 to obtain the template shown in Figure 5.19(b). We then replicate this template n/2 times to show the computation for a vector of length n, obtaining the data-.ow representation shown in Figure 5.20. We see here that there is still a critical path of n mul operations in this graph¡ªthere are half as many iterations, but each iteration has two multiplication operations in sequence. Since the critical path was the limiting factor for the performance of the code without loop unrolling, it remains so with simple loop unrolling. 
Figure 5.18 

Graphical representation of inner-loop code for combine5. Each iteration has two mulss instructions, each of which is translated into a load and a mul operation. 

mulss (%rax,%rdx,4), %xmm0 
mulss 4(%rax,%rdx,4), %xmm0 
addq $2,%rdx cmpq %rdx,%rbp jg loop 
Figure 5.19 

Abstracting combine5 operations as data-.ow graph. We rearrange, sim-plify, and abstract the representation of Fig-ure 5.18 to show the data dependencies between successive iterations (a). We see that each iteration must perform two multipli-cations in sequence (b). 


(a) (b) 

Aside Getting the compiler to unroll loops 
Loop unrolling can easily be performed by a compiler. Many compilers do it routinely whenever the optimization level is set suf.ciently high. gcc will perform loop unrolling when invoked with command-line option ¡®-funroll-loops¡¯. 
Figure 5.20 

Data-.ow representation of combine5 operating on a vector of length 
n. Even though the loop has been unrolled by a factor of 2, there are still n mul operations along the critical path. 
Critical path 
data[0] 
data[1] 
data[2] 
data[3] 


data[n-2] 

data[n-1] 


5.9 
Enhancing 
Parallelism 

At this point, our functions have hit the bounds imposed by the latencies of the arithmetic units. As we have noted, however, the functional units performing addition and multiplication are all fully pipelined, meaning that they can start new operations every clock cycle. Our code cannot take advantage of this capability, even with loop unrolling, since we are accumulating the value as a single variable acc. We cannot compute a new value for acc until the preceding computation has completed. Even though the functional unit can start a new operation every clock cycle, it will only start one every Lcycles, where Lis the latency of the combining operation. We will now investigate ways to break this sequential dependency and get performance better than the latency bound. 
5.9.1 Multiple Accumulators 
For a combining operation that is associative and commutative, such as integer addition or multiplication, we can improve performance by splitting the set of combining operations into two or more parts and combining the results at the end. For example, let Pn denote the product of elements a0,a1,...,an.1: 
n.1 P = a
ni i=0 
Assuming nis even, we can also write this as P = PE ¡Á PO , where PE is the product of the elements with even indices, and PO is the product of the elements 
nnn n 
n 
with odd indices: n/2.1 PE = a
n 2i i=0 n/2.1 PO = a
n 2i+1 i=0 
Figure 5.21 shows code that uses this method. It uses both two-way loop unrolling, to combine more elements per iteration, and two-way parallelism, accumulating elements with even index in variable acc0 and elements with odd index in variable acc1. As before, we include a second loop to accumulate any remaining array elements for the case where the vector length is not a multiple of 2. We then apply the combining operation to acc0 and acc1 to compute the .nal result. 
Comparing loop unrolling alone to loop unrolling with two-way parallelism, we obtain the following performance: 
Integer Floating point Function Page Method +*+ F * D * 
combine4 493 Accumulate in temporary 2.00 3.00 3.00 4.00 5.00 combine5 510 Unroll by ¡Á2 2.00 1.50 3.00 4.00 5.00 combine6 515 Unroll ¡Á2, parallelism ¡Á2 1.50 1.50 1.50 2.00 2.50 
Latency bound 1.00 3.00 3.00 4.00 5.00 Throughput bound 1.00 1.00 1.00 1.00 1.00 
Figure 5.22 demonstrates the effect of applying this transformation to achieve k-way loop unrolling and k-way parallelism for values up to k= 6. We can see that 

1 /* Unroll loop by 2, 2-way parallelism */ 2 void combine6(vec_ptr v, data_t *dest) 3 { 4 long int i; 5 long int length = vec_length(v); 6 long int limit = length-1; 7 data_t *data = get_vec_start(v); 8 data_t acc0 = IDENT; 9 data_t acc1 = IDENT; 
10 11 /* Combine 2 elements at a time */ 12 for (i = 0; i < limit; i+=2) { 13 acc0 = acc0 OP data[i]; 14 acc1 = acc1 OP data[i+1]; 
15 
} 16 17 /* Finish any remaining elements */ 18 for (; i < length; i++) { 19 acc0 = acc0 OP data[i]; 

20 
} 21 *dest = acc0 OP acc1; 


22 } 

Figure 5.21 Unrolling loop by 2 and using two-way parallelism. This approach makes use of the pipelining capability of the functional units. 
the CPEs for all of our combining cases improve with increasing values of k.For integer multiplication, and for the .oating-point operations, we see a CPE value of L/k, where L is the latency of the operation, up to the throughput bound of 1.00. We also see integer addition reaching its throughput bound of 1.00 with k = 3. Of course, we also reached this bound for integer addition with standard unrolling. 
Figure 5.22 6.00 CPE performance for k-5.00way loop unrolling with k-way parallelism. All of 4.00 the CPEs improve with this transformation, up to the 3.00 limiting value of 1.00. 
2.00 
1.00 
0.00 
CPE 

123456 
Unrolling factor K 





mulss (%rax,%rdx,4), %xmm0 
mulss 4(%rax,%rdx,4), %xmm1 
addq $2,%rdx cmpq %rdx,%rbp jg loop 

(a) (b) 
To understand the performance of combine6, we start with the code and oper-ation sequence shown in Figure 5.23. We can derive a template showing the data dependencies between iterations through the process shown in Figure 5.24. As with combine5, the inner loop contains two mulss operations, but these instruc-tions translate into mul operations that read and write separate registers, with no data dependency between them (Figure 5.24(b)). We then replicate this tem-plate n/2 times (Figure 5.25), modeling the execution of the function on a vector 
Critical paths
Figure 5.25 

Data-.ow representation of combine6 operating on a vector of length n. 
We now have two critical paths, each containing n/2 operations. 


of length n. We see that we now have two critical paths, one corresponding to computing the product of even-numbered elements (program value acc0) and one for the odd-numbered elements (program value acc1). Each of these criti-cal paths contain only n/2 operations, thus leading to a CPE of 4.00/2. A similar analysis explains our observed CPE of L/2 for operations with latency L for the different combinations of data type and combining operation. Operationally, we are exploiting the pipelining capabilities of the functional unit to increase their utilization by a factor of 2. When we apply this transformation for larger values of k, we .nd that we cannot reduce the CPE below 1.00. Once we reach this point, several of the functional units are operating at maximum capacity. 
We have seen in Chapter 2 that two¡¯s-complement arithmetic is commuta-tive and associative, even when over.ow occurs. Hence, for an integer data type, the result computed by combine6 will be identical to that computed by combine5 under all possible conditions. Thus, an optimizing compiler could potentially con-vert the code shown in combine4 .rst to a two-way unrolled variant of combine5 by loop unrolling, and then to that of combine6 by introducing parallelism. Many compilers do loop unrolling automatically, but relatively few then introduce this form of parallelism. 
On the other hand, .oating-point multiplication and addition are not as-sociative. Thus, combine5 and combine6 could produce different results due to rounding or over.ow. Imagine, for example, a product computation in which all of the elements with even indices were numbers with very large absolute value, while those with odd indices were very close to 0.0. In such a case, product PE
n 
might over.ow, or PO might under.ow, even though computing product P pro-
nn 
ceeds normally. In most real-life applications, however, such patterns are unlikely. Since most physical phenomena are continuous, numerical data tend to be reason-ably smooth and well-behaved. Even when there are discontinuities, they do not generally cause periodic patterns that lead to a condition such as that sketched ear-lier. It is unlikely that multiplying the elements in strict order gives fundamentally better accuracy than does multiplying two groups independently and then mul-tiplying those products together. For most applications, achieving a performance gain of 2¡Á outweighs the risk of generating different results for strange data pat-terns. Nevertheless, a program developer should check with potential users to see if there are particular conditions that may cause the revised algorithm to be un-acceptable. 
5.9.2 Reassociation Transformation 
We now explore another way to break the sequential dependencies and thereby improve performance beyond the latency bound. We saw that the simple loop un-rolling of combine5 did not change the set of operations performed in combining the vector elements to form their sum or product. By a very small change in the code, however, we can fundamentally change the way the combining is performed, and also greatly increase the program performance. 
Figure 5.26 shows a function combine7 that differs from the unrolled code of 
combine5 (Figure 5.16) only in the way the elements are combined in the inner 
loop. In combine5, the combining is performed by the statement 
12 acc = (acc OP data[i]) OP data[i+1]; 
while in combine7 it is performed by the statement 
12 acc = acc OP (data[i] OP data[i+1]); 
differing only in how two parentheses are placed. We call this a reassociation trans-formation, because the parentheses shift the order in which the vector elements are combined with the accumulated value acc. 
To an untrained eye, the two statements may seem essentially the same, but when we measure the CPE, we get surprising results: 

Section 5.9  Enhancing Parallelism  519  
Integer  Floating point  
Function  Page  Method  +  *  +  F *  D *  
combine4 493 Accumulate in temporary 2.00 3.00 3.00 4.00 5.00 combine5 510 Unroll by ¡Á2 2.00 1.50 3.00 4.00 5.00 combine6 515 Unroll by ¡Á2, parallelism ¡Á2 1.50 1.50 1.50 2.00 2.50 combine7 519 Unroll ¡Á2 and reassociate 2.00 1.51 1.50 2.00 2.97 


Latency bound 1.00 3.00 3.00 4.00 5.00 Throughput bound 1.00 1.00 1.00 1.00 1.00 
The integer multiplication case nearly matches the performance of the ver-sion with simple unrolling (combine5), while the .oating-point cases match the performance of the version with parallel accumulators (combine6), doubling the performance relative to simple unrolling. (The CPE of 2.97 shown for double-precision multiplication is most likely the result of a measurement error, with the true value being 2.50. In our experiments, we found the measured CPEs for combine7 to be more variable than for the other functions.) 
Figure 5.27 demonstrates the effect of applying the reassociation transforma-tion to achieve k-way loop unrolling with reassociation. We can see that the CPEs for all of our combining cases improve with increasing values of k. For integer 
1 /* Change associativity of combining operation */ 2 void combine7(vec_ptr v, data_t *dest) 3 { 4 long int i; 5 long int length = vec_length(v); 6 long int limit = length-1; 7 data_t *data = get_vec_start(v); 8 data_t acc = IDENT; 9 
10 /* Combine 2 elements at a time */ 11 for (i = 0; i < limit; i+=2) { 12 acc = acc OP (data[i] OP data[i+1]); 
13 
} 14 15 /* Finish any remaining elements */ 16 for (; i < length; i++) { 17 acc = acc OP data[i]; 

18 
} 19 *dest = acc; 


20 } 

Figure 5.26 Unrolling loop by 2 and then reassociating the combining operation. This approach also increases the number of operations that can be performed in parallel. 
Figure 5.27  6.00  
CPE performance for k-way loop unrolling with  5.00  
reassociation. All of the  4.00  

CPEs improve with this 
transformation, up to the 
CPE
3.00 

limiting value of 1.00. 
2.00 
1.00 0.00 

123456 
Unrolling factor K 
multiplication and for the .oating-point operations, we see a CPE value of nearly L/k, where L is the latency of the operation, up to the throughput bound of 1.00. We also see integer addition reaching CPE of 1.00 for k = 3, achieving both the throughput and the latency bounds. 
Figure 5.28 illustrates how the code for the inner loop of combine7 (for the case of single-precision product) gets decoded into operations and the resulting data dependencies. We see that the load operations resulting from the movss and the .rst mulss instructions load vector elements i and i + 1 from memory, and the .rst mul operation multiplies them together. The second mul operation then multiples this result by the accumulated value acc. Figure 5.29 shows how we rearrange, re.ne, and abstract the operations of Figure 5.28 to get a template rep-resenting the data dependencies for one iteration (Figure 5.29(b)). As with the templates for combine5 and combine7, we have two load and two mul operations, 

movss (%rax,%rdx,4), %xmm0 
mulss 4(%rax,%rdx,4), %xmm0 
mulss %xmm0, %xmm1 addq $2,%rdx cmpq %rdx,%rbp jg loop 




data[i ] 
data[i +1] 

(a) (b) 
but only one of the mul operations forms a data-dependency chain between loop registers. When we then replicate this template n/2 times to show the computa-tions performed in multiplying n vector elements (Figure 5.30), we see that we only have n/2 operations along the critical path. The .rst multiplication within each iteration can be performed without waiting for the accumulated value from the previous iteration. Thus, we reduce the minimum possible CPE by a factor of 2. As we increase k, we continue to have only one operation per iteration along the critical path. 
In performing the reassociation transformation, we once again change the order in which the vector elements will be combined together. For integer addition and multiplication, the fact that these operations are associative implies that this reordering will have no effect on the result. For the .oating-point cases, we must once again assess whether this reassociation is likely to signi.cantly affect the outcome. We would argue that the difference would be immaterial for most applications. 
We can now explain the surprising improvement we saw with simple loop unrolling (combine5) for the case of integer multiplication. In compiling this code, gcc performed the reassociation that we have shown in combine7, and hence it achieved the same performance. It also performed the transformation for code with higher degrees of unrolling. gcc recognizes that it can safely perform this transformation for integer operations, but it also recognizes that it cannot transform the .oating-point cases due to the lack of associativity. It would be gratifying to .nd that gcc performed this transformation recognizing that the resulting code would run faster, but unfortunately this seems not to be the case. In our experiments, we found that very minor changes to the C code caused gcc 
Figure 5.30 

Data-.ow representation of combine7 operating on a vector of length n. 
We have a single critical path, but it contains only n/2 operations. 

to associate the operations differently, sometimes causing the generated code to speed up, and sometimes to slow down, relative to what would be achieved by a straightforward compilation. Optimizing compilers must choose which factors they try to optimize, and it appears that gcc does not use maximizing instruction-level parallelism as one of its optimization criteria when selecting how to associate integer operations. 
In summary, a reassociation transformation can reduce the number of opera-tions along the critical path in a computation, resulting in better performance by better utilizing the pipelining capabilities of the functional units. Most compilers will not attempt any reassociations of .oating-point operations, since these oper-ations are not guaranteed to be associative. Current versions of gcc do perform reassociations of integer operations, but not always with good effects. In general, we have found that unrolling a loop and accumulating multiple values in parallel is a more reliable way to achieve improved program performance. 

Practice Problem 5.8 
Consider the following function for computing the product of an array of ninte-gers. We have unrolled the loop by a factor of 3. 
double aprod(double a[], int n) 
{ int i; double x, y, z; doubler=1; for (i= 0; i< n-2; i+=3) { 
x = a[i]; y = a[i+1]; z = a[i+2]; 
r=r*x*y*z; /* Product computation */ } for (;i<n; i++) 
r *= a[i]; return r; } 
For the line labeled Product computation, we can use parentheses to create .ve different associations of the computation, as follows: 
r=((r*x)*y)*z; /*A1 */ r=(r*(x*y))*z; /*A2 */ r=r*((x*y)*z); /*A3 */ r=r*(x*(y* z));/*A4 */ r=(r*x)*(y*z); /*A5 */ 
Assume we run these functions on a machine where double-precision multi-plication has a latency of 5 clock cycles. Determine the lower bound on the CPE set by the data dependencies of the multiplication. (Hint: It helps to draw a pictorial representation of how r is computed on every iteration.) 
Web Aside OPT:SIMD Achieving greater parallelism with SIMD instructions 
As described in Section 3.1, Intel introduced the SSE instructions in 1999, where SSE is the acronym for ¡°Streaming SIMD Extensions,¡± and, in turn, SIMD (pronounced ¡°sim-dee¡±) is the acronym for ¡°Single-Instruction, Multiple-Data.¡± The idea behind the SIMD execution model is that each 16-byte XMM register can hold multiple values. In our examples, we consider the cases where they can hold either four integer or single-precision values, or two double-precision values. SSE instructions can then perform vector operations on these registers, such as adding or multiplying four or two sets of values in parallel. For example, if XMM register %xmm0 contains four single-precision .oating-point numbers, which we denote a0,...,a3, and %rcx contains the memory address of a sequence of four single-precision .oating-point numbers, which we denote b0,...,b3, then the instruction 
mulps (%rcs), %xmm0 
will read the four values from memory and perform four multiplications in parallel, computing ai ¡û ai .bi, for 0 ¡Ü i ¡Ü 3. We see that a single instruction is able to generate a computation over multiple data values, hence the term ¡°SIMD.¡± 
gcc supports extensions to the C language that let programmers express a program in terms of vector operations that can be compiled into the SIMD instructions of SSE. This coding style is preferable to writing code directly in assembly language, since gcc can also generate code for the SIMD instructions found on other processors. 
Using a combination of gcc instructions, loop unrolling, and multiple accumulators, we are able to achieve the following performance for our combining functions: 
Integer Floating point Method +*+ F * D * 
SSE + 8-way unrolling 0.25 0.55 0.25 0.24 0.58 Throughput bound 0.25 0.50 0.25 0.25 0.50 
As this chart shows, using SSE instructions lowers the throughput bound, and we have nearly achieved these bounds for all .ve cases. The throughput bound of 0.25 for integer addition and single-precision addition and multiplication is due to the fact that the SSE instruction can perform four of these in parallel, and it has an issue time of 1. The double-precision instructions can only perform two in parallel, giving a throughput bound of 0.50. The integer multiplication operation has a throughput bound of 0.50 for a different reason¡ªalthough it can perform four in parallel, it has an issue time of 2. In fact, this instruction is only available for SSE versions 4 and higher (requiring command-line .ag ¡®-msse4¡¯). 
5.10 
Summary 
of 
Results 
for 
Optimizing 
Combining 
Code 

Our efforts at maximizing the performance of a routine that adds or multiplies the elements of a vector have clearly paid off. The following summarizes the results we obtain with scalar code, not making use of the SIMD parallelism provided by SSE vector instructions: 
Integer Floating point 

Function Page Method +*+ F * D * 
combine1 485 Abstract -O1 12.00 12.00 12.00 12.01 13.00 combine6 515 Unroll by ¡Á2, parallelism ¡Á2 1.50 1.50 1.50 2.00 2.50 
Unroll by ¡Á5, parallelism ¡Á5 1.01 1.00 1.00 1.00 1.00 

Latency bound 1.00 3.00 3.00 4.00 5.00 Throughput bound 1.00 1.00 1.00 1.00 1.00 
By using multiple optimizations, we have been able to achieve a CPE close to 
1.00 for all combinations of data type and operation using ordinary C code, a per-formance improvement of over 10X compared to the original version combine1. 

As covered in Web Aside opt:simd, we can improve performance even further by making use of gcc¡¯s support for SIMD vector instructions: 
Integer Floating point Function Method +*+ F * D * 
SIMD code SIMD + 8-way unrolling 0.25 0.55 0.25 0.24 0.58 
Throughput bound 0.25 0.50 0.25 0.25 0.50 
The processor can sustain up to four combining operations per cycle for integer and single-precision data, and two per cycle for double-precision data. This represents a performance of over 6 giga.ops (billions of .oating-point operations per second) on a processor now commonly found in laptop and desktop machines. 
Compare this performance to that of the Cray 1S, a breakthrough supercom-puter introduced in 1976. This machine cost around $8 million and consumed 115 kilowatts of electricity to get its peak performance of 0.25 giga.ops, over 20 times slower than we measured here. 
Several factors limit our performance for this computation to a CPE of 1.00 when using scalar instructions, and a CPE of either 0.25 (32-bit data) or 0.50 (64-bit data) when using SIMD instructions. First, the processor can only read 16 bytes from the data cache on each cycle, and then only by reading into an XMM register. Second, the multiplier and adder units can only start a new operation every clock cycle (in the case of SIMD instructions, each of these ¡°operations¡± actually computes two or four sums or products). Thus, we have succeeded in producing the fastest possible versions of our combining function for this machine. 
5.11 
Some 
Limiting 
Factors 

We have seen that the critical path in a data-.ow graph representation of a program indicates a fundamental lower bound on the time required to execute a program. That is, if there is some chain of data dependencies in a program where the sum of all of the latencies along that chain equals T , then the program will require at least T cycles to execute. 
We have also seen that the throughput bounds of the functional units also impose a lower bound on the execution time for a program. That is, assume that a program requires a total of N computations of some operation, that the microprocessor has only m functional units capable of performing that operation, and that these units have an issue time of i. Then the program will require at least 
N . i/m cycles to execute. 
In this section, we will consider some other factors that limit the performance of programs on actual machines. 
5.11.1 Register Spilling 
The bene.ts of loop parallelism are limited by the ability to express the compu-tation in assembly code. In particular, the IA32 instruction set only has a small number of registers to hold the values being accumulated. If we have a degree of parallelism p that exceeds the number of available registers, then the compiler will resort to spilling, storing some of the temporary values on the stack. Once this happens, the performance can drop signi.cantly. As an illustration, compare the performance of our parallel accumulator code for integer sum on x86-64 vs. IA32: 
Degree of unrolling 
Machine1 2 3 4 5 6 
IA32 2.12 1.76 1.45 1.39 1.90 1.99 x86-64 2.00 1.50 1.00 1.00 1.01 1.00 
We see that for IA32, the lowest CPE is achieved when just k = 4 values are accumulated in parallel, and it gets worse for higher values of k. We also see that we cannot get down to the CPE of 1.00 achieved for x86-64. 
Examining the IA32 code for the case of k = 5 shows the effect of the small number of registers with IA32: 
IA32 code. Unroll X5, accumulate X5, data_t = int, OP = + 
iin %edx, data in %eax, limit at %ebp-20 
1 .L291: loop: 
2 imull (%eax,%edx,4), %ecx x0 = x0 * data[i] 
3 movl -16(%ebp), %ebx Get x1 
4 imull 4(%eax,%edx,4), %ebx x1 = x1 * data[i+1] 
5 movl %ebx, -16(%ebp) Store x1 
6 imull 8(%eax,%edx,4), %edi x2 = x2 * data[i+2] 
7 imull 12(%eax,%edx,4), %esi x3 = x3 * data[i+3] 
8 movl -28(%ebp), %ebx Get x4 
9 imull 16(%eax,%edx,4), %ebx x4 = x4 * daa[i+4] 10 movl %ebx, -28(%ebp) Store x4 11 addl $5, %edx i+= 5 12 cmpl %edx, -20(%ebp) Compare limit:i 13 jg .L291 If >, goto loop 
We see here that accumulator values acc1 and acc4 have been ¡°spilled¡± onto the stack, at offsets .16 and .28 relative to %ebp. In addition, the termination value limit is kept on the stack at offset .20. The loads and stores associated with reading these values from memory and then storing them back negates any value obtained by accumulating multiple values in parallel. 
We can now see the merit of adding eight additional registers in the extension of IA32 to x86-64. The x86-64 code is able to accumulate up to 12 values in parallel without spilling any registers. 
5.11.2 Branch Prediction and Misprediction Penalties 
We demonstrated via experiments in Section 3.6.6 that a conditional branch can incur a signi.cant misprediction penalty when the branch prediction logic does not correctly anticipate whether or not a branch will be taken. Now that we have learned something about how processors operate, we can understand where this penalty arises. 

Modern processors work well ahead of the currently executing instructions, reading new instructions from memory and decoding them to determine what operations to perform on what operands. This instruction pipelining works well as long as the instructions follow in a simple sequence. When a branch is encountered, the processor must guess which way the branch will go. For the case of a conditional jump, this means predicting whether or not the branch will be taken. For an instruction such as an indirect jump (as we saw in the code to jump to an address speci.ed by a jump table entry) or a procedure return, this means predicting the target address. In this discussion, we focus on conditional branches. 
In a processor that employs speculative execution, the processor begins exe-cuting the instructions at the predicted branch target. It does this in a way that avoids modifying any actual register or memory locations until the actual out-come has been determined. If the prediction is correct, the processor can then ¡°commit¡± the results of the speculatively executed instructions by storing them in registers or memory. If the prediction is incorrect, the processor must discard all of the speculatively executed results and restart the instruction fetch process at the correct location. The misprediction penalty is incurred in doing this, be-cause the instruction pipeline must be re.lled before useful results are gener-ated. 
We saw in Section 3.6.6 that recent versions of x86 processors have conditional move instructions and that gcc can generate code that uses these instructions when compiling conditional statements and expressions, rather than the more traditional realizations based on conditional transfers of control. The basic idea for translating into conditional moves is to compute the values along both branches of a conditional expression or statement, and then use conditional moves to select the desired value. We saw in Section 4.5.10 that conditional move instructions can be implemented as part of the pipelined processing of ordinary instructions. There is no need to guess whether or not the condition will hold, and hence no penalty for guessing incorrectly. 
How then can a C programmer make sure that branch misprediction penalties do not hamper a program¡¯s ef.ciency? Given the 44 clock-cycle misprediction penalty we saw for the Intel Core i7, the stakes are very high. There is no simple answer to this question, but the following general principles apply. 
Do Not Be Overly Concerned about Predictable Branches 
We have seen that the effect of a mispredicted branch can be very high, but that does not mean that all program branches will slow a program down. In fact, the branch prediction logic found in modern processors is very good at discerning regular patterns and long-term trends for the different branch instructions. For example, the loop-closing branches in our combining routines would typically be predicted as being taken, and hence would only incur a misprediction penalty on the last time around. 
As another example, consider the small performance gain we observed 
when shifting from combine2 to combine3, when we took the function get_vec_ 
element out of the inner loop of the function, as is reproduced below: 
Integer Floating point Function Page Method +* + F * D * 
combine2 486 Move vec_length 8.03 8.09 10.09 11.09 12.08 combine3 491 Direct data access 6.01 8.01 10.01 11.01 12.02 
The CPE hardly changed, even though this function uses two conditionals to check whether the vector index is within bounds. These checks always determine that the index is within bounds, and hence they are highly predictable. 
As a way to measure the performance impact of bounds checking, consider the following combining code, where we have modi.ed the inner loop of combine4 by replacing the access to the data element with the result of performing an inline substitution of the code for get_vec_element. We will call this new version combine4b. This code performs bounds checking and also references the vector elements through the vector data structure. 
1 /* Include bounds check in loop */ 2 void combine4b(vec_ptr v, data_t *dest) 3 { 4 long int i; 5 long int length = vec_length(v); 6 data_t acc = IDENT; 7 8 for (i = 0; i < length; i++) { 9 if(i>= 0&& i<v->len){ 
10 acc = acc OP v->data[i]; 
11 } 
12 } 13 *dest = acc; 
14 } 
We can then directly compare the CPE for the functions with and without bounds checking: 
Integer Floating point Function Page Method +*+ F * D * 
combine4 493 No bounds checking 1.00 3.00 3.00 4.00 5.00 combine4b 493 Bounds checking 4.00 4.00 4.00 4.00 5.00 
Although the performance of the version with bounds checking is not quite as good, it increases the CPE by at most 2 clock cycles. This is a fairly small difference, considering that the bounds checking code performs two conditional branches and it also requires a load operation to implement the expression v->len.The processor is able to predict the outcomes of these branches, and so none of this evaluation has much effect on the fetching and processing of the instructions that form the critical path in the program execution. 

Write Code Suitable for Implementation with Conditional Moves 
Branch prediction is only reliable for regular patterns. Many tests in a program are completely unpredictable, dependent on arbitrary features of the data, such as whether a number is negative or positive. For these, the branch prediction logic will do very poorly, possibly giving a prediction rate of 50%¡ªno better than random guessing. (In principle, branch predictors can have prediction rates less than 50%, but such cases are very rare.) For inherently unpredictable cases, program performance can be greatly enhanced if the compiler is able to generate code using conditional data transfers rather than conditional control transfers. This cannot be controlled directly by the C programmer, but some ways of expressing conditional behavior can be more directly translated into conditional moves than others. 
We have found that gcc is able to generate conditional moves for code written in a more ¡°functional¡± style, where we use conditional operations to compute values and then update the program state with these values, as opposed to a more ¡°imperative¡± style, where we use conditionals to selectively update program state. 
There are no strict rules for these two styles, and so we illustrate with an example. Suppose we are given two arrays of integers a and b, and at each position i, we want to set a[i] to the minimum of a[i] and b[i], and b[i] to the maximum. 
An imperative style of implementing this function is to check at each position i and swap the two elements if they are out of order: 
1 /* Rearrange two vectors so that for each i, b[i] >= a[i] */ 
2 void minmax1(int a[], int b[], int n) { 
3 int i; 

4 for (i=0;i<n; i++) { 
5 if (a[i] > b[i]) { 
6 int t = a[i]; 
7 a[i] = b[i]; 
8 b[i] = t; 
9 
} 

10 
} 

11 
} 



Our measurements for this function on random data show a CPE of around 
14.50 for random data, and 3.00¨C4.00 for predictable data, a clear sign of a high misprediction penalty. 
A functional style of implementing this function is to compute the minimum and maximum values at each position i and then assign these values to a[i] and b[i], respectively: 
1 /* Rearrange two vectors so that for each i, b[i] >= a[i] */ 2 void minmax2(int a[], int b[], int n) { 3 int i; 4 for (i=0;i<n; i++) { 5 int min = a[i] < b[i] ? a[i] : b[i]; 6 int max = a[i] < b[i] ? b[i] : a[i]; 7 a[i] = min; 8 b[i] = max; 
9 
} 

10 
} 


Our measurements for this function show a CPE of around 5.0 regardless of whether the data are arbitrary or predictable. (We also examined the generated assembly code to make sure that it indeed used conditional moves.) 
As discussed in Section 3.6.6, not all conditional behavior can be implemented with conditional data transfers, and so there are inevitably cases where program-mers cannot avoid writing code that will lead to conditional branches for which the processor will do poorly with its branch prediction. But, as we have shown, a little cleverness on the part of the programmer can sometimes make code more amenable to translation into conditional data transfers. This requires some amount of experimentation, writing different versions of the function and then examining the generated assembly code and measuring performance. 
Practice Problem 5.9 
The traditional implementation of the merge step of mergesort requires three loops: 
1 void merge(int src1[], int src2[], int dest[], int n) { 2 int i1= 0; 3 int i2= 0; 4 int id= 0; 5 while (i1<n&&i2<n){ 6 if (src1[i1] < src2[i2]) 7 dest[id++] = src1[i1++]; 8 else 9 dest[id++] = src2[i2++]; 
10 } 11 while (i1 < n) 12 dest[id++] = src1[i1++]; 13 while (i2 < n) 14 dest[id++] = src2[i2++]; 
15 } 

The branches caused by comparing variables i1 and i2 to n have good pre-diction performance¡ªthe only mispredictions occur when they .rst become false. The comparison between values src1[i1] and src2[i2] (line 6), on the other hand, is highly unpredictable for typical data. This comparison controls a condi-tional branch, yielding a CPE (where the number of elements is 2n) of around 17.50. 
Rewrite the code so that the effect of the conditional statement in the .rst loop (lines 6¨C9) can be implemented with a conditional move. 
5.12 
Understanding 
Memory 
Performance 

All of the code we have written thus far, and all the tests we have run, access relatively small amounts of memory. For example, the combining routines were measured over vectors of length less than 1000 elements, requiring no more than 8000 bytes of data. All modern processors contain one or more cache memories to provide fast access to such small amounts of memory. In this section, we will further investigate the performance of programs that involve load (reading from memory into registers) and store (writing from registers to memory) operations, considering only the cases where all data are held in cache. In Chapter 6, we go into much more detail about how caches work, their performance characteristics, and how to write code that makes best use of caches. 
As Figure 5.11 shows, modern processors have dedicated functional units to perform load and store operations, and these units have internal buffers to hold sets of outstanding requests for memory operations. For example, the Intel Core i7 load unit¡¯s buffer can hold up to 48 read requests, while the store unit¡¯s buffer can hold up to 32 write requests [99]. Each of these units can typically initiate one operation every clock cycle. 
5.12.1 Load Performance 
The performance of a program containing load operations depends on both the pipelining capability and the latency of the load unit. In our experiments with com-bining operations on a Core i7, we saw that the CPE never got below 1.00, except when using SIMD operations. One factor limiting the CPE for our examples is that they all require reading one value from memory for each element computed. Since the load unit can only initiate one load operation every clock cycle, the CPE cannot be less than 1.00. For applications where we must load k values for every element computed, we can never achieve a CPE lower than k (see, for example, Problem 5.17). 
In our examples so far, we have not seen any performance effects due to the latency of load operations. The addresses for our load operations depended only on the loop index i, and so the load operations did not form part of a performance-limiting critical path. 
To determine the latency of the load operation on a machine, we can set up a computation with a sequence of load operations, where the outcome of one 
1 typedef struct ELE { 2 struct ELE *next; 3 int data; 
4 } list_ele, *list_ptr; 5 6 int list_len(list_ptr ls) { 7 int len=0; 8 while (ls) { 9 len++; 
10 ls = ls->next; 
11 } 12 return len; 
13 } Figure 5.31 Linked list functions. These illustrate the latency of the load operation. 
determines the address for the next. As an example, consider the function list_ len in Figure 5.31, which computes the length of a linked list. In the loop of this function, each successive value of variable ls depends on the value read by the pointer reference ls->next. Our measurements show that function list_len has a CPE of 4.00, which we claim is a direct indication of the latency of the load operation. To see this, consider the assembly code for the loop. (We show the x86-64 version of the code. The IA32 code is very similar.) 
len in %eax,lsin %rdi 
1 .L11: loop: 
2 addl $1, %eax Increment len 
3 movq (%rdi), %rdi ls = ls->next 
4 testq %rdi, %rdi Test ls 
5 jne .L11 If nonnull, goto loop 
The movq instruction on line 3 forms the critical bottleneck in this loop. Each successive value of register %rdi depends on the result of a load operation having the value in %rdi as its address. Thus, the load operation for one iteration cannot begin until the one for the previous iteration has completed. The CPE of 4.00 for this function is determined by the latency of the load operation. 
5.12.2 Store Performance 
In all of our examples thus far, we analyzed only functions that reference mem-ory mostly with load operations, reading from a memory location into a register. Its counterpart, the store operation, writes a register value to memory. The per-formance of this operation, particularly in relation to its interactions with load operations, involves several subtle issues. 
As with the load operation, in most cases, the store operation can operate in a 
fully pipelined mode, beginning a new store on every cycle. For example, consider 
the functions shown in Figure 5.32 that set the elements of an array dest of length 

1 /* Set elements of array to 0 */ 
2 void clear_array(int *dest, int n) { 
3 int i; 

4 for (i=0;i<n; i++) 
5 dest[i] = 0; 
6 } 1 /* Set elements of array to 0, Unrolled X4 */ 2 void clear_array_4(int *dest, int n) { 3 int i; 4 int limit = n-3; 5 for(i=0; i<limit; i+=4) { 6 dest[i] = 0; 7 dest[i+1] = 0; 
8 dest[i+2] = 0; 9 dest[i+3] = 0; 
10 } 11 for (; i < limit; i++) 12 dest[i] = 0; 
13 } 

Figure 5.32 Functions to set array elements to 0. These illustrate the pipelining of the store operation. 
n to zero. Our measurements for the .rst version show a CPE of 2.00. By unrolling the loop four times, as shown in the code for clear_array_4, we achieve a CPE of 1.00. Thus, we have achieved the optimum of one new store operation per cycle. 
Unlike the other operations we have considered so far, the store operation does not affect any register values. Thus, by their very nature a series of store operations cannot create a data dependency. Only a load operation is affected by the result of a store operation, since only a load can read back the memory value that has been written by the store. The function write_read shown in Figure 5.33 illustrates the potential interactions between loads and stores. This .gure also shows two example executions of this function, when it is called for a two-element array a, with initial contents .10 and 17, and with argument cnt equal to 3. These executions illustrate some subtleties of the load and store operations. 
In Example A of Figure 5.33, argument src is a pointer to array element a[0], while dest is a pointer to array element a[1]. In this case, each load by the pointer reference *src will yield the value .10. Hence, after two iterations, the array elements will remain .xed at .10 and .9, respectively. The result of the read from src is not affected by the write to dest. Measuring this example over a larger number of iterations gives a CPE of 2.00. 
In Example B of Figure 5.33, both arguments src and dest are pointers to array element a[0]. In this case, each load by the pointer reference *src will yield the value stored by the previous execution of the pointer reference *dest.Asa consequence, a series of ascending values will be stored in this location. In general, 
1 /* Write to dest, read from src */ 2 void write_read(int *src, int *dest, int n) 3 { 4 int cnt=n; 5 int val=0; 6 7 while (cnt--) { 8 *dest = val; 9 val = (*src)+1; 
10 
} 

11 
} 


Example A: write_read(&a[0],&a[1],3) 
Initial 
3 
Iter. 1 
2 
Iter. 2 
1 
Iter. 3 
0
cnt a val 
10  17  

10  0  

10  9  

10  9  

0 
9 
9 
9 
Example B: write_read(&a[0],&a[0],3) 
Initial 
3 
Iter. 1 
2 
Iter. 2 
1 
Iter. 3 
0
cnt a val 
10  17  

0  17  

1  17  

2  17  

0 
1 
2 
3 
Figure 5.33 Code to write and read memory locations, along with illustrative executions. This function highlights the interactions between stores and loads when arguments src and dest are equal. 
if function write_read is called with arguments src and dest pointing to the same memory location, and with argument cnt having some value n>0, the net effect is to set the location to n. 1. This example illustrates a phenomenon we will call a write/read dependency¡ªthe outcome of a memory read depends on a recent memory write. Our performance measurements show that Example B has a CPE of 6.00. The write/read dependency causes a slowdown in the processing. 
To see how the processor can distinguish between these two cases and why one runs slower than the other, we must take a more detailed look at the load and store execution units, as shown in Figure 5.34. The store unit contains a store buffer containing the addresses and data of the store operations that have been issued to the store unit, but have not yet been completed, where completion involves updating the data cache. This buffer is provided so that a series of store operations can be executed without having to wait for each one to update the cache. When 
Figure 5.34 

Detail of load and store units. The store unit maintains a buffer of pending writes. The load unit must check its address with those in the store unit to detect a write/read dependency. 


a load operation occurs, it must check the entries in the store buffer for matching addresses. If it .nds a match (meaning that any of the bytes being written have the same address as any of the bytes being read), it retrieves the corresponding data entry as the result of the load operation. 
Figure 5.35 shows the assembly code for the inner loop of write_read, and a graphical representation of the operations generated by the instruction decoder. The instruction movl %eax,(%ecx) is translated into two operations: The s_addr instruction computes the address for the store operation, creates an entry in the store buffer, and sets the address .eld for that entry. The s_data operation sets the data .eld for the entry. As we will see, the fact that these two computations are performed independently can be important to program performance. 
In addition to the data dependencies between the operations caused by the writing and reading of registers, the arcs on the right of the operators denote a set of implicit dependencies for these operations. In particular, the address computation of the s_addr operation must clearly precede the s_data operation. In addition, the load operation generated by decoding the instruction movl (%ebx), 
Figure 5.35 

Graphical representation of inner-loop code for write_read. The .rst movl instruction is decoded into separate operations to compute the store address and to store the data to memory. 


Figure 5.36 

Abstracting the opera-tions for write_read. We .rst rearrange the opera-tors of Figure 5.35 (a) and then show only those oper-ations that use values from one iteration to produce new values for the next (b). 

(a) (b) 
%eax must check the addresses of any pending store operations, creating a data dependency between it and the s_addr operation. The .gure shows a dashed arc between the s_data and load operations. This dependency is conditional: if the two addresses match, the load operation must wait until the s_data has deposited its result into the store buffer, but if the two addresses differ, the two operations can proceed independently. 
Figure 5.36 illustrates more clearly the data dependencies between the oper-ations for the inner loop of write_read. In Figure 5.36(a), we have rearranged the operations to allow the dependencies to be seen more clearly. We have la-beled the three dependencies involving the load and store operations for special attention. The arc labeled (1) represents the requirement that the store address must be computed before the data can be stored. The arc labeled (2) represents the need for the load operation to compare its address with that for any pend-ing store operations. Finally, the dashed arc labeled (3) represents the conditional data dependency that arises when the load and store addresses match. 
Figure 5.36(b) illustrates what happens when we take away those operations that do not directly affect the .ow of data from one iteration to the next. The data-.ow graph shows just two chains of dependencies: the one on the left, with data values being stored, loaded, and incremented (only for the case of matching addresses), and the one on the right, decrementing variable cnt. 
We can now understand the performance characteristics of function write_ read. Figure 5.37 illustrates the data dependencies formed by multiple iterations of its inner loop. For the case of Example A of Figure 5.33, with differing source and destination addresses, the load and store operations can proceed independently, and hence the only critical path is formed by the decrementing of variable cnt. This would lead us to predict a CPE of just 1.00, rather than the measured CPE of 
2.00. We have found similar behavior for any function where data are both being stored and loaded within a loop. Apparently the effort to compare load addresses with those of the pending store operations forms an additional bottleneck. For 
Figure 5.37 

Data-.ow representation of function write_read. 
When the two addresses do not match, the only critical path is formed by the decrementing of cnt (Example A). When they do match, the chain of data being stored, loaded, and incremented forms the critical path (Example B). 
Example A Example B 
Critical path Critical path 


load 



the case of Example B, with matching source and destination addresses, the data dependency between the s_data and load instructions causes a critical path to form involving data being stored, loaded, and incremented. We found that these three operations in sequence require a total of 6 clock cycles. 
As these two examples show, the implementation of memory operations in-volves many subtleties. With operations on registers, the processor can determine which instructions will affect which others as they are being decoded into opera-tions. With memory operations, on the other hand, the processor cannot predict which will affect which others until the load and store addresses have been com-puted. Ef.cient handling of memory operations is critical to the performance of many programs. The memory subsystem makes use of many optimizations, such as the potential parallelism when operations can proceed independently. 
Practice Problem 5.10 
As another example of code with potential load-store interactions, consider the following function to copy the contents of one array to another: 
1 void copy_array(int *src, int *dest, int n) 2 { 3 int i; 4 for (i=0;i<n; i++) 5 dest[i] = src[i]; 
6 } 
Suppose a is an array of length 1000 initialized so that each element a[i] equals i. 
A. What would be the effect of the call copy_array(a+1,a,999)? 
B. What would be the effect of the call copy_array(a,a+1,999)? 
C. Our performance measurements indicate that the call of part A has a CPE of 2.00, while the call of part B has a CPE of 5.00. To what factor do you attribute this performance difference? 
D. What performance would you expect for the call copy_array(a,a,999)? 
Practice Problem 5.11 
We saw that our measurements of the pre.x-sum function psum1 (Figure 5.1) yield a CPE of 10.00 on a machine where the basic operation to be performed, .oating-point addition, has a latency of just 3 clock cycles. Let us try to understand why our function performs so poorly. 
The following is the assembly code for the inner loop of the function: 
psum1.  a  in  %rdi,pin  %rsi,iin  %rax,  cnt  in  %rdx  
1  .L5:  loop:  
2  movss  -4(%rsi,%rax,4),  %xmm0  Get  p[i-1]  
3  addss  (%rdi,%rax,4),  %xmm0  Add  a[i]  
4  movss  %xmm0,  (%rsi,%rax,4)  Store  at  p[i]  
5  addq  $1,  %rax  Increment  i  
6  cmpq  %rax,  %rdx  Compare  cnt:i  
7  jg  .L5  If  >,  goto  loop  

Perform an analysis similar to those shown for combine3 (Figure 5.14) and for write_read (Figure 5.36) to diagram the data dependencies created by this loop, and hence the critical path that forms as the computation proceeds. 
Explain why the CPE is so high. (You may not be able to justify the exact CPE, but you should be able to describe why it runs more slowly than one might expect.) 

Practice Problem 5.12 
Rewrite the code for psum1 (Figure 5.1) so that it does not need to repeatedly retrieve the value of p[i] from memory. You do not need to use loop unrolling. We measured the resulting code to have a CPE of 3.00, limited by the latency of .oating-point addition. 
5.13 
Life 
in 
the 
Real 
World: 
Performance 
Improvement 
Techniques 

Although we have only considered a limited set of applications, we can draw important lessons on how to write ef.cient code. We have described a number of basic strategies for optimizing program performance: 
1. 
High-level design. Choose appropriate algorithms and data structures for the problem at hand. Be especially vigilant to avoid algorithms or coding tech-niques that yield asymptotically poor performance. 

2. 
Basic coding principles. Avoid optimization blockers so that a compiler can 


generate ef.cient code. Eliminate excessive function calls. Move computations out of loops when possible. Consider selective compromises of program modularity to gain greater ef.ciency. Eliminate unnecessary memory references. Introduce temporary variables to hold intermediate results. Store a result in an array or global variable only when the .nal value has been computed. 
3. Low-level optimizations. 
Unroll loops to reduce overhead and to enable further optimizations. Find ways to increase instruction-level parallelism by techniques such as multiple accumulators and reassociation. Rewrite conditional operations in a functional style to enable compilation via conditional data transfers. 
A .nal word of advice to the reader is to be vigilant to avoid introducing errors as you rewrite programs in the interest of ef.ciency. It is very easy to make mistakes when introducing new variables, changing loop bounds, and making the code more complex overall. One useful technique is to use checking code to test each version of a function as it is being optimized, to ensure no bugs are introduced during this process. Checking code applies a series of tests to the new versions of a function and makes sure they yield the same results as the original. The set of test cases must become more extensive with highly optimized code, since there are more cases to consider. For example, checking code that uses loop unrolling requires testing for many different loop bounds to make sure it handles all of the different possible numbers of single-step iterations required at the end. 
5.14 
Identifying 
and 
Eliminating 
Performance 
Bottlenecks 

Up to this point, we have only considered optimizing small programs, where there is some clear place in the program that limits its performance and therefore should be the focus of our optimization efforts. When working with large programs, even knowing where to focus our optimization efforts can be dif.cult. In this section we describe how to use code pro.lers, analysis tools that collect performance data about a program as it executes. We also present a general principle of system optimization known as Amdahl¡¯s law. 
5.14.1 Program Pro.ling 
Program pro.ling involves running a version of a program in which instrumenta-tion code has been incorporated to determine how much time the different parts of the program require. It can be very useful for identifying the parts of a program we should focus on in our optimization efforts. One strength of pro.ling is that it can be performed while running the actual program on realistic benchmark data. 
Unix systems provide the pro.ling program gprof. This program generates two forms of information. First, it determines how much CPU time was spent for each of the functions in the program. Second, it computes a count of how many times each function gets called, categorized by which function performs the call. Both forms of information can be quite useful. The timings give a sense of the relative importance of the different functions in determining the overall run time. The calling information allows us to understand the dynamic behavior of the program. 
Pro.ling with gprof requires three steps, as shown for a C program prog.c, which runs with command line argument file.txt: 
1. 
The program must be compiled and linked for pro.ling. With gcc (and other C compilers) this involves simply including the run-time .ag ¡®-pg¡¯ on the command line: 

2. 
The program is then executed as usual: 


unix> gcc -O1 -pg prog.c -o prog 
unix> ./prog file.txt 
It runs slightly (around a factor of 2) slower than normal, but otherwise the only difference is that it generates a .le gmon.out. 
3. gprof is invoked to analyze the data in gmon.out. 
unix> gprof prog 
The .rst part of the pro.le report lists the times spent executing the different 
functions, sorted in descending order. As an example, the following listing shows 
this part of the report for the three most time-consuming functions in a program: 

% cumulative self self total time seconds seconds calls s/call s/call name 97.58 173.05 173.05 1 173.05 173.05 sort_words 
2.36 177.24 4.19 965027 0.00 0.00 find_ele_rec 0.12 177.46 0.22 12511031 0.00 0.00 Strlen 
Each row represents the time spent for all calls to some function. The .rst column indicates the percentage of the overall time spent on the function. The second shows the cumulative time spent by the functions up to and including the one on this row. The third shows the time spent on this particular function, and the fourth shows how many times it was called (not counting recursive calls). In our example, the function sort_words was called only once, but this single call required 173.05 seconds, while the function find_ele_rec was called 965,027 times (not including recursive calls), requiring a total of 4.19 seconds. Function Strlen computes the length of a string by calling the library function strlen. Library function calls are normally not shown in the results by gprof. Their times are usually reported as part of the function calling them. By creating the ¡°wrapper function¡± Strlen, we can reliably track the calls to strlen, showing that it was called 12,511,031 times, but only requiring a total of 0.22 seconds. 
The second part of the pro.le report shows the calling history of the functions. The following is the history for a recursive function find_ele_rec: 
158655725 find_ele_rec [5] 4.19 0.02 965027/965027 insert_string [4] 
[5] 2.4 4.19 0.02 965027+158655725 find_ele_rec [5] 0.01 0.01 363039/363039 new_ele [10] 0.00 0.01 363039/363039 save_string [13] 
158655725 find_ele_rec [5] 

This history shows both the functions that called find_ele_rec, as well as the functions that it called. The .rst two lines show the calls to the function: 158,655,725 calls by itself recursively, and 965,027 calls by function insert_string (which is itself called 965,027 times). Function find_ele_rec in turn called two other functions, save_string and new_ele, each a total of 363,039 times. 
From this calling information, we can often infer useful information about the program behavior. For example, the function find_ele_rec is a recursive procedure that scans the linked list for a hash bucket looking for a particular string. For this function, comparing the number of recursive calls with the number of top-level calls provides statistical information about the lengths of the traversals through these lists. Given that their ratio is 164.4, we can infer that the program scanned an average of around 164 elements each time. 
Some properties of gprof are worth noting: 
. The timing is not very precise. It is based on a simple interval counting scheme in which the compiled program maintains a counter for each function record-ing the time spent executing that function. The operating system causes the program to be interrupted at some regular time interval ¦Ä. Typical values of 
¦Ä range between 1.0 and 10.0 milliseconds. It then determines what function the program was executing when the interrupt occurred and increments the counter for that function by ¦Ä. Of course, it may happen that this function just started executing and will shortly be completed, but it is assigned the full cost of the execution since the previous interrupt. Some other function may run between two interrupts and therefore not be charged any time at all. 
Over a long duration, this scheme works reasonably well. Statistically, ev-ery function should be charged according to the relative time spent executing it. For programs that run for less than around 1 second, however, the numbers should be viewed as only rough estimates. 
. The calling information is quite reliable. The compiled program maintains a counter for each combination of caller and callee. The appropriate counter is incremented every time a procedure is called. 
. By default, the timings for library functions are not shown. Instead, these times are incorporated into the times for the calling functions. 
5.14.2 Using a Pro.ler to Guide Optimization 
As an example of using a pro.ler to guide program optimization, we created an ap-plication that involves several different tasks and data structures. This application analyzes the n-gram statistics of a text document, where an n-gram is a sequence of n words occurring in a document. For n = 1, we collect statistics on individual words, for n = 2 on pairs of words, and so on. For a given value of n, our program reads a text .le, creates a table of unique n-grams specifying how many times each one occurs, then sorts the n-grams in descending order of occurrence. 
As a benchmark, we ran it on a .le consisting of the complete works of William Shakespeare totaling 965,028 words, of which 23,706 are unique. We found that for n = 1 even a poorly written analysis program can readily process the entire .le in under 1 second, and so we set n = 2 to make things more challenging. For the case of n = 2, n-grams are referred to as bigrams (pronounced ¡°bye-grams¡±). We determined that Shakespeare¡¯s works contain 363,039 unique bigrams. The most common is ¡°I am,¡± occurring 1,892 times. The phrase ¡°to be¡± occurs 1,020 times. Fully 266,018 of the bigrams occur only once. 
Our program consists of the following parts. We created multiple versions, 
starting with simple algorithms for the different parts and then replacing them 
with more sophisticated ones: 
1. 
Each word is read from the .le and converted to lowercase. Our initial version used the function lower1 (Figure 5.7), which we know to have quadratic run time due to repeated calls to strlen. 

2. 
A hash function is applied to the string to create a number between 0 and s . 1, for a hash table with s buckets. Our initial function simply summed the ASCII codes for the characters modulo s. 

3. 
Each hash bucket is organized as a linked list. The program scans down this list looking for a matching entry. If one is found, the frequency for this n-gram 




(a) All versions 
6 
5 

Sort
4 

List Lower
3 


Strlen Hash
2 



Rest 1 
0 
(b) All but the slowest version 

Figure 5.38 Pro.le resultss for different versions of n-gram frequency counting program. Time is divided according to the different major operations in the program. 
is incremented. Otherwise, a new list element is created. Our initial version performed this operation recursively, inserting new elements at the end of the list. 
4. Once the table has been generated, we sort all of the elements according to the frequencies. Our initial version used insertion sort. 
Figure 5.38 shows the pro.le results for six different versions of our n-gram-
frequency analysis program. For each version, we divide the time into the follow-
ing categories: 

Sort: Sorting n-grams by frequency List: Scanning the linked list for a matching n-gram, inserting a new element if necessary Lower: Converting strings to lowercase 
CPU seconds 


Strlen: Computing string lengths Hash: Computing the hash function Rest: The sum of all other functions 
As part (a) of the .gure shows, our initial version required nearly 3 minutes, with most of the time spent sorting. This is not surprising, since insertion sort has quadratic run time, and the program sorted 363,039 values. 
In our next version, we performed sorting using the library function qsort, which is based on the quicksort algorithm, having run time O(n log n). This version is labeled ¡°Quicksort¡± in the .gure. The more ef.cient sorting algorithm reduces the time spent sorting to become negligible, and the overall run time to around 
4.7 seconds. Part (b) of the .gure shows the times for the remaining version on a scale where we can see them more clearly. 
With improved sorting, we now .nd that list scanning becomes the bottleneck. Thinking that the inef.ciency is due to the recursive structure of the function, we replaced it by an iterative one, shown as ¡°Iter .rst.¡± Surprisingly, the run time increases to around 5.9 seconds. On closer study, we .nd a subtle difference between the two list functions. The recursive version inserted new elements at the end of the list, while the iterative one inserted them at the front. To maximize performance, we want the most frequent n-grams to occur near the beginnings of the lists. That way, the function will quickly locate the common cases. Assuming that n-grams are spread uniformly throughout the document, we would expect the .rst occurrence of a frequent one to come before that of a less frequent one. By inserting new n-grams at the end, the .rst function tended to order n-grams in descending order of frequency, while the second function tended to do just the opposite. We therefore created a third list-scanning function that uses iteration, but inserts new elements at the end of this list. With this version, shown as ¡°Iter last,¡± the time dropped to around 4.2 seconds, slightly better than with the recursive version. These measurements demonstrate the importance of running experiments on a program as part of an optimization effort. We initially assumed that converting recursive code to iterative code would improve its performance and did not consider the distinction between adding to the end or to the beginning of a list. 
Next, we consider the hash table structure. The initial version had only 1021 buckets (typically, the number of buckets is chosen to be a prime number to enhance the ability of the hash function to distribute keys uniformly among the buckets). For a table with 363,039 entries, this would imply an average load of 363039/1021 = 355.6. That explains why so much of the time is spent performing list operations¡ªthe searches involve testing a signi.cant number of candidate n-grams. It also explains why the performance is so sensitive to the list ordering. We then increased the number of buckets to 199,999, reducing the average load to 1.8. Oddly enough, however, our overall run time only drops to 3.9 seconds, a difference of only 0.3 seconds. 
On further inspection, we can see that the minimal performance gain with a larger table was due to a poor choice of hash function. Simply summing the charac-ter codes for a string does not produce a very wide range of values. In particular, the maximum code value for a letter is 122, and so a string of n characters will generate a sum of at most 122n. The longest bigram in our document, ¡°honori.ca-bilitudinitatibus thou,¡± sums to just 3371, and so most of the buckets in our hash table will go unused. In addition, a commutative hash function, such as addition, does not differentiate among the different possible orderings of characters with a string. For example, the words ¡°rat¡± and ¡°tar¡± will generate the same sums. 

We switched to a hash function that uses shift and Exclusive-Or operations. With this version, shown as ¡°Better hash,¡± the time drops to 0.4 seconds. A more systematic approach would be to study the distribution of keys among the buckets more carefully, making sure that it comes close to what one would expect if the hash function had a uniform output distribution. 
Finally, we have reduced the run time to the point where most of the time is spent in strlen, and most of the calls to strlen occur as part of the lowercase con-version. We have already seen that function lower1 has quadratic performance, especially for long strings. The words in this document are short enough to avoid the disastrous consequences of quadratic performance; the longest bigram is just 32 characters. Still, switching to lower2, shown as ¡°Linear lower,¡± yields a signif-icant performance, with the overall time dropping to around 0.2 seconds. 
With this exercise, we have shown that code pro.ling can help drop the time required for a simple application from nearly 3 minutes down to well under 1 second. The pro.ler helps us focus our attention on the most time-consuming parts of the program and also provides useful information about the procedure call structure. Some of the bottlenecks in our code, such as using a quadratic sort routine, are easy to anticipate, while others, such as whether to append to the beginning or end of a list, emerge only through a careful analysis. 
We can see that pro.ling is a useful tool to have in the toolbox, but it should not be the only one. The timing measurements are imperfect, especially for shorter (less than 1 second) run times. More signi.cantly, the results apply only to the particular data tested. For example, if we had run the original function on data consisting of a smaller number of longer strings, we would have found that the lowercase conversion routine was the major performance bottleneck. Even worse, if it only pro.led documents with short words, we might never detect hidden bottlenecks such as the quadratic performance of lower1. In general, pro.ling can help us optimize for typical cases, assuming we run the program on representative data, but we should also make sure the program will have respectable performance for all possible cases. This mainly involves avoiding algorithms (such as insertion sort) and bad programming practices (such as lower1) that yield poor asymptotic performance. 
5.14.3 Amdahl¡¯s Law 
Gene Amdahl, one of the early pioneers in computing, made a simple but insight-ful observation about the effectiveness of improving the performance of one part of a system. This observation has come to be known as Amdahl¡¯s law. The main idea is that when we speed up one part of a system, the effect on the overall sys-tem performance depends on both how signi.cant this part was and how much it sped up. Consider a system in which executing some application requires time Told. Suppose some part of the system requires a fraction ¦Á of this time, and that we improve its performance by a factor of k. That is, the component originally re-quired time ¦ÁTold, and it now requires time (¦ÁTold)/k. The overall execution time would thus be 
Tnew = (1 . ¦Á)Told + (¦ÁTold)/k 
= Told[(1 . ¦Á) + ¦Á/k] 
From this, we can compute the speedup S = Told/Tnew as 
1 
S = (5.4) 
(1 . ¦Á) + ¦Á/k 
As an example, consider the case where a part of the system that initially consumed 60% of the time (¦Á = 0.6) is sped up by a factor of 3 (k = 3). Then we get a speedup of 1/[0.4 + 0.6/3] = 1.67. Thus, even though we made a substantial improvement to a major part of the system, our net speedup was signi.cantly less. This is the major insight of Amdahl¡¯s law¡ªto signi.cantly speed up the entire system, we must improve the speed of a very large fraction of the overall system. 
Practice Problem 5.13 
Suppose you work as a truck driver, and you have been hired to carry a load of potatoes from Boise, Idaho, to Minneapolis, Minnesota, a total distance of 2500 kilometers. You estimate you can average 100 km/hr driving within the speed limits, requiring a total of 25 hours for the trip. 
A. You hear on the news that Montana has just abolished its speed limit, which constitutes 1500 km of the trip. Your truck can travel at 150 km/hr. What will be your speedup for the trip? 
B. You can buy a new turbocharger for your truck at www.fasttrucks.com. 
They stock a variety of models, but the faster you want to go, the more it will cost. How fast must you travel through Montana to get an overall speedup for your trip of 5/3? 
Practice Problem 5.14 
The marketing department at your company has promised your customers that the next software release will show a 2¡Á performance improvement. You have been assigned the task of delivering on that promise. You have determined that only 80% of the system can be improved. How much (i.e., what value of k) would you need to improve this part to meet the overall performance target? 

One interesting special case of Amdahl¡¯s law is to consider the effect of setting k to ¡Þ. That is, we are able to take some part of the system and speed it up to the point at which it takes a negligible amount of time. We then get 
1 
S¡Þ= (5.5) 
(1 . ¦Á) 

So, for example, if we can speed up 60% of the system to the point where it re-quires close to no time, our net speedup will still only be 1/0.4 = 2.5. We saw this performance with our dictionary program as we replaced insertion sort by quick-sort. The initial version spent 173.05 of its 177.57 seconds performing insertion sort, giving ¦Á = 0.975. With quicksort, the time spent sorting becomes negligible, giving a predicted speedup of 39.3. In fact, the actual measured speedup was a bit less: 173.05/4.72 = 37.6, due to inaccuracies in the pro.ling measurements. We were able to gain a large speedup because sorting constituted a very large fraction of the overall execution time. 
Amdahl¡¯s law describes a general principle for improving any process. In addition to applying to speeding up computer systems, it can guide a company trying to reduce the cost of manufacturing razor blades, or a student trying to improve his or her gradepoint average. Perhaps it is most meaningful in the world of computers, where we routinely improve performance by factors of 2 or more. Such high factors can only be achieved by optimizing large parts of a system. 
5.15 
Summary 

Although most presentations on code optimization describe how compilers can generate ef.cient code, much can be done by an application programmer to assist the compiler in this task. No compiler can replace an inef.cient algorithm or data structure by a good one, and so these aspects of program design should remain a primary concern for programmers. We also have seen that optimization block-ers, such as memory aliasing and procedure calls, seriously restrict the ability of compilers to perform extensive optimizations. Again, the programmer must take primary responsibility for eliminating these. These should simply be considered parts of good programming practice, since they serve to eliminate unneeded work. 
Tuning performance beyond a basic level requires some understanding of the processor¡¯s microarchitecture, describing the underlying mechanisms by which the processor implements its instruction set architecture. For the case of out-of-order processors, just knowing something about the operations, latencies, and issue times of the functional units establishes a baseline for predicting program performance. 
We have studied a series of techniques, including loop unrolling, creating multiple accumulators, and reassociation, that can exploit the instruction-level parallelism provided by modern processors. As we get deeper into the optimiza-tion, it becomes important to study the generated assembly code, and to try to understand how the computation is being performed by the machine. Much can be gained by identifying the critical paths determined by the data dependencies in the program, especially between the different iterations of a loop. We can also compute a throughput bound for a computation, based on the number of oper-ations that must be computed and the number and issue times of the units that perform those operations. 
Programs that involve conditional branches or complex interactions with the memory system are more dif.cult to analyze and optimize than the simple loop programs we .rst considered. The basic strategy is to try to make branches more predictable or make them amenable to implementation using conditional data transfers. We must also watch out for the interactions between store and load operations. Keeping values in local variables, allowing them to be stored in registers, can often be helpful. 
When working with large programs, it becomes important to focus our op-timization efforts on the parts that consume the most time. Code pro.lers and related tools can help us systematically evaluate and improve program perfor-mance. We described gprof, a standard Unix pro.ling tool. More sophisticated pro.lers are available, such as the vtune program development system from In-tel, and valgrind, commonly available on Linux systems. These tools can break down the execution time below the procedure level, to estimate the performance of each basic block of the program. (A basic block is a sequence of instructions that has no transfers of control out of its middle, and so the block is always executed in its entirety.) 
Amdahl¡¯s law provides a simple but powerful insight into the performance gains obtained by improving just one part of the system. The gain depends both on how much we improve this part and how large a fraction of the overall time this part originally required. 
Bibliographic 
Notes 

Our focus has been to describe code optimization from the programmer¡¯s per-spective, demonstrating how to write code that will make it easier for compilers to generate ef.cient code. An extended paper by Chellappa, Franchetti, and P ¡§
uschel 
[19] takes a similar approach, but goes into more detail with respect to the pro-cessor¡¯s characteristics. 
Many publications describe code optimization from a compiler¡¯s perspective, formulating ways that compilers can generate more ef.cient code. Muchnick¡¯s book is considered the most comprehensive [76]. Wadleigh and Crawford¡¯s book on software optimization [114] covers some of the material we have presented, but it also describes the process of getting high performance on parallel machines. An early paper by Mahlke et al. [71] describes how several techniques developed for compilers that map programs onto parallel machines can be adapted to exploit the instruction-level parallelism of modern processors. This paper covers the code transformations we presented, including loop unrolling, multiple accumulators (which they refer to as accumulator variable expansion), and reassociation (which they refer to as tree height reduction). 
Our presentation of the operation of an out-of-order processor is fairly brief and abstract. More complete descriptions of the general principles can be found in advanced computer architecture textbooks, such as the one by Hennessy and Pat-terson [49, Ch. 2¨C3]. Shen and Lipasti¡¯s book [96] provides an in-depth treatment of modern processor design. 

Amdahl¡¯s law is presented in most books on computer architecture. With its major focus on quantitative system evaluation, Hennessy and Patterson¡¯s book [49, Ch. 1] provides a particularly good treatment of the subject. 
Homework 
Problems 

5.15 ¡ô¡ô 

Suppose we wish to write a procedure that computes the inner product of two vectors u and v. An abstract version of the function has a CPE of 16¨C17 with x86-64 and 26¨C29 with IA32 for integer, single-precision, and double-precision data. By doing the same sort of transformations we did to transform the abstract program combine1 into the more ef.cient combine4, we get the following code: 
1 /* Accumulate in temporary */ 2 void inner4(vec_ptr u, vec_ptr v, data_t *dest) 3 { 4 long int i; 5 int length = vec_length(u); 6 data_t *udata = get_vec_start(u); 7 data_t *vdata = get_vec_start(v); 8 data_t sum = (data_t) 0; 9 
10 for (i = 0; i < length; i++) { 11 sum = sum + udata[i] * vdata[i]; 
12 } 13 *dest = sum; 
14 } 

Our measurements show that this function has a CPE of 3.00 for integer and .oating-point data. For data type float, the x86-64 assembly code for the inner loop is as follows: 
inner4: data_t = float udata in %rbx, vdata in %rax, limit in %rcx, iin %rdx, sum in %xmm1 
1 .L87: loop: 2 movss (%rbx,%rdx,4), %xmm0 Get udata[i] 3 mulss (%rax,%rdx,4), %xmm0 Multiply by vdata[i] 4 addss %xmm0, %xmm1 Add to sum 5 addq $1, %rdx Increment i 6 cmpq %rcx, %rdx Compare i:limit 7 jl .L87 If <, goto loop 
Assume that the functional units have the characteristics listed in Figure 5.12. 
A. Diagram how this instruction sequence would be decoded into operations and show how the data dependencies between them would create a critical path of operations, in the style of Figures 5.13 and 5.14. 
B. For data type float, what lower bound on the CPE is determined by the critical path? 
C. Assuming similar instruction sequences for the integer code as well, what lower bound on the CPE is determined by the critical path for integer data? 
D. Explain how the two .oating-point versions can have CPEs of 3.00, even though the multiplication operation requires either 4 or 5 clock cycles. 
5.16 ¡ô 
Write a version of the inner product procedure described in Problem 5.15 that uses four-way loop unrolling. For x86-64, our measurements of the unrolled version give a CPE of 2.00 for integer data but still 3.00 for both single and double precision. 
A. Explain why any version of any inner product procedure cannot achieve a CPE less than 2.00. 
B. Explain why the performance for .oating-point data did not improve with loop unrolling. 
5.17 ¡ô 
Write a version of the inner product procedure described in Problem 5.15 that uses four-way loop unrolling with four parallel accumulators. Our measurements for this function with x86-64 give a CPE of 2.00 for all types of data. 
A. What factor limits the performance to a CPE of 2.00? 
B. Explain why the version with integer data on IA32 achieves a CPE of 2.75, worse than the CPE of 2.25 achieved with just four-way loop unrolling. 
5.18 ¡ô 
Write a version of the inner product procedure described in Problem 5.15 that uses four-way loop unrolling along with reassociation to enable greater parallelism. Our measurements for this function give a CPE of 2.00 with x86-64 and 2.25 with IA32 for all types of data. 
5.19 ¡ô¡ô 
The library function memset has the following prototype: 
void *memset(void *s, int c, size_t n); 
This function .lls n bytes of the memory area starting at s with copies of the low-order byte of c. For example, it can be used to zero out a region of memory by giving argument 0 for c, but other values are possible. 

The following is a straightforward implementation of memset: 
1 /* Basic implementation of memset */ 2 void *basic_memset(void *s, int c, size_t n) 3 { 4 size_t cnt = 0; 5 unsigned char *schar = s; 6 while (cnt < n) { 7 *schar++ = (unsigned char) c; 8 cnt++; 
9 } 10 return s; 
11 } 

Implement a more ef.cient version of the function by using a word of data type unsigned long to pack four (for IA32) or eight (for x86-64) copies of c, and then step through the region using word-level writes. You might .nd it helpful to do additional loop unrolling as well. On an Intel Core i7 machine, we were able to reduce the CPE from 2.00 for the straightforward implementation to 0.25 for IA32 and 0.125 for x86-64, i.e., writing either 4 or 8 bytes on every clock cycle. 
Here are some additional guidelines. In this discussion, let K denote the value of sizeof(unsigned long) for the machine on which you run your program. 
. You may not call any library functions. . Your code should work for arbitrary values of n, including when it is not a multiple of K. You can do this in a manner similar to the way we .nish the last few iterations with loop unrolling. . You should write your code so that it will compile and run correctly regardless of the value of K. Make use of the operation sizeof to do this. . On some machines, unaligned writes can be much slower than aligned ones. (On some non-x86 machines, they can even cause segmentation faults.) Write your code so that it starts with byte-level writes until the destination address is a multiple of K, then do word-level writes, and then (if necessary) .nish with byte-level writes. . Beware of the case where cnt is small enough that the upper bounds on some of the loops become negative. With expressions involving the sizeof operator, the testing may be performed with unsigned arithmetic. (See Sec-tion 2.2.8 and Problem 2.72.) 
5.20 ¡ô¡ô¡ô 

We considered the task of polynomial evaluation in Problems 5.5 and 5.6, with both a direct evaluation and an evaluation by Horner¡¯s method. Try to write faster versions of the function using the optimization techniques we have explored, including loop unrolling, parallel accumulation, and reassociation. You will .nd many different ways of mixing together Horner¡¯s scheme and direct evaluation with these optimization techniques. 
Ideally, you should be able to reach a CPE close to the number of cycles between successive .oating-point additions and multiplications with your machine (typically 1). At the very least, you should be able to achieve a CPE less than the latency of .oating-point addition for your machine. 
5.21 ¡ô¡ô¡ô 
In Problem 5.12, we were able to reduce the CPE for the pre.x-sum computation to 3.00, limited by the latency of .oating-point addition on this machine. Simple loop unrolling does not improve things. 
Using a combination of loop unrolling and reassociation, write code for pre-.x sum that achieves a CPE less than the latency of .oating-point addition on your machine. Doing this requires actually increasing the number of additions per-formed. For example, our version with two-way unrolling requires three additions per iteration, while our version with three-way unrolling requires .ve. 
5.22 ¡ô 
Suppose you are given the task of improving the performance of a program consisting of three parts. Part A requires 20% of the overall run time, part B requires 30%, and part C requires 50%. You determine that for $1000 you could either speed up part B by a factor of 3.0 or part C by a factor of 1.5. Which choice would maximize performance? 
Solutions 
to 
Practice 
Problems 

Solution to Problem 5.1 (page 478) 
This problem illustrates some of the subtle effects of memory aliasing. As the following commented code shows, the effect will be to set the value at 
xp to zero: 
4 *xp = *xp+*xp;/*2x */ 5 *xp = *xp-*xp;/*2x-2x=0*/ 6 *xp = *xp-*xp;/* 0-0=0*/ 
This example illustrates that our intuition about program behavior can often be wrong. We naturally think of the case where xp and yp are distinct but overlook the possibility that they might be equal. Bugs often arise due to conditions the programmer does not anticipate. 
Solution to Problem 5.2 (page 482) 
This problem illustrates the relationship between CPE and absolute performance. It can be solved using elementary algebra. We .nd that for n ¡Ü 2, Version 1 is the fastest. Version 2 is fastest for 3 ¡Ü n ¡Ü 7, and Version 3 is fastest for n ¡Ý 8. 
Solution to Problem 5.3 (page 490) 
This is a simple exercise, but it is important to recognize that the four statements of a for loop¡ªinitial, test, update, and body¡ªget executed different numbers of times. 

Code min max incr square 
A. 191 90 90 
B. 91190 90 
C. 1190 90 
Solution to Problem 5.4 (page 494) 
This assembly code demonstrates a clever optimization opportunity detected by gcc. It is worth studying this code carefully to better understand the subtleties of code optimization. 
A. In the less optimized code, register %xmm0 is simply used as a temporary value, both set and used on each loop iteration. In the more optimized code, it is used more in the manner of variable x in combine4, accumulating the product of the vector elements. The difference with combine4, however, is that location dest is updated on each iteration by the second movss instruction. 
We can see that this optimized version operates much like the following C code: 
1 /* Make sure dest updated on each iteration */ 
2 void combine3w(vec_ptr v, data_t *dest) 
3 { 

4 long int i; 
5 long int length = vec_length(v); 
6 data_t *data = get_vec_start(v); 
7 data_t acc = IDENT; 
8 

9 for (i = 0; i < length; i++) { 
10 acc = acc OP data[i]; 
11 *dest = acc; 
12 
} 

13 
} 



B. The two versions of combine3 will have identical functionality, even with memory aliasing. 
C. This transformation can be made without changing the program behavior, because, with the exception of the .rst iteration, the value read from dest at the beginning of each iteration will be the same value written to this register at the end of the previous iteration. Therefore, the combining instruction can simply use the value already in %xmm0 at the beginning of the loop. 
Solution to Problem 5.5 (page 507) 
Polynomial evaluation is a core technique for solving many problems. For example, polynomial functions are commonly used to approximate trigonometric functions in math libraries. 
A. The function performs 2n multiplications and n additions. 
B. We can see that the performance limiting computation here is the repeated computation of the expression xpwr=x* xpwr. This requires a double-precision, .oating-point multiplication (5 clock cycles), and the computation for one iteration cannot begin until the one for the previous iteration has completed. The updating of result only requires a .oating-point addition (3 clock cycles) between successive iterations. 
Solution to Problem 5.6 (page 508) 
This problem demonstrates that minimizing the number of operations in a com-putation may not improve its performance. 
A. The function performs n multiplications and n additions, half the number of multiplications as the original function poly. 
B. We can see that the performance limiting computation here is the repeated computation of the expression result = a[i] + x*result. Starting from the value of result from the previous iteration, we must .rst multiply it by x (5 clock cycles) and then add it to a[i] (3 cycles) before we have the value for this iteration. Thus, each iteration imposes a minimum latency of 8 cycles, exactly our measured CPE. 
C. Although each iteration in function poly requires two multiplications rather than one, only a single multiplication occurs along the critical path per iteration. 
Solution to Problem 5.7 (page 510) 
The following code directly follows the rules we have stated for unrolling a loop by some factor k: 
1 void unroll5(vec_ptr v, data_t *dest) 2 { 3 long int i; 4 long int length = vec_length(v); 5 long int limit = length-4; 6 data_t *data = get_vec_start(v); 7 data_t acc = IDENT; 8 9 /* Combine 5 elements at a time */ 
10 for (i = 0; i < limit; i+=5) { 11 acc = acc OP data[i] OP data[i+1]; 12 acc = acc OP data[i+2] OP data[i+3]; 13 acc = acc OP data[i+4]; 
14 } 15 16 /* Finish any remaining elements */ 17 for (; i < length; i++) { 18 acc = acc OP data[i]; 

A1: ((r*x)*y)*z A2: (r*(x*y))*z A3: r*((x*y)*z) A4: r*(x*(y*z)) A5: (r*x)*(y*z) 



* 
* 
* 
* 



19  }  
20  *dest  =  acc;  
21  }  

Solution to Problem 5.8 (page 523) 
This problem demonstrates how small changes in a program can yield dramatic performance differences, especially on a machine with out-of-order execution. Figure 5.39 diagrams the three multiplication operations for a single iteration of the function. In this .gure, the operations shown as blue boxes are along the critical path¡ªthey need to be computed in sequence to compute a new value for loop variable r. The operations shown as light boxes can be computed in parallel with the critical path operations. For a loop with c operations along the critical path, each iteration will require a minimum of 5c clock cycles and will compute the product for three elements, giving a lower bound on the CPE of 5c/3. This implies lower bounds of 5.00 for A1, 3.33 for A2 and A5, and 1.67 for A3 and A4. 
We ran these functions on an Intel Core i7, and indeed obtained CPEs of 5.00 for A1, and 1.67 for A3 and A4. For some reason, A2 and A5 achieved CPEs of just 3.67, indicating that the functions required 11 clock cycles per iteration rather than the predicted 10. 
Solution to Problem 5.9 (page 530) 
This is another demonstration that a slight change in coding style can make it much easier for the compiler to detect opportunities to use conditional moves: 
while (i1<n&&i2<n){ 
int v1 = src1[i1]; 
int v2 = src2[i2]; 
int take1 = v1 < v2; 
dest[id++] = take1 ? v1 : v2; 
i1 += take1; 
i2 += (1-take1); } 
We measured a CPE of around 11.50 for this version of the code, a signi.cant improvement over the original CPE of 17.50. 
Solution to Problem 5.10 (page 538) 
This problem requires you to analyze the potential load-store interactions in a program. 
A. It will set each element a[i] to i + 1, for 0 ¡Ü i ¡Ü 998. 
B. It will set each element a[i] to 0, for 1 ¡Ü i ¡Ü 999. 
C. In the second case, the load of one iteration depends on the result of the store from the previous iteration. Thus, there is a write/read dependency between successive iterations. It is interesting to note that the CPE of 5.00 is 1 less than we measured for Example B of function write_read. This is due to the fact that write_read increments the value before storing it, requiring one clock cycle. 
D. It will give a CPE of 2.00, the same as for Example A, since there are no dependencies between stores and subsequent loads. 
Solution to Problem 5.11 (page 538) 
We can see that this function has a write/read dependency between successive iterations¡ªthe destination value p[i] on one iteration matches the source value p[i-1] on the next. 
Solution to Problem 5.12 (page 539) 
Here is a revised version of the function: 
1 void psum1a(float a[], float p[], long int n) 2 { 3 long int i; 4 /* last_val holds p[i-1]; val holds p[i] */ 5 float last_val, val; 6 last_val = p[0] = a[0]; 7 for (i=1;i<n; i++) { 8 val = last_val + a[i]; 9 p[i] = val; 
10 last_val = val; 
11 
} 

12 
} 


We introduce a local variable last_val. At the start of iteration i, it holds the value of p[i-1]. We then compute val to be the value of p[i] and to be the new value for last_val. 
This version compiles to the following assembly code: 
psum1a. a in %rdi,pin %rsi,iin %rax, cnt in %rdx, last_val in %xmm0 1 .L18: loop: 2 addss (%rdi,%rax,4), %xmm0 last_val = val = last_val + a[i] 

3  movss  %xmm0,  (%rsi,%rax,4)  Store val in p[i]  
4  addq  $1,  %rax  Increment i  
5  cmpq  %rax,  %rdx  Compare cnt:i  
6  jg  .L18  If >, goto loop  

This code holds last_val in %xmm0, avoiding the need to read p[i-1] from memory, and thus eliminating the write/read dependency seen in psum1. 
Solution to Problem 5.13 (page 546) 
This problem illustrates that Amdahl¡¯s law applies to more than just computer systems. 
A. In terms of Equation 5.4, we have ¦Á = 0.6 and k = 1.5. More directly, travel-ing the 1500 kilometers through Montana will require 10 hours, and the rest of the trip also requires 10 hours. This will give a speedup of 25/(10 + 10) = 1.25. 
B. In terms of Equation 5.4, we have ¦Á = 0.6, and we require S = 5/3, from which we can solve for k. More directly, to speed up the trip by 5/3, we must decrease the overall time to 15 hours. The parts outside of Montana will still require 10 hours, so we must drive through Montana in 5 hours. This requires traveling at 300 km/hr, which is pretty fast for a truck! 
Solution to Problem 5.14 (page 546) 
Amdahl¡¯s law is best understood by working through some examples. This one requires you to look at Equation 5.4 from an unusual perspective. 
This problem is a simple application of the equation. You are given S = 2 and ¦Á = .8, and you must then solve for k: 
1
2 = 
(1 . 0.8) + 0.8/k 0.4 + 1.6/k = 1.0 k = 2.67 
This page intentionally left blank 

CHAPTER 
6 
The 
Memory 
Hierarchy 
6.1 
Storage 
Technologies 
561 
6.2 
Locality 
586 
6.3 
The 
Memory 
Hierarchy 
591 
6.4 
Cache 
Memories 
596 
6.5 
Writing 
Cache-friendly 
Code 
615 
6.6 
Putting 
It 
Together: 
The 
Impact 
of 
Caches 
on 
Program 
Performance 
620 
6.7 
Summary 
629 
Bibliographic 
Notes 
630 
Homework 
Problems 
631 
Solutions 
to 
Practice 
Problems 
642 
To this point in our study of systems, we have relied on a simple model of a computer system as a CPU that executes instructions and a memory system that holds instructions and data for the CPU. In our simple model, the memory system is a linear array of bytes, and the CPU can access each memory location in a constant amount of time. While this is an effective model as far as it goes, it does not re.ect the way that modern systems really work. 
In practice, a memory system is a hierarchy of storage devices with different capacities, costs, and access times. CPU registers hold the most frequently used data. Small, fast cache memories nearby the CPU act as staging areas for a subset of the data and instructions stored in the relatively slow main memory. The main memory stages data stored on large, slow disks, which in turn often serve as staging areas for data stored on the disks or tapes of other machines connected by networks. 
Memory hierarchies work because well-written programs tend to access the storage at any particular level more frequently than they access the storage at the next lower level. So the storage at the next level can be slower, and thus larger and cheaper per bit. The overall effect is a large pool of memory that costs as much as the cheap storage near the bottom of the hierarchy, but that serves data to programs at the rate of the fast storage near the top of the hierarchy. 
As a programmer, you need to understand the memory hierarchy because it has a big impact on the performance of your applications. If the data your program needs are stored in a CPU register, then they can be accessed in zero cycles during the execution of the instruction. If stored in a cache, 1 to 30 cycles. If stored in main memory, 50 to 200 cycles. And if stored in disk tens of millions of cycles! 
Here, then, is a fundamental and enduring idea in computer systems: if you understand how the system moves data up and down the memory hierarchy, then you can write your application programs so that their data items are stored higher in the hierarchy, where the CPU can access them more quickly. 
This idea centers around a fundamental property of computer programs known as locality. Programs with good locality tend to access the same set of data items over and over again, or they tend to access sets of nearby data items. Programs with good locality tend to access more data items from the upper levels of the memory hierarchy than programs with poor locality, and thus run faster. For example, the running times of different matrix multiplication kernels that perform the same number of arithmetic operations, but have different degrees of locality, can vary by a factor of 20! 
In this chapter, we will look at the basic storage technologies¡ªSRAM mem-ory, DRAM memory, ROM memory, and rotating and solid state disks¡ªand describe how they are organized into hierarchies. In particular, we focus on the cache memories that act as staging areas between the CPU and main memory, be-cause they have the most impact on application program performance. We show you how to analyze your C programs for locality and we introduce techniques for improving the locality in your programs. You will also learn an interesting way to characterize the performance of the memory hierarchy on a particular machine as a ¡°memory mountain¡± that shows read access times as a function of locality. 

6.1 
Storage 
Technologies 

Much of the success of computer technology stems from the tremendous progress in storage technology. Early computers had a few kilobytes of random-access memory. The earliest IBM PCs didn¡¯t even have a hard disk. That changed with the introduction of the IBM PC-XT in 1982, with its 10-megabyte disk. By the year 2010, typical machines had 150,000 times as much disk storage, and the amount of storage was increasing by a factor of 2 every couple of years. 
6.1.1 Random-Access Memory 
Random-access memory (RAM) comes in two varieties¡ªstatic and dynamic. Static RAM (SRAM) is faster and signi.cantly more expensive than Dynamic RAM (DRAM). SRAM is used for cache memories, both on and off the CPU chip. DRAM is used for the main memory plus the frame buffer of a graphics system. Typically, a desktop system will have no more than a few megabytes of SRAM, but hundreds or thousands of megabytes of DRAM. 
Static RAM 

SRAM stores each bit in a bistable memory cell. Each cell is implemented with a six-transistor circuit. This circuit has the property that it can stay inde.nitely in either of two different voltage con.gurations, or states. Any other state will be unstable¡ªstarting from there, the circuit will quickly move toward one of the stable states. Such a memory cell is analogous to the inverted pendulum illustrated in Figure 6.1. 
The pendulum is stable when it is tilted either all the way to the left or all the way to the right. From any other position, the pendulum will fall to one side or the other. In principle, the pendulum could also remain balanced in a vertical position inde.nitely, but this state is metastable¡ªthe smallest disturbance would make it start to fall, and once it fell it would never return to the vertical position. 
Due to its bistable nature, an SRAM memory cell will retain its value indef-initely, as long as it is kept powered. Even when a disturbance, such as electrical noise, perturbs the voltages, the circuit will return to the stable value when the disturbance is removed. 
Figure 6.1 
Inverted pendulum. 

Like an SRAM cell, the pendulum has only two 
Stable left 
Unstable Stable right

stable con.gurations, or states. 



Transistors Relative Relative per bit access time Persistent? Sensitive? cost Applications 
SRAM6 1¡Á Yes No 100¡Á Cache memory DRAM 1 10¡Á No Yes 1¡Á Main mem, frame buffers 
Figure 6.2 Characteristics of DRAM and SRAM memory. 
Dynamic RAM 
DRAM stores each bit as charge on a capacitor. This capacitor is very small¡ª typically around 30 femtofarads, that is, 30 ¡Á 10.15 farads. Recall, however, that a farad is a very large unit of measure. DRAM storage can be made very dense¡ª each cell consists of a capacitor and a single access transistor. Unlike SRAM, however, a DRAM memory cell is very sensitive to any disturbance. When the capacitor voltage is disturbed, it will never recover. Exposure to light rays will cause the capacitor voltages to change. In fact, the sensors in digital cameras and camcorders are essentially arrays of DRAM cells. 
Various sources of leakage current cause a DRAM cell to lose its charge within a time period of around 10 to 100 milliseconds. Fortunately, for computers operating with clock cycle times measured in nanoseconds, this retention time is quite long. The memory system must periodically refresh every bit of memory by reading it out and then rewriting it. Some systems also use error-correcting codes, where the computer words are encoded a few more bits (e.g., a 32-bit word might be encoded using 38 bits), such that circuitry can detect and correct any single erroneous bit within a word. 
Figure 6.2 summarizes the characteristics of SRAM and DRAM memory. SRAM is persistent as long as power is applied. Unlike DRAM, no refresh is necessary. SRAM can be accessed faster than DRAM. SRAM is not sensitive to disturbances such as light and electrical noise. The trade-off is that SRAM cells use more transistors than DRAM cells, and thus have lower densities, are more expensive, and consume more power. 
Conventional DRAMs 
The cells (bits) in a DRAM chip are partitioned into d supercells, each consisting of w DRAM cells. A d ¡Á w DRAM stores a total of dw bits of information. The supercells are organized as a rectangular array with r rows and c columns, where rc = d. Each supercell has an address of the form (i, j), where i denotes the row, and j denotes the column. 
For example, Figure 6.3 shows the organization of a 16 ¡Á 8 DRAM chip with d = 16 supercells, w = 8 bits per supercell, r = 4 rows, and c = 4 columns. The shaded box denotes the supercell at address (2, 1). Information .ows in and out of the chip via external connectors called pins. Each pin carries a 1-bit signal. Figure 6.3 shows two of these sets of pins: eight data pins that can transfer 1 byte in or out of the chip, and two addr pins that carry two-bit row and column supercell addresses. Other pins that carry control information are not shown. 


Aside A note on terminology 
The storage community has never settled on a standard name for a DRAM array element. Computer architects tend to refer to it as a ¡°cell,¡± overloading the term with the DRAM storage cell. Circuit designers tend to refer to it as a ¡°word,¡± overloading the term with a word of main memory. To avoid confusion, we have adopted the unambiguous term ¡°supercell.¡± 
Each DRAM chip is connected to some circuitry, known as the memory controller, that can transfer w bits at a time to and from each DRAM chip. To read the contents of supercell (i, j), the memory controller sends the row address i to the DRAM, followed by the column address j . The DRAM responds by sending the contents of supercell (i, j) back to the controller. The row address i is called a RAS (Row Access Strobe) request. The column address j is called a CAS (Column Access Strobe) request. Notice that the RAS and CAS requests share the same DRAM address pins. 
For example, to read supercell (2, 1) from the 16 ¡Á 8 DRAM in Figure 6.3, the memory controller sends row address 2, as shown in Figure 6.4(a). The DRAM responds by copying the entire contents of row 2 into an internal row buffer. Next, the memory controller sends column address 1, as shown in Figure 6.4(b). The DRAM responds by copying the 8 bits in supercell (2, 1) from the row buffer and sending them to the memory controller. 
One reason circuit designers organize DRAMs as two-dimensional arrays instead of linear arrays is to reduce the number of address pins on the chip. For example, if our example 128-bit DRAM were organized as a linear array of 16 supercells with addresses 0 to 15, then the chip would need four address pins instead of two. The disadvantage of the two-dimensional array organization is that addresses must be sent in two distinct steps, which increases the access time. 
DRAM chip DRAM chip 

Cols 0123 0 1 

Rows 
2 3 




Internal row buffer 

(a) Select row 2 (RAS request). (b) Select column 1 (CAS request). 
Memory Modules 
DRAM chips are packaged in memory modules that plug into expansion slots on the main system board (motherboard). Common packages include the 168-pin dual inline memory module (DIMM), which transfers data to and from the memory controller in 64-bit chunks, and the 72-pin single inline memory module (SIMM), which transfers data in 32-bit chunks. 
Figure 6.5 shows the basic idea of a memory module. The example module stores a total of 64 MB (megabytes) using eight 64-Mbit 8M ¡Á 8 DRAM chips, numbered 0 to 7. Each supercell stores 1 byte of main memory, and each 64-bit doubleword1 at byte address A in main memory is represented by the eight supercells whose corresponding supercell address is (i, j). In the example in Figure 6.5, DRAM 0 stores the .rst (lower-order) byte, DRAM 1 stores the next byte, and so on. 
To retrieve a 64-bit doubleword at memory address A, the memory controller converts A to a supercell address (i, j) and sends it to the memory module, which then broadcasts i and j to each DRAM. In response, each DRAM outputs the 8-bit contents of its (i, j) supercell. Circuitry in the module collects these outputs and forms them into a 64-bit doubleword, which it returns to the memory controller. 
Main memory can be aggregated by connecting multiple memory modules to the memory controller. In this case, when the controller receives an address A, the controller selects the module k that contains A, converts A to its (i, j) form, and sends (i, j) to module k. 
1. IA32 would call this 64-bit quantity a ¡°quadword.¡± 



: Supercell (i,j) 
64 MB memory module consisting of 8 8M 8 DRAMs 
bits 0-7 
Memory controller 
64-bit doubleword to CPU chip 

Figure 6.5 Reading the contents of a memory module. 
Practice Problem 6.1 
In the following, let r be the number of rows in a DRAM array, c the number of columns, b the number of bits needed to address the rows, and b the number 
rc 
of bits needed to address the columns. For each of the following DRAMs, deter-mine the power-of-two array dimensions that minimize max(b ,b ), the maximum 
rc 

number of bits needed to address the rows or columns of the array. 
Organization r cbb max(b ,b )
rc rc 
16 ¡Á 1 
16 ¡Á 4 128 ¡Á 8 512 ¡Á 4 1024 ¡Á 4 

Enhanced DRAMs 
There are many kinds of DRAM memories, and new kinds appear on the mar-ket with regularity as manufacturers attempt to keep up with rapidly increasing processor speeds. Each is based on the conventional DRAM cell, with optimiza-tions that improve the speed with which the basic DRAM cells can be accessed. 
. Fast page mode DRAM (FPM DRAM). A conventional DRAM copies an entire row of supercells into its internal row buffer, uses one, and then discards the rest. FPM DRAM improves on this by allowing consecutive accesses to the same row to be served directly from the row buffer. For example, to read four supercells from row i of a conventional DRAM, the memory controller must send four RAS/CAS requests, even though the row address i is identical in each case. To read supercells from the same row of an FPM DRAM, the memory controller sends an initial RAS/CAS request, followed by three CAS requests. The initial RAS/CAS request copies row i into the row buffer and returns the supercell addressed by the CAS. The next three supercells are served directly from the row buffer, and thus more quickly than the initial supercell. 
. Extended data out DRAM (EDO DRAM). An enhanced form of FPM DRAM that allows the individual CAS signals to be spaced closer together in time. 
. Synchronous DRAM (SDRAM). Conventional, FPM, and EDO DRAMs are asynchronous in the sense that they communicate with the memory controller using a set of explicit control signals. SDRAM replaces many of these control signals with the rising edges of the same external clock signal that drives the memory controller. Without going into detail, the net effect is that an SDRAM can output the contents of its supercells at a faster rate than its asynchronous counterparts. 
. Double Data-Rate Synchronous DRAM (DDR SDRAM). DDR SDRAM is an enhancement of SDRAM that doubles the speed of the DRAM by using both clock edges as control signals. Different types of DDR SDRAMs are characterized by the size of a small prefetch buffer that increases the effective bandwidth: DDR (2 bits), DDR2 (4 bits), and DDR3 (8 bits). 
. Rambus DRAM (RDRAM). This is an alternative proprietary technology with a higher maximum bandwidth than DDR SDRAM. 
. Video RAM (VRAM). Used in the frame buffers of graphics systems. VRAM is similar in spirit to FPM DRAM. Two major differences are that (1) VRAM output is produced by shifting the entire contents of the internal buffer in sequence, and (2) VRAM allows concurrent reads and writes to the memory. Thus, the system can be painting the screen with the pixels in the frame buffer (reads) while concurrently writing new values for the next update (writes). 

Aside Historical popularity of DRAM technologies 
Until 1995, most PCs were built with FPM DRAMs. From 1996 to 1999, EDO DRAMs dominated the market, while FPM DRAMs all but disappeared. SDRAMs .rst appeared in 1995 in high-end systems, and by 2002 most PCs were built with SDRAMs and DDR SDRAMs. By 2010, most server and desktop systems were built with DDR3 SDRAMs. In fact, the Intel Core i7 supports only DDR3 SDRAM. 
Nonvolatile Memory 
DRAMs and SRAMs are volatile in the sense that they lose their information if the supply voltage is turned off. Nonvolatile memories, on the other hand, retain their information even when they are powered off. There are a variety of nonvolatile memories. For historical reasons, they are referred to collectively as read-only memories (ROMs), even though some types of ROMs can be written to as well as read. ROMs are distinguished by the number of times they can be reprogrammed (written to) and by the mechanism for reprogramming them. 
A programmable ROM (PROM) can be programmed exactly once. PROMs include a sort of fuse with each memory cell that can be blown once by zapping it with a high current. 
An erasable programmable ROM (EPROM) has a transparent quartz window that permits light to reach the storage cells. The EPROM cells are cleared to zeros by shining ultraviolet light through the window. Programming an EPROM is done by using a special device to write ones into the EPROM. An EPROM can be erased and reprogrammed on the order of 1000 times. An electrically erasable PROM (EEPROM) is akin to an EPROM, but does not require a physically separate programming device, and thus can be reprogrammed in-place on printed circuit cards. An EEPROM can be reprogrammed on the order of 105 times before it wears out. 
Flash memory is a type of nonvolatile memory, based on EEPROMs, that has become an important storage technology. Flash memories are everywhere, providing fast and durable nonvolatile storage for a slew of electronic devices, including digital cameras, cell phones, music players, PDAs, and laptop, desktop, and server computer systems. In Section 6.1.3, we will look in detail at a new form of .ash-based disk drive, known as a solid state disk (SSD), that provides a faster, sturdier, and less power-hungry alternative to conventional rotating disks. 
Programs stored in ROM devices are often referred to as .rmware. When a computer system is powered up, it runs .rmware stored in a ROM. Some systems provide a small set of primitive input and output functions in .rmware, for example, a PC¡¯s BIOS (basic input/output system) routines. Complicated devices such as graphics cards and disk drive controllers also rely on .rmware to translate I/O (input/output) requests from the CPU. 
Accessing Main Memory 
Data .ows back and forth between the processor and the DRAM main memory over shared electrical conduits called buses. Each transfer of data between the CPU and memory is accomplished with a series of steps called a bus transaction. A read transaction transfers data from the main memory to the CPU. A write transaction transfers data from the CPU to the main memory. 
A bus is a collection of parallel wires that carry address, data, and control signals. Depending on the particular bus design, data and address signals can share the same set of wires, or they can use different sets. Also, more than two devices can share the same bus. The control wires carry signals that synchronize the transaction and identify what kind of transaction is currently being performed. For example, 
Figure 6.6 

Example bus structure that connects the CPU and main memory. 

is this transaction of interest to the main memory, or to some other I/O device such as a disk controller? Is the transaction a read or a write? Is the information on the bus an address or a data item? 
Figure 6.6 shows the con.guration of an example computer system. The main components are the CPU chip, a chipset that we will call an I/O bridge (which includes the memory controller), and the DRAM memory modules that make up main memory. These components are connected by a pair of buses: a system bus that connects the CPU to the I/O bridge, and a memory bus that connects the I/O bridge to the main memory. 
The I/O bridge translates the electrical signals of the system bus into the electrical signals of the memory bus. As we will see, the I/O bridge also connects the system bus and memory bus to an I/O bus that is shared by I/O devices such as disks and graphics cards. For now, though, we will focus on the memory bus. 

Aside A note on bus designs 
Bus design is a complex and rapidly changing aspect of computer systems. Different vendors develop different bus architectures as a way to differentiate their products. For example, Intel systems use chipsets known as the northbridge and the southbridge to connect the CPU to memory and I/O devices, respectively. In older Pentium and Core 2 systems, a front side bus (FSB) connects the CPU to the northbridge. Systems from AMD replace the FSB with the HyperTransport interconnect, while newer Intel Core i7 systems use the QuickPath interconnect. The details of these different bus architectures are beyond the scope of this text. Instead, we will use the high-level bus architecture from Figure 6.6 as a running example throughout the text. It is a simple but useful abstraction that allows us to be concrete, and captures the main ideas without being tied too closely to the detail of any proprietary designs. 
Consider what happens when the CPU performs a load operation such as 
movl A,%eax 
where the contents of address A are loaded into register %eax. Circuitry on the CPU chip called the bus interface initiates a read transaction on the bus. The read transaction consists of three steps. First, the CPU places the address A on the system bus. The I/O bridge passes the signal along to the memory bus (Figure 6.7(a)). Next, the main memory senses the address signal on the memory 
Register file 
%eax 


Main
I/O 
memory
bridge 
0
A 

X  


A 

(a)
 CPU places address A on the memory bus. 

(b)
 Main memory reads A from the bus, retrieves word x, and places it on the bus. Register file 



%eax 


Main memory 0 
A

X  



(c) CPU reads word x from the bus, and copies it into register %eax. 
Figure 6.7 Memory read transaction for a load operation: movl A,%eax. 
bus, reads the address from the memory bus, fetches the data word from the DRAM, and writes the data to the memory bus. The I/O bridge translates the memory bus signal into a system bus signal, and passes it along to the system bus (Figure 6.7(b)). Finally, the CPU senses the data on the system bus, reads it from the bus, and copies it to register %eax (Figure 6.7(c)). 
Conversely, when the CPU performs a store instruction such as 
movl %eax,A 

where the contents of register %eax are written to address A, the CPU initiates a write transaction. Again, there are three basic steps. First, the CPU places the address on the system bus. The memory reads the address from the memory bus and waits for the data to arrive (Figure 6.8(a)). Next, the CPU copies the data word in %eax to the system bus (Figure 6.8(b)). Finally, the main memory reads the data word from the memory bus and stores the bits in the DRAM (Figure 6.8(c)). 
%eax 

Main
I/O 
memory
bridge 

0
A 
A 
(a) CPU places address A on the memory bus. Main memory reads it and waits for the data word. 
0 A 



y  


Main memory 0 
A 
(c) Main memory reads data word y from the bus and stores it at address A. 
Figure 6.8 Memory write transaction for a store operation: movl %eax,A. 
6.1.2 Disk Storage 
Disks are workhorse storage devices that hold enormous amounts of data, on the order of hundreds to thousands of gigabytes, as opposed to the hundreds or thousands of megabytes in a RAM-based memory. However, it takes on the order of milliseconds to read information from a disk, a hundred thousand times longer than from DRAM and a million times longer than from SRAM. 
Disk Geometry 
Disks are constructed from platters. Each platter consists of two sides, or surfaces, that are coated with magnetic recording material. A rotating spindle in the center of the platter spins the platter at a .xed rotational rate, typically between 5400 and 
Gaps 
Cylinder k 


Surface 0 Surface 1 
Platter 0 Surface 2 Surface 3 
Platter 1 
Surface 4 Surface 5 
Platter 2 

(b) Multiple-platter view 

15,000 revolutions per minute (RPM). A disk will typically contain one or more of these platters encased in a sealed container. 
Figure 6.9(a) shows the geometry of a typical disk surface. Each surface consists of a collection of concentric rings called tracks. Each track is partitioned into a collection of sectors. Each sector contains an equal number of data bits (typically 512 bytes) encoded in the magnetic material on the sector. Sectors are separated by gaps where no data bits are stored. Gaps store formatting bits that identify sectors. 
A disk consists of one or more platters stacked on top of each other and encased in a sealed package, as shown in Figure 6.9(b). The entire assembly is often referred to as a disk drive, although we will usually refer to it as simply a disk. We will sometime refer to disks as rotating disks to distinguish them from .ash-based solid state disks (SSDs), which have no moving parts. 
Disk manufacturers describe the geometry of multiple-platter drives in terms of cylinders, where a cylinder is the collection of tracks on all the surfaces that are equidistant from the center of the spindle. For example, if a drive has three platters and six surfaces, and the tracks on each surface are numbered consistently, then cylinder k is the collection of the six instances of track k. 
Disk Capacity 

The maximum number of bits that can be recorded by a disk is known as its max-imum capacity, or simply capacity. Disk capacity is determined by the following technology factors: 
.  Recording density (bits/in): The number of bits that can be squeezed into a  
1-inch segment of a track.  
.  Track density (tracks/in): The number of tracks that can be squeezed into a  
1-inch segment of the radius extending from the center of the platter.  

. Areal density (bits/in2): The product of the recording density and the track density. 
Disk manufacturers work tirelessly to increase areal density (and thus capac-ity), and this is doubling every few years. The original disks, designed in an age of low areal density, partitioned every track into the same number of sectors, which was determined by the number of sectors that could be recorded on the innermost track. To maintain a .xed number of sectors per track, the sectors were spaced far-ther apart on the outer tracks. This was a reasonable approach when areal densities were relatively low. However, as areal densities increased, the gaps between sec-tors (where no data bits were stored) became unacceptably large. Thus, modern high-capacity disks use a technique known as multiple zone recording, where the set of cylinders is partitioned into disjoint subsets known as recording zones. Each zone consists of a contiguous collection of cylinders. Each track in each cylinder in a zone has the same number of sectors, which is determined by the number of sec-tors that can be packed into the innermost track of the zone. Note that diskettes (.oppy disks) still use the old-fashioned approach, with a constant number of sectors per track. 
The capacity of a disk is given by the following formula: 
# bytes average # sectors # tracks # surfaces # platters 
Disk capacity =¡Á ¡Á¡Á¡Á 
sector track surface platter disk 
For example, suppose we have a disk with 5 platters, 512 bytes per sector, 20,000 tracks per surface, and an average of 300 sectors per track. Then the capacity of the disk is: 
512 bytes 300 sectors 20,000 tracks 2 surfaces 5 platters 
Disk capacity =¡Á¡Á ¡Á¡Á 
sector track surface platter disk 
= 30,720,000,000 bytes 
= 30.72 GB. 
Notice that manufacturers express disk capacity in units of gigabytes (GB), where 1GB = 109 bytes. 

Aside How much is a gigabyte? 
Unfortunately, the meanings of pre.xes such as kilo (K), mega (M), giga (G), and tera (T ) depend 
on the context. For measures that relate to the capacity of DRAMs and SRAMs, typically K = 210, M = 220, G = 230 = 240 
, and T . For measures related to the capacity of I/O devices such as disks and networks, typically K = 103, M = 106, G = 109, and T = 1012. Rates and throughputs usually use these pre.x values as well. 
Fortunately, for the back-of-the-envelope estimates that we typically rely on, either assump-tion works .ne in practice. For example, the relative difference between 220 = 1,048,576 and 106 = 1,000,000 is small: (220 . 106)/106 ¡Ö 5%. Similarly for 230 = 1,073,741,824 and 109 = 1,000,000,000: (230 . 109)/109 ¡Ö 7%. 
The disk surface spins at a fixed 
The read/write head
rotational rate 
is attached to the end 


Read/write headsof the arm and flies over the disk surface on a thin cushion of air Arm 
By moving radially, the arm can position the read/write head over any track Spindle (a) Single-platter view (b) Multiple-platter view 
Figure 6.10 Disk dynamics. 


Practice Problem 6.2 
What is the capacity of a disk with two platters, 10,000 cylinders, an average of 400 sectors per track, and 512 bytes per sector? 
Disk Operation 

Disks read and write bits stored on the magnetic surface using a read/write head connected to the end of an actuator arm, as shown in Figure 6.10(a). By moving the arm back and forth along its radial axis, the drive can position the head over any track on the surface. This mechanical motion is known as a seek. Once the head is positioned over the desired track, then as each bit on the track passes underneath, the head can either sense the value of the bit (read the bit) or alter the value of the bit (write the bit). Disks with multiple platters have a separate read/write head for each surface, as shown in Figure 6.10(b). The heads are lined up vertically and move in unison. At any point in time, all heads are positioned on the same cylinder. 
The read/write head at the end of the arm .ies (literally) on a thin cushion of air over the disk surface at a height of about 0.1 microns and a speed of about 80 km/h. This is analogous to placing the Sears Tower on its side and .ying it around the world at a height of 2.5 cm (1 inch) above the ground, with each orbit of the earth taking only 8 seconds! At these tolerances, a tiny piece of dust on the surface is like a huge boulder. If the head were to strike one of these boulders, the head would cease .ying and crash into the surface (a so-called head crash). For this reason, disks are always sealed in airtight packages. 
Disks read and write data in sector-sized blocks. The access time for a sector has three main components: seek time, rotational latency, and transfer time: 
. Seek time: To read the contents of some target sector, the arm .rst positions the head over the track that contains the target sector. The time required to move the arm is called the seek time. The seek time, Tseek, depends on the previous position of the head and the speed that the arm moves across the surface. The average seek time in modern drives, Tavg seek, measured by taking the mean of several thousand seeks to random sectors, is typically on the order of 3 to 9 ms. The maximum time for a single seek, Tmax seek, can be as high as 20 ms. 
. Rotational latency: Once the head is in position over the track, the drive waits for the .rst bit of the target sector to pass under the head. The performance of this step depends on both the position of the surface when the head arrives at the target sector and the rotational speed of the disk. In the worst case, the head just misses the target sector and waits for the disk to make a full rotation. Thus, the maximum rotational latency, in seconds, is given by 
1 60 secs 
T ¡Á
max rotation = 
RPM 1 min 
The average rotational latency, Tavg rotation, is simply half of Tmax rotation. 
. Transfer time: When the .rst bit of the target sector is under the head, the drive can begin to read or write the contents of the sector. The transfer time for one sector depends on the rotational speed and the number of sectors per track. Thus, we can roughly estimate the average transfer time for one sector in seconds as 
1 1 60 secs 
T =¡Á ¡Á
avg transfer 
RPM (average # sectors/track) 1 min 
We can estimate the average time to access the contents of a disk sector as the sum of the average seek time, the average rotational latency, and the average transfer time. For example, consider a disk with the following parameters: 
Parameter Value 
Rotational rate  7200 RPM  
Tavg seek  9ms  
Average # sectors/track  400  

For this disk, the average rotational latency (in ms) is 
T
avg rotation = 1/2 ¡Á Tmax rotation = 1/2 ¡Á (60 secs / 7200 RPM) ¡Á 1000 ms/sec ¡Ö 4ms 
The average transfer time is Tavg transfer = 60 / 7200 RPM ¡Á 1 / 400 sectors/track ¡Á 1000 ms/sec ¡Ö 0.02 ms 

Putting it all together, the total estimated access time is 
Taccess = Tavg seek + Tavg rotation + Tavg transfer 
= 9ms + 4ms + 0.02 ms = 13.02 ms 

This example illustrates some important points: 
. The time to access the 512 bytes in a disk sector is dominated by the seek time and the rotational latency. Accessing the .rst byte in the sector takes a long time, but the remaining bytes are essentially free. 
. Since the seek time and rotational latency are roughly the same, twice the seek time is a simple and reasonable rule for estimating disk access time. 
. The access time for a doubleword stored in SRAM is roughly 4 ns, and 60 ns for DRAM. Thus, the time to read a 512-byte sector-sized block from memory is roughly 256 ns for SRAM and 4000 ns for DRAM. The disk access time, roughly 10 ms, is about 40,000 times greater than SRAM, and about 2500 times greater than DRAM. The difference in access times is even more dramatic if we compare the times to access a single word. 
Practice Problem 6.3 
Estimate the average time (in ms) to access a sector on the following disk: 
Parameter Value 
Rotational rate 15,000 RPM T 8ms 
avg seek 

Average # sectors/track 500 
Logical Disk Blocks 
As we have seen, modern disks have complex geometries, with multiple surfaces and different recording zones on those surfaces. To hide this complexity from the operating system, modern disks present a simpler view of their geometry as a sequence of B sector-sized logical blocks, numbered 0, 1,...,B . 1. A small hardware/.rmware device in the disk package, called the disk controller, maintains the mapping between logical block numbers and actual (physical) disk sectors. 
When the operating system wants to perform an I/O operation such as reading a disk sector into main memory, it sends a command to the disk controller asking it to read a particular logical block number. Firmware on the controller performs a fast table lookup that translates the logical block number into a (surface, track, sector) triple that uniquely identi.es the corresponding physical sector. Hardware on the controller interprets this triple to move the heads to the appropriate cylinder, waits for the sector to pass under the head, gathers up the bits sensed by the head into a small memory buffer on the controller, and copies them into main memory. 
Aside Formatted disk capacity 
Before a disk can be used to store data, it must be formatted by the disk controller. This involves .lling in the gaps between sectors with information that identi.es the sectors, identifying any cylinders with surface defects and taking them out of action, and setting aside a set of cylinders in each zone as spares that can be called into action if one or more cylinders in the zone goes bad during the lifetime of the disk. The formatted capacity quoted by disk manufacturers is less than the maximum capacity because of the existence of these spare cylinders. 
Practice Problem 6.4 
Suppose thata1MB.le consisting of 512-byte logical blocks is stored on a disk drive with the following characteristics: 
Parameter Value 
Rotational rate  10,000 RPM  
Tavg seek  5ms  
Average # sectors/track  1000  
Surfaces  4  
Sector size  512 bytes  

For each case below, suppose that a program reads the logical blocks of the .le sequentially, one after the other, and that the time to position the head over the .rst block is Tavg seek + Tavg rotation. 
A. Best case: Estimate the optimal time (in ms) required to read the .le given the best possible mapping of logical blocks to disk sectors (i.e., sequential). 
B. Random case: Estimate the time (in ms) required to read the .le if blocks are mapped randomly to disk sectors. 
Connecting I/O Devices 
Input/output (I/O) devices such as graphics cards, monitors, mice, keyboards, and disks are connected to the CPU and main memory using an I/O bus such as Intel¡¯s Peripheral Component Interconnect (PCI) bus. Unlike the system bus and memory buses, which are CPU-speci.c, I/O buses such as PCI are designed to be independent of the underlying CPU. For example, PCs and Macs both incorporate the PCI bus. Figure 6.11 shows a typical I/O bus structure (modeled on PCI) that connects the CPU, main memory, and I/O devices. 
Although the I/O bus is slower than the system and memory buses, it can accommodate a wide variety of third-party I/O devices. For example, the bus in Figure 6.11 has three different types of devices attached to it. 


.  A Universal Serial Bus (USB) controller is a conduit for devices attached to  
a USB bus, which is a wildly popular standard for connecting a variety of  
peripheral I/O devices, including keyboards, mice, modems, digital cameras,  
game controllers, printers, external disk drives, and solid state disks. USB 2.0  
buses have a maximum bandwidth of 60 MB/s. USB 3.0 buses have a maximum  
bandwidth of 600 MB/s.  
.  A graphics card (or adapter) contains hardware and software logic that is  
responsible for painting the pixels on the display monitor on behalf of the  
CPU.  
.  A host bus adapter that connects one or more disks to the I/O bus using  
a communication protocol de.ned by a particular host bus interface.The  
two most popular such interfaces for disks are SCSI (pronounced ¡°scuzzy¡±)  
and SATA (pronounced ¡°sat-uh¡±). SCSI disks are typically faster and more  
expensive than SATA drives. A SCSI host bus adapter (often called a SCSI  
controller) can support multiple disk drives, as opposed to SATA adapters,  
which can only support one drive.  
Additional devices such as network adapters can be attached to the I/O bus by  

plugging the adapter into empty expansion slots on the motherboard that provide a direct electrical connection to the bus. 

(a) The CPU initiates a disk read by writing a command, logical block number, and destination memory address to the memory-mapped address associated with the disk. 
CPU chip 

Mouse Keyboard Monitor 
(b) The disk controller reads the sector and performs a DMA transfer into main memory. 
Accessing Disks 
While a detailed description of how I/O devices work and how they are pro-grammed is outside our scope here, we can give you a general idea. For example, Figure 6.12 summarizes the steps that take place when a CPU reads data from a disk. 
The CPU issues commands to I/O devices using a technique called memory-mapped I/O (Figure 6.12(a)). In a system with memory-mapped I/O, a block of 


(c) When the DMA transfer is complete, the disk controller noti.es the CPU with an interrupt. 
addresses in the address space is reserved for communicating with I/O devices. Each of these addresses is known as an I/O port. Each device is associated with (or mapped to) one or more ports when it is attached to the bus. 
As a simple example, suppose that the disk controller is mapped to port 0xa0. Then the CPU might initiate a disk read by executing three store instructions to address 0xa0: The .rst of these instructions sends a command word that tells the disk to initiate a read, along with other parameters such as whether to interrupt the CPU when the read is .nished. (We will discuss interrupts in Section 8.1.) The second instruction indicates the logical block number that should be read. The third instruction indicates the main memory address where the contents of the disk sector should be stored. 
After it issues the request, the CPU will typically do other work while the disk is performing the read. Recall that a 1 GHz processor witha1ns clock cycle can potentially execute 16 million instructions in the 16 ms it takes to read the disk. Simply waiting and doing nothing while the transfer is taking place would be enormously wasteful. 
After the disk controller receives the read command from the CPU, it trans-lates the logical block number to a sector address, reads the contents of the sector, and transfers the contents directly to main memory, without any intervention from the CPU (Figure 6.12(b)). This process, whereby a device performs a read or write bus transaction on its own, without any involvement of the CPU, is known as direct memory access (DMA). The transfer of data is known as a DMA transfer. 
After the DMA transfer is complete and the contents of the disk sector are safely stored in main memory, the disk controller noti.es the CPU by sending an interrupt signal to the CPU (Figure 6.12(c)). The basic idea is that an interrupt signals an external pin on the CPU chip. This causes the CPU to stop what it is currently working on and jump to an operating system routine. The routine records the fact that the I/O has .nished and then returns control to the point where the CPU was interrupted. 
Geometry attribute  Value  
Platters  4  
Surfaces (read/write heads)  8  
Surface diameter  3.5 in.  
Sector size  512 bytes  
Zones  15  
Cylinders  50,864  
Recording density (max)  628,000 bits/in.  
Track density  85,000 tracks/in.  
Areal density (max)  53.4 Gbits/sq. in.  
Formatted capacity  146.8 GB  
Figure 6.13 Seagate Cheetah 15K.4 geometry and performance. Source: www.seagate.com. 



Performance attribute  Value  
Rotational rate Avg. rotational latency Avg. seek time Sustained transfer rate  15,000 RPM 2 ms 4 ms 58¨C96 MB/s  

Anatomy of a Commercial Disk 
Disk manufacturers publish a lot of useful high-level technical information on their Web pages. For example, the Cheetah 15K.4 is a SCSI disk .rst manufactured by Seagate in 2005. If we consult the online product manual on the Seagate Web page, we can glean the geometry and performance information shown in Figure 6.13. 
Disk manufacturers rarely publish detailed technical information about the geometry of the individual recording zones. However, storage researchers at Carnegie Mellon University have developed a useful tool, called DIXtrac, that automatically discovers a wealth of low-level information about the geometry and performance of SCSI disks [92]. For example, DIXtrac is able to discover the detailed zone geometry of our example Seagate disk, which we¡¯ve shown in Fig-ure 6.14. Each row in the table characterizes one of the 15 zones. The .rst column gives the zone number, with zone 0 being the outermost and zone 14 the inner-most. The second column gives the number of sectors contained in each track in that zone. The third column shows the number of cylinders assigned to that zone, where each cylinder consists of eight tracks, one from each surface. Simi-larly, the fourth column gives the total number of logical blocks assigned to each zone, across all eight surfaces. (The tool was not able to extract valid data for the innermost zone, so these are omitted.) 
The zone map reveals some interesting facts about the Seagate disk. First, 
more sectors are packed into the outer zones (which have a larger circumference) 
than the inner zones. Second, each zone has more sectors than logical blocks (check this yourself). These spare sectors form a pool of spare cylinders.Ifthe recording material on a sector goes bad, the disk controller will automatically remap the logical blocks on that cylinder to an available spare. So we see that the notion of a logical block not only provides a simpler interface to the operating system, it also provides a level of indirection that enables the disk to be more robust. This general idea of indirection is very powerful, as we will see when we study virtual memory in Chapter 9. 

Zone  Sectors  Cylinders  Logical blocks  
number  per track  per zone  per zone  
(outer) 0  864  3201  22,076,928  
1  844  3200  21,559,136  
2  816  3400  22,149,504  
3  806  3100  19,943,664  
4  795  3100  19,671,480  
5  768  3400  20,852,736  
6  768  3450  21,159,936  
7  725  3650  21,135,200  
8  704  3700  20,804,608  
9  672  3700  19,858,944  
10  640  3700  18,913,280  
11  603  3700  17,819,856  
12  576  3707  17,054,208  
13  528  3060  12,900,096  
(inner) 14  ¡ª  ¡ª  ¡ª  
Figure 6.14 Seagate Cheetah 15K.4 zone map. Source: DIXtrac automatic disk drive characterization tool [92]. Data for zone 14 not available. 


Practice Problem 6.5 
Use the zone map in Figure 6.14 to determine the number of spare cylinders in the following zones: 
A. Zone 0 
B. Zone 8 

6.1.3 Solid State Disks 
A solid state disk (SSD) is a storage technology, based on .ash memory (Sec-tion 6.1.1), that in some situations is an attractive alternative to the conventional rotating disk. Figure 6.15 shows the basic idea. An SSD package plugs into a stan-dard disk slot on the I/O bus (typically USB or SATA) and behaves like any other 
Solid state disk (SSD) 

Flash memory Block 0 Block B-1 

Figure 6.15 Solid state disk (SSD). 
Reads  Writes  
Sequential read throughput  250 MB/s  Sequential write throughput  170 MB/s  
Random read throughput  140 MB/s  Random write throughput  14 MB/s  
Random read access time  30 ¦Ìs  Random write access time  300 ¦Ìs  

Figure 6.16 Performance characteristics of a typical solid state disk. Source: Intel X25-E SATA solid state drive product manual. 
disk, processing requests from the CPU to read and write logical disk blocks. An SSD package consists of one or more .ash memory chips, which replace the me-chanical drive in a conventional rotating disk, and a .ash translation layer, which is a hardware/.rmware device that plays the same role as a disk controller, trans-lating requests for logical blocks into accesses of the underlying physical device. 
SSDs have different performance characteristics than rotating disks. As shown in Figure 6.16, sequential reads and writes (where the CPU accesses logical disk blocks in sequential order) have comparable performance, with sequential read-ing somewhat faster than sequential writing. However, when logical blocks are accessed in random order, writing is an order of magnitude slower than reading. 
The difference between random reading and writing performance is caused by a fundamental property of the underlying .ash memory. As shown in Figure 6.15, a .ash memory consists of a sequence of B blocks, where each block consists of P pages. Typically, pages are 512¨C4KB in size, and a block consists of 32¨C128 pages, with total block sizes ranging from 16 KB to 512 KB. Data is read and written in units of pages. A page can be written only after the entire block to which it belongs has been erased (typically this means that all bits in the block are set to 1). However, once a block is erased, each page in the block can be written once with no further erasing. A blocks wears out after roughly 100,000 repeated writes. Once a block wears out it can no longer be used. 

Random writes are slow for two reasons. First, erasing a block takes a rela-tively long time, on the order of 1 ms, which is more than an order of magnitude longer than it takes to access a page. Second, if a write operation attempts to modify a page p that contains existing data (i.e., not all ones), then any pages in the same block with useful data must be copied to a new (erased) block before the write to page p can occur. Manufacturers have developed sophisticated logic in the .ash translation layer that attempts to amortize the high cost of erasing blocks and to minimize the number of internal copies on writes, but it is unlikely that random writing will ever perform as well as reading. 
SSDs have a number of advantages over rotating disks. They are built of semiconductor memory, with no moving parts, and thus have much faster random access times than rotating disks, use less power, and are more rugged. However, there are some disadvantages. First, because .ash blocks wear out after repeated writes, SSDs have the potential to wear out as well. Wear leveling logic in the .ash translation layer attempts to maximize the lifetime of each block by spreading erasures evenly across all blocks, but the fundamental limit remains. Second, SSDs are about 100 times more expensive per byte than rotating disks, and thus the typical storage capacities are 100 times less than rotating disks. However, SSD prices are decreasing rapidly as they become more popular, and the gap between the two appears to be decreasing. 
SSDs have completely replaced rotating disks in portable music devices, are popular as disk replacements in laptops, and have even begun to appear in desk-tops and servers. While rotating disks are here to stay, it is clear that SSDs are an important new storage technology. 
Practice Problem 6.6 
As we have seen, a potential drawback of SSDs is that the underlying .ash memory can wear out. For example, one major manufacturer guarantees 1 petabyte (1015 bytes) of random writes for their SSDs before they wear out. Given this assump-tion, estimate the lifetime (in years) of the SSD in Figure 6.16 for the following workloads: 
A. Worst case for sequential writes: The SSD is written to continuously at a rate of 170 MB/s (the average sequential write throughput of the device). 
B. Worst case for random writes: The SSD is written to continuously at a rate of 14 MB/s (the average random write throughput of the device). 
C. Average case: The SSD is written to at a rate of 20 GB/day (the average daily write rate assumed by some computer manufacturers in their mobile computer workload simulations). 
6.1.4 Storage Technology Trends 
There are several important concepts to take away from our discussion of storage technologies. 
Metric 1980 1985 1990 1995 2000 2005 2010 2010:1980 
$/MB 19,200 2900 320 256 100 75 60 320 Access (ns) 300 150 35 15 3 2 1.5 200 
(a) SRAM trends 

Metric 1980 1985 1990 1995 2000 2005 2010 2010:1980 
$/MB 8000 880 100 30 1 .1 0.06 130,000 Access (ns) 375 200 100 70 60 50 40 9 Typical size (MB) 0.064 0.256 4 16 64 2000 8,000 125,000 
(b) DRAM trends 

Metric 1980 1985 1990 1995 2000 2005 2010 2010:1980 
$/MB 500 100 8 0.30 0.01 0.005 0.0003 1,600,000 Seektime(ms) 87 75 28 10 8 5 3 29 Typical size (MB) 1 10 160 1000 20,000 160,000 1,500,000 1,500,000 
(c) Rotating disk trends 
Metric 1980 1985 1990 1995 2000 2003 2005 2010 2010:1980 
Intel CPU 8080 80286 80386 Pent. P-III Pent. 4 Core 2 Core i7 ¡ª 
Clock rate (MHz) 1 6 20 150 600 3300 2000 2500 2500 Cycle time (ns) 1000 166 50 6 1.6 0.30 0.50 0.4 2500 Cores 111111 2 4 4 Eff. cycle time (ns) 1000 166 50 6 1.6 0.30 0.25 0.10 10,000 
(d) CPU trends 

Figure 6.17 Storage and processing technology trends. 
Different storage technologies have different price and performance trade-offs. 
SRAM is somewhat faster than DRAM, and DRAM is much faster than disk. On the other hand, fast storage is always more expensive than slower storage. SRAM costs more per byte than DRAM. DRAM costs much more than disk. SSDs split the difference between DRAM and rotating disk. 
The price and performance properties of different storage technologies are changing at dramatically different rates. Figure 6.17 summarizes the price and performance properties of storage technologies since 1980, when the .rst PCs were introduced. The numbers were culled from back issues of trade magazines and the Web. Although they were collected in an informal survey, the numbers reveal some interesting trends. 
Since 1980, both the cost and performance of SRAM technology have im-
proved at roughly the same rate. Access times have decreased by a factor of about 
200 and cost per megabyte by a factor of 300 (Figure 6.17(a)). However, the trends for DRAM and disk are much more dramatic and divergent. While the cost per megabyte of DRAM has decreased by a factor of 130,000 (more than .ve orders of magnitude!), DRAM access times have decreased by only a factor of 10 or so (Fig-ure 6.17(b)). Disk technology has followed the same trend as DRAM and in even more dramatic fashion. While the cost of a megabyte of disk storage has plum-meted by a factor of more than 1,000,000 (more than six orders of magnitude!) since 1980, access times have improved much more slowly, by only a factor of 30 or so (Figure 6.17(c)). These startling long-term trends highlight a basic truth of memory and disk technology: it is easier to increase density (and thereby reduce cost) than to decrease access time. 

DRAM and disk performance are lagging behind CPU performance.As we see in Figure 6.17(d), CPU cycle times improved by a factor of 2500 between 1980 and 2010. If we look at the effective cycle time¡ªwhich we de.ne to be the cycle time of an individual CPU (processor) divided by the number of its processor cores¡ªthen the improvement between 1980 and 2010 is even greater, a factor of 10,000. The split in the CPU performance curve around 2003 re.ects the introduction of multi-core processors (see aside on next page). After this split, cycle times of individual cores actually increased a bit before starting to decrease again, albeit at a slower rate than before. 
Note that while SRAM performance lags, it is roughly keeping up. However, the gap between DRAM and disk performance and CPU performance is actually widening. Until the advent of multi-core processors around 2003, this performance gap was a function of latency, with DRAM and disk access times increasing more slowly than the cycle time of an individual processor. However, with the introduction of multiple cores, this performance gap is increasingly a function of throughput, with multiple processor cores issuing requests to the DRAM and disk in parallel. 
The various trends are shown quite clearly in Figure 6.18, which plots the access and cycle times from Figure 6.17 on a semi-log scale. 
1980 1985 1990 1995 2000 2003 2005 2010 
Year 


As we will see in Section 6.4, modern computers make heavy use of SRAM-based caches to try to bridge the processor-memory gap. This approach works because of a fundamental property of application programs known as locality, which we discuss next. 

Aside When cycle time stood still: the advent of multi-core processors 
The history of computers is marked by some singular events that caused profound changes in the industry and the world. Interestingly, these in.ection points tend to occur about once per decade: the development of Fortran in the 1950s, the introduction of the IBM 360 in the early 1960s, the dawn of the Internet (then called ARPANET) in the early 1970s, the introduction of the IBM PC in the early 1980s, and the creation of the World Wide Web in the early 1990s. 
The most recent such event occurred early in the 21st century, when computer manufacturers ran headlong into the so-called ¡°power wall,¡± discovering that they could no longer increase CPU clock frequencies as quickly because the chips would then consume too much power. The solution was to improve performance by replacing a single large processor with multiple smaller processor cores, each a complete processor capable of executing programs independently and in parallel with the other cores. This multi-core approach works in part because the power consumed by a processor is proportional to P = fCv2, where f is the clock frequency, C is the capacitance, and v is the voltage. The capacitance C is roughly proportional to the area, so the power drawn by multiple cores can be held constant as long as the total area of the cores is constant. As long as feature sizes continue to shrink at the exponential Moore¡¯s law rate, the number of cores in each processor, and thus its effective performance, will continue to increase. 
From this point forward, computers will get faster not because the clock frequency increases, but because the number of cores in each processor increases, and because architectural innovations increase the ef.ciency of programs running on those cores. We can see this trend clearly in Figure 6.18. CPU cycle time reached its lowest point in 2003 and then actually started to rise before leveling off and starting to decline again at a slower rate than before. However, because of the advent of multi-core processors (dual-core in 2004 and quad-core in 2007), the effective cycle time continues to decrease at close to its previous rate. 
Practice Problem 6.7 
Using the data from the years 2000 to 2010 in Figure 6.17(c), estimate the year when you will be able to buy a petabyte (1015 bytes) of rotating disk storage for $500. Assume constant dollars (no in.ation). 
6.2 
Locality 

Well-written computer programs tend to exhibit good locality. That is, they tend to reference data items that are near other recently referenced data items, or that were recently referenced themselves. This tendency, known as the principle of locality, is an enduring concept that has enormous impact on the design and performance of hardware and software systems. 

Locality is typically described as having two distinct forms: temporal locality and spatial locality. In a program with good temporal locality, a memory location that is referenced once is likely to be referenced again multiple times in the near future. In a program with good spatial locality, if a memory location is referenced once, then the program is likely to reference a nearby memory location in the near future. 
Programmers should understand the principle of locality because, in general, programs with good locality run faster than programs with poor locality. All levels of modern computer systems, from the hardware, to the operating system, to application programs, are designed to exploit locality. At the hardware level, the principle of locality allows computer designers to speed up main memory accesses by introducing small fast memories known as cache memories that hold blocks of the most recently referenced instructions and data items. At the operating system level, the principle of locality allows the system to use the main memory as a cache of the most recently referenced chunks of the virtual address space. Similarly, the operating system uses main memory to cache the most recently used disk blocks in the disk .le system. The principle of locality also plays a crucial role in the design of application programs. For example, Web browsers exploit temporal locality by caching recently referenced documents on a local disk. High-volume Web servers hold recently requested documents in front-end disk caches that satisfy requests for these documents without requiring any intervention from the server. 
6.2.1 Locality of References to Program Data 
Consider the simple function in Figure 6.19(a) that sums the elements of a vector. Does this function have good locality? To answer this question, we look at the reference pattern for each variable. In this example, the sum variable is referenced once in each loop iteration, and thus there is good temporal locality with respect to sum. On the other hand, since sum is a scalar, there is no spatial locality with respect to sum. 
As we see in Figure 6.19(b), the elements of vector v are read sequentially, one after the other, in the order they are stored in memory (we assume for convenience that the array starts at address 0). Thus, with respect to variable v, the function has good spatial locality but poor temporal locality since each vector element 
1  int  sumvec(int  v[N])  
2  {  
3  int  i,  sum  =  0;  Address  0  4  8  12  16  20  24  28  
4  Contents  v0  v1  v2  v3  v4  v5  v6  v7  
5 6  for  (i=0;i<N; sum += v[i];  i++)  Access order  1  2  3  4  5  6  7  8  
7  return  sum;  
8  }  
(a)  (b)  
Figure 6.19 (a) A function with good locality. (b) Reference pattern for vector v (N = 8). Notice how the vector elements are accessed in the same order that they are stored in memory. 


1 int sumarrayrows(int a[M][N]) 2 { 3 inti, j, sum =0; Address 0 4 8 12 16 20 4 Contents 
a00 a01 a02 a10 a11 a12 

5 for (i=0;i<M; i++) 
Accessorder 1 2 3 4 5 6 

6 for (j=0;j<N; j++) 7 sum += a[i][j]; 8 return sum; 
9 } 

(a) (b) 
Figure 6.20 (a) Another function with good locality. (b) Reference pattern for array a (M =2, N = 3). There is good spatial locality because the array is accessed in the same row-major order in which it is stored in memory. 
is accessed exactly once. Since the function has either good spatial or temporal locality with respect to each variable in the loop body, we can conclude that the sumvec function enjoys good locality. 
A function such as sumvec that visits each element of a vector sequentially is said to have a stride-1 reference pattern (with respect to the element size). We will sometimes refer to stride-1 reference patterns as sequential reference patterns. Visiting every kth element of a contiguous vector is called a stride-k reference pattern. Stride-1 reference patterns are a common and important source of spatial locality in programs. In general, as the stride increases, the spatial locality decreases. 
Stride is also an important issue for programs that reference multidimensional arrays. For example, consider the sumarrayrows function in Figure 6.20(a) that sums the elements of a two-dimensional array. The doubly nested loop reads the elements of the array in row-major order. That is, the inner loop reads the elements of the .rst row, then the second row, and so on. The sumarrayrows function enjoys good spatial locality because it references the array in the same row-major order that the array is stored (Figure 6.20(b)). The result is a nice stride-1 reference pattern with excellent spatial locality. 
Seemingly trivial changes to a program can have a big impact on its locality. For example, the sumarraycols function in Figure 6.21(a) computes the same result as the sumarrayrows function in Figure 6.20(a). The only difference is that we have interchanged the i and j loops. What impact does interchanging the loops have on its locality? The sumarraycols function suffers from poor spatial locality because it scans the array column-wise instead of row-wise. Since C arrays are laid out in memory row-wise, the result is a stride-N reference pattern, as shown in Figure 6.21(b). 
6.2.2 Locality of Instruction Fetches 
Since program instructions are stored in memory and must be fetched (read) by the CPU, we can also evaluate the locality of a program with respect to its instruction fetches. For example, in Figure 6.19 the instructions in the body of the 

1 int sumarraycols(int a[M][N]) 2 { 3 inti,j, sum=0; Address 0 4 8 12 16 20 4 Contents 
a00 a01 a02 a10 a11 a12 

5 for (j=0;j<N; j++) 
Accessorder 1 3 5 2 4 6 

6 for (i=0;i<M; i++) 7 sum += a[i][j]; 8 return sum; 
9 } 

(a) (b) 
Figure 6.21 (a) A function with poor spatial locality. (b) Reference pattern for array a (M =2, N = 3). The function has poor spatial locality because it scans memory with a stride-N reference pattern. 
for loop are executed in sequential memory order, and thus the loop enjoys good spatial locality. Since the loop body is executed multiple times, it also enjoys good temporal locality. 
An important property of code that distinguishes it from program data is that it is rarely modi.ed at run time. While a program is executing, the CPU reads its instructions from memory. The CPU rarely overwrites or modi.es these instructions. 
6.2.3 Summary of Locality 
In this section, we have introduced the fundamental idea of locality and have identi.ed some simple rules for qualitatively evaluating the locality in a program: 
. Programs that repeatedly reference the same variables enjoy good temporal locality. 
. For programs with stride-k reference patterns, the smaller the stride the better the spatial locality. Programs with stride-1 reference patterns have good spa-tial locality. Programs that hop around memory with large strides have poor spatial locality. 
. Loops have good temporal and spatial locality with respect to instruction fetches. The smaller the loop body and the greater the number of loop it-erations, the better the locality. 
Later in this chapter, after we have learned about cache memories and how they work, we will show you how to quantify the idea of locality in terms of cache hits and misses. It will also become clear to you why programs with good locality typically run faster than programs with poor locality. Nonetheless, knowing how to glance at a source code and getting a high-level feel for the locality in the program is a useful and important skill for a programmer to master. 
Practice Problem 6.8 
Permute the loops in the following function so that it scans the three-dimensional array a with a stride-1 reference pattern. 
1 int sumarray3d(int a[N][N][N]) 2 { 3 int i,j,k, sum=0; 4 5 for (i=0;i<N; i++) { 6 for (j=0;j<N; j++) { 7 for (k=0;k<N; k++) { 8 sum += a[k][i][j]; 
9 
} 

10 
} 


11 } 12 return sum; 
13 } 
Practice Problem 6.9 
The three functions in Figure 6.22 perform the same operation with varying de-grees of spatial locality. Rank-order the functions with respect to the spatial local-ity enjoyed by each. Explain how you arrived at your ranking. 

(a) An array of structs (b) The clear1 function 1 #define N 1000 1 void clear1(point *p, int n) 22 { 3 typedef struct { 3 int i, j; 
4 int vel[3]; 4 5 int acc[3]; 5 for (i=0;i<n; i++) { 
6 } point; 6 for (j=0;j<3; j++) 77 p[i].vel[j] = 0; 8 point p[N]; 8 for (j=0;j<3; j++) 
9 p[i].acc[j] = 0; 
10 } 

11 } Figure 6.22 Code examples for Practice Problem 6.9. 
(c) The clear2 function (d) The clear3 function 1 void clear2(point *p, int n) 1 void clear3(point *p, int n) 2 { 2 { 3 int i, j; 3 int i, j; 44 5 for (i=0;i<n; i++) { 5 for (j=0;j<3; j++) { 6 for (j=0;j<3; j++) { 6 for (i=0;i<n; i++) 
7 p[i].vel[j] = 0; 7 p[i].vel[j] = 0; 8 p[i].acc[j] = 0; 8 for (i=0;i<n; i++) 
9 } 9 p[i].acc[j] = 0; 
10 } 10 } 
11 } 11 } Figure 6.22 (continued) Code examples for Practice Problem 6.9. 
6.3 
The 
Memory 
Hierarchy 

Sections 6.1 and 6.2 described some fundamental and enduring properties of storage technology and computer software: 
. Storage technology: Different storage technologies have widely different ac-cess times. Faster technologies cost more per byte than slower ones and have less capacity. The gap between CPU and main memory speed is widening. 
. Computer software: Well-written programs tend to exhibit good locality. 
In one of the happier coincidences of computing, these fundamental properties of hardware and software complement each other beautifully. Their complemen-tary nature suggests an approach for organizing memory systems, known as the memory hierarchy, that is used in all modern computer systems. Figure 6.23 shows a typical memory hierarchy. In general, the storage devices get slower, cheaper, and larger as we move from higher to lower levels. At the highest level (L0) are a small number of fast CPU registers that the CPU can access in a single clock cycle. Next are one or more small to moderate-sized SRAM-based cache memories that can be accessed in a few CPU clock cycles. These are followed by a large DRAM-based main memory that can be accessed in tens to hundreds of clock cycles. Next are slow but enormous local disks. Finally, some systems even include an addi-tional level of disks on remote servers that can be accessed over a network. For example, distributed .le systems such as the Andrew File System (AFS) or the Network File System (NFS) allow a program to access .les that are stored on re-mote network-connected servers. Similarly, the World Wide Web allows programs to access remote .les stored on Web servers anywhere in the world. 

Smaller, faster, and 

L1: costlier (per byte) storage 
L2: devices 
L3: 
Larger, slower, and cheaper (per byte) storage devices 



CPU registers hold words retrieved from cache memory. L1 cache holds cache lines retrieved from L2 cache. 


L2 cache holds cache lines retrieved from L3 cache. 
L3 cache holds cache lines retrieved from memory. 
Main memory holds disk blocks retrieved from local disks. 

Local disks hold files retrieved from disks on remote network servers. 

Aside Other memory hierarchies 
We have shown you one example of a memory hierarchy, but other combinations are possible, and indeed common. For example, many sites back up local disks onto archival magnetic tapes. At some of these sites, human operators manually mount the tapes onto tape drives as needed. At other sites, tape robots handle this task automatically. In either case, the collection of tapes represents a level in the memory hierarchy, below the local disk level, and the same general principles apply. Tapes are cheaper per byte than disks, which allows sites to archive multiple snapshots of their local disks. The trade-off is that tapes take longer to access than disks. As another example, solid state disks are playing an increasingly important role in the memory hierarchy, bridging the gulf between DRAM and rotating disk. 
6.3.1 Caching in the Memory Hierarchy 
In general, a cache (pronounced ¡°cash¡±) is a small, fast storage device that acts as a staging area for the data objects stored in a larger, slower device. The process of using a cache is known as caching (pronounced ¡°cashing¡±). 
The central idea of a memory hierarchy is that for each k, the faster and smaller storage device at level k serves as a cache for the larger and slower storage device at level k + 1. In other words, each level in the hierarchy caches data objects from the next lower level. For example, the local disk serves as a cache for .les (such as Web pages) retrieved from remote disks over the network, the main memory serves as a cache for data on the local disks, and so on, until we get to the smallest cache of all, the set of CPU registers. 


Smaller, faster, more expensive Level k: device at level k caches a subset of the blocks from level k1. 

Data is copied between levels in block-sized transfer units. 

Larger, slower, cheaper storage Level k1: 
device at level k1 is partitioned into blocks. 

0  1  2  3  
4  5  6  7  
8  9  10  11  
12  13  14  15  

Figure 6.24 The basic principle of caching in a memory hierarchy. 
Figure 6.24 shows the general concept of caching in a memory hierarchy. The storage at level k + 1 is partitioned into contiguous chunks of data objects called blocks. Each block has a unique address or name that distinguishes it from other blocks. Blocks can be either .xed-sized (the usual case) or variable-sized (e.g., the remote HTML .les stored on Web servers). For example, the level k + 1 storage in Figure 6.24 is partitioned into 16 .xed-sized blocks, numbered 0 to 15. 
Similarly, the storage at level k is partitioned into a smaller set of blocks that are the same size as the blocks at level k + 1. At any point in time, the cache at level k contains copies of a subset of the blocks from level k + 1. For example, in Figure 6.24, the cache at level k has room for four blocks and currently contains copies of blocks 4, 9, 14, and 3. 
Data is always copied back and forth between level k and level k + 1 in block-sized transfer units. It is important to realize that while the block size is .xed between any particular pair of adjacent levels in the hierarchy, other pairs of levels can have different block sizes. For example, in Figure 6.23, transfers between L1 and L0 typically use one-word blocks. Transfers between L2 and L1 (and L3 and L2, and L4 and L3) typically use blocks of 8 to 16 words. And transfers between L5 and L4 use blocks with hundreds or thousands of bytes. In general, devices lower in the hierarchy (further from the CPU) have longer access times, and thus tend to use larger block sizes in order to amortize these longer access times. 
Cache Hits 

When a program needs a particular data object d from level k + 1, it .rst looks for d in one of the blocks currently stored at level k.If d happens to be cached at level k, then we have what is called a cache hit. The program reads d directly from level k, which by the nature of the memory hierarchy is faster than reading d from level k + 1. For example, a program with good temporal locality might read a data object from block 14, resulting in a cache hit from level k. 
Cache Misses 
If, on the other hand, the data object d is not cached at level k, then we have what is called a cache miss. When there is a miss, the cache at level k fetches the block containing d from the cache at level k + 1, possibly overwriting an existing block if the level k cache is already full. 
This process of overwriting an existing block is known as replacing or evicting the block. The block that is evicted is sometimes referred to as a victim block. The decision about which block to replace is governed by the cache¡¯s replacement policy. For example, a cache with a random replacement policy would choose a random victim block. A cache with a least-recently used (LRU) replacement policy would choose the block that was last accessed the furthest in the past. 
After the cache at level k has fetched the block from level k + 1, the program can read d from level k as before. For example, in Figure 6.24, reading a data object from block 12 in the level k cache would result in a cache miss because block 12 is not currently stored in the level k cache. Once it has been copied from level k + 1 to level k, block 12 will remain there in expectation of later accesses. 
Kinds of Cache Misses 
It is sometimes helpful to distinguish between different kinds of cache misses. If the cache at level k is empty, then any access of any data object will miss. An empty cache is sometimes referred to as a cold cache, and misses of this kind are called compulsory misses or cold misses. Cold misses are important because they are often transient events that might not occur in steady state, after the cache has been warmed up by repeated memory accesses. 
Whenever there is a miss, the cache at level k must implement some placement policy that determines where to place the block it has retrieved from level k + 1. The most .exible placement policy is to allow any block from level k + 1to be stored in any block at level k. For caches high in the memory hierarchy (close to the CPU) that are implemented in hardware and where speed is at a premium, this policy is usually too expensive to implement because randomly placed blocks are expensive to locate. 
Thus, hardware caches typically implement a more restricted placement policy that restricts a particular block at level k + 1 to a small subset (sometimes a singleton) of the blocks at level k. For example, in Figure 6.24, we might decide that a block i at level k + 1 must be placed in block (i mod 4) at level k. For example, blocks 0, 4, 8, and 12 at level k + 1 would map to block 0 at level k; blocks 1, 5, 9, and 13 would map to block 1; and so on. Notice that our example cache in Figure 6.24 uses this policy. 
Restrictive placement policies of this kind lead to a type of miss known as a con.ict miss, in which the cache is large enough to hold the referenced data objects, but because they map to the same cache block, the cache keeps missing. For example, in Figure 6.24, if the program requests block 0, then block 8, then block 0, then block 8, and so on, each of the references to these two blocks would miss in the cache at level k, even though this cache can hold a total of four blocks. 

Programs often run as a sequence of phases (e.g., loops) where each phase accesses some reasonably constant set of cache blocks. For example, a nested loop might access the elements of the same array over and over again. This set of blocks is called the working set of the phase. When the size of the working set exceeds the size of the cache, the cache will experience what are known as capacity misses. In other words, the cache is just too small to handle this particular working set. 
Cache Management 
As we have noted, the essence of the memory hierarchy is that the storage device at each level is a cache for the next lower level. At each level, some form of logic must manage the cache. By this we mean that something has to partition the cache storage into blocks, transfer blocks between different levels, decide when there are hits and misses, and then deal with them. The logic that manages the cache can be hardware, software, or a combination of the two. 
For example, the compiler manages the register .le, the highest level of the cache hierarchy. It decides when to issue loads when there are misses, and determines which register to store the data in. The caches at levels L1, L2, and L3 are managed entirely by hardware logic built into the caches. In a system with virtual memory, the DRAM main memory serves as a cache for data blocks stored on disk, and is managed by a combination of operating system software and address translation hardware on the CPU. For a machine with a distributed .le system such as AFS, the local disk serves as a cache that is managed by the AFS client process running on the local machine. In most cases, caches operate automatically and do not require any speci.c or explicit actions from the program. 
6.3.2 Summary of Memory Hierarchy Concepts 
To summarize, memory hierarchies based on caching work because slower storage is cheaper than faster storage and because programs tend to exhibit locality: 
. Exploiting temporal locality. Because of temporal locality, the same data ob-jects are likely to be reused multiple times. Once a data object has been copied into the cache on the .rst miss, we can expect a number of subsequent hits on that object. Since the cache is faster than the storage at the next lower level, these subsequent hits can be served much faster than the original miss. 
. Exploiting spatial locality. Blocks usually contain multiple data objects. Be-cause of spatial locality, we can expect that the cost of copying a block after a miss will be amortized by subsequent references to other objects within that block. 
Caches are used everywhere in modern systems. As you can see from Fig-ure 6.25, caches are used in CPU chips, operating systems, distributed .le systems, and on the World Wide Web. They are built from and managed by various com-binations of hardware and software. Note that there are a number of terms and acronyms in Figure 6.25 that we haven¡¯t covered yet. We include them here to demonstrate how common caches are. 
Type  What cached  Where cached  Latency (cycles)  Managed by  
CPU registers  4-byte or 8-byte word  On-chip CPU registers  0  Compiler  
TLB  Address translations  On-chip TLB  0  Hardware MMU  
L1 cache  64-byte block  On-chip L1 cache  1  Hardware  
L2 cache  64-byte block  On/off-chip L2 cache  10  Hardware  
L3 cache  64-byte block  On/off-chip L3 cache  30  Hardware  
Virtual memory  4-KB page  Main memory  100  Hardware + OS  
Buffer cache  Parts of .les  Main memory  100  OS  
Disk cache  Disk sectors  Disk controller  100,000  Controller .rmware  
Network cache  Parts of .les  Local disk  10,000,000  AFS/NFS client  
Browser cache  Web pages  Local disk  10,000,000  Web browser  
Web cache  Web pages  Remote server disks  1,000,000,000  Web proxy server  
Figure 6.25 The ubiquity of caching in modern computer systems. Acronyms: TLB: translation lookaside buffer, MMU: memory management unit, OS: operating system, AFS: Andrew File System, NFS: Network File System. 


6.4 
Cache 
Memories 

The memory hierarchies of early computer systems consisted of only three levels: CPU registers, main DRAM memory, and disk storage. However, because of the increasing gap between CPU and main memory, system designers were compelled to insert a small SRAM cache memory, called an L1 cache (Level 1 cache) between the CPU register .le and main memory, as shown in Figure 6.26. The L1 cache can be accessed nearly as fast as the registers, typically in 2 to 4 clock cycles. 
As the performance gap between the CPU and main memory continued to increase, system designers responded by inserting an additional larger cache, called an L2 cache, between the L1 cache and main memory, that can be accessed in about 10 clock cycles. Some modern systems include an additional even larger cache, called an L3 cache, which sits between the L2 cache and main memory in the memory hierarchy and can be accessed in 30 or 40 cycles. While there is considerable variety in the arrangements, the general principles are the same. For our discussion in the next section, we will assume a simple memory hierarchy with a single L1 cache between the CPU and main memory. 


6.4.1 Generic Cache Memory Organization 
Consider a computer system where each memory address has m bits that form M= 2m unique addresses. As illustrated in Figure 6.27(a), a cache for such a machine is organized as an array of S= 2s cache sets. Each set consists of Ecache lines. Each line consists of a data block of B= 2b bytes, a valid bit that indicates whether or not the line contains meaningful information, and t= m. (b+ s)tag bits (a subset of the bits from the current block¡¯s memory address) that uniquely identify the block stored in the cache line. 
In general, a cache¡¯s organization can be characterized by the tuple (S,E,B,m). The size (or capacity) of a cache, C, is stated in terms of the ag-gregate size of all the blocks. The tag bits and valid bit are not included. Thus, C= S¡Á E¡Á B. 
When the CPU is instructed by a load instruction to read a word from ad-dress Aof main memory, it sends the address Ato the cache. If the cache is holding a copy of the word at address A, it sends the word immediately back to the CPU. 
Figure 6.27 1 valid bit t tag bits 
B 2b bytesGeneral organization per line per line per cache block 
of cache (S,E,B,m). 




(a) A cache is an array of sets. Each 
Set 0: 

E lines per set 
set contains one or more lines. Each line contains a valid bit, some tag bits, and a 
Set 1:

block of data. (b) The 
S 2s sets 

cache organization induces a partition of the maddress bits into t tag bits, s set index bits, and bblock offset Set S 1: bits. 



(a) t bits s bits b bits 
Address: 
m1 0 

Tag Set index Block offset 
(b) 

Fundamental parameters 
Parameter Description 
S = 2s Number of sets E Number of lines per set B = 2b Block size (bytes) m = log2(M) Number of physical (main memory) address bits 
Derived quantities 
Parameter Description 
M = 2m Maximum number of unique memory addresses 
s = log2(S) Number of set index bits 
b = log2(B) Number of block offset bits 
t = m . (s + b) Number of tag bits 
C = B ¡Á E ¡Á S Cache size (bytes) not including overhead such as the valid and tag bits 
Figure 6.28 Summary of cache parameters. 
So how does the cache know whether it contains a copy of the word at address A? The cache is organized so that it can .nd the requested word by simply inspect-ing the bits of the address, similar to a hash table with an extremely simple hash function. Here is how it works: 
The parameters S and B induce a partitioning of the m address bits into the three .elds shown in Figure 6.27(b). The s set index bits in A form an index into the array of S sets. The .rst set is set 0, the second set is set 1, and so on. When interpreted as an unsigned integer, the set index bits tell us which set the word must be stored in. Once we know which set the word must be contained in, the t tag bits in A tell us which line (if any) in the set contains the word. A line in the set contains the word if and only if the valid bit is set and the tag bits in the line match the tag bits in the address A. Once we have located the line identi.ed by the tag in the set identi.ed by the set index, then the b block offset bits give us the offset of the word in the B-byte data block. 
As you may have noticed, descriptions of caches use a lot of symbols. Fig-ure 6.28 summarizes these symbols for your reference. 
Practice Problem 6.10 
The following table gives the parameters for a number of different caches. For each cache, determine the number of cache sets (S), tag bits (t), set index bits (s), and block offset bits (b). 

Section 6.4  Cache Memories  599  
Cache  m  C  B  E  S  t  s b  
1.  32  1024  4  1  
2.  32  1024  8  4  
3.  32  1024  32  32  

6.4.2 Direct-Mapped Caches 
Caches are grouped into different classes based on E, the number of cache lines per set. A cache with exactly one line per set (E = 1) is known as a direct-mapped cache (see Figure 6.29). Direct-mapped caches are the simplest both to implement and to understand, so we will use them to illustrate some general concepts about how caches work. 
Suppose we have a system with a CPU, a register .le, an L1 cache, and a main memory. When the CPU executes an instruction that reads a memory word w, it requests the word from the L1 cache. If the L1 cache has a cached copy of w, then we have an L1 cache hit, and the cache quickly extracts w and returns it to the CPU. Otherwise, we have a cache miss, and the CPU must wait while the L1 cache requests a copy of the block containing w from the main memory. When the requested block .nally arrives from memory, the L1 cache stores the block in one of its cache lines, extracts word w from the stored block, and returns it to the CPU. The process that a cache goes through of determining whether a request is a hit or a miss and then extracting the requested word consists of three steps: (1) set selection, (2) line matching, and (3) word extraction. 
Set Selection in Direct-Mapped Caches 
In this step, the cache extracts the s set index bits from the middle of the address for w. These bits are interpreted as an unsigned integer that corresponds to a set number. In other words, if we think of the cache as a one-dimensional array of sets, then the set index bits form an index into this array. Figure 6.30 shows how set selection works for a direct-mapped cache. In this example, the set index bits 000012 are interpreted as an integer index that selects set 1. 
Line Matching in Direct-Mapped Caches 
Now that we have selected some set i in the previous step, the next step is to determine if a copy of the word w is stored in one of the cache lines contained in 
Figure 6.29 
Set 0:

Direct-mapped cache (E = 1). There is exactly 
Set 1: 
one line per set. 

. . . 



Tag Set index Block offset 
Figure 6.30 Set selection in a direct-mapped cache. 
set i. In a direct-mapped cache, this is easy and fast because there is exactly one line per set. A copy of w is contained in the line if and only if the valid bit is set and the tag in the cache line matches the tag in the address of w. 
Figure 6.31 shows how line matching works in a direct-mapped cache. In this example, there is exactly one cache line in the selected set. The valid bit for this line is set, so we know that the bits in the tag and block are meaningful. Since the tag bits in the cache line match the tag bits in the address, we know that a copy of the word we want is indeed stored in the line. In other words, we have a cache hit. On the other hand, if either the valid bit were not set or the tags did not match, then we would have had a cache miss. 
Word Selection in Direct-Mapped Caches 
Once we have a hit, we know that w is somewhere in the block. This last step determines where the desired word starts in the block. As shown in Figure 6.31, the block offset bits provide us with the offset of the .rst byte in the desired word. Similar to our view of a cache as an array of lines, we can think of a block as an array of bytes, and the byte offset as an index into that array. In the example, the block offset bits of 1002 indicate that the copy of w starts at byte 4 in the block. (We are assuming that words are 4 bytes long.) 
Line Replacement on Misses in Direct-Mapped Caches 
If the cache misses, then it needs to retrieve the requested block from the next level in the memory hierarchy and store the new block in one of the cache lines of 

Figure 6.31 1? (1) The valid bit must be set. 
Line matching and word 
01234567

selection in a direct-
Selected set (i):

mapped cache. Within the cache block, w0 denotes 
(3) If (1) and (2), then

the low-order byte of the 
(2) The tag bits in the 
 ? cache hit, and

word w, w1 the next byte, cache line must 
block offset selects and so on. match the tag bits 
starting byte. in the address. t bits s bits b bits 0110 i 100 m1 0 Tag Set index Block offset 

Address bits  
Address  Tag bits  Index bits  Offset bits  Block number  
(decimal)  (t= 1)  (s= 2)  (b= 1)  (decimal)  

0  0  00  0  0  
1  0  00  1  0  
2  0  01  0  1  
3  0  01  1  1  
4  0  10  0  2  
5  0  10  1  2  
6  0  11  0  3  
7  0  11  1  3  
8  1  00  0  4  
9  1  00  1  4  
10  1  01  0  5  
11  1  01  1  5  
12  1  10  0  6  
13  1  10  1  6  
14  1  11  0  7  
15  1  11  1  7  
Figure 6.32 4-bit address space for example direct-mapped cache. 


the set indicated by the set index bits. In general, if the set is full of valid cache lines, then one of the existing lines must be evicted. For a direct-mapped cache, where each set contains exactly one line, the replacement policy is trivial: the current line is replaced by the newly fetched line. 
Putting It Together: A Direct-Mapped Cache in Action 
The mechanisms that a cache uses to select sets and identify lines are extremely simple. They have to be, because the hardware must perform them in a few nanoseconds. However, manipulating bits in this way can be confusing to us humans. A concrete example will help clarify the process. Suppose we have a direct-mapped cache described by 
(S,E,B,m)= (4,1,2,4) 

In other words, the cache has four sets, one line per set, 2 bytes per block, and 4-bit addresses. We will also assume that each word is a single byte. Of course, these assumptions are totally unrealistic, but they will help us keep the example simple. 
When you are .rst learning about caches, it can be very instructive to enumer-ate the entire address space and partition the bits, as we¡¯ve done in Figure 6.32 for our 4-bit example. There are some interesting things to notice about this enumer-ated space: 
. The concatenation of the tag and index bits uniquely identi.es each block in memory. For example, block 0 consists of addresses 0 and 1, block 1 consists of addresses 2 and 3, block 2 consists of addresses 4 and 5, and so on. 
. Since there are eight memory blocks but only four cache sets, multiple blocks map to the same cache set (i.e., they have the same set index). For example, blocks 0 and 4 both map to set 0, blocks 1 and 5 both map to set 1, and so on. 
. Blocks that map to the same cache set are uniquely identi.ed by the tag. For example, block 0 has a tag bit of 0 while block 4 has a tag bit of 1, block 1 has a tag bit of 0 while block 5 has a tag bit of 1, and so on. 
Let us simulate the cache in action as the CPU performs a sequence of reads. Remember that for this example, we are assuming that the CPU reads 1-byte words. While this kind of manual simulation is tedious and you may be tempted to skip it, in our experience students do not really understand how caches work until they work their way through a few of them. 
Initially, the cache is empty (i.e., each valid bit is zero): 
Set Valid Tag block[0] block[1] 
0 1 2 3  0 0 0 0  

Each row in the table represents a cache line. The .rst column indicates the set that the line belongs to, but keep in mind that this is provided for convenience and is not really part of the cache. The next three columns represent the actual bits in each cache line. Now, let us see what happens when the CPU performs a sequence of reads: 
1. Read word at address 0. Since the valid bit for set 0 is zero, this is a cache miss. The cache fetches block 0 from memory (or a lower-level cache) and stores the block in set 0. Then the cache returns m[0] (the contents of memory location 0) from block[0] of the newly fetched cache line. 
Set Valid Tag block[0] block[1] 
0 1 2 3  1 0 0 0  0  m[0]  m[1]  

2. 
Read word at address 1. This is a cache hit. The cache immediately returns m[1] from block[1] of the cache line. The state of the cache does not change. 

3. 
Read word at address 13. Since the cache line in set 2 is not valid, this is a cache miss. The cache loads block 6 into set 2 and returns m[13] from block[1] of the new cache line. 



Set Valid Tag block[0] block[1] 
0 1 2 3  1 0 1 0  0 1  m[0] m[12]  m[1] m[13]  

4. Read word at address 8. This is a miss. The cache line in set 0 is indeed valid, but the tags do not match. The cache loads block 4 into set 0 (replacing the line that was there from the read of address 0) and returns m[8] from block[0] of the new cache line. 
Set Valid Tag block[0] block[1] 
0 1 2 3  1 0 1 0  1 1  m[8] m[12]  m[9] m[13]  

5. Read word at address 0. This is another miss, due to the unfortunate fact that we just replaced block 0 during the previous reference to address 8. This kind of miss, where we have plenty of room in the cache but keep alternating references to blocks that map to the same set, is an example of a con.ict miss. 
Set Valid Tag block[0] block[1] 
0 1 2 3  1 0 1 0  0 1  m[0] m[12]  m[1] m[13]  

Con.ict Misses in Direct-Mapped Caches 
Con.ict misses are common in real programs and can cause baf.ing performance problems. Con.ict misses in direct-mapped caches typically occur when programs access arrays whose sizes are a power of 2. For example, consider a function that computes the dot product of two vectors: 
1  float  dotprod(float  x[8],  float  y[8])  
2  {  
3  float  sum  =  0.0;  
4  int  i;  
5  
6  for  (i=0;i<8;  i++)  
7  sum  +=  x[i]  * y[i];  
8  return  sum;  
9  }  

This function has good spatial locality with respect to x and y, and so we might expect it to enjoy a good number of cache hits. Unfortunately, this is not always true. 
Suppose that .oats are 4 bytes, that x is loaded into the 32 bytes of contiguous memory starting at address 0, and that y starts immediately after x at address 32. For simplicity, suppose that a block is 16 bytes (big enough to hold four .oats) and that the cache consists of two sets, for a total cache size of 32 bytes. We will assume that the variable sum is actually stored in a CPU register and thus does not require a memory reference. Given these assumptions, each x[i] and y[i] will map to the identical cache set: 
Element Address Set index Element Address Set index 
x[0] x[1] x[2] x[3] x[4] x[5] x[6] x[7]  0 4 8 12 16 20 24 28  0 0 0 0 1 1 1 1  y[0] y[1] y[2] y[3] y[4] y[5] y[6] y[7]  32 36 40 44 48 52 56 60  0 0 0 0 1 1 1 1  

At run time, the .rst iteration of the loop references x[0], a miss that causes the block containing x[0]¨Cx[3] to be loaded into set 0. The next reference is to y[0], another miss that causes the block containing y[0]¨Cy[3] to be copied into set 0, overwriting the values of x that were copied in by the previous reference. During the next iteration, the reference to x[1] misses, which causes the x[0]¨C x[3] block to be loaded back into set 0, overwriting the y[0]¨Cy[3] block. So now we have a con.ict miss, and in fact each subsequent reference to x and y will result in a con.ict miss as we thrash back and forth between blocks of x and y. The term thrashing describes any situation where a cache is repeatedly loading and evicting the same sets of cache blocks. 
The bottom line is that even though the program has good spatial locality and we have room in the cache to hold the blocks for both x[i] and y[i], each reference results in a con.ict miss because the blocks map to the same cache set. It is not unusual for this kind of thrashing to result in a slowdown by a factor of 2 or 
3. Also, be aware that even though our example is extremely simple, the problem is real for larger and more realistic direct-mapped caches. 
Luckily, thrashing is easy for programmers to .x once they recognize what is going on. One easy solution is to put B bytes of padding at the end of each array. For example, instead of de.ning x to be float x[8], we de.ne it to be float x[12]. Assuming y starts immediately after x in memory, we have the following mapping of array elements to sets: 

Element Address Set index Element Address Set index 
x[0] x[1] x[2] x[3] x[4] x[5] x[6] x[7]  0 4 8 12 16 20 24 28  0 0 0 0 1 1 1 1  y[0] y[1] y[2] y[3] y[4] y[5] y[6] y[7]  48 52 56 60 64 68 72 76  1 1 1 1 0 0 0 0  

With the padding at the end of x, x[i] and y[i] now map to different sets, which eliminates the thrashing con.ict misses. 
Practice Problem 6.11 
In the previous dotprod example, what fraction of the total references to x and y will be hits once we have padded array x? 
Practice Problem 6.12 
In general, if the high-order sbits of an address are used as the set index, contigu-ous chunks of memory blocks are mapped to the same cache set. 
A. How many blocks are in each of these contiguous array chunks? 
B. Consider the following code that runs on a system with a cache of the form (S,E,B,m)= (512,1,32,32): 
int array[4096]; 
for(i =0; i <4096; i++) sum += array[i]; 
What is the maximum number of array blocks that are stored in the cache at any point in time? 
Aside Why index with the middle bits? 
You may be wondering why caches use the middle bits for the set index instead of the high-order bits. There is a good reason why the middle bits are better. Figure 6.33 shows why. If the high-order bits are used as an index, then some contiguous memory blocks will map to the same cache set. For example, in the .gure, the .rst four blocks map to the .rst cache set, the second four blocks map to the second set, and so on. If a program has good spatial locality and scans the elements of an array sequentially, then the cache can only hold a block-sized chunk of the array at any point in time. This is an inef.cient use of the cache. Contrast this with middle-bit indexing, where adjacent blocks always map to different cache lines. In this case, the cache can hold an entire C-sized chunk of the array, where Cis the cache size. 
High-order Middle-order bit indexing bit indexing 
0000 

0000 
0001 
0001 
0010 
0010 
0011 
0011 
0100 
0100 Four-set cache 
0101 
0101 
00 0110 0110 
01 0111 0111 
10 1000 1000 
11 

1001 
1001 
1010 
1010 
1011 
1011 
1100 
1100 
1101 
1101 
1110 
1110 
1111 
1111 
Set index bits 

Why caches index with the middle bits. 

Figure 6.33 
6.4.3 Set Associative Caches 
The problem with con.ict misses in direct-mapped caches stems from the con-straint that each set has exactly one line (or in our terminology, E= 1). A set associative cache relaxes this constraint so each set holds more than one cache line. A cache with 1 <E<C/Bis often called an E-way set associative cache. We will discuss the special case, where E= C/B, in the next section. Figure 6.34 shows 
the organization of a two-way set associative cache. 
Figure 6.34 

Set associative cache (1 <E<C/B). In a set associative cache, each set contains more than one line. This particular example shows a two-way set associative cache. 
Set 0: 
Set 1: E 2 lines per set 

. . . 
Set S 1: 
Valid  Tag  Cache block  
Valid  Tag  Cache block  

Set 0: 

Selected set 


Set 1: 

. . . 
Set S 1:

t bits 
s bits b bits 0 0 0 0 1 
Valid  Tag  Cache block  
Valid  Tag  Cache block  


m1 0 Tag Set index Block offset 
Figure 6.35 Set selection in a set associative cache. 
Set Selection in Set Associative Caches 
Set selection is identical to a direct-mapped cache, with the set index bits identi-fying the set. Figure 6.35 summarizes this principle. 
Line Matching and Word Selection in Set Associative Caches 
Line matching is more involved in a set associative cache than in a direct-mapped cache because it must check the tags and valid bits of multiple lines in order to determine if the requested word is in the set. A conventional memory is an array of values that takes an address as input and returns the value stored at that address. An associative memory, on the other hand, is an array of (key, value) pairs that takes as input the key and returns a value from one of the (key, value) pairs that matches the input key. Thus, we can think of each set in a set associative cache as a small associative memory where the keys are the concatenation of the tag and valid bits, and the values are the contents of a block. 
Figure 6.36 shows the basic idea of line matching in an associative cache. An important idea here is that any line in the set can contain any of the memory blocks 

Figure 6.36  1?  (1) The valid bit must be set.  
Line matching and  
 
word selection in a set  
associative cache.  Selected set (i):  
(2) The tag bits in one of the cache lines  ?  
must match the tag  
bits in the address.  t bits  s bits  b bits 
0110  i  100 
m  1  0  

Tag Set index Block offset cache hit, and block offset selects starting byte. 


that map to that set. So the cache must search each line in the set, searching for a valid line whose tag matches the tag in the address. If the cache .nds such a line, then we have a hit and the block offset selects a word from the block, as before. 
Line Replacement on Misses in Set Associative Caches 
If the word requested by the CPU is not stored in any of the lines in the set, then we have a cache miss, and the cache must fetch the block that contains the word from memory. However, once the cache has retrieved the block, which line should it replace? Of course, if there is an empty line, then it would be a good candidate. But if there are no empty lines in the set, then we must choose one of the nonempty lines and hope that the CPU does not reference the replaced line anytime soon. 
It is very dif.cult for programmers to exploit knowledge of the cache replace-ment policy in their codes, so we will not go into much detail about it here. The simplest replacement policy is to choose the line to replace at random. Other more sophisticated policies draw on the principle of locality to try to minimize the prob-ability that the replaced line will be referenced in the near future. For example, a least-frequently-used (LFU) policy will replace the line that has been referenced the fewest times over some past time window. A least-recently-used (LRU) policy will replace the line that was last accessed the furthest in the past. All of these policies require additional time and hardware. But as we move further down the memory hierarchy, away from the CPU, the cost of a miss becomes more expen-sive and it becomes more worthwhile to minimize misses with good replacement policies. 
6.4.4 Fully Associative Caches 
A fully associative cache consists of a single set (i.e., E = C/B) that contains all of the cache lines. Figure 6.37 shows the basic organization. 
Set Selection in Fully Associative Caches 
Set selection in a fully associative cache is trivial because there is only one set, summarized in Figure 6.38. Notice that there are no set index bits in the address, which is partitioned into only a tag and a block offset. 
Line Matching and Word Selection in Fully Associative Caches 
Line matching and word selection in a fully associative cache work the same as with a set associative cache, as we show in Figure 6.39. The difference is mainly a question of scale. Because the cache circuitry must search for many matching 
Figure 6.37 

Fully associative cache (E = C/B). In a fully 
Set 0:

associative cache, a single set contains all of the lines. 




The entire cache is one set, so by default set 0 is always selected. Set 0: 
. . . 

t bits  b bits  Valid  Tag  Cache block  
m  1  0  
Tag  Block offset  

Figure 6.38 Set selection in a fully associative cache. Notice that there are no set index bits.
 1? (1) The valid bit must be set. 

t bits  b bits 
0110  100  
m  1  0  
Tag  Block offset  

Figure 6.39 Line matching and word selection in a fully associative cache. 
tags in parallel, it is dif.cult and expensive to build an associative cache that is both large and fast. As a result, fully associative caches are only appropriate for small caches, such as the translation lookaside buffers (TLBs) in virtual memory systems that cache page table entries (Section 9.6.2). 
Practice Problem 6.13 
The problems that follow will help reinforce your understanding of how caches work. Assume the following: 
.  The memory is byte addressable.  
.  Memory accesses are to 1-byte words (not to 4-byte words).  
.  Addresses are 13 bits wide.  
.  The cache is two-way set associative (E = 2), with a 4-byte block size (B = 4)  
and eight sets (S = 8).  

The contents of the cache are as follows, with all numbers given in hexadecimal notation. 
2-way set associative cache 
Line 0 Line 1 

Set index Tag Valid Byte 0 Byte 1 Byte 2 Byte 3 Tag Valid Byte 0 Byte 1 Byte 2 Byte 3 
0 1 2 3 4 5 6 7  09 45 EB 06 C7 71 91 46  1 1 0 0 1 1 1 0  86 60 ¡ª ¡ª 06 0B A0 ¡ª  30 4F ¡ª ¡ª 78 DE B7 ¡ª  3F E0 ¡ª ¡ª 07 18 26 ¡ª  10 23 ¡ª ¡ª C5 4B 2D ¡ª  00 38 0B 32 05 6E F0 DE  0 1 0 1 1 0 0 1  ¡ª 00 ¡ª 12 40 ¡ª ¡ª 12  ¡ª BC ¡ª 08 67 ¡ª ¡ª C0  ¡ª 0B ¡ª 7B C2 ¡ª ¡ª 88  ¡ª 37 ¡ª AD 3B ¡ª ¡ª 37  

The following .gure shows the format of an address (one bit per box). Indicate (by labeling the diagram) the .elds that would be used to determine the following: 
CO The cache block offset 
CI  The cache set index  
CT  The cache tag  
12  11  10  9  8  7  6  5  4  3  2  1  0  


Practice Problem 6.14 
Suppose a program running on the machine in Problem 6.13 references the 1-byte word at address 0x0E34. Indicate the cache entry accessed and the cache byte value returned in hex. Indicate whether a cache miss occurs. If there is a cache miss, enter ¡°¨C¡± for ¡°Cache byte returned.¡± 
A. Address format (one bit per box): 
12 11 10 9 8 7 6 5 4 3 2 1 0 

B.  Memory reference:  
Parameter  Value  
Cache block offset (CO) Cache set index (CI) Cache tag (CT) Cache hit? (Y/N) Cache byte returned  0x 0x 0x 0x  


Practice Problem 6.15 
Repeat Problem 6.14 for memory address 0x0DD5. 
A. Address format (one bit per box): 
12 11 10 9 8 7 6 5 4 3 2 1 0 

B.  Memory reference:  
Parameter  Value  
Cache block offset (CO) Cache set index (CI) Cache tag (CT) Cache hit? (Y/N) Cache byte returned  0x 0x 0x 0x  

Practice Problem 6.16 
Repeat Problem 6.14 for memory address 0x1FE4. 
A. Address format (one bit per box): 
12 11 10 9 8 7 6 5 4 3 2 1 0 

B.  Memory reference:  
Parameter  Value  
Cache block offset (CO) Cache set index (CI) Cache tag (CT) Cache hit? (Y/N) Cache byte returned  0x 0x 0x 0x  

Practice Problem 6.17 
For the cache in Problem 6.13, list all of the hex memory addresses that will hit in set 3. 
6.4.5 Issues with Writes 
As we have seen, the operation of a cache with respect to reads is straightforward. First, look for a copy of the desired word w in the cache. If there is a hit, return w immediately. If there is a miss, fetch the block that contains w from the next lower level of the memory hierarchy, store the block in some cache line (possibly evicting a valid line), and then return w. 
The situation for writes is a little more complicated. Suppose we write a word w that is already cached (a write hit). After the cache updates its copy of w, what does it do about updating the copy of w in the next lower level of the hierarchy? The simplest approach, known as write-through, is to immediately write w¡¯s cache block to the next lower level. While simple, write-through has the disadvantage of causing bus traf.c with every write. Another approach, known as write-back, defers the update as long as possible by writing the updated block to the next lower level only when it is evicted from the cache by the replacement algorithm. Because of locality, write-back can signi.cantly reduce the amount of bus traf.c, but it has the disadvantage of additional complexity. The cache must maintain an additional dirty bit for each cache line that indicates whether or not the cache block has been modi.ed. 
Another issue is how to deal with write misses. One approach, known as write-allocate, loads the corresponding block from the next lower level into the cache and then updates the cache block. Write-allocate tries to exploit spatial locality of writes, but it has the disadvantage that every miss results in a block transfer from the next lower level to cache. The alternative, known as no-write-allocate, bypasses the cache and writes the word directly to the next lower level. Write-through caches are typically no-write-allocate. Write-back caches are typically write-allocate. 
Optimizing caches for writes is a subtle and dif.cult issue, and we are only scratching the surface here. The details vary from system to system and are often proprietary and poorly documented. To the programmer trying to write reason-ably cache-friendly programs, we suggest adopting a mental model that assumes write-back write-allocate caches. There are several reasons for this suggestion. 
As a rule, caches at lower levels of the memory hierarchy are more likely to use write-back instead of write-through because of the larger transfer times. For example, virtual memory systems (which use main memory as a cache for the blocks stored on disk) use write-back exclusively. But as logic densities increase, the increased complexity of write-back is becoming less of an impediment and we are seeing write-back caches at all levels of modern systems. So this assumption matches current trends. Another reason for assuming a write-back write-allocate approach is that it is symmetric to the way reads are handled, in that write-back write-allocate tries to exploit locality. Thus, we can develop our programs at a high level to exhibit good spatial and temporal locality rather than trying to optimize for a particular memory system. 
6.4.6 Anatomy of a Real Cache Hierarchy 
So far, we have assumed that caches hold only program data. But in fact, caches can hold instructions as well as data. A cache that holds instructions only is called an i-cache. A cache that holds program data only is called a d-cache. A cache that holds both instructions and data is known as a uni.ed cache. Modern processors include separate i-caches and d-caches. There are a number of reasons for this. With two separate caches, the processor can read an instruction word and a data word at the same time. I-caches are typically read-only, and thus simpler. The two caches are often optimized to different access patterns and can have different block sizes, associativities, and capacities. Also, having separate caches ensures that data accesses do not create con.ict misses with instruction accesses, and vice versa, at the cost of a potential increase in capacity misses. 
Section 6.4 Cache Memories 613 


Figure 6.40 shows the cache hierarchy for the Intel Core i7 processor. Each CPU chip has four cores. Each core has its own private L1 i-cache, L1 d-cache, and L2 uni.ed cache. All of the cores share an on-chip L3 uni.ed cache. An interesting feature of this hierarchy is that all of the SRAM cache memories are contained in the CPU chip. 
Figure 6.41 summarizes the basic characteristics of the Core i7 caches. 
Cache type  Access time (cycles)  Cache size (C)  Assoc. (E)  Block size (B)  Sets (S)  
L1 i-cache  4  32 KB  8  64 B  64  
L1 d-cache  4  32 KB  8  64 B  64  
L2 uni.ed cache  11  256 KB  8  64 B  512  
L3 uni.ed cache  30¨C40  8 MB  16  64 B  8192  
Figure 6.41 Characteristics of the Intel Core i7 cache hierarchy. 


6.4.7 Performance Impact of Cache Parameters 
Cache performance is evaluated with a number of metrics: 
. Miss rate. The fraction of memory references during the execution of a pro-gram, or a part of a program, that miss. It is computed as #misses/#references. . Hit rate. The fraction of memory references that hit. It is computed as 1 . miss rate. 
. Hit time. The time to deliver a word in the cache to the CPU, including the time for set selection, line identi.cation, and word selection. Hit time is on the order of several clock cycles for L1 caches. 
. Miss penalty. Any additional time required because of a miss. The penalty for L1 misses served from L2 is on the order of 10 cycles; from L3, 40 cycles; and from main memory, 100 cycles. 
Optimizing the cost and performance trade-offs of cache memories is a subtle exercise that requires extensive simulation on realistic benchmark codes and thus is beyond our scope. However, it is possible to identify some of the qualitative trade-offs. 
Impact of Cache Size 
On the one hand, a larger cache will tend to increase the hit rate. On the other hand, it is always harder to make large memories run faster. As a result, larger caches tend to increase the hit time. This is especially important for on-chip L1 caches that must have a short hit time. 
Impact of Block Size 
Large blocks are a mixed blessing. On the one hand, larger blocks can help increase the hit rate by exploiting any spatial locality that might exist in a program. However, for a given cache size, larger blocks imply a smaller number of cache lines, which can hurt the hit rate in programs with more temporal locality than spatial locality. Larger blocks also have a negative impact on the miss penalty, since larger blocks cause larger transfer times. Modern systems usually compromise with cache blocks that contain 32 to 64 bytes. 
Impact of Associativity 
The issue here is the impact of the choice of the parameter E, the number of cache lines per set. The advantage of higher associativity (i.e., larger values of E) is that it decreases the vulnerability of the cache to thrashing due to con.ict misses. However, higher associativity comes at a signi.cant cost. Higher associativity is expensive to implement and hard to make fast. It requires more tag bits per line, additional LRU state bits per line, and additional control logic. Higher associativity can increase hit time, because of the increased complexity, and it can also increase the miss penalty because of the increased complexity of choosing a victim line. 

The choice of associativity ultimately boils down to a trade-off between the hit time and the miss penalty. Traditionally, high-performance systems that pushed the clock rates would opt for smaller associativity for L1 caches (where the miss penalty is only a few cycles) and a higher degree of associativity for the lower levels, where the miss penalty is higher. For example, in Intel Core i7 systems, the L1 and L2 caches are 8-way associative, and the L3 cache is 16-way. 
Impact of Write Strategy 
Write-through caches are simpler to implement and can use a write buffer that works independently of the cache to update memory. Furthermore, read misses are less expensive because they do not trigger a memory write. On the other hand, write-back caches result in fewer transfers, which allows more bandwidth to memory for I/O devices that perform DMA. Further, reducing the number of transfers becomes increasingly important as we move down the hierarchy and the transfer times increase. In general, caches further down the hierarchy are more likely to use write-back than write-through. 
Aside Cache lines, sets, and blocks: What¡¯s the difference? 
It is easy to confuse the distinction between cache lines, sets, and blocks. Let¡¯s review these ideas and make sure they are clear: 
. A block is a .xed-sized packet of information that moves back and forth between a cache and main memory (or a lower-level cache). . A line is a container in a cache that stores a block, as well as other information such as the valid bit and the tag bits. . A set is a collection of one or more lines. Sets in direct-mapped caches consist of a single line. Sets in set associative and fully associative caches consist of multiple lines. 
In direct-mapped caches, sets and lines are indeed equivalent. However, in associative caches, sets and lines are very different things and the terms cannot be used interchangeably. 
Since a line always stores a single block, the terms ¡°line¡± and ¡°block¡± are often used interchange-ably. For example, systems professionals usually refer to the ¡°line size¡± of a cache, when what they really mean is the block size. This usage is very common, and shouldn¡¯t cause any confusion, so long as you understand the distinction between blocks and lines. 
6.5 
Writing 
Cache-friendly 
Code 

In Section 6.2, we introduced the idea of locality and talked in qualitative terms about what constitutes good locality. Now that we understand how cache memo-ries work, we can be more precise. Programs with better locality will tend to have lower miss rates, and programs with lower miss rates will tend to run faster than programs with higher miss rates. Thus, good programmers should always try to write code that is cache friendly, in the sense that it has good locality. Here is the basic approach we use to try to ensure that our code is cache friendly. 
1. 
Make the common case go fast. Programs often spend most of their time in a few core functions. These functions often spend most of their time in a few loops. So focus on the inner loops of the core functions and ignore the rest. 

2. 
Minimize the number of cache misses in each inner loop.All other things being equal, such as the total number of loads and stores, loops with better miss rates will run faster. 


To see how this works in practice, consider the sumvec function from Section 6.2: 
1 int sumvec(int v[N]) 2 { 3 int i,sum = 0; 4 5 for (i=0;i<N; i++) 6 sum += v[i]; 7 return sum; 
8 } 
Is this function cache friendly? First, notice that there is good temporal locality in the loop body with respect to the local variables i and sum. In fact, because these are local variables, any reasonable optimizing compiler will cache them in the register .le, the highest level of the memory hierarchy. Now consider the stride-1 references to vector v. In general, if a cache has a block size of B bytes, then a stride-k reference pattern (where k is expressed in words) results in an average of min (1,(wordsize ¡Á k)/B) misses per loop iteration. This is minimized for k = 1, so the stride-1 references to v are indeed cache friendly. For example, suppose that v is block aligned, words are 4 bytes, cache blocks are 4 words, and the cache is initially empty (a cold cache). Then, regardless of the cache organization, the references to v will result in the following pattern of hits and misses: 
v[i] i = 0 i = 1 i = 2 i = 3 i = 4 i = 5 i = 6 i = 7 
Access order, [h]it or [m]iss  1 [m]  2 [h]  3 [h]  4 [h]  5 [m]  6 [h]  7 [h]  8 [h]  

In this example, the reference to v[0] misses and the corresponding block, which contains v[0]¨Cv[3], is loaded into the cache from memory. Thus, the next three references are all hits. The reference to v[4] causes another miss as a new block is loaded into the cache, the next three references are hits, and so on. In general, three out of four references will hit, which is the best we can do in this case with a cold cache. 
To summarize, our simple sumvec example illustrates two important points about writing cache-friendly code: 
. Repeated references to local variables are good because the compiler can cache them in the register .le (temporal locality). 

. Stride-1 reference patterns are good because caches at all levels of the memory hierarchy store data as contiguous blocks (spatial locality). 
Spatial locality is especially important in programs that operate on multi-dimensional arrays. For example, consider the sumarrayrows function from Sec-tion 6.2, which sums the elements of a two-dimensional array in row-major order: 
1 int sumarrayrows(int a[M][N]) 2 { 3 inti,j, sum=0; 4 5 for (i=0;i<M; i++) 6 for (j=0;j<N; j++) 7 sum += a[i][j]; 8 return sum; 
9 } 

Since C stores arrays in row-major order, the inner loop of this function has the same desirable stride-1 access pattern as sumvec. For example, suppose we make the same assumptions about the cache as for sumvec. Then the references to the array a will result in the following pattern of hits and misses: 
a[i][j] j = 0 j = 1 j = 2 j = 3 j = 4 j = 5 j = 6 j = 7 
i=0 i=1 i=2 i=3  1 [m] 9 [m] 17 [m] 25 [m]  2 [h] 10 [h] 18 [h] 26 [h]  3 [h] 11 [h] 19 [h] 27 [h]  4 [h] 12 [h] 20 [h] 28 [h]  5 [m] 13 [m] 21 [m] 29 [m]  6 [h] 14 [h] 22 [h] 30 [h]  7 [h] 15 [h] 23 [h] 31 [h]  8 [h] 16 [h] 24 [h] 32 [h]  

But consider what happens if we make the seemingly innocuous change of permuting the loops: 
1 int sumarraycols(int a[M][N]) 2 { 3 inti,j, sum=0; 4 5 for (j=0;j<N; j++) 6 for (i=0;i<M; i++) 7 sum += a[i][j]; 8 return sum; 
9 } 

In this case, we are scanning the array column by column instead of row by row. If we are lucky and the entire array .ts in the cache, then we will enjoy the same miss rate of 1/4. However, if the array is larger than the cache (the more likely case), then each and every access of a[i][j] will miss! 
a[i][j] j = 0 j = 1 j = 2 j = 3 j = 4 j = 5 j = 6 j = 7 
i=0 i=1 i=2 i=3  1 [m] 2 [m] 3 [m] 4 [m]  5 [m] 6 [m] 7 [m] 8 [m]  9 [m] 10 [m] 11 [m] 12 [m]  13 [m] 14 [m] 15 [m] 16 [m]  17 [m] 18 [m] 19 [m] 20 [m]  21 [m] 22 [m] 23 [m] 24 [m]  25 [m] 26 [m] 27 [m] 28 [m]  29 [m] 30 [m] 31 [m] 32 [m]  

Higher miss rates can have a signi.cant impact on running time. For example, on our desktop machine, sumarrayrows runs twice as fast as sumarraycols.To summarize, programmers should be aware of locality in their programs and try to write programs that exploit it. 
Practice Problem 6.18 
Transposing the rows and columns of a matrix is an important problem in signal processing and scienti.c computing applications. It is also interesting from a local-ity point of view because its reference pattern is both row-wise and column-wise. For example, consider the following transpose routine: 
1 typedef int array[2][2]; 2 3 void transpose1(array dst, array src) 4 { 5 int i, j; 6 7 for (i=0;i<2; i++) { 8 for (j=0;j<2; j++) { 9 dst[j][i] = src[i][j]; 
10 
} 

11 
} 

12 
} 


Assume this code runs on a machine with the following properties: 
. sizeof(int) == 4. . The src array starts at address 0 and the dst array starts at address 16 (decimal). . There is a single L1 data cache that is direct-mapped, write-through, and write-
allocate, with a block size of 8 bytes. . The cache has a total size of 16 data bytes and the cache is initially empty. . Accesses to the src and dst arrays are the only sources of read and write 
misses, respectively. 
A. For each row and col, indicate whether the access to src[row][col] and dst[row][col] is a hit (h) or a miss (m). For example, reading src[0][0] is a miss and writing dst[0][0] is also a miss. 

Section 6.5  Writing Cache-friendly Code  619  
dst array  src array  
Col 0  Col 1  Col 0  Col 1  

Row 0  m  
Row 1  

Row 0  m  
Row 1  


B. Repeat the problem for a cache with 32 data bytes. 
Practice Problem 6.19 
The heart of the recent hit game SimAquarium is a tight loop that calculates the average position of 256 algae. You are evaluating its cache performance on a machine with a 1024-byte direct-mapped data cache with 16-byte blocks (B = 16). You are given the following de.nitions: 
1 struct algae_position { 2 int x; 3 int y; 4 }; 5 6 struct algae_position grid[16][16]; 7 int total_x = 0, total_y = 0; 8 int i, j; 
You should also assume the following: 
. sizeof(int) == 4. . grid begins at memory address 0. . The cache is initially empty. . The only memory accesses are to the entries of the array grid. Variables i, j, 
total_x, and total_y are stored in registers. 
Determine the cache performance for the following code: 
1 for(i=0; i<16; i++) { 2 for (j= 0; j < 16;j++){ 3 total_x += grid[i][j].x; 
4 } 

5 } 6 7 for(i=0; i<16; i++) { 8 for (j= 0; j < 16;j++){ 9 total_y += grid[i][j].y; 
10 
} 

11 
} 



A. What is the total number of reads? 
B. What is the total number of reads that miss in the cache? 
C. What is the miss rate? 
Practice Problem 6.20 
Given the assumptions of Problem 6.19, determine the cache performance of the following code: 
1 for (i= 0; i< 16;i++){ 2 for(j = 0; j <16;j++){ 3 total_x += grid[j][i].x; 4 total_y += grid[j][i].y; 
5 
} 

6 
} 


A. What is the total number of reads? 
B. What is the total number of reads that miss in the cache? 
C. What is the miss rate? 
D. What would the miss rate be if the cache were twice as big? 
Practice Problem 6.21 
Given the assumptions of Problem 6.19, determine the cache performance of the following code: 
1 for (i= 0; i< 16;i++){ 2 for(j = 0; j <16;j++){ 3 total_x += grid[i][j].x; 4 total_y += grid[i][j].y; 
5 
} 

6 
} 


A. What is the total number of reads? 
B. What is the total number of reads that miss in the cache? 
C. What is the miss rate? 
D. What would the miss rate be if the cache were twice as big? 
6.6 
Putting 
It 
Together: 
The 
Impact 
of 
Caches 
on 
Program 
Performance 

This section wraps up our discussion of the memory hierarchy by studying the im-pact that caches have on the performance of programs running on real machines. 

6.6.1 The Memory Mountain 
The rate that a program reads data from the memory system is called the read throughput, or sometimes the read bandwidth. If a program reads n bytes over a period of s seconds, then the read throughput over that period is n/s, typically expressed in units of megabytes per second (MB/s). 
If we were to write a program that issued a sequence of read requests from a tight program loop, then the measured read throughput would give us some insight into the performance of the memory system for that particular sequence of reads. Figure 6.42 shows a pair of functions that measure the read throughput for a particular read sequence. 
The test function generates the read sequence by scanning the .rst elems elements of an array with a stride of stride.The run function is a wrapper that calls the test function and returns the measured read throughput. The call to the test function in line 29 warms the cache. The fcyc2 function in line 30 calls the test function with arguments elems and estimates the running time of the test function in CPU cycles. Notice that the size argument to the run function is in units of bytes, while the corresponding elems argument to the test function is in units of array elements. Also, notice that line 31 computes MB/s as 106 bytes/s, as opposed to 220 bytes/s. 
The size and stride arguments to the run function allow us to control the degree of temporal and spatial locality in the resulting read sequence. Smaller values of size result in a smaller working set size, and thus better temporal locality. Smaller values of stride result in better spatial locality. If we call the run function repeatedly with different values of size and stride, then we can recover a fascinating two-dimensional function of read throughput versus temporal and spatial locality. This function is called a memory mountain. 
Every computer has a unique memory mountain that characterizes the ca-pabilities of its memory system. For example, Figure 6.43 shows the memory mountain for an Intel Core i7 system. In this example, the size varies from 2 KB to 64 MB, and the stride varies from 1 to 64 elements, where each element is an 8-byte double. 
The geography of the Core i7 mountain reveals a rich structure. Perpendicular to the size axis are four ridges that correspond to the regions of temporal locality where the working set .ts entirely in the L1 cache, the L2 cache, the L3 cache, and main memory, respectively. Notice that there is an order of magnitude difference between the highest peak of the L1 ridge, where the CPU reads at a rate of over 6 GB/s, and the lowest point of the main memory ridge, where the CPU reads at a rate of 600 MB/s. 
There is a feature of the L1 ridge that should be pointed out. For very large strides, notice how the read throughput drops as the working set size approaches 2 KB (falling off the back side of the ridge). Since the L1 cache holds the entire working set, this feature does not re.ect the true L1 cache performance. It is an artifact of overheads of calling the test function and setting up to execute the loop. For large strides in small working set sizes, these overheads are not amortized, as they are with the larger sizes. 
code/mem/mountain/mountain.c 

1 double data[MAXELEMS]; /* The global array we¡¯ll be traversing */ 2 3 /* 4 * test -Iterate over first "elems" elements of array "data" 5 * with stride of "stride". 6 */ 7 void test(int elems, int stride) /* The test function */ 8 { 9 int i; 
10 double result = 0.0; 
11 volatile double sink; 
12 

13 for (i = 0; i < elems; i += stride) { 
14 result += data[i]; 
15 
} 16 sink = result; /* So compiler doesn¡¯t optimize away the loop */ 

17 
} 18 19 /* 20 * run -Run test(elems, stride) and return read throughput (MB/s). 21 * "size" is in bytes, "stride" is in array elements, and 22 * Mhz is CPU clock frequency in Mhz. 23 */ 24 double run(int size, int stride, double Mhz) 25 { 26 double cycles; 27 int elems = size / sizeof(double); 28 29 test(elems, stride); /* warm up the cache */ 30 cycles = fcyc2(test, elems, stride, 0); /* call test(elems,stride) */ 31 return (size / stride) / (cycles / Mhz); /* convert cycles to MB/s */ 


32 } 
code/mem/mountain/mountain.c 

Figure 6.42 Functions that measure and compute read throughput. We can generate a memory mountain for a particular computer by calling the run function with different values of size (which corresponds to temporal locality) and stride (which corresponds to spatial locality). 
On each of the L2, L3, and main memory ridges, there is a slope of spatial locality that falls downhill as the stride increases, and spatial locality decreases. Notice that even when the working set is too large to .t in any of the caches, the highest point on the main memory ridge is a factor of 7 higher than its lowest point. So even when a program has poor temporal locality, spatial locality can still come to the rescue and make a signi.cant difference. 
Read throughput (MB/s) 

7000 L1 
6000 
5000 
4000 
L2 
3000 

2000 
L3 

1000 Core i7 
2.67 GHz 32 KB L1 d-cache 256 KB L2 cache 8 MB L3 cache 
Ridges of temporal locality 
Slopes of spatial locality 
0 
s1s3s5s7s9s11 
Stride (x8 bytes) 
Mem 
s13
s15
s3264M16M4M1M
256k64k16k4k 
Size (bytes) 

Figure 6.43 The memory mountain. 
There is a particularly interesting .at ridge line that extends perpendicular to the stride axis for strides of 1 and 2, where the read throughput is a relatively constant 4.5 GB/s. This is apparently due to a hardware prefetching mechanism in the Core i7 memory system that automatically identi.es memory referencing patterns and attempts to fetch those blocks into cache before they are accessed. While the details of the particular prefetching algorithm are not documented, it is clear from the memory mountain that the algorithm works best for small strides¡ª yet another reason to favor sequential accesses in your code. 
If we take a slice through the mountain, holding the stride constant as in Fig-ure 6.44, we can see the impact of cache size and temporal locality on performance. For sizes up to 32 KB, the working set .ts entirely in the L1 d-cache, and thus reads are served from L1 at the peak throughput of about 6 GB/s. For sizes up to 256 KB, the working set .ts entirely in the uni.ed L2 cache, and for sizes up to 8M, the working set .ts entirely in the uni.ed L3 cache. Larger working set sizes are served primarily from main memory. 
The dips in read throughputs at the leftmost edges of the L1, L2, and L3 cache regions¡ªwhere the working set sizes of 32 KB, 256 KB, and 8 MB are equal to their respective cache sizes¡ªare interesting. It is not entirely clear why these dips occur. The only way to be sure is to perform a detailed cache simulation, but it 
624  Chapter 6  The Memory Hierarchy  
Main memory  L3 cache  L2 cache  L1 cache  
region  region  region  region  


64M32M16M8M4M2M1M512k256k128k64k32k16k8k4k2k 
Working set size (bytes) 
is likely that the drops are caused by other data and code blocks that make it impossible to .t the entire array in the respective cache. 
Slicing through the memory mountain in the opposite direction, holding the working set size constant, gives us some insight into the impact of spatial locality on the read throughput. For example, Figure 6.45 shows the slice for a .xed working set size of 4 MB. This slice cuts along the L3 ridge in Figure 6.43, where the working set .ts entirely in the L3 cache, but is too large for the L2 cache. 
Notice how the read throughput decreases steadily as the stride increases from one to eight doublewords. In this region of the mountain, a read miss in L2 causes a block to be transferred from L3 to L2. This is followed by some number of hits on the block in L2, depending on the stride. As the stride increases, the ratio of L2 misses to L2 hits increases. Since misses are served more slowly than hits, the read throughput decreases. Once the stride reaches eight doublewords, which on this system equals the block size of 64 bytes, every read request misses in L2 and must be served from L3. Thus, the read throughput for strides of at least eight doublewords is a constant rate determined by the rate that cache blocks can be transferred from L3 into L2. 
To summarize our discussion of the memory mountain, the performance of the memory system is not characterized by a single number. Instead, it is a mountain of temporal and spatial locality whose elevations can vary by over an order of magnitude. Wise programmers try to structure their programs so that they run in the peaks instead of the valleys. The aim is to exploit temporal locality so that 


s1s2s3s4s5s6s7s8s9s10s11 s12s13s14s15s16s32s64 
Stride (x8 bytes) 

Figure 6.45 A slope of spatial locality. The graph shows a slice through Figure 6.43 with size=4 MB. 
heavily used words are fetched from the L1 cache, and to exploit spatial locality so that as many words as possible are accessed from a single L1 cache line. 
Practice Problem 6.22 
Use the memory mountain in Figure 6.43 to estimate the time, in CPU cycles, to read an 8-byte word from the L1 d-cache. 
6.6.2 Rearranging Loops to Increase Spatial Locality 
Consider the problem of multiplying a pair of n ¡Á n matrices: C = AB. For exam-
ple, if n = 2, then  
c11 c21  c12 c22  =  a11 a21  a12 a22  b11 b21  b12 b22  
where  

c11 = a11b11 + a12b21 c12 = a11b12 + a12b22 c21 = a21b11 + a22b21 c22 = a21b12 + a22b22 A matrix multiply function is usually implemented using three nested loops, which are identi.ed by their indexes i, j , and k. If we permute the loops and make some other minor code changes, we can create the six functionally equivalent versions 
(a) Version ijk (b) Version jik 
code/mem/matmult/mm.c code/mem/matmult/mm.c 

1 for (i=0;i<n; i++) 1 for (j=0;j<n; j++) 2 for (j=0;j<n; j++) { 2 for (i=0;i<n; i++) { 3 sum = 0.0; 3 sum = 0.0; 4 for (k=0;k<n; k++) 4 for (k=0;k<n; k++) 5 sum += A[i][k]*B[k][j]; 5 sum += A[i][k]*B[k][j]; 6 C[i][j] += sum; 6 C[i][j] += sum; 
7 } 7 } 
code/mem/matmult/mm.c code/mem/matmult/mm.c 

(c) Version jki (d) Version kji 
code/mem/matmult/mm.c code/mem/matmult/mm.c 

1 for (j=0;j<n; j++) 1 for (k=0;k<n; k++) 2 for (k=0;k<n; k++) { 2 for (j=0;j<n; j++) { 3 r = B[k][j]; 3 r = B[k][j]; 4 for (i=0;i<n; i++) 4 for (i=0;i<n; i++) 5 C[i][j] += A[i][k]*r; 5 C[i][j] += A[i][k]*r; 
6 } 6 } 
code/mem/matmult/mm.c code/mem/matmult/mm.c 

(e) Version kij (f) Version ikj 
code/mem/matmult/mm.c code/mem/matmult/mm.c 

1 for (k=0;k<n; k++) 1 for (i=0;i<n; i++) 2 for (i=0;i<n; i++) { 2 for (k=0;k<n; k++) { 3 r = A[i][k]; 3 r = A[i][k]; 4 for (j=0;j<n; j++) 4 for (j=0;j<n; j++) 5 C[i][j] += r*B[k][j]; 5 C[i][j] += r*B[k][j]; 
6 } 6 } 
code/mem/matmult/mm.c code/mem/matmult/mm.c 

Figure 6.46 Six versions of matrix multiply. Each version is uniquely identi.ed by the ordering of its loops. 
of matrix multiply shown in Figure 6.46. Each version is uniquely identi.ed by the 
ordering of its loops. At a high level, the six versions are quite similar. If addition is associative, 
then each version computes an identical result.2 Each version performs O(n3) total 
2. As we learned in Chapter 2, .oating-point addition is commutative, but in general not associative. In practice, if the matrices do not mix extremely large values with extremely small ones, as often is true when the matrices store physical properties, then the assumption of associativity is reasonable. 

Section 6.6  Putting It Together: The Impact of Caches on Program Performance  627  
Matrix multiply version (class)  Loads per iter.  Stores per iter.  A misses per iter.  B misses per iter.  C misses per iter.  Total misses per iter.  
ij k & jik (AB) jki & kj i (AC) kij & ikj (BC)  2 2 2  0 1 1  0.25 1.00 0.00  1.00 0.00 0.25  0.00 1.00 0.25  1.25 2.00 0.50  
Figure 6.47 Analysis of matrix multiply inner loops. The six versions partition into three equivalence classes, denoted by the pair of arrays that are accessed in the inner loop. 


operations and an identical number of adds and multiplies. Each of the n 2 elements of A and B is read n times. Each of the n 2 elements of C is computed by summing n values. However, if we analyze the behavior of the innermost loop iterations, we .nd that there are differences in the number of accesses and the locality. For the purposes of this analysis, we make the following assumptions: 
. Each array is an n ¡Á n array of double, with sizeof(double) == 8. . There is a single cache with a 32-byte block size (B = 32). . The array size n is so large that a single matrix row does not .t in the L1 cache. . The compiler stores local variables in registers, and thus references to local 
variables inside loops do not require any load or store instructions. 
Figure 6.47 summarizes the results of our inner loop analysis. Notice that the six versions pair up into three equivalence classes, which we denote by the pair of matrices that are accessed in the inner loop. For example, versions ij k and jik are members of Class AB because they reference arrays A and B (but not C) in their innermost loop. For each class, we have counted the number of loads (reads) and stores (writes) in each inner loop iteration, the number of references to A, B, and C that will miss in the cache in each loop iteration, and the total number of cache misses per iteration. 
The inner loops of the Class AB routines (Figure 6.46(a) and (b)) scan a row of array A with a stride of 1. Since each cache block holds four doublewords, the miss rate for A is 0.25 misses per iteration. On the other hand, the inner loop scans a column of B with a stride of n. Since n is large, each access of array B results in a miss, for a total of 1.25 misses per iteration. 
The inner loops in the Class AC routines (Figure 6.46(c) and (d)) have some problems. Each iteration performs two loads and a store (as opposed to the Class AB routines, which perform two loads and no stores). Second, the inner loop scans the columns of A and C with a stride of n. The result is a miss on each load, for a total of two misses per iteration. Notice that interchanging the loops has decreased the amount of spatial locality compared to the Class AB routines. 
The BC routines (Figure 6.46(e) and (f)) present an interesting trade-off: With two loads and a store, they require one more memory operation than the AB routines. On the other hand, since the inner loop scans both B and C row-wise 
Figure 6.48 60 
Core i7 matrix multiply performance. Legend: jki 
and kji: Class AC; ijk and jik: Class AB; kij and ikj: Class BC. 
Cycles per inner loop iteration 
50 40 30 20 10 0 


jki kji ijk jik kij ikj 
with a stride-1 access pattern, the miss rate on each array is only 0.25 misses per iteration, for a total of 0.50 misses per iteration. 
Figure 6.48 summarizes the performance of different versions of matrix mul-tiply on a Core i7 system. The graph plots the measured number of CPU cycles per inner loop iteration as a function of array size (n). 

There are a number of interesting points to notice about this graph:  
.  For large values of n, the fastest version runs almost 20 times faster than the  
slowest version, even though each performs the same number of .oating-point  
arithmetic operations.  
.  Pairs of versions with the same number of memory references and misses per  
iteration have almost identical measured performance.  
.  The two versions with the worst memory behavior, in terms of the number of  
accesses and misses per iteration, run signi.cantly slower than the other four  
versions, which have fewer misses or fewer accesses, or both.  
.  Miss rate, in this case, is a better predictor of performance than the total  
number of memory accesses. For example, the Class BC routines, with 0.5  
misses per iteration, perform much better than the Class AB routines, with  
1.25 misses per iteration, even though the Class BC routines perform more  
memory references in the inner loop (two loads and one store) than the  
Class AB routines (two loads).  
.  For large values of n, the performance of the fastest pair of versions (kij and  
ikj) is constant. Even though the array is much larger than any of the SRAM  
cache memories, the prefetching hardware is smart enough to recognize the  
stride-1 access pattern, and fast enough to keep up with memory accesses  
in the tight inner loop. This is a stunning accomplishment by the Intel engi- 


neers who designed this memory system, providing even more incentive for programmers to develop programs with good spatial locality. 
Web Aside MEM:BLOCKING Using blocking to increase temporal locality 
There is an interesting technique called blocking that can improve the temporal locality of inner loops. The general idea of blocking is to organize the data structures in a program into large chunks called blocks. (In this context, ¡°block¡± refers to an application-level chunk of data, not to a cache block.) The program is structured so that it loads a chunk into the L1 cache, does all the reads and writes that it needs to on that chunk, then discards the chunk, loads in the next chunk, and so on. 
Unlike the simple loop transformations for improving spatial locality, blocking makes the code harder to read and understand. For this reason, it is best suited for optimizing compilers or frequently executed library routines. Still, the technique is interesting to study and understand because it is a general concept that can produce big performance gains on some systems. 
6.6.3 Exploiting Locality in Your Programs 
As we have seen, the memory system is organized as a hierarchy of storage devices, with smaller, faster devices toward the top and larger, slower devices toward the bottom. Because of this hierarchy, the effective rate that a program can access memory locations is not characterized by a single number. Rather, it is a wildly varying function of program locality (what we have dubbed the memory mountain) that can vary by orders of magnitude. Programs with good locality access most of their data from fast cache memories. Programs with poor locality access most of their data from the relatively slow DRAM main memory. 
Programmers who understand the nature of the memory hierarchy can ex-ploit this understanding to write more ef.cient programs, regardless of the speci.c memory system organization. In particular, we recommend the following tech-niques: 
. Focus your attention on the inner loops, where the bulk of the computations and memory accesses occur. . Try to maximize the spatial locality in your programs by reading data objects sequentially, with stride 1, in the order they are stored in memory. . Try to maximize the temporal locality in your programs by using a data object as often as possible once it has been read from memory. 
6.7 
Summary 


The basic storage technologies are random-access memories (RAMs), nonvolatile memories (ROMs), and disks. RAM comes in two basic forms. Static RAM (SRAM) is faster and more expensive, and is used for cache memories both on and off the CPU chip. Dynamic RAM (DRAM) is slower and less expensive, and is used for the main memory and graphics frame buffers. Nonvolatile memories, also called read-only memories (ROMs), retain their information even if the sup-ply voltage is turned off, and they are used to store .rmware. Rotating disks are mechanical nonvolatile storage devices that hold enormous amounts of data at a low cost per bit, but with much longer access times than DRAM. Solid state disks (SSDs) based on nonvolatile .ash memory are becoming increasingly attractive alternatives to rotating disks for some applications. 
In general, faster storage technologies are more expensive per bit and have smaller capacities. The price and performance properties of these technologies are changing at dramatically different rates. In particular, DRAM and disk access times are much larger than CPU cycle times. Systems bridge these gaps by orga-nizing memory as a hierarchy of storage devices, with smaller, faster devices at the top and larger, slower devices at the bottom. Because well-written programs have good locality, most data are served from the higher levels, and the effect is a memory system that runs at the rate of the higher levels, but at the cost and capacity of the lower levels. 
Programmers can dramatically improve the running times of their programs by writing programs with good spatial and temporal locality. Exploiting SRAM-based cache memories is especially important. Programs that fetch data pri-marily from cache memories can run much faster than programs that fetch data primarily from memory. 
Bibliographic 
Notes 

Memory and disk technologies change rapidly. In our experience, the best sources of technical information are the Web pages maintained by the manufacturers. Companies such as Micron, Toshiba, and Samsung provide a wealth of current technical information on memory devices. The pages for Seagate, Maxtor, and Western Digital provide similarly useful information about disks. 
Textbooks on circuit and logic design provide detailed information about memory technology [56, 85]. IEEE Spectrum published a series of survey articles on DRAM [53]. The International Symposium on Computer Architecture (ISCA) is a common forum for characterizations of DRAM memory performance [34, 35]. 
Wilkes wrote the .rst paper on cache memories [116]. Smith wrote a clas-sic survey [101]. Przybylski wrote an authoritative book on cache design [82]. Hennessy and Patterson provide a comprehensive discussion of cache design is-sues [49]. 
Stricker introduced the idea of the memory mountain as a comprehensive characterization of the memory system in [111], and suggested the term ¡°memory mountain¡± informally in later presentations of the work. Compiler researchers work to increase locality by automatically performing the kinds of manual code transformations we discussed in Section 6.6 [22, 38, 63, 68, 75, 83, 118]. Carter and colleagues have proposed a cache-aware memory controller [18]. Seward devel-oped an open-source cache pro.ler, called cacheprof, that characterizes the miss behavior of C programs on an arbitrary simulated cache (www.cacheprof.org). 
Other researchers have developed cache oblivious algorithms that are designed to run well without any explicit knowledge of the structure of the underlying cache memory [36, 42, 43]. 

There is a large body of literature on building and using disk storage. Many storage researchers look for ways to aggregate individual disks into larger, more robust, and more secure storage pools [20, 44, 45, 79, 119]. Others look for ways to use caches and locality to improve the performance of disk accesses [12, 21]. Systems such as Exokernel provide increased user-level control of disk and mem-ory resources [55]. Systems such as the Andrew File System [74] and Coda [91] extend the memory hierarchy across computer networks and mobile notebook computers. Schindler and Ganger developed an interesting tool that automatically characterizes the geometry and performance of SCSI disk drives [92]. Researchers are investigating techniques for building and using Flash-based SSDs [8, 77]. 
Homework 
Problems 

6.23 ¡ô¡ô 

Suppose you are asked to design a rotating disk where the number of bits per track is constant. You know that the number of bits per track is determined by the circumference of the innermost track, which you can assume is also the circumference of the hole. Thus, if you make the hole in the center of the disk larger, the number of bits per track increases, but the total number of tracks decreases. If you let r denote the radius of the platter, and x.r the radius of the hole, what value of x maximizes the capacity of the disk? 
6.24 ¡ô 

Estimate the average time (in ms) to access a sector on the following disk: 
Parameter Value Rotational rate 15,000 RPM T 4ms 
avg seek 

Average # sectors/track 800 
6.25 ¡ô¡ô 

Suppose thata2MB.le consisting of 512-byte logical blocks is stored on a disk drive with the following characteristics: 
Parameter Value Rotational rate 15,000 RPM T 4ms 
avg seek 

Average # sectors/track 1000 Surfaces 8 Sector size 512 bytes 
For each case below, suppose that a program reads the logical blocks of the .le sequentially, one after the other, and that the time to position the head over the .rst block is Tavg seek + Tavg rotation. 
A. Best case: Estimate the optimal time (in ms) required to read the .le over all possible mappings of logical blocks to disk sectors. 
B. Random case: Estimate the time (in ms) required to read the .le if blocks are mapped randomly to disk sectors. 
6.26 ¡ô 
The following table gives the parameters for a number of different caches. For each cache, .ll in the missing .elds in the table. Recall that m is the number of physical address bits, C is the cache size (number of data bytes), B is the block size in bytes, E is the associativity, S is the number of cache sets, t is the number of tag bits, s is the number of set index bits, and b is the number of block offset bits. 
Cache mCBE Stsb 
1. 2. 3. 4. 5. 6.  32 32 32 32 32 32  1024 1024 1024 1024 1024 1024  4 4 8 8 32 32  4 256 1 128 1 4  
6.27  ¡ô  

The following table gives the parameters for a number of different caches. Your task is to .ll in the missing .elds in the table. Recall that m is the number of physical address bits, C is the cache size (number of data bytes), B is the block size in bytes, E is the associativity, S is the number of cache sets, t is the number of tag bits, s is the number of set index bits, and b is the number of block offset bits. 
Cache mCBE St sb 
1.32 81 2183 2. 322048 128 23 7 2 3. 3210242 8 64 1 4.321024 2 16 23 4 
6.28 ¡ô 
This problem concerns the cache in Problem 6.13. 
A. List all of the hex memory addresses that will hit in set 1. 
B. List all of the hex memory addresses that will hit in set 6. 
6.29 ¡ô¡ô 
This problem concerns the cache in Problem 6.13. 
A. List all of the hex memory addresses that will hit in set 2. 
B. List all of the hex memory addresses that will hit in set 4. 

C. List all of the hex memory addresses that will hit in set 5. 
D. List all of the hex memory addresses that will hit in set 7. 
6.30 ¡ô¡ô 

Suppose we have a system with the following properties: . The memory is byte addressable. . Memory accesses are to 1-byte words (not to 4-byte words). . Addresses are 12 bits wide. . The cache is two-way set associative (E = 2), with a 4-byte block size (B = 4) and four sets (S = 4). The contents of the cache are as follows, with all addresses, tags, and values given in hexadecimal notation: 
Set index Tag Valid Byte 0 Byte 1 Byte 2 Byte 3 
0  00 83  1 1  40 FE  41 97  42 CC  43 D0  
1  00 83  1 0  44 ¡ª  45 ¡ª  46 ¡ª  47 ¡ª  
2  00 40  1 0  48 ¡ª  49 ¡ª  4A ¡ª  4B ¡ª  
3  FF 00  1 0  9A ¡ª  C0 ¡ª  03 ¡ª  FF ¡ª  

A. The following diagram shows the format of an address (one bit per box). Indicate (by labeling the diagram) the .elds that would be used to determine the following: 
CO CI CT  The cache block offset The cache set index The cache tag  
11  10  9  8  7  6  5  4  3  2  1  0  


B. For each of the following memory accesses indicate if it will be a cache hit or miss when carried out in sequence as listed. Also give the value of a read if it can be inferred from the information in the cache. 
Operation Address Hit? Read value (or unknown) 
Read 0x834 
Write 0x836 Read 0xFFD 
6.31 ¡ô 
Suppose we have a system with the following properties: 
. The memory is byte addressable. . Memory accesses are to 1-byte words (not to 4-byte words). . Addresses are 13 bits wide. . The cache is four-way set associative (E = 4), with a 4-byte block size (B = 4) 
and eight sets (S = 8). 
Consider the following cache state. All addresses, tags, and values are given in hexadecimal format. The Index column contains the set index for each set of four lines. The Tag columns contain the tag value for each line. The V columns contain the valid bit for each line. The Bytes 0¨C3 columns contain the data for each line, numbered left-to-right starting with byte 0 on the left. 
4-way set associative cache 

Index Tag V Bytes 0¨C3 Tag V Bytes 0¨C3 Tag V Bytes 0¨C3 Tag V Bytes 0¨C3 
0 1 2 3 4 5 6 7  F0 BC BC BE 7E 98 38 8A  1 0 1 0 1 0 0 1  ED 03 54 2F 32 A9 5D 04  32 3E 9E 7E 21 76 4D 2A  0A CD 1E 3D 1C 2B F7 32  A2 38 FA A8 2C EE DA 6A  8A A0 B6 C0 8A 54 BC 9E  1 0 1 1 1 0 1 0  BF 16 DC 27 22 BC 69 B1  80 7B 81 95 C2 91 C2 86  1D ED B2 A4 DC D5 8C 56  FC 5A 14 74 34 92 74 0E  14 BC 00 C4 BC 98 8A CC  1 1 0 0 1 1 1 1  EF 8E B6 07 BA 80 A8 96  09 4C 1F 11 DD BA CE 30  86 DF 7B 6B 37 9B 7F 47  2A 18 44 D8 D8 F6 DA F2  BC E4 74 BC DC BC 38 BC  0 1 0 0 0 1 1 1  25 FB 10 C7 E7 48 FA F8  44 B7 F5 B7 A2 16 93 1D  6F 12 B8 AF 39 81 EB 42  1A 02 2E C2 BA 0A 48 30  

A. What is size (C) of this cache in bytes? 
B. The box that follows shows the format of an address (one bit per box). Indicate (by labeling the diagram) the .elds that would be used to determine the following: 
CO  The cache block offset  
CI  The cache set index  
CT  The cache tag  
12  11  10  9  8  7  6  5  4  3  2  1  0  


6.32 ¡ô¡ô 
Supppose that a program using the cache in Problem 6.31 references the 1-byte word at address 0x071A. Indicate the cache entry accessed and the cache byte value returned in hex. Indicate whether a cache miss occurs. If there is a cache miss, enter ¡°¨C¡± for ¡°Cache byte returned¡±. Hint: Pay attention to those valid bits! 
A. Address format (one bit per box): 
12 11 10 9 8 7 6 5 4 3 2 1 0 


B. Memory reference: 
Parameter Value Block offset (CO) 0x 
Index (CI) 0x Cache tag (CT) 0x Cache hit? (Y/N) Cache byte returned 0x 
6.33 ¡ô¡ô 

Repeat Problem 6.32 for memory address 0x16E8. 
A. Address format (one bit per box): 
12 11 10 9 8 7 6 5 4 3 2 1 0 

B. Memory reference: 
Parameter Value 
Cache offset (CO) 0x 
Cache index (CI) 0x Cache tag (CT) 0x Cache hit? (Y/N) Cache byte returned 0x 
6.34 ¡ô¡ô 

For the cache in Problem 6.31, list the eight memory addresses (in hex) that will hit in set 2. 
6.35 ¡ô¡ô 

Consider the following matrix transpose routine: 
1 typedef int array[4][4]; 2 3 void transpose2(array dst, array src) 4 { 5 int i, j; 6 7 for (i=0;i<4; i++) { 8 for (j=0;j<4; j++) { 9 dst[j][i] = src[i][j]; 
10 
} 

11 
} 

12 
} 



Assume this code runs on a machine with the following properties: 
. sizeof(int) == 4. . The src array starts at address 0 and the dst array starts at address 64 (decimal). . There is a single L1 data cache that is direct-mapped, write-through, write-
allocate, with a block size of 16 bytes. . The cache has a total size of 32 data bytes and the cache is initially empty. . Accesses to the src and dst arrays are the only sources of read and write 
misses, respectively. 
A. For each row and col, indicate whether the access to src[row][col] and dst[row][col] is a hit (h) or a miss (m). For example, reading src[0][0] is a miss and writing dst[0][0] is also a miss. 
dst array src array Col0 Col1 Col2 Col3 Col0 Col1 Col2 Col3 
Row 0 
m  




Row 0 Row 1 
Row 1 Row 2 
Row 2 Row 3 
Row 3 
m  




6.36 ¡ô¡ô 
Repeat Problem 6.35 for a cache with a total size of 128 data bytes. 
dst array src array Col0 Col1 Col2 Col3 Col0 Col1 Col2 Col3 Row 0 
Row 0 Row 1 
Row 1 Row 2 
Row 2 Row 3 
Row 3 
6.37 ¡ô¡ô 
This problem tests your ability to predict the cache behavior of C code. You are given the following code to analyze: 
1 int x[2][128]; 2 int i; 3 int sum=0; 4 5 for (i= 0; i< 128; i++) { 6 sum += x[0][i] * x[1][i]; 
7 } 

Assume we execute this under the following conditions: 
. sizeof(int) = 4. . Array x begins at memory address 0x0 and is stored in row-major order. . In each case below, the cache is initially empty. . The only memory accesses are to the entries of the array x. All other variables 
are stored in registers. 
Given these assumptions, estimate the miss rates for the following cases: 
A. Case 1: Assume the cache is 512 bytes, direct-mapped, with 16-byte cache blocks. What is the miss rate? 
B. Case 2: What is the miss rate if we double the cache size to 1024 bytes? 
C. Case 3: Now assume the cache is 512 bytes, two-way set associative using an LRU replacement policy, with 16-byte cache blocks. What is the cache miss rate? 
D. For Case 3, will a larger cache size help to reduce the miss rate? Why or why not? 
E. For Case 3, will a larger block size help to reduce the miss rate? Why or why not? 
6.38 ¡ô¡ô 

This is another problem that tests your ability to analyze the cache behavior of C code. Assume we execute the three summation functions in Figure 6.49 under the following conditions: 
. sizeof(int) == 4. . The machine has a 4KB direct-mapped cache with a 16-byte block size. . Within the two loops, the code uses memory accesses only for the array data. 
The loop indices and the value sum are held in registers. . Array a is stored starting at memory address 0x08000000. 
Fill in the table for the approximate cache miss rate for the two cases N = 64 and N = 60. 
Function N=64 N=60 sumA 
sumB sumC 
6.39 ¡ô 

3M. decides to make Post-It. notes by printing yellow squares on white pieces of paper. As part of the printing process, they need to set the CMYK (cyan, magenta, yellow, black) value for every point in the square. 3M hires you to determine 
1 typedef int array_t[N][N]; 2 3 int sumA(array_t a) 4 { 5 int i, j; 6 int sum=0; 7 for (i=0;i<N; i++) 8 for (j=0;j<N; j++) { 9 sum += a[i][j]; 
10 
} 11 return sum; 

12 
} 13 14 int sumB(array_t a) 15 { 16 int i, j; 17 int sum=0; 18 for (j=0;j<N; j++) 19 for (i=0;i<N; i++) { 20 sum += a[i][j]; 

21 
} 22 return sum; 

23 
} 24 25 int sumC(array_t a) 26 { 27 int i, j; 28 int sum=0; 29 for (j=0;j<N; j+=2) 30 for (i=0;i<N; i+=2) { 31 sum += (a[i][j] + a[i+1][j] 32 + a[i][j+1] + a[i+1][j+1]); 

33 
} 34 return sum; 


35 } Figure 6.49 Functions referenced in Problem 6.38. 
the ef.ciency of the following algorithms on a machine with a 2048-byte direct-mapped data cache with 32-byte blocks. You are given the following de.nitions: 
1 struct point_color { 2 int c; 3 int m; 4 int y; 5 int k; 6 }; 

7 8 struct point_color square[16][16]; 9 int i, j; 
Assume the following: 
. sizeof(int) == 4. . square begins at memory address 0. . The cache is initially empty. . The only memory accesses are to the entries of the array square. Variables i 
and j are stored in registers. 
Determine the cache performance of the following code: 
1 for(i=0; i<16; i++){ 2 for (j= 0; j < 16;j++){ 3 square[i][j].c = 0; 4 square[i][j].m = 0; 5 square[i][j].y = 1; 6 square[i][j].k = 0; 
7 
} 

8 
} 



A. What is the total number of writes? 
B. What is the total number of writes that miss in the cache? 
C. What is the miss rate? 
6.40 ¡ô 

Given the assumptions in Problem 6.39, determine the cache performance of the following code: 
1 for(i=0; i<16; i++){ 2 for (j= 0; j < 16;j++){ 3 square[j][i].c = 0; 4 square[j][i].m = 0; 5 square[j][i].y = 1; 6 square[j][i].k = 0; 
7 
} 

8 
} 



A. What is the total number of writes? 
B. What is the total number of writes that miss in the cache? 
C. What is the miss rate? 
6.41 ¡ô 
Given the assumptions in Problem 6.39, determine the cache performance of the following code: 
1 for (i= 0; i< 16;i++){ 2 for(j = 0; j <16;j++){ 3 square[i][j].y = 1; 
4 } 
5 } 6 for (i= 0; i< 16;i++){ 7 for(j = 0; j <16;j++){ 8 square[i][j].c = 0; 9 square[i][j].m = 0; 
10 square[i][j].k = 0; 
11 
} 

12 
} 


A. What is the total number of writes? 
B. What is the total number of writes that miss in the cache? 
C. What is the miss rate? 
6.42 ¡ô¡ô 
You are writing a new 3D game that you hope will earn you fame and fortune. You are currently working on a function to blank the screen buffer before drawing the next frame. The screen you are working with is a 640 ¡Á 480 array of pixels. The machine you are working on has a 64 KB direct-mapped cache with 4-byte lines. The C structures you are using are as follows: 
1 struct pixel { 2 char r; 3 char g; 4 char b; 5 char a; 6 }; 7 8 struct pixel buffer[480][640]; 9 int i, j; 
10 char *cptr; 11 int *iptr; 
Assume the following: 
. sizeof(char) == 1 and sizeof(int) == 4. . buffer begins at memory address 0. . The cache is initially empty. . The only memory accesses are to the entries of the array buffer. Variables i, 
j, cptr, and iptr are stored in registers. 

What percentage of writes in the following code will miss in the cache? 
1 for (j = 0; j < 640; j++) { 2 for (i= 0; i < 480; i++){ 3 buffer[i][j].r = 0; 4 buffer[i][j].g = 0; 5 buffer[i][j].b = 0; 6 buffer[i][j].a = 0; 
7 
} 

8 
} 


6.43 ¡ô¡ô 

Given the assumptions in Problem 6.42, what percentage of writes in the following code will miss in the cache? 
1 char *cptr = (char *) buffer; 2 for (; cptr < (((char *) buffer) + 640 * 480 * 4); cptr++) 3 *cptr = 0; 
6.44 ¡ô¡ô 

Given the assumptions in Problem 6.42, what percentage of writes in the following code will miss in the cache? 
1 int *iptr = (int *)buffer; 2 for (; iptr < ((int *)buffer + 640*480); iptr++) 3 *iptr = 0; 
6.45 ¡ô¡ô¡ô 

Download the mountain program from the CS:APP2 Web site and run it on your favorite PC/Linux system. Use the results to estimate the sizes of the caches on your system. 
6.46 ¡ô¡ô¡ô¡ô 

In this assignment, you will apply the concepts you learned in Chapters 5 and 6 to the problem of optimizing code for a memory-intensive application. Consider a procedure to copy and transpose the elements of an N ¡Á N matrix of type int. That is, for source matrix S and destination matrix D, we want to copy each element si,j to dj,i. This code can be written with a simple loop, 
1 void transpose(int *dst, int *src, int dim) 2 { 3 int i, j; 4 5 for (i = 0; i < dim; i++) 6 for (j= 0; j < dim; j++) 7 dst[j*dim + i] = src[i*dim + j]; 
8 } 

where the arguments to the procedure are pointers to the destination (dst) and source (src) matrices, as well as the matrix size N (dim). Your job is to devise a transpose routine that runs as fast as possible. 
6.47 ¡ô¡ô¡ô¡ô 
This assignment is an intriguing variation of Problem 6.46. Consider the problem of converting a directed graph g into its undirected counterpart g. The graph g has an edge from vertex u to vertex v if and only if there is an edge from u to v or from v to u in the original graph g. The graph g is represented by its adjacency matrix Gas follows. If N is the number of vertices in g, then Gis an N ¡Á N matrix and its entries are all either 0 or 1. Suppose the vertices of g are named v0,v1,v2,...,vN.1. Then G[i][j] is 1 if there is an edge from vi to vj and is 0 otherwise. Observe that the elements on the diagonal of an adjacency matrix are always 1 and that the adjacency matrix of an undirected graph is symmetric. This code can be written with a simple loop: 
1 void col_convert(int *G, int dim) { 2 int i, j; 3 4 for(i=0; i<dim;i++) 5 for (j=0; j< dim; j++) 6 G[j*dim + i] = G[j*dim + i] || G[i*dim + j]; 
7 } 
Your job is to devise a conversion routine that runs as fast as possible. As 
before, you will need to apply concepts you learned in Chapters 5 and 6 to come 
up with a good solution. 
Solutions 
to 
Practice 
Problems 

Solution to Problem 6.1 (page 565) 
The idea here is to minimize the number of address bits by minimizing the aspect ratio max(r,c)/min(r,c). In other words, the squarer the array, the fewer the address bits. 
Organization r cbbc max(b ,bc)
rr 
16 ¡Á 1  4  4  2  2  2  
16 ¡Á 4  4  4  2  2  2  
128 ¡Á 8  16  8  4  3  4  
512 ¡Á 4  32  16  5  4  5  
1024 ¡Á 4  32  32  5  5  5  

Solution to Problem 6.2 (page 573) 
The point of this little drill is to make sure you understand the relationship between cylinders and tracks. Once you have that straight, just plug and chug: 

512 bytes 400 sectors 10,000 tracks 2 surfaces 2 platters 
Disk capacity =¡Á¡Á ¡Á¡Á 
sector track surface platter disk 
= 8,192,000,000 bytes = 8.192 GB 
Solution to Problem 6.3 (page 575) 
This solution to this problem is a straightforward application of the formula for disk access time. The average rotational latency (in ms) is 
T

avg rotation = 1/2 ¡Á Tmax rotation = 1/2 ¡Á (60 secs / 15,000 RPM) ¡Á 1000 ms/sec ¡Ö 2ms 
The average transfer time is 
Tavg transfer = (60 secs / 15,000 RPM) ¡Á 1/500 sectors/track ¡Á 1000 ms/sec 
¡Ö 0.008 ms 
Putting it all together, the total estimated access time is 
Taccess = Tavg seek + Tavg rotation + Tavg transfer 
= 8ms + 2ms + 0.008 ms ¡Ö 10 ms 

Solution to Problem 6.4 (page 576) 
This is a good check of your understanding of the factors that affect disk perfor-mance. First we need to determine a few basic properties of the .le and the disk. The .le consists of 2000, 512-byte logical blocks. For the disk, Tavg seek =5ms, 
Tmax rotation =6ms, and Tavg rotation =3ms. 
A. Best case: In the optimal case, the blocks are mapped to contiguous sectors, on the same cylinder, that can be read one after the other without moving the head. Once the head is positioned over the .rst sector it takes two full rotations (1000 sectors per rotation) of the disk to read all 2000 blocks. So the total time to read the .le is Tavg seek + Tavg rotation + 2 . Tmax rotation = 5 + 3 + 12 = 20 ms. 
B. Random case: In this case, where blocks are mapped randomly to sectors, reading each of the 2000 blocks requires Tavg seek + Tavg rotation ms, so the to-tal time to read the .le is (Tavg seek + Tavg rotation) . 2000 = 16,000 ms (16 sec-onds!). 
You can see now why it¡¯s often a good idea to defragment your disk drive! 
Solution to Problem 6.5 (page 581) 
This problem, based on the zone map in Figure 6.14, is a good test of your understanding of disk geometry, and it also enables you to derive an interesting characteristic of a real disk drive. 
A. Zone 0. There are a total of 864 ¡Á 8 ¡Á 3201 = 22,125,312 sectors and 22,076,928 logical blocks assigned to zone 0, for a total of 22,125,312 . 22,076,928 = 48,384 spare sectors. Given that there are 864 ¡Á 8 = 6912 sec-tors per cylinder, there are 48,384/6912 = 7 spare cylinders in zone 0. 
B. Zone 8. A similar analysis reveals there are ((3700 ¡Á 5632) . 20,804,608)/ 5632 = 6 spare cylinders in zone 8. 
Solution to Problem 6.6 (page 583) 
This is a simple problem that will give you some interesting insights into feasibility of SSDs. Recall that for disks, 1 PB = 109 MB. Then the following straightforward translation of units yields the following predicted times for each case: 
A. Worst case sequential writes (170 MB/s): 109 ¡Á (1/170) ¡Á (1/(86,400 ¡Á 365)) ¡Ö 0.2 years. 
B. Worst case random writes (14 MB/s): 109 ¡Á (1/14) ¡Á (1/(86,400 ¡Á 365)) ¡Ö 2.25 years. 
C. Average case (20 GB/day): 109 ¡Á (1/20,000) ¡Á (1/365) ¡Ö 140 years. 
Solution to Problem 6.7 (page 586) 
In the 10-year period between 2000 and 2010, the unit price of rotating disk dropped by a factor of about 30, which means the price is dropping by roughly a factor of 2 every 2 years. Assuming this trend continues, a petabyte of storage, which costs about $300,000 in 2010, will drop below $500 after about ten of these factor-of-2 reductions. Since these are occurring every 2 years, we can expect a petabyte of storage to be available for $500 around the year 2030. 
Solution to Problem 6.8 (page 590) 
To create a stride-1 reference pattern, the loops must be permuted so that the rightmost indices change most rapidly. 
1 int sumarray3d(int a[N][N][N]) 2 { 3 int i,j,k, sum=0; 4 5 for (k=0;k<N; k++) { 6 for (i=0;i<N; i++) { 7 for (j=0;j<N; j++) { 8 sum += a[k][i][j]; 
9 
} 

10 
} 


11 } 12 return sum; 
13 } 

This is an important idea. Make sure you understand why this particular loop permutation results in a stride-1 access pattern. 
Solution to Problem 6.9 (page 590) 
The key to solving this problem is to visualize how the array is laid out in memory and then analyze the reference patterns. Function clear1 accesses the array using a stride-1 reference pattern and thus clearly has the best spatial locality. Function clear2 scans each of the Nstructs in order, which is good, but within each struct it hops around in a non-stride-1 pattern at the following offsets from the beginning of the struct: 0, 12, 4, 16, 8, 20. So clear2 has worse spatial locality than clear1. Function clear3 not only hops around within each struct, but it also hops from struct to struct. So clear3 exhibits worse spatial locality than clear2 and clear1. 
Solution to Problem 6.10 (page 598) 
The solution is a straightforward application of the de.nitions of the various cache parameters in Figure 6.28. Not very exciting, but you need to understand how the cache organization induces these partitions in the address bits before you can really understand how caches work. 
Cache mCBE Stsb 
1. 
321024 4 1 2562282 

2. 
321024 8 4 32 2453 

3. 
3210243232 1 27 05 


Solution to Problem 6.11 (page 605) 
The padding eliminates the con.ict misses. Thus, three-fourths of the references are hits. 
Solution to Problem 6.12 (page 605) 
Sometimes, understanding why something is a bad idea helps you understand why the alternative is a good idea. Here, the bad idea we are looking at is indexing the cache with the high-order bits instead of the middle bits. 
A. With high-order bit indexing, each contiguous array chunk consists of 2t blocks, where tis the number of tag bits. Thus, the .rst 2t contiguous blocks of the array would map to set 0, the next 2t blocks would map to set 1, and so on. 
B. For a direct-mapped cache where (S,E,B,m)= (512,1,32,32), the cache capacity is 512 32-byte blocks, and there are t= 18 tag bits in each cache line. Thus, the .rst 218 blocks in the array would map to set 0, the next 218 blocks to set 1. Since our array consists of only (4096 . 4)/32 = 512 blocks, all of the blocks in the array map to set 0. Thus, the cache will hold at most one array block at any point in time, even though the array is small enough to .t entirely in the cache. Clearly, using high-order bit indexing makes poor use of the cache. 
Solution to Problem 6.13 (page 609) 
The 2 low-order bits are the block offset (CO), followed by 3 bits of set index (CI), with the remaining bits serving as the tag (CT): 
12 11 10 9 8 7 6 5432 1 
CT  CT  CT  CT  CT  CT  CT  CT  CI  CI  CI  CO  CO  

Solution to Problem 6.14 (page 610) Address: 0x0E34 A. Address format (one bit per box):  
12 11 10 9 8 7  6  5  4  3  2  1  0  
CT CT CT CT CT CT  CT  CT  CI  CI  CI  CO  CO  
B. Memory reference:  
Parameter Value Cache block offset (CO) 0x0 Cache set index (CI) 0x5 Cache tag (CT) 0x71 Cache hit? (Y/N) Y Cache byte returned 0xB  
Solution to Problem 6.15 (page 611) Address: 0x0DD5 A. Address format (one bit per box):  
12 11 10 9 8 7  6  5  4  3  2  1  0  
CT CT CT CT CT CT  CT  CT  CI  CI  CI  CO  CO  
B. Memory reference:  
Parameter Value  

0  1  1  1  0  0  0  1  1  0  1  0  0  


0  1  1  0  1  1  1  0  1  0  1  0  1  


Cache block offset (CO) 0x1 Cache set index (CI) 0x5 Cache tag (CT) 0x6E Cache hit? (Y/N) N Cache byte returned ¡ª 

Solutions to Practice Problems  647  
Solution to Problem 6.16 (page 611) Address: 0x1FE4 A. Address format (one bit per box): 12 11 10 9 8 7  6  5  4  3  2  1  0  

1  1  1  1  1  1  1  1  0  0  1  0  0  

CT  CT  CT  CT  CT  CT  CT  CT  CI  CI  CI  CO  CO  
B. Memory reference:  
Parameter  Value  
Cache block offset Cache set index Cache tag Cache hit? (Y/N) Cache byte returned  0x0 0x1 0xFF N ¡ª  

Solution to Problem 6.17 (page 611) 
This problem is a sort of inverse version of Problems 6.13¨C6.16 that requires you to work backward from the contents of the cache to derive the addresses that will hit in a particular set. In this case, set 3 contains one valid line with a tag of 0x32. Since there is only one valid line in the set, four addresses will hit. These addresses have the binary form 0 0110 0100 11xx. Thus, the four hex addresses that hit in set 3 are 
0x064C, 0x064D, 0x064E, and 0x064F 
Solution to Problem 6.18 (page 618) 
A. The key to solving this problem is to visualize the picture in Figure 6.50. Notice that each cache line holds exactly one row of the array, that the cache is exactly large enough to hold one array, and that for all i, row i of src and dst maps to the same cache line. Because the cache is too small to hold both arrays, references to one array keep evicting useful lines from the other array. For example, the write to dst[0][0] evicts the line that was loaded when we read src[0][0]. So when we next read src[0][1], we have a miss. 
dst array src array Col0 Col1 Col0 Col1 
Row 0 
Row 0 
Row 1 
Row 1 

m  m  
m  m  

m  m  
m  h  


Figure 6.50 
Main memory 
0 
Cache

Figure for Problem 6.18. 
src 

Line 0
 16 
Line 1
dst 

B. When the cache is 32 bytes, it is large enough to hold both arrays. Thus, the only misses are the initial cold misses. 

dst array  src array  
Col 0  Col 1  Col 0  Col 1  
Row 0  m  h  Row 0  m  h  
Row 1  m  h  Row 1  m  h  



Solution to Problem 6.19 (page 619) 
Each 16-byte cache line holds two contiguous algae_position structures. Each loop visits these structures in memory order, reading one integer element each time. So the pattern for each loop is miss, hit, miss, hit, and so on. Notice that for this problem we could have predicted the miss rate without actually enumerating the total number of reads and misses. 
A. What is the total number of read accesses? 512 reads. 
B. What is the total number of read accesses that miss in the cache? 256 misses. 
C. What is the miss rate? 256/512 = 50%. 
Solution to Problem 6.20 (page 620) 
The key to this problem is noticing that the cache can only hold 1/2 of the ar-ray. So the column-wise scan of the second half of the array evicts the lines that were loaded during the scan of the .rst half. For example, reading the .rst ele-ment of grid[8][0] evicts the line that was loaded when we read elements from grid[0][0]. This line also contained grid[0][1]. So when we begin scanning the next column, the reference to the .rst element of grid[0][1] misses. 
A. What is the total number of read accesses? 512 reads. 
B. What is the total number of read accesses that miss in the cache? 256 misses. 
C. What is the miss rate? 256/512 = 50%. 
D. What would the miss rate be if the cache were twice as big? If the cache were twice as big, it could hold the entire grid array. The only misses would be the initial cold misses, and the miss rate would be 1/4 = 25%. 
Solution to Problem 6.21 (page 620) 
This loop has a nice stride-1 reference pattern, and thus the only misses are the initial cold misses. 
A. What is the total number of read accesses? 512 reads. 
B. What is the total number of read accesses that miss in the cache? 128 misses. 
C. What is the miss rate? 128/512 = 25%. 

D. What would the miss rate be if the cache were twice as big? Increasing the cache size by any amount would not change the miss rate, since cold misses are unavoidable. 
Solution to Problem 6.22 (page 625) 

The peak throughput from L1 is about 6500 MB/s, the clock frequency is 2670 MHz, and the individual read accesses are in units of 8-byte doubles. Thus, from this graph we can estimate that it takes roughly 2670/6500 ¡Á 8 = 3.2 ¡Ö 4 cycles to access a word from L1 on this machine. 
This page intentionally left blank 

Part 
II 
Running 
Programs 
on 
a 
System 
O
ur exploration of computer systems continues with a closer look at the systems software that builds and runs application programs. The linker combines different parts of our programs into a sin-gle .le that can be loaded into memory and executed by the processor. Modern operating systems cooperate with the hardware to provide each program with the illusion that it has exclusive use of a processor and the main memory, when in reality, multiple programs are running on the sys-tem at any point in time. 
In the .rst part of this book, you developed a good understanding of the interaction between your programs and the hardware. Part II of the book will broaden your view of systems by giving you a solid understand-ing of the interactions between your programs and the operating system. You will learn how to use services provided by the operating system to build system-level programs such as Unix shells and dynamic memory allocation packages. 
This page intentionally left blank 

CHAPTER 
7 
Linking 
7.1 Compiler Drivers 655 
7.2 Static Linking 657 
7.3 Object Files 657 
7.4 Relocatable Object Files 658 
7.5 Symbols and Symbol Tables 660 
7.6 Symbol Resolution 663 
7.7 Relocation 672 
7.8 Executable Object Files 678 
7.9 Loading Executable Object Files 679 
7.10 Dynamic Linking with Shared Libraries 681 
7.11 Loading and Linking Shared Libraries from Applications 683 
7.12 Position-Independent Code (PIC) 687 
7.13 Tools for Manipulating Object Files 690 7.14 Summary 691 Bibliographic Notes 691 Homework Problems 692 Solutions to Practice Problems 698 
Linking is the process of collecting and combining various pieces of code and data into a single .le that can be loaded (copied) into memory and executed. Linking can be performed at compile time, when the source code is translated into machine code; at load time, when the program is loaded into memory and executed by the loader; and even at run time, by application programs. On early computer systems, linking was performed manually. On modern systems, linking is performed automatically by programs called linkers. 
Linkers play a crucial role in software development because they enable separate compilation. Instead of organizing a large application as one monolithic source .le, we can decompose it into smaller, more manageable modules that can be modi.ed and compiled separately. When we change one of these modules, we simply recompile it and relink the application, without having to recompile the other .les. 
Linking is usually handled quietly by the linker, and is not an important 
issue for students who are building small programs in introductory programming 
classes. So why bother learning about linking? 
. Understanding linkers will help you build large programs. Programmers who build large programs often encounter linker errors caused by missing modules, missing libraries, or incompatible library versions. Unless you understand how a linker resolves references, what a library is, and how a linker uses a library to resolve references, these kinds of errors will be baf.ing and frustrating. 
. Understanding linkers will help you avoid dangerous programming errors.The decisions that Unix linkers make when they resolve symbol references can silently affect the correctness of your programs. Programs that incorrectly de-.ne multiple global variables pass through the linker without any warnings in the default case. The resulting programs can exhibit baf.ing run-time behav-ior and are extremely dif.cult to debug. We will show you how this happens and how to avoid it. 
. Understanding linking will help you understand how language scoping rules are implemented.For example, what is the difference between global and local variables? What does it really mean when you de.ne a variable or function with the static attribute? 
. Understanding linking will help you understand other important systems con-cepts. The executable object .les produced by linkers play key roles in impor-tant systems functions such as loading and running programs, virtual memory, paging, and memory mapping. 
. Understanding linking will enable you to exploit shared libraries. For many years, linking was considered to be fairly straightforward and uninteresting. However, with the increased importance of shared libraries and dynamic linking in modern operating systems, linking is a sophisticated process that provides the knowledgeable programmer with signi.cant power. For exam-ple, many software products use shared libraries to upgrade shrink-wrapped binaries at run time. Also, most Web servers rely on dynamic linking of shared libraries to serve dynamic content. 

This chapter provides a thorough discussion of all aspects of linking, from traditional static linking, to dynamic linking of shared libraries at load time, to dynamic linking of shared libraries at run time. We will describe the basic mechanisms using real examples, and we will identify situations in which linking issues can affect the performance and correctness of your programs. 
To keep things concrete and understandable, we will couch our discussion in the context of an x86 system running Linux and using the standard ELF object .le format. For clarity, we will focus our discussion on linking 32-bit code, which is easier to understand than linking 64-bit code.1 However, it is important to realize that the basic concepts of linking are universal, regardless of the operating system, the ISA, or the object .le format. Details may vary, but the concepts are the same. 
7.1 
Compiler 
Drivers 

Consider the C program in Figure 7.1. It consists of two source .les, main.c and swap.c. Function main() calls swap, which swaps the two elements in the external global array buf. Granted, this is a strange way to swap two numbers, but it will serve as a small running example throughout this chapter that will allow us to make some important points about how linking works. 
Most compilation systems provide a compiler driver that invokes the language preprocessor, compiler, assembler, and linker, as needed on behalf of the user. For example, to build the example program using the GNU compilation system, we might invoke the gcc driver by typing the following command to the shell: 
unix> gcc -O2 -g -o p main.c swap.c 
Figure 7.2 summarizes the activities of the driver as it translates the example program from an ASCII source .le into an executable object .le. (If you want to see these steps for yourself, run gcc with the -v option.) The driver .rst runs the C preprocessor (cpp), which translates the C source .le main.c into an ASCII intermediate .le main.i: 
cpp [other arguments] main.c /tmp/main.i 
Next, the driver runs the C compiler (cc1), which translates main.i into an ASCII assembly language .le main.s. 
cc1 /tmp/main.i main.c -O2 [other arguments] -o /tmp/main.s 
Then, the driver runs the assembler (as), which translates main.s into a relocatable object .le main.o: 
as [other arguments] -o /tmp/main.o /tmp/main.s 
1. You can generate 32-bit code on an x86-64 system using gcc -m32. 
(a) main.c (b) swap.c 
code/link/main.c code/link/swap.c 

1 /* main.c */ 1 /* swap.c */ 2 void swap(); 2 extern int buf[]; 33 4 int buf[2] = {1, 2}; 4 int *bufp0 = &buf[0]; 55 int *bufp1; 6 int main() 6 7 { 7 void swap() 8 swap(); 8 { 9 return 0; 9 int temp; 
10 } 10 11 bufp1 = &buf[1];
code/link/main.c 
12 temp = *bufp0; 13 *bufp0 = *bufp1; 14 *bufp1 = temp; 
15 } 
code/link/swap.c 

Figure 7.1 Example program 1: The example program consists of two source .les, main.c and swap.c. The main function initializes a two-element array of ints, and then calls the swap function to swap the pair. 
main.c swap.c Source files 


swap.o Relocatable 

The driver goes through the same process to generate swap.o. Finally, it runs the linker program ld, which combines main.o and swap.o, along with the necessary system object .les, to create the executable object .le p: 
ld -o p [system object files and args] /tmp/main.o /tmp/swap.o To run the executable p, we type its name on the Unix shell¡¯s command line: unix> ./p 

The shell invokes a function in the operating system called the loader, which copies the code and data in the executable .le p into memory, and then transfers control to the beginning of the program. 
7.2 
Static 
Linking 

Static linkers such as the Unix ld program take as input a collection of relocatable object .les and command-line arguments and generate as output a fully linked executable object .le that can be loaded and run. The input relocatable object .les consist of various code and data sections. Instructions are in one section, initialized global variables are in another section, and uninitialized variables are in yet another section. 
To build the executable, the linker must perform two main tasks: 
. Symbol resolution. Object .les de.ne and reference symbols. The purpose of symbol resolution is to associate each symbol reference with exactly one symbol de.nition. 
. Relocation. Compilers and assemblers generate code and data sections that start at address 0. The linker relocates these sections by associating a memory location with each symbol de.nition, and then modifying all of the references to those symbols so that they point to this memory location. 
The sections that follow describe these tasks in more detail. As you read, keep in mind some basic facts about linkers: Object .les are merely collections of blocks of bytes. Some of these blocks contain program code, others contain program data, and others contain data structures that guide the linker and loader. A linker concatenates blocks together, decides on run-time locations for the concatenated blocks, and modi.es various locations within the code and data blocks. Linkers have minimal understanding of the target machine. The compilers and assemblers that generate the object .les have already done most of the work. 
7.3 
Object 
Files 

Object .les come in three forms: 
. Relocatable object .le. Contains binary code and data in a form that can be combined with other relocatable object .les at compile time to create an executable object .le. 
. Executable object .le. Contains binary code and data in a form that can be copied directly into memory and executed. . Shared object .le. A special type of relocatable object .le that can be loaded into memory and linked dynamically, at either load time or run time. 
Compilers and assemblers generate relocatable object .les (including shared object .les). Linkers generate executable object .les. Technically, an object module 
is a sequence of bytes, and an object .le is an object module stored on disk in a .le. However, we will use these terms interchangeably. 
Object .le formats vary from system to system. The .rst Unix systems from Bell Labs used the a.out format. (To this day, executables are still referred to as a.out .les.) Early versions of System V Unix used the Common Object File format (COFF). Windows NT uses a variant of COFF called the Portable Executable (PE) format. Modern Unix systems¡ªsuch as Linux, later versions of System V Unix, BSD Unix variants, and Sun Solaris¡ªuse the Unix Executable and Linkable Format (ELF). Although our discussion will focus on ELF, the basic concepts are similar, regardless of the particular format. 
7.4 
Relocatable 
Object 
Files 

Figure 7.3 shows the format of a typical ELF relocatable object .le. The ELF header begins with a 16-byte sequence that describes the word size and byte ordering of the system that generated the .le. The rest of the ELF header contains information that allows a linker to parse and interpret the object .le. This includes the size of the ELF header, the object .le type (e.g., relocatable, executable, or shared), the machine type (e.g., IA32), the .le offset of the section header table, and the size and number of entries in the section header table. The locations and sizes of the various sections are described by the section header table, which contains a .xed sized entry for each section in the object .le. 
Sandwiched between the ELF header and the section header table are the 
sections themselves. A typical ELF relocatable object .le contains the following 
sections: 
.text: The machine code of the compiled program. .rodata: Read-only data such as the format strings in printf statements, and jump tables for switch statements (see Problem 7.14). 

ELF header  
.text  
.rodata  
.data  
.bss  
.symtab  
.rel.text  
.rel.data  
.debug  
.line  
.strtab  
Section header table  

0
Figure 7.3 

Typical ELF relocatable object .le. 
Sections 
Describes object file sections 

.data: Initialized global C variables. Local C variables are maintained at run time on the stack, and do not appear in either the .data or .bss sections. 
.bss: Uninitialized global C variables. This section occupies no actual space in the object .le; it is merely a place holder. Object .le formats distin-guish between initialized and uninitialized variables for space ef.ciency: uninitialized variables do not have to occupy any actual disk space in the object .le. 
.symtab: A symbol table with information about functions and global vari-ables that are de.ned and referenced in the program. Some programmers mistakenly believe that a program must be compiled with the -g option to get symbol table information. In fact, every relocatable object .le has a symbol table in .symtab. However, unlike the symbol table inside a compiler, the .symtab symbol table does not contain entries for local variables. 
.rel.text: A list of locations in the .text section that will need to be modi.ed when the linker combines this object .le with others. In general, any instruction that calls an external function or references a global variable will need to be modi.ed. On the other hand, instructions that call local functions do not need to be modi.ed. Note that relocation information is not needed in executable object .les, and is usually omitted unless the user explicitly instructs the linker to include it. 
.rel.data: Relocation information for any global variables that are refer-enced or de.ned by the module. In general, any initialized global variable whose initial value is the address of a global variable or externally de.ned function will need to be modi.ed. 
.debug: A debugging symbol table with entries for local variables and typedefs de.ned in the program, global variables de.ned and referenced in the program, and the original C source .le. It is only present if the compiler driver is invoked with the -g option. 
.line: A mapping between line numbers in the original C source program and machine code instructions in the .text section. It is only present if the compiler driver is invoked with the -g option. 
.strtab: A string table for the symbol tables in the .symtab and .debug sections, and for the section names in the section headers. A string table is a sequence of null-terminated character strings. 
Aside Why is uninitialized data called .bss? 
The use of the term .bss to denote uninitialized data is universal. It was originally an acronym for the ¡°Block Storage Start¡± instruction from the IBM 704 assembly language (circa 1957) and the acronym has stuck. A simple way to remember the difference between the .data and .bss sections is to think of ¡°bss¡± as an abbreviation for ¡°Better Save Space!¡± 
7.5 
Symbols 
and 
Symbol 
Tables 

Each relocatable object module, m, has a symbol table that contains information about the symbols that are de.ned and referenced by m. In the context of a linker, there are three different kinds of symbols: 
. Global symbols that are de.ned by module m and that can be referenced by other modules. Global linker symbols correspond to nonstatic C functions and global variables that are de.ned without the C static attribute. 
. Global symbols that are referenced by module m but de.ned by some other module. Such symbols are called externals and correspond to C functions and variables that are de.ned in other modules. 
. Local symbols that are de.ned and referenced exclusively by module m. Some local linker symbols correspond to C functions and global variables that are de.ned with the static attribute. These symbols are visible anywhere within module m, but cannot be referenced by other modules. The sections in an object .le and the name of the source .le that corresponds to module m also get local symbols. 
It is important to realize that local linker symbols are not the same as local program variables. The symbol table in .symtab does not contain any symbols that correspond to local nonstatic program variables. These are managed at run time on the stack and are not of interest to the linker. 
Interestingly, local procedure variables that are de.ned with the C static attribute are not managed on the stack. Instead, the compiler allocates space in .data or .bss for each de.nition and creates a local linker symbol in the symbol table with a unique name. For example, suppose a pair of functions in the same module de.ne a static local variable x: 
1 int f() 2 { 3 static intx=0; 4 return x; 
5 } 6 7 int g() 8 { 9 static intx=1; 
10 return x; 
11 } 
In this case, the compiler allocates space for two integers in .data and exports a pair of unique local linker symbols to the assembler. For example, it might use x.1 for the de.nition in function f and x.2 for the de.nition in function g. 

New to C? Hiding variable and function names with static 
C programmers use the static attribute to hide variable and function declarations inside modules, much as you would use public and private declarations in Java and C++. C source .les play the role of modules. Any global variable or function declared with the static attribute is private to that module. Similarly, any global variable or function declared without the static attribute is public and can be accessed by any other module. It is good programming practice to protect your variables and functions with the static attribute wherever possible. 
Symbol tables are built by assemblers, using symbols exported by the compiler into the assembly-language .s .le. An ELF symbol table is contained in the .symtab section. It contains an array of entries. Figure 7.4 shows the format of each entry. 
The name is a byte offset into the string table that points to the null-terminated string name of the symbol. The value is the symbol¡¯s address. For relocatable modules, the value is an offset from the beginning of the section where the object is de.ned. For executable object .les, the value is an absolute run-time address. The size is the size (in bytes) of the object. The type is usually either data or function. The symbol table can also contain entries for the individual sections and for the path name of the original source .le. So there are distinct types for these objects as well. The binding .eld indicates whether the symbol is local or global. 
Each symbol is associated with some section of the object .le, denoted by the section .eld, which is an index into the section header table. There are three special pseudo sections that don¡¯t have entries in the section header table: ABS is for symbols that should not be relocated. UNDEF is for unde.ned sym-bols, that is, symbols that are referenced in this object module but de.ned else-where. COMMON is for uninitialized data objects that are not yet allocated. For COMMON symbols, the value .eld gives the alignment requirement, and size gives the minimum size. 
code/link/elfstructs.c 

1 typedef struct { 2 int name; /* String table offset */ 3 int value; /* Section offset, or VM address */ 4 int size; /* Object size in bytes */ 5 char type:4, /* Data, func, section, or src file name (4 bits) */ 6 binding:4; /* Local or global (4 bits) */ 7 char reserved; /* Unused */ 8 char section; /* Section header index, ABS, UNDEF, */ 9 /* Or COMMON */ 
10 } Elf_Symbol; 
code/link/elfstructs.c 

Figure 7.4 ELF symbol table entry. type and binding are four bits each. 
For example, here are the last three entries in the symbol table for main.o,as 
displayed by the GNU readelf tool. The .rst eight entries, which are not shown, 
are local symbols that the linker uses internally. 
Num: Value Size Type Bind Ot Ndx Name 
8: 0 8 OBJECT GLOBAL 0 3 buf 
9: 0 17 FUNC GLOBAL 0 1 main 
10: 0 0 NOTYPE GLOBAL 0 UND swap 
In this example, we see an entry for the de.nition of global symbol buf,an 8-byte object located at an offset (i.e., value) of zero in the .data section. This is followed by the de.nition of the global symbol main, a 17-byte function located at an offset of zero in the .text section. The last entry comes from the reference for the external symbol swap. Readelf identi.es each section by an integer index. Ndx=1 denotes the .text section, and Ndx=3 denotes the .data section. 
Similarly, here are the symbol table entries for swap.o: 
Num: Value Size Type Bind Ot Ndx Name 
8: 0 4 OBJECT GLOBAL 0 3 bufp0 
9: 0 0 NOTYPE GLOBAL 0 UND buf 
10: 0 39 FUNC GLOBAL 0 1 swap 
11: 4 4 OBJECT GLOBAL 0 COM bufp1 
First, we see an entry for the de.nition of the global symbol bufp0, which is a 4-byte initialized object starting at offset 0 in .data. The next symbol comes from the reference to the external buf symbol in the initialization code for bufp0. This is followed by the global symbol swap, a 39-byte function at an offset of zero in .text. The last entry is the global symbol bufp1, a 4-byte uninitialized data object (with a 4-byte alignment requirement) that will eventually be allocated as a .bss object when this module is linked. 
Practice Problem 7.1 
This problem concerns the swap.o module from Figure 7.1(b). For each symbol that is de.ned or referenced in swap.o, indicate whether or not it will have a symbol table entry in the .symtab section in module swap.o. If so, indicate the module that de.nes the symbol (swap.o or main.o), the symbol type (local, global, or extern), and the section (.text, .data,or .bss) it occupies in that module. 
Symbol swap.o .symtab entry? Symbol type Module where de.ned Section 
buf 
bufp0 bufp1 swap temp 

7.6 
Symbol 
Resolution 

The linker resolves symbol references by associating each reference with exactly one symbol de.nition from the symbol tables of its input relocatable object .les. Symbol resolution is straightforward for references to local symbols that are de-.ned in the same module as the reference. The compiler allows only one de.nition of each local symbol per module. The compiler also ensures that static local vari-ables, which get local linker symbols, have unique names. 
Resolving references to global symbols, however, is trickier. When the com-piler encounters a symbol (either a variable or function name) that is not de.ned in the current module, it assumes that it is de.ned in some other module, gener-ates a linker symbol table entry, and leaves it for the linker to handle. If the linker is unable to .nd a de.nition for the referenced symbol in any of its input modules, it prints an (often cryptic) error message and terminates. For example, if we try to compile and link the following source .le on a Linux machine, 
1 void foo(void); 
2 
3 int main() { 
4 foo(); 

5 return 0; 
6 } 

then the compiler runs without a hitch, but the linker terminates when it cannot resolve the reference to foo: 
unix> gcc -Wall -O2 -o linkerror linkerror.c /tmp/ccSz5uti.o: In function ¡®main¡¯: /tmp/ccSz5uti.o(.text+0x7): undefined reference to ¡®foo¡¯ collect2: ld returned 1 exit status 
Symbol resolution for global symbols is also tricky because the same symbol might be de.ned by multiple object .les. In this case, the linker must either .ag an error or somehow choose one of the de.nitions and discard the rest. The approach adopted by Unix systems involves cooperation between the compiler, assembler, and linker, and can introduce some baf.ing bugs to the unwary programmer. 
Aside Mangling of linker symbols in C++ and Java 
Both C++ and Java allow overloaded methods that have the same name in the source code but different 
parameter lists. So how does the linker tell the difference between these different overloaded functions? 
Overloaded functions in C++ and Java work because the compiler encodes each unique method and 
parameter list combination into a unique name for the linker. This encoding process is called mangling, 
and the inverse process demangling. 
Happily, C++ and Java use compatible mangling schemes. A mangled class name consists of the 
integer number of characters in the name followed by the original name. For example, the class Foo 
is encoded as 3Foo. A method is encoded as the original method name, followed by __, followed by the mangled class name, followed by single letter encodings of each argument. For example, Foo::bar(int, long) is encoded as bar__3Fooil. Similar schemes are used to mangle global variable and template names. 
7.6.1 How Linkers Resolve Multiply De.ned Global Symbols 
At compile time, the compiler exports each global symbol to the assembler as either strong or weak, and the assembler encodes this information implicitly in the symbol table of the relocatable object .le. Functions and initialized global variables get strong symbols. Uninitialized global variables get weak symbols. For the example program in Figure 7.1, buf, bufp0, main, and swap are strong symbols; bufp1 is a weak symbol. 
Given this notion of strong and weak symbols, Unix linkers use the following rules for dealing with multiply de.ned symbols: 
. Rule 1: Multiple strong symbols are not allowed. . Rule 2: Given a strong symbol and multiple weak symbols, choose the strong symbol. . Rule 3: Given multiple weak symbols, choose any of the weak symbols. 
For example, suppose we attempt to compile and link the following two C modules: 
1 /* foo1.c */ 1 /* bar1.c */ 2 int main() 2 int main() 3 { 3 { 4 return 0; 4 return 0; 
5 } 5 } 
In this case, the linker will generate an error message because the strong symbol main is de.ned multiple times (rule 1): 
unix> gcc foo1.c bar1.c /tmp/cca015022.o: In function ¡®main¡¯: /tmp/cca015022.o(.text+0x0): multiple definition of ¡®main¡¯ /tmp/cca015021.o(.text+0x0): first defined here 
Similarly, the linker will generate an error message for the following modules because the strong symbol x is de.ned twice (rule 1): 
1 /* foo2.c */ 
1 /* bar2.c */ 
2 int x = 15213; 
2 int x = 15213; 
3 
3 
4 int main() 
4 void f() 
5 { 5 { 
6 return 0; 
6 } 
7 } 

However, if x is uninitialized in one module, then the linker will quietly choose the strong symbol de.ned in the other (rule 2): 
1 /* foo3.c */ 1 /* bar3.c */ 2 #include <stdio.h> 
2 int x; 3 void f(void); 3 
44 void f() 5 int x = 15213; 5 { 
66 x = 15212; 7 int main() 7 } 8 { 9 f(); 10 printf("x = %d\n", x); 11 return 0; 
12 } 

At run time, function f changes the value of x from 15213 to 15212, which might come as an unwelcome surprise to the author of function main! Notice that the linker normally gives no indication that it has detected multiple de.nitions of x: 
unix> gcc -o foobar3 foo3.c bar3.c unix> ./foobar3 x = 15212 
The same thing can happen if there are two weak de.nitions of x (rule 3): 
1 /* foo4.c */ 1 /* bar4.c */ 2 #include <stdio.h> 2 int x; 3 void f(void); 3 44 void f() 
5 int x; 5 { 66 x = 15212; 7 int main() 7 } 8 { 9 x = 15213; 10 f(); 11 printf("x = %d\n", x); 12 return 0; 
13 } 

The application of rules 2 and 3 can introduce some insidious run-time bugs that are incomprehensible to the unwary programmer, especially if the duplicate symbol de.nitions have different types. Consider the following example, in which x is de.ned as an int in one module and a double in another: 
1 /* foo5.c */ 1 /* bar5.c */ 2 #include <stdio.h> 2 double x; 3 void f(void); 3 
44 void f() 5 int x = 15213; 5 { 6 int y = 15212; 6 x = -0.0; 
77 } 8 int main() 9 { 
10 f(); 11 printf("x = 0x%x y = 0x%x \n", 12 x, y); 13 return 0; 
14 } 
On an IA32/Linux machine, doubles are 8 bytes and ints are 4 bytes. Thus, the assignment x = -0.0 in line 6 of bar5.c will overwrite the memory locations for x and y (lines 5 and 6 in foo5.c) with the double-precision .oating-point representation of negative zero! 
linux> gcc -o foobar5 foo5.c bar5.c linux> ./foobar5 x = 0x0 y = 0x80000000 
This is a subtle and nasty bug, especially because it occurs silently, with no warning from the compilation system, and because it typically manifests itself much later in the execution of the program, far away from where the error occurred. In a large system with hundreds of modules, a bug of this kind is extremely hard to .x, especially because many programmers are not aware of how linkers work. When in doubt, invoke the linker with a .ag such as the gcc -fno-common .ag, which triggers an error if it encounters multiply de.ned global symbols. 
Practice Problem 7.2 
In this problem, let REF(x.i) --> DEF(x.k) denote that the linker will associate an arbitrary reference to symbol x in module i to the de.nition of x in module k. For each example that follows, use this notation to indicate how the linker would resolve references to the multiply de.ned symbol in each module. If there is a link-time error (rule 1), write ¡°ERROR.¡± If the linker arbitrarily chooses one of the de.nitions (rule 3), write ¡°UNKNOWN.¡± 
A. /* Module 1 */ /* Module 2 */ 
int main() int main; { int p2() }{ 
} 

(a) 
REF(main.1) --> DEF( . ) 

(b) 
REF(main.2) --> DEF( . ) 

B. 
/* Module 1 */ /* Module 2 */ 


void main() int main=1; { int p2() }{ } 
(a) 
REF(main.1) --> DEF( . ) 

(b) 
REF(main.2) --> DEF( . ) 

C. 
/* Module 1 */ /* Module 2 */ 


int x; double x=1.0; void main() int p2() {{ }} 
(a) 
REF(x.1) --> DEF( . ) 

(b) 
REF(x.2) --> DEF( . ) 


7.6.2 Linking with Static Libraries 
So far, we have assumed that the linker reads a collection of relocatable object .les and links them together into an output executable .le. In practice, all compilation systems provide a mechanism for packaging related object modules into a single .le called a static library, which can then be supplied as input to the linker. When it builds the output executable, the linker copies only the object modules in the library that are referenced by the application program. 
Why do systems support the notion of libraries? Consider ANSI C, which de.nes an extensive collection of standard I/O, string manipulation, and integer math functions such as atoi, printf, scanf, strcpy, and rand. They are available to every C program in the libc.a library. ANSI C also de.nes an extensive collection of .oating-point math functions such as sin, cos, and sqrt in the libm.a library. 
Consider the different approaches that compiler developers might use to pro-vide these functions to users without the bene.t of static libraries. One approach would be to have the compiler recognize calls to the standard functions and to generate the appropriate code directly. Pascal, which provides a small set of stan-dard functions, takes this approach, but it is not feasible for C, because of the large number of standard functions de.ned by the C standard. It would add signi.cant complexity to the compiler and would require a new compiler version each time a function was added, deleted, or modi.ed. To application programmers, however, this approach would be quite convenient because the standard functions would always be available. 
Another approach would be to put all of the standard C functions in a single 
relocatable object module, say, libc.o, that application programmers could link 
into their executables: 
unix> gcc main.c /usr/lib/libc.o 
This approach has the advantage that it would decouple the implementation of the standard functions from the implementation of the compiler, and would still be reasonably convenient for programmers. However, a big disadvantage is that every executable .le in a system would now contain a complete copy of the collection of standard functions, which would be extremely wasteful of disk space. (On a typical system, libc.a is about 8 MB and libm.a is about 1 MB.) Worse, each running program would now contain its own copy of these functions in memory, which would be extremely wasteful of memory. Another big disadvantage is that any change to any standard function, no matter how small, would require the library developer to recompile the entire source .le, a time-consuming operation that would complicate the development and maintenance of the standard functions. 
We could address some of these problems by creating a separate relocatable .le for each standard function and storing them in a well-known directory. How-ever, this approach would require application programmers to explicitly link the appropriate object modules into their executables, a process that would be error prone and time consuming: 
unix> gcc main.c /usr/lib/printf.o /usr/lib/scanf.o ... 
The notion of a static library was developed to resolve the disadvantages of these various approaches. Related functions can be compiled into separate object modules and then packaged in a single static library .le. Application programs can then use any of the functions de.ned in the library by specifying a single .le name on the command line. For example, a program that uses functions from the standard C library and the math library could be compiled and linked with a command of the form 
unix> gcc main.c /usr/lib/libm.a /usr/lib/libc.a 
At link time, the linker will only copy the object modules that are referenced by the program, which reduces the size of the executable on disk and in memory. On the other hand, the application programmer only needs to include the names of a few library .les. (In fact, C compiler drivers always pass libc.a to the linker, so the reference to libc.a mentioned previously is unnecessary.) 
On Unix systems, static libraries are stored on disk in a particular .le format known as an archive. An archive is a collection of concatenated relocatable object .les, with a header that describes the size and location of each member object .le. Archive .lenames are denoted with the .a suf.x. To make our discussion of libraries concrete, suppose that we want to provide the vector routines in Figure 7.5 in a static library called libvector.a. 

(a) addvec.o (b) multvec.o 
code/link/addvec.c code/link/multvec.c 

1 void addvec(int *x, int *y, 1 void multvec(int *x, int *y, 2 int *z, int n) 2 int *z, int n) 3 { 3 { 4 int i; 4 int i; 55 6 for (i=0;i<n; i++) 6 for (i=0;i<n; i++) 7 z[i] = x[i] + y[i]; 7 z[i] = x[i] * y[i]; 
8 } 8 } 
code/link/addvec.c code/link/multvec.c 

Figure 7.5 Member object .les in libvector.a. 
To create the library, we would use the ar tool as follows: 
unix> gcc -c addvec.c multvec.c unix> ar rcs libvector.a addvec.o multvec.o 
To use the library, we might write an application such as main2.c in Figure 7.6, which invokes the addvec library routine. (The include (header) .le vector.h de.nes the function prototypes for the routines in libvector.a.) 
code/link/main2.c 

1 /* main2.c */ 2 #include <stdio.h> 3 #include "vector.h" 4 5 int x[2] = {1, 2}; 6 int y[2] = {3, 4}; 7 int z[2]; 8 9 int main() 
10 { 11 addvec(x, y, z, 2); 12 printf("z = [%d %d]\n", z[0], z[1]); 13 return 0; 
14 } 
code/link/main2.c 

Figure 7.6 Example program 2: This program calls member functions in the static libvector.a library. 
Source files 


Static libraries 
printf.o and any other 
Relocatable 
modules called by printf.o
object files 

Figure 7.7 Linking with static libraries. 
To build the executable, we would compile and link the input .les main.o and libvector.a: 
unix> gcc -O2 -c main2.c unix> gcc -static -o p2 main2.o ./libvector.a 
Figure 7.7 summarizes the activity of the linker. The -static argument tells the compiler driver that the linker should build a fully linked executable object .le that can be loaded into memory and run without any further linking at load time. When the linker runs, it determines that the addvec symbol de.ned by addvec.o is referenced by main.o, so it copies addvec.o into the executable. Since the program doesn¡¯t reference any symbols de.ned by multvec.o, the linker does not copy this module into the executable. The linker also copies the printf.o module from libc.a, along with a number of other modules from the C run-time system. 
7.6.3 How Linkers Use Static Libraries to Resolve References 
While static libraries are useful and essential tools, they are also a source of confusion to programmers because of the way the Unix linker uses them to resolve external references. During the symbol resolution phase, the linker scans the relocatable object .les and archives left to right in the same sequential order that they appear on the compiler driver¡¯s command line. (The driver automatically translates any .c .les on the command line into .o .les.) During this scan, the linker maintains a set E of relocatable object .les that will be merged to form the executable, a set U of unresolved symbols (i.e., symbols referred to, but not yet de.ned), and a set D of symbols that have been de.ned in previous input .les. Initially, E, U , and D are empty. 
. For each input .le f on the command line, the linker determines if f is an object .le or an archive. If f is an object .le, the linker adds f to E, updates U and D to re.ect the symbol de.nitions and references in f , and proceeds to the next input .le. 

. If f is an archive, the linker attempts to match the unresolved symbols in U against the symbols de.ned by the members of the archive. If some archive member, m, de.nes a symbol that resolves a reference in U , then m is added to E, and the linker updates U and D to re.ect the symbol de.nitions and references in m. This process iterates over the member object .les in the archive until a .xed point is reached where U and D no longer change. At this point, any member object .les not contained in E are simply discarded and the linker proceeds to the next input .le. 
. If U is nonempty when the linker .nishes scanning the input .les on the command line, it prints an error and terminates. Otherwise, it merges and relocates the object .les in E to build the output executable .le. 
Unfortunately, this algorithm can result in some baf.ing link-time errors because the ordering of libraries and object .les on the command line is signi.cant. If the library that de.nes a symbol appears on the command line before the object .le that references that symbol, then the reference will not be resolved and linking will fail. For example, consider the following: 
unix> gcc -static ./libvector.a main2.c /tmp/cc9XH6Rp.o: In function ¡®main¡¯: /tmp/cc9XH6Rp.o(.text+0x18): undefined reference to ¡®addvec¡¯ 
What happened? When libvector.a is processed, U is empty, so no member object .les from libvector.a are added to E. Thus, the reference to addvec is never resolved and the linker emits an error message and terminates. 
The general rule for libraries is to place them at the end of the command line. If the members of the different libraries are independent, in that no member references a symbol de.ned by another member, then the libraries can be placed at the end of the command line in any order. 
If, on the other hand, the libraries are not independent, then they must be ordered so that for each symbol s that is referenced externally by a member of an archive, at least one de.nition of s follows a reference to s on the command line. For example, suppose foo.c calls functions in libx.a and libz.a that call func-tions in liby.a. Then libx.a and libz.a must precede liby.a on the command line: 
unix> gcc foo.c libx.a libz.a liby.a 
Libraries can be repeated on the command line if necessary to satisfy the dependence requirements. For example, suppose foo.c calls a function in libx.a that calls a function in liby.a that calls a function in libx.a. Then libx.a must be repeated on the command line: 
unix> gcc foo.c libx.a liby.a libx.a 
Alternatively, we could combine libx.a and liby.a into a single archive. 
Practice Problem 7.3 
Let a and b denote object modules or static libraries in the current directory, and let a¡úb denote that a depends on b, in the sense that b de.nes a symbol that is referenced by a. For each of the following scenarios, show the minimal command line (i.e., one with the least number of object .le and library arguments) that will allow the static linker to resolve all symbol references. 
A. p.o ¡ú libx.a. 
B. p.o ¡ú libx.a ¡ú liby.a. 
C. p.o ¡ú libx.a ¡ú liby.a and liby.a ¡ú libx.a ¡úp.o. 
7.7 
Relocation 

Once the linker has completed the symbol resolution step, it has associated each symbol reference in the code with exactly one symbol de.nition (i.e., a symbol table entry in one of its input object modules). At this point, the linker knows the exact sizes of the code and data sections in its input object modules. It is now ready to begin the relocation step, where it merges the input modules and assigns run-time addresses to each symbol. Relocation consists of two steps: 
. Relocating sections and symbol de.nitions. In this step, the linker merges all sections of the same type into a new aggregate section of the same type. For example, the .data sections from the input modules are all merged into one section that will become the .data section for the output executable object .le. The linker then assigns run-time memory addresses to the new aggregate sections, to each section de.ned by the input modules, and to each symbol de.ned by the input modules. When this step is complete, every instruction and global variable in the program has a unique run-time memory address. 
. Relocating symbol references within sections. In this step, the linker modi.es every symbol reference in the bodies of the code and data sections so that they point to the correct run-time addresses. To perform this step, the linker relies on data structures in the relocatable object modules known as relocation entries, which we describe next. 
7.7.1 Relocation Entries 
When an assembler generates an object module, it does not know where the code and data will ultimately be stored in memory. Nor does it know the locations of any externally de.ned functions or global variables that are referenced by the module. So whenever the assembler encounters a reference to an object whose ultimate 
code/link/elfstructs.c 

1 typedef struct { 2 int offset; /* Offset of the reference to relocate */ 3 int symbol:24, /* Symbol the reference should point to */ 4 type:8; /* Relocation type */ 5 } Elf32_Rel; 
code/link/elfstructs.c 

Figure 7.8 ELF relocation entry. Each entry identi.es a reference that must be relocated. 
location is unknown, it generates a relocation entry that tells the linker how to modify the reference when it merges the object .le into an executable. Relocation entries for code are placed in .rel.text. Relocation entries for initialized data are placed in .rel.data. 
Figure 7.8 shows the format of an ELF relocation entry. The offset is the section offset of the reference that will need to be modi.ed. The symbol identi.es the symbol that the modi.ed reference should point to. The type tells the linker how to modify the new reference. 
ELF de.nes 11 different relocation types, some quite arcane. We are con-cerned with only the two most basic relocation types: 
. R_386_PC32: Relocate a reference that uses a 32-bit PC-relative address. Recall from Section 3.6.3 that a PC-relative address is an offset from the current run-time value of the program counter (PC). When the CPU executes an instruction using PC-relative addressing, it forms the effective address (e.g., the target of the call instruction) by adding the 32-bit value encoded in the instruction to the current run-time value of the PC, which is always the address of the next instruction in memory. 
. R_386_32: Relocate a reference that uses a 32-bit absolute address. With absolute addressing, the CPU directly uses the 32-bit value encoded in the instruction as the effective address, without further modi.cations. 
7.7.2 Relocating Symbol References 
Figure 7.9 shows the pseudo code for the linker¡¯s relocation algorithm. Lines 1 and 2 iterate over each section s and each relocation entry r associated with each section. For concreteness, assume that each section s is an array of bytes and that each relocation entry r is a struct of type Elf32_Rel, as de.ned in Figure 7.8. Also, assume that when the algorithm runs, the linker has already chosen run-time addresses for each section (denoted ADDR(s)) and each sym-bol (denoted ADDR(r.symbol)). Line 3 computes the address in the s array of the 4-byte reference that needs to be relocated. If this reference uses PC-relative 
1 foreach section s { 2 foreach relocation entry r { 3 refptr=s+ r.offset; /* ptr to reference to be relocated */ 
5 /* Relocate a PC-relative reference */ 6 if (r.type == R_386_PC32) { 7 refaddr = ADDR(s) + r.offset; /* ref¡¯s run-time address */ 8 *refptr = (unsigned) (ADDR(r.symbol) + *refptr -refaddr); 
9 } 10 11 /* Relocate an absolute reference */ 12 if (r.type == R_386_32) 13 *refptr = (unsigned) (ADDR(r.symbol) + *refptr); 
14 } 
15 } Figure 7.9 Relocation algorithm. 
addressing, then it is relocated by lines 5¨C9. If the reference uses absolute address-ing, then it is relocated by lines 11¨C13. 
Relocating PC-Relative References 
Recall from our running example in Figure 7.1(a) that the main routine in the .text section of main.o calls the swap routine, which is de.ned in swap.o. Here is the disassembled listing for the call instruction, as generated by the GNU objdump tool: 
6: e8 fc ff ff ff call 7 <main+0x7> swap(); 
7: R_386_PC32 swap relocation entry 
From this listing, we see that the call instruction begins at section offset 0x6 and consists of the 1-byte opcode 0xe8, followed by the 32-bit reference 0xfffffffc (.4 decimal), which is stored in little-endian byte order. We also see a relocation entry for this reference displayed on the following line. (Recall that relocation entries and instructions are actually stored in different sections of the object .le. The objdump tool displays them together for convenience.) The relocation entry r consists of three .elds: 
r.offset = 0x7 r.symbol = swap r.type = R_386_PC32 
These .elds tell the linker to modify the 32-bit PC-relative reference starting at offset 0x7 so that it will point to the swap routine at run time. Now, suppose that the linker has determined that 
ADDR(s) = ADDR(.text) = 0x80483b4 
and 

ADDR(r.symbol) = ADDR(swap) = 0x80483c8 
Using the algorithm in Figure 7.9, the linker .rst computes the run-time address of the reference (line 7): 
refaddr = ADDR(s) + r.offset 
= 0x80483b4 + 0x7 
= 0x80483bb 
It then updates the reference from its current value (.4) to 0x9 so that it will point to the swap routine at run time (line 8): 
*refptr = (unsigned) (ADDR(r.symbol) + *refptr -refaddr) 
= (unsigned) (0x80483c8 + (-4) -0x80483bb) 
= (unsigned) (0x9) 
In the resulting executable object .le, the call instruction has the following relocated form: 
80483ba: e8 09 00 00 00 call 80483c8 <swap> swap(); 
At run time, the call instruction will be stored at address 0x80483ba. When the CPU executes the call instruction, the PC has a value of 0x80483bf, which is the address of the instruction immediately following the call instruction. To execute the instruction, the CPU performs the following steps: 
1. 
push PC onto stack 

2. 
PC <-PC + 0x9 = 0x80483bf + 0x9 = 0x80483c8 


Thus, the next instruction to execute is the .rst instruction of the swap routine, which of course is what we want! 
You may wonder why the assembler created the reference in the call instruc-tion with an initial value of .4. The assembler uses this value as a bias to account for the fact that the PC always points to the instruction following the current in-struction. On a different machine with different instruction sizes and encodings, the assembler for that machine would use a different bias. This is a powerful trick that allows the linker to blindly relocate references, blissfully unaware of the in-struction encodings for a particular machine. 
Relocating Absolute References 
Recall that in our example program in Figure 7.1, the swap.o module initializes the global pointer bufp0 to the address of the .rst element of the global buf array: 
int *bufp0 = &buf[0]; 
Since bufp0 is an initialized data object, it will be stored in the .data section of the swap.o relocatable object module. Since it is initialized to the address of a global array, it will need to be relocated. Here is the disassembled listing of the .data section from swap.o: 
00000000 <bufp0>: 
0: 00000000 int *bufp0 = &buf[0]; 
0: R_386_32 buf Relocation entry 
We see that the .data section contains a single 32-bit reference, the bufp0 pointer, which has a value of 0x0. The relocation entry tells the linker that this is a 32-bit absolute reference, beginning at offset 0, which must be relocated so that it points to the symbol buf. Now, suppose that the linker has determined that 
ADDR(r.symbol) = ADDR(buf) = 0x8049454 
The linker updates the reference using line 13 of the algorithm in Figure 7.9: 
*refptr = (unsigned) (ADDR(r.symbol) + *refptr) = (unsigned) (0x8049454 + 0) = (unsigned) (0x8049454) 
In the resulting executable object .le, the reference has the following relocated form: 
0804945c <bufp0>: 804945c: 54 94 04 08 Relocated! 
In words, the linker has decided that at run time the variable bufp0 will be located at memory address 0x804945c and will be initialized to 0x8049454, which is the run-time address of the buf array. 
The .text section in the swap.o module contains .ve absolute references that 
are relocated in a similar way (see Problem 7.12). Figure 7.10 shows the relocated 
.text and .data sections in the .nal executable object .le. 
Practice Problem 7.4 
This problem concerns the relocated program in Figure 7.10. 
A. What is the hex address of the relocated reference to swap in line 5? 
B. What is the hex value of the relocated reference to swap in line 5? 
C. Suppose the linker had decided for some reason to locate the .text sec-tion at 0x80483b8 instead of 0x80483b4. What would the hex value of the relocated reference in line 5 be in this case? 

(a) Relocated .text section 
code/link/p-exe.d  
1  080483b4  <main>:  
2  80483b4:  55  push  %ebp  
3  80483b5:  89 e5  mov  %esp,%ebp  
4  80483b7:  83 ec 08  sub  $0x8,%esp  
5  80483ba:  e8 09 00  00  00  call  80483c8 <swap>  swap();  
6  80483bf:  31 c0  xor  %eax,%eax  
7  80483c1:  89 ec  mov  %ebp,%esp  
8  80483c3:  5d  pop  %ebp  
9  80483c4:  c3  ret  
10  80483c5:  90  nop  
11  80483c6:  90  nop  
12  80483c7:  90  nop  
13  080483c8  <swap>:  
14  80483c8:  55  push  %ebp  
15  80483c9:  8b 15 5c  94  04  08  mov  0x804945c,%edx  Get *bufp0  
16  80483cf:  a1 58 94  04  08  mov  0x8049458,%eax  Get buf[1]  
17  80483d4:  89 e5  mov  %esp,%ebp  
18  80483d6:  c7 05 48  95  04  08  58  movl  $0x8049458,0x8049548  bufp1 = &buf[1]  
19  80483dd:  94 04 08  
20  80483e0:  89 ec  mov  %ebp,%esp  
21  80483e2:  8b 0a  mov  (%edx),%ecx  
22  80483e4:  89 02  mov  %eax,(%edx)  
23  80483e6:  a1 48 95  04  08  mov  0x8049548,%eax  Get *bufp1  
24  80483eb:  89 08  mov  %ecx,(%eax)  
25  80483ed:  5d  pop  %ebp  
26  80483ee:  c3  ret  
code/link/p-exe.d  
(b) Relocated .data section  
code/link/pdata-exe.d  
1  08049454  <buf>:  
2  8049454:  01  00  00  00  02  00  00  00  
3  0804945c  <bufp0>:  
4  804945c:  54  94  04  08  Relocated!  
code/link/pdata-exe.d  
Figure 7.10 Relocated .text and .data sections for executable .le p. The original C code is in Figure 7.1. 


7.8 
Executable 
Object 
Files 

We have seen how the linker merges multiple object modules into a single exe-cutable object .le. Our C program, which began life as a collection of ASCII text .les, has been transformed into a single binary .le that contains all of the informa-tion needed to load the program into memory and run it. Figure 7.11 summarizes the kinds of information in a typical ELF executable .le. 
The format of an executable object .le is similar to that of a relocatable object .le. The ELF header describes the overall format of the .le. It also includes the program¡¯s entry point, which is the address of the .rst instruction to execute when the program runs. The .text, .rodata, and .data sections are similar to those in a relocatable object .le, except that these sections have been relocated to their eventual run-time memory addresses. The .init section de.nes a small function, called _init, that will be called by the program¡¯s initialization code. Since the executable is fully linked (relocated), it needs no .rel sections. 
ELF executables are designed to be easy to load into memory, with contigu-ous chunks of the executable .le mapped to contiguous memory segments. This mapping is described by the segment header table. Figure 7.12 shows the segment header table for our example executable p, as displayed by objdump. 
From the segment header table, we see that two memory segments will be initialized with the contents of the executable object .le. Lines 1 and 2 tell us that the .rst segment (the code segment) is aligned toa4KB(212) boundary, has read/execute permissions, starts at memory address 0x08048000, has a total memory size of 0x448 bytes, and is initialized with the .rst 0x448 bytes of the executable object .le, which includes the ELF header, the segment header table, and the .init, .text, and .rodata sections. 
0 
Maps contiguous file sections to runtime memory segments 
ELF header  
Segment header table  
.init  
.text  
.rodata  
.data  
.bss  
.symtab  
.debug  
.line  
.strtab  
Section header table  

Read-only memory segment (code segment) 


Read/write memory segment (data segment) 
Symbol table and debugging info are not loaded into memory 
Describes object file sections 

Figure 7.11 Typical ELF executable object .le. 
code/link/p-exe.d 

Read-only code segment 
1 LOAD off 0x00000000 vaddr 0x08048000 paddr 0x08048000 align 2**12 
2 filesz 0x00000448 memsz 0x00000448 flags r-x 
Read/write data segment 
3 LOAD off 0x00000448 vaddr 0x08049448 paddr 0x08049448 align 2**12 
4 filesz 0x000000e8 memsz 0x00000104 flags rw-
code/link/p-exe.d 

Figure 7.12 Segment header table for the example executable p. Legend: off: .le offset, vaddr/paddr: virtual/physical address, align: segment alignment, filesz: segment size in the object .le, memsz: segment size in memory, flags: run-time permissions. 
Lines 3 and 4 tell us that the second segment (the data segment) is aligned to a 4 KB boundary, has read/write permissions, starts at memory address 0x08049448, has a total memory size of 0x104 bytes, and is initialized with the 0xe8 bytes starting at .le offset 0x448, which in this case is the beginning of the .data section. The remaining bytes in the segment correspond to .bss data that will be initialized to zero at run time. 
7.9 
Loading 
Executable 
Object 
Files 

To run an executable object .le p, we can type its name to the Unix shell¡¯s command line: 
unix> ./p 

Since p does not correspond to a built-in shell command, the shell assumes that p is an executable object .le, which it runs for us by invoking some memory-resident operating system code known as the loader. Any Unix program can invoke the loader by calling the execve function, which we will describe in detail in Section 8.4.5. The loader copies the code and data in the executable object .le from disk into memory, and then runs the program by jumping to its .rst instruction, or entry point. This process of copying the program into memory and then running it is known as loading. 
Every Unix program has a run-time memory image similar to the one in Fig-ure 7.13. On 32-bit Linux systems, the code segment starts at address 0x08048000. The data segment follows at the next 4 KB aligned address. The run-time heap fol-lows on the .rst 4 KB aligned address past the read/write segment and grows up via calls to the malloc library. (We will describe malloc and the heap in detail in Section 9.9.) There is also a segment that is reserved for shared libraries. The user stack always starts at the largest legal user address and grows down (toward lower memory addresses). The segment starting above the stack is reserved for 
Figure 7.13 

Linux run-time memory image. 
0x08048000 0 

Memory invisible to user code 
%esp (stack pointer) 
brk 
Loaded from the executable file 
the code and data in the memory-resident part of the operating system known as the kernel. 
When the loader runs, it creates the memory image shown in Figure 7.13. Guided by the segment header table in the executable, it copies chunks of the executable into the code and data segments. Next, the loader jumps to the pro-gram¡¯s entry point, which is always the address of the _start symbol. The startup code at the _start address is de.ned in the object .le crt1.o and is the same for all C programs. Figure 7.14 shows the speci.c sequence of calls in the startup code. After calling initialization routines from the .text and .init sections, the startup code calls the atexit routine, which appends a list of routines that should be called when the application terminates normally. The exit function runs the functions registered by atexit, and then returns control to the operating system 
1 0x080480c0 <_start>: /* Entry point in .text */ 2 call __libc_init_first /* Startup code in .text */ 3 call _init /* Startup code in .init */ 4 call atexit /* Startup code in .text */ 5 call main /* Application main routine */ 6 call _exit /* Returns control to OS */ 7 /* Control never reaches here */ 
Figure 7.14 Pseudo-code for the crt1.o startup routine in every C program. Note: The code that pushes the arguments for each function is not shown. 

by calling _exit. Next, the startup code calls the application¡¯s main routine, which begins executing our C code. After the application returns, the startup code calls the _exit routine, which returns control to the operating system. 
Aside How do loaders really work? 
Our description of loading is conceptually correct, but intentionally not entirely accurate. To understand how loading really works, you must understand the concepts of processes, virtual memory, and memory mapping, which we haven¡¯t discussed yet. As we encounter these concepts later in Chapters 8 and 9, we will revisit loading and gradually reveal the mystery to you. 
For the impatient reader, here is a preview of how loading really works: Each program in a Unix system runs in the context of a process with its own virtual address space. When the shell runs a program, the parent shell process forks a child process that is a duplicate of the parent. The child process invokes the loader via the execve system call. The loader deletes the child¡¯s existing virtual memory segments, and creates a new set of code, data, heap, and stack segments. The new stack and heap segments are initialized to zero. The new code and data segments are initialized to the contents of the executable .le by mapping pages in the virtual address space to page-sized chunks of the executable .le. Finally, the loader jumps to the _start address, which eventually calls the application¡¯s main routine. Aside from some header information, there is no copying of data from disk to memory during loading. The copying is deferred until the CPU references a mapped virtual page, at which point the operating system automatically transfers the page from disk to memory using its paging mechanism. 
Practice Problem 7.5 
A. Why does every C program need a routine called main? 
B. Have you ever wondered why a C main routine can end with a call to exit,a return statement, or neither, and yet the program still terminates properly? Explain. 
7.10 
Dynamic 
Linking 
with 
Shared 
Libraries 

The static libraries that we studied in Section 7.6.2 address many of the issues associated with making large collections of related functions available to applica-tion programs. However, static libraries still have some signi.cant disadvantages. Static libraries, like all software, need to be maintained and updated periodically. If application programmers want to use the most recent version of a library, they must somehow become aware that the library has changed, and then explicitly relink their programs against the updated library. 
Another issue is that almost every C program uses standard I/O functions such as printf and scanf. At run time, the code for these functions is duplicated in the text segment of each running process. On a typical system that is running 50¨C100 processes, this can be a signi.cant waste of scarce memory system resources. (An interesting property of memory is that it is always a scarce resource, regardless of how much there is in a system. Disk space and kitchen trash cans share this same property.) 
Shared libraries are modern innovations that address the disadvantages of static libraries. A shared library is an object module that, at run time, can be loaded at an arbitrary memory address and linked with a program in memory. This process is known as dynamic linking and is performed by a program called a dynamic linker. 
Shared libraries are also referred to as shared objects, and on Unix systems 
are typically denoted by the .so suf.x. Microsoft operating systems make heavy 
use of shared libraries, which they refer to as DLLs (dynamic link libraries). 
Shared libraries are ¡°shared¡± in two different ways. First, in any given .le system, there is exactly one .so .le for a particular library. The code and data in this .so .le are shared by all of the executable object .les that reference the library, as opposed to the contents of static libraries, which are copied and embedded in the executables that reference them. Second, a single copy of the .text section of a shared library in memory can be shared by different running processes. We will explore this in more detail when we study virtual memory in Chapter 9. 
Figure 7.15 summarizes the dynamic linking process for the example program in Figure 7.6. To build a shared library libvector.so of our example vector arithmetic routines in Figure 7.5, we would invoke the compiler driver with the following special directive to the linker: 
unix> gcc -shared -fPIC -o libvector.so addvec.c multvec.c 
The -fPIC .ag directs the compiler to generate position-independent code (more on this in the next section). The -shared .ag directs the linker to create a shared object .le. 
Once we have created the library, we would then link it into our example program in Figure 7.6: 
unix> gcc -o p2 main2.c ./libvector.so 
This creates an executable object .le p2 in a form that can be linked with libvector.so at run time. The basic idea is to do some of the linking statically when the executable .le is created, and then complete the linking process dynam-ically when the program is loaded. 
It is important to realize that none of the code or data sections from libvector.so are actually copied into the executable p2 at this point. Instead, the linker copies some relocation and symbol table information that will allow references to code and data in libvector.so to be resolved at run time. 
When the loader loads and runs the executable p2, it loads the partially linked executable p2, using the techniques discussed in Section 7.9. Next, it notices that p2 

Figure 7.15 main2.c vector.h 
Dynamic linking with shared libraries. 
Relocatable object file 
Partially linked executable object file 
Fully linked executable in memory 


contains a .interp section, which contains the path name of the dynamic linker, which is itself a shared object (e.g., ld-linux.so on Linux systems). Instead of passing control to the application, as it would normally do, the loader loads and runs the dynamic linker. 
The dynamic linker then .nishes the linking task by performing the following relocations: 
. Relocating the text and data of libc.so into some memory segment. . Relocating the text and data of libvector.so into another memory segment. . Relocating any references in p2 to symbols de.ned by libc.so and libvec-
tor.so. 

Finally, the dynamic linker passes control to the application. From this point on, the locations of the shared libraries are .xed and do not change during execution of the program. 
7.11 
Loading 
and 
Linking 
Shared 
Libraries 
from 
Applications 

Up to this point, we have discussed the scenario in which the dynamic linker loads and links shared libraries when an application is loaded, just before it executes. However, it is also possible for an application to request the dynamic linker to load and link arbitrary shared libraries while the application is running, without having to link in the applications against those libraries at compile time. 
Dynamic linking is a powerful and useful technique. Here are some examples in the real world: 
. Distributing software. Developers of Microsoft Windows applications fre-quently use shared libraries to distribute software updates. They generate a new copy of a shared library, which users can then download and use as a replacement for the current version. The next time they run their application, it will automatically link and load the new shared library. 
. Building high-performance Web servers.Many Web servers generate dynamic content, such as personalized Web pages, account balances, and banner ads. Early Web servers generated dynamic content by using fork and execve to create a child process and run a ¡°CGI program¡± in the context of the child. However, modern high-performance Web servers can generate dynamic content using a more ef.cient and sophisticated approach based on dynamic linking. 
The idea is to package each function that generates dynamic content in a shared library. When a request arrives from a Web browser, the server dynamically loads and links the appropriate function and then calls it directly, as opposed to using fork and execve to run the function in the context of a child process. The function remains cached in the server¡¯s address space, so subsequent requests can be handled at the cost of a simple function call. This can have a signi.cant impact on the throughput of a busy site. Further, existing functions can be updated and new functions can be added at run time, without stopping the server. 
Linux systems provide a simple interface to the dynamic linker that allows appli-cation programs to load and link shared libraries at run time. 
#include <dlfcn.h> void *dlopen(const char *filename, int flag); 
Returns: ptr to handle if OK, NULL on error 
The dlopen function loads and links the shared library filename. The external symbols in filename are resolved using libraries previously opened with the RTLD_ GLOBAL .ag. If the current executable was compiled with the -rdynamic .ag, then its global symbols are also available for symbol resolution. The flag argument must include either RTLD_NOW, which tells the linker to resolve references to external symbols immediately, or the RTLD_LAZY .ag, which instructs the linker to defer symbol resolution until code from the library is executed. Either of these values can be or¡¯d with the RTLD_GLOBAL .ag. 

#include <dlfcn.h> void *dlsym(void *handle, char *symbol); 
Returns: ptr to symbol if OK, NULL on error 

The dlsym function takes a handle to a previously opened shared library and a symbol name, and returns the address of the symbol, if it exists, or NULL otherwise. 
#include <dlfcn.h> int dlclose (void *handle); 
Returns: 0 if OK, .1 on error 

The dlclose function unloads the shared library if no other shared libraries are still using it. 
#include  <dlfcn.h>  
const  char  *dlerror(void);  
Returns: error msg if previous call to dlopen, dlsym,  
or dlclose failed, NULL if previous call was OK  

The dlerror function returns a string describing the most recent error that oc-curred as a result of calling dlopen, dlsym,or dlclose, or NULL if no error occurred. 
Figure 7.16 shows how we would use this interface to dynamically link our libvector.so shared library (Figure 7.5), and then invoke its addvec routine. To compile the program, we would invoke gcc in the following way: 
unix> gcc -rdynamic -O2 -o p3 dll.c -ldl 
Aside Shared libraries and the Java Native Interface 
Java de.nes a standard calling convention called Java Native Interface (JNI) that allows ¡°native¡± C and C++ functions to be called from Java programs. The basic idea of JNI is to compile the native C function, say, foo, into a shared library, say foo.so. When a running Java program attempts to invoke function foo, the Java interpreter uses the dlopen interface (or something like it) to dynamically link and load foo.so, and then call foo. 
code/link/dll.c  
1  #include  <stdio.h>  
2  #include  <stdlib.h>  
3  #include  <dlfcn.h>  
4  
5  int  x[2]  =  {1,  2};  
6  int  y[2]  =  {3,  4};  
7  int  z[2];  
8  
9  int  main()  
10  {  
11  void *handle;  
12  void (*addvec)(int *, int *, int *, int);  
13  char *error;  
14  
15  /*  Dynamically  load  shared  library  that  contains  addvec()  */  
16  handle  =  dlopen("./libvector.so",  RTLD_LAZY);  
17  if  (!handle)  {  
18  fprintf(stderr,  "%s\n",  dlerror());  
19  exit(1);  
20  }  
21  
22  /*  Get  a  pointer  to  the  addvec()  function  we  just  loaded  */  
23  addvec  =  dlsym(handle,  "addvec");  
24  if  ((error  =  dlerror())  !=  NULL)  {  
25  fprintf(stderr,  "%s\n",  error);  
26  exit(1);  
27  }  
28  
29  /*  Now  we  can  call  addvec()  just  like  any  other  function  */  
30  addvec(x,  y,  z,  2);  
31  printf("z  =  [%d  %d]\n",  z[0],  z[1]);  
32  
33  /*  Unload  the  shared  library  */  
34  if  (dlclose(handle)  <  0)  {  
35  fprintf(stderr,  "%s\n",  dlerror());  
36  exit(1);  
37  }  
38  return  0;  
39  }  
code/link/dll.c  
Figure 7.16 An application program that dynamically loads and links the shared library libvector.so. 



7.12 
Position-Independent 
Code 
(PIC) 

A key purpose of shared libraries is to allow multiple running processes to share the same library code in memory and thus save precious memory resources. So how can multiple processes share a single copy of a program? One approach would be to assign a priori a dedicated chunk of the address space to each shared library, and then require the loader to always load the shared library at that address. While straightforward, this approach creates some serious problems. It would be an inef.cient use of the address space because portions of the space would be allocated even if a process didn¡¯t use the library. Second, it would be dif.cult to manage. We would have to ensure that none of the chunks overlapped. Every time a library were modi.ed, we would have to make sure that it still .t in its assigned chunk. If not, then we would have to .nd a new chunk. And if we created a new library, we would have to .nd room for it. Over time, given the hundreds of libraries and versions of libraries in a system, it would be dif.cult to keep the address space from fragmenting into lots of small unused but unusable holes. Even worse, the assignment of libraries to memory would be different for each system, thus creating even more management headaches. 
A better approach is to compile library code so that it can be loaded and executed at any address without being modi.ed by the linker. Such code is known as position-independent code (PIC). Users direct GNU compilation systems to generate PIC code with the -fPIC option to gcc. 
On IA32 systems, calls to procedures in the same object module require no special treatment, since the references are PC-relative, with known offsets, and thus are already PIC (see Problem 7.4). However, calls to externally de.ned procedures and references to global variables are not normally PIC, since they require relocation at link time. 
PIC Data References 
Compilers generate PIC references to global variables by exploiting the following interesting fact: No matter where we load an object module (including shared object modules) in memory, the data segment is always allocated immediately after the code segment. Thus, the distance between any instruction in the code segment and any variable in the data segment is a run-time constant, independent of the absolute memory locations of the code and data segments. 
To exploit this fact, the compiler creates a table called the global offset table (GOT) at the beginning of the data segment. The GOT contains an entry for each global data object that is referenced by the object module. The compiler also generates a relocation record for each entry in the GOT. At load time, the dynamic linker relocates each entry in the GOT so that it contains the appropriate absolute address. Each object module that references global data has its own GOT. 
At run time, each global variable is referenced indirectly through the GOT using code of the form 
call L1 
L1: popl %ebx ebx contains the current PC addl $VAROFF, %ebx ebx points to the GOT entry for var movl (%ebx), %eax reference indirect through the GOT movl (%eax), %eax 
In this fascinating piece of code, the call to L1 pushes the return address (which happens to be the address of the popl instruction) on the stack. The popl instruc-tion then pops this address into %ebx. The net effect of these two instructions is to move the value of the PC into register %ebx. 
The addl instruction adds a constant offset to %ebx so that it points to the appropriate entry in the GOT, which contains the absolute address of the data item. At this point, the global variable can be referenced indirectly through the GOT entry contained in %ebx. In this example, the two movl instructions load the contents of the global variable (indirectly through the GOT) into register %eax. 
PIC code has performance disadvantages. Each global variable reference now requires .ve instructions instead of one, with an additional memory reference to the GOT. Also, PIC code uses an additional register to hold the address of the GOT entry. On machines with large register .les, this is not a major issue. On register-starved IA32 systems, however, losing even one register can trigger spilling of the registers onto the stack. 
PIC Function Calls 
It would certainly be possible for PIC code to use the same approach for resolving external procedure calls: 
call L1 
L1: popl %ebx ebx contains the current PC addl $PROCOFF, %ebx ebx points to GOT entry for proc call *(%ebx) call indirect through the GOT 
However, this approach would require three additional instructions for each run-time procedure call. Instead, ELF compilation systems use an interesting tech-nique, called lazy binding, that defers the binding of procedure addresses until the .rst time the procedure is called. There is a nontrivial run-time overhead the .rst time the procedure is called, but each call thereafter only costs a single instruction and a memory reference for the indirection. 
Lazy binding is implemented with a compact yet somewhat complex interac-tion between two data structures: the GOT and the procedure linkage table (PLT). If an object module calls any functions that are de.ned in shared libraries, then it has its own GOT and PLT. The GOT is part of the .data section. The PLT is part of the .text section. 
Figure 7.17 shows the format of the GOT for the example program main2.o from Figure 7.6. The .rst three GOT entries are special: GOT[0] contains the address of the .dynamic segment, which contains information that the dynamic linker uses to bind procedure addresses, such as the location of the symbol table and relocation information. GOT[1] contains some information that de.nes this module. GOT[2] contains an entry point into the lazy binding code of the dynamic linker. 

Section 7.12  Position-Independent Code (PIC)  689  
Address  Entry  Contents  Description  
08049674 08049678 0804967c 08049680 08049684  GOT[0] GOT[1] GOT[2] GOT[3] GOT[4]  0804969c 4000a9f8 4000596f 0804845a 0804846a  address of .dynamic section identifying info for the linker entry point in dynamic linker address of pushl in PLT[1] (printf) address of pushl in PLT[2] (addvec)  
Figure 7.17 The global offset table (GOT) for executable p2. The original code is in Figures 7.5 and 7.6. 


Each procedure that is de.ned in a shared object and called by main2.o gets an entry in the GOT, starting with entry GOT[3]. For the example program, we have shown the GOT entries for printf, which is de.ned in libc.so, and addvec, which is de.ned in libvector.so. 
Figure 7.18 shows the PLT for our example program p2. The PLT is an array of 16-byte entries. The .rst entry, PLT[0], is a special entry that jumps into the dynamic linker. Each called procedure has an entry in the PLT, starting at PLT[1]. In the .gure, PLT[1] corresponds to printf and PLT[2] corresponds to addvec. 
PLT[0]  
08048444:  ff  35  78  96  04  08  pushl  0x8049678  push &GOT[1]  
804844a:  ff  25  7c  96  04  08  jmp  *0x804967c  jmp  to  *GOT[2](linker)  
8048450:  00  00  padding  
8048452:  00  00  padding  

PLT[1] <printf> 8048454: ff 25 80 96 04 08 jmp *0x8049680 jmp to *GOT[3] 804845a: 68 00 00 00 00 pushl $0x0 ID for printf 804845f: e9 e0 ff ff ff jmp 8048444 jmp to PLT[0] 
PLT[2] <addvec> 8048464: ff 25 84 96 04 08 jmp *0x8049684 jump to *GOT[4] 804846a: 68 08 00 00 00 pushl $0x8 ID for addvec 804846f: e9 d0 ff ff ff jmp 8048444 jmp to PLT[0] 
<other PLT entries> 
Figure 7.18 The procedure linkage table (PLT) for executable p2. The original code is in Figures 7.5 and 7.6. 
Initially, after the program has been dynamically linked and begins executing, 
procedures printf and addvec are bound to the .rst instruction in their respective 
PLT entries. For example, the call to addvec has the form 
80485bb: e8 a4 fe ff ff call 8048464 <addvec> 
When addvec is called the .rst time, control passes to the .rst instruction in PLT[2], which does an indirect jump through GOT[4]. Initially, each GOT entry contains the address of the pushl entry in the corresponding PLT entry. So the indirect jump in the PLT simply transfers control back to the next instruction in PLT[2]. This instruction pushes an ID for the addvec symbol onto the stack. The last instruction jumps to PLT[0], which pushes another word of identifying information on the stack from GOT[1], and then jumps into the dynamic linker indirectly through GOT[2]. The dynamic linker uses the two stack entries to determine the location of addvec, overwrites GOT[4] with this address, and passes control to addvec. 
The next time addvec is called in the program, control passes to PLT[2] as before. However, this time the indirect jump through GOT[4] transfers control to addvec. The only additional overhead from this point on is the memory reference for the indirect jump. 
7.13 
Tools 
for 
Manipulating 
Object 
Files 

There are a number of tools available on Unix systems to help you understand and manipulate object .les. In particular, the GNU binutils package is especially helpful and runs on every Unix platform. 
ar: Creates static libraries, and inserts, deletes, lists, and extracts members. strings: Lists all of the printable strings contained in an object .le. strip: Deletes symbol table information from an object .le. nm: Lists the symbols de.ned in the symbol table of an object .le. size: Lists the names and sizes of the sections in an object .le. readelf: Displays the complete structure of an object .le, including all of the 
information encoded in the ELF header; subsumes the functionality of size and nm. 
objdump: The mother of all binary tools. Can display all of the information in an object .le. Its most useful function is disassembling the binary instructions in the .text section. 
Unix systems also provide the ldd program for manipulating shared libraries: 
ldd: Lists the shared libraries that an executable needs at run time. 

7.14 
Summary 

Linking can be performed at compile time by static linkers, and at load time and run time by dynamic linkers. Linkers manipulate binary .les called object .les, which come in three different forms: relocatable, executable, and shared. Relocatable object .les are combined by static linkers into an executable object .le that can be loaded into memory and executed. Shared object .les (shared libraries) are linked and loaded by dynamic linkers at run time, either implicitly when the calling program is loaded and begins executing, or on demand, when the program calls functions from the dlopen library. 
The two main tasks of linkers are symbol resolution, where each global symbol in an object .le is bound to a unique de.nition, and relocation, where the ultimate memory address for each symbol is determined and where references to those objects are modi.ed. 
Static linkers are invoked by compiler drivers such as gcc. They combine multiple relocatable object .les into a single executable object .le. Multiple object .les can de.ne the same symbol, and the rules that linkers use for silently resolving these multiple de.nitions can introduce subtle bugs in user programs. 
Multiple object .les can be concatenated in a single static library. Linkers use libraries to resolve symbol references in other object modules. The left-to-right sequential scan that many linkers use to resolve symbol references is another source of confusing link-time errors. 
Loaders map the contents of executable .les into memory and run the pro-gram. Linkers can also produce partially linked executable object .les with un-resolved references to the routines and data de.ned in a shared library. At load time, the loader maps the partially linked executable into memory and then calls a dynamic linker, which completes the linking task by loading the shared library and relocating the references in the program. 
Shared libraries that are compiled as position-independent code can be loaded anywhere and shared at run time by multiple processes. Applications can also use the dynamic linker at run time in order to load, link, and access the functions and data in shared libraries. 
Bibliographic 
Notes 

Linking is not well documented in the computer systems literature. Since it lies at the intersection of compilers, computer architecture, and operating systems, linking requires understanding of code generation, machine-language program-ming, program instantiation, and virtual memory. It does not .t neatly into any of the usual computer systems specialties and thus is not well covered by the clas-sic texts in these areas. However, Levine¡¯s monograph provides a good general reference on the subject [66]. The original speci.cations for ELF and DWARF (a speci.cation for the contents of the .debug and .line sections) are described in [52]. 
Some interesting research and commercial activity centers around the notion of binary translation, where the contents of an object .le are parsed, analyzed, and modi.ed. Binary translation can be used for three different purposes [64]: to emulate one system on another system, to observe program behavior, or to perform system-dependent optimizations that are not possible at compile time. Commercial products such as VTune, Purify, and BoundsChecker use binary translation to provide programmers with detailed observations of their programs. Valgrind is a popular open-source alternative. 
The Atom system provides a .exible mechanism for instrumenting Alpha executable object .les and shared libraries with arbitrary C functions [103]. Atom has been used to build a myriad of analysis tools that trace procedure calls, pro.le instruction counts and memory referencing patterns, simulate memory system behavior, and isolate memory referencing errors. Etch [90] and EEL [64] provide roughly similar capabilities on different platforms. The Shade system uses binary translation for instruction pro.ling [23]. Dynamo [5] and Dyninst [15] provide mechanisms for instrumenting and optimizing executables in memory at run time. Smith and his colleagues have investigated binary translation for program pro.ling and optimization [121]. 
Homework 
Problems 

7.6 ¡ô 
Consider the following version of the swap.c function that counts the number of times it has been called: 
1 extern int buf[]; 2 3 int *bufp0 = &buf[0]; 4 static int *bufp1; 5 6 static void incr() 7 { 8 static int count=0; 9 
10 count++; 
11 } 12 13 void swap() 14 { 15 int temp; 16 17 incr(); 18 bufp1 = &buf[1]; 19 temp = *bufp0; 20 *bufp0 = *bufp1; 21 *bufp1 = temp; 
22 } 

For each symbol that is de.ned and referenced in swap.o, indicate if it will have a symbol table entry in the .symtab section in module swap.o. If so, indicate the module that de.nes the symbol (swap.o or main.o), the symbol type (local, global, or extern), and the section (.text, .data,or .bss) it occupies in that module. 
Symbol swap.o .symtab entry? Symbol type Module where de.ned Section 
buf 
bufp0 bufp1 swap temp incr count 
7.7 ¡ô 

Without changing any variable names, modify bar5.c on page 666 so that foo5.c prints the correct values of x and y (i.e., the hex representations of integers 15213 and 15212). 
7.8 ¡ô 

In this problem, let REF(x.i) --> DEF(x.k) denote that the linker will associate an arbitrary reference to symbol x in module i to the de.nition of x in module k. For each example below, use this notation to indicate how the linker would resolve references to the multiply de.ned symbol in each module. If there is a link-time error (rule 1), write ¡°ERROR.¡± If the linker arbitrarily chooses one of the de.nitions (rule 3), write ¡°UNKNOWN.¡± 
A. /* Module 1 */ /* Module 2 */ 
int main() static int main=1; 
{ int p2() 
}{ 
} 

(a) 
REF(main.1) --> DEF( . ) 

(b) 
REF(main.2) --> DEF( . ) 

B. 
/* Module 1 */ /* Module 2 */ 


int x; double x; void main() int p2() {{ }} 
(a) 
REF(x.1) --> DEF( . ) 

(b) 
REF(x.2) --> DEF( . ) 


Linking 
C. /* Module 1 */ /* Module 2 */ 
int x=1; double x=1.0; void main() int p2() {{ 
}} 
(a) 
REF(x.1) --> DEF( . ) 

(b) 
REF(x.2) --> DEF( . ) 


7.9 ¡ô 
Consider the following program, which consists of two object modules: 
1 /* foo6.c */ 1 /* bar6.c */ 2 void p2(void); 2 #include <stdio.h> 33 
4 int main() 4 char main; 5 { 5 6 p2(); 6 void p2() 7 return 0; 7 { 8 } 8 printf("0x%x\n", main); 
9 } 
When this program is compiled and executed on a Linux system, it prints the string ¡°0x55\n¡± and terminates normally, even though p2 never initializes variable main. Can you explain this? 
7.10 ¡ô 
Let a and b denote object modules or static libraries in the current directory, and let a¡úb denote that a depends on b, in the sense that b de.nes a symbol that is referenced by a. For each of the following scenarios, show the minimal command line (i.e., one with the least number of object .le and library arguments) that will allow the static linker to resolve all symbol references: 
A. p.o ¡ú libx.a ¡ú p.o 
B. p.o ¡ú libx.a ¡ú liby.a and liby.a ¡ú libx.a 
C. p.o ¡ú libx.a ¡ú liby.a ¡ú libz.a and liby.a ¡ú libx.a ¡ú libz.a 
7.11 ¡ô 
The segment header in Figure 7.12 indicates that the data segment occupies 0x104 bytes in memory. However, only the .rst 0xe8 bytes of these come from the sections of the executable .le. What causes this discrepancy? 
7.12 ¡ô¡ô 
The swap routine in Figure 7.10 contains .ve relocated references. For each relo-cated reference, give its line number in Figure 7.10, its run-time memory address, and its value. The original code and relocation entries in the swap.o module are shown in Figure 7.19. 

1 00000000 <swap>: 2 0: 55 push %ebp 
3 
1: 8b 15 00 00 00 00 mov 0x0,%edx Get *bufp0=&buf[0] 4 3: R_386_32 bufp0 Relocation entry 

5 
7: a1 04 00 00 00 mov 0x4,%eax Get buf[1] 

6 
8: R_386_32 buf Relocation entry 7 c: 89 e5 mov %esp,%ebp 8 e: c7 05 00 00 00 00 04 movl $0x4,0x0 bufp1 = &buf[1]; 9 15: 000000 


10 10: R_386_32 bufp1 Relocation entry 11 14: R_386_32 buf Relocation entry 12 18: 89 ec mov %ebp,%esp 13 1a: 8b 0a mov (%edx),%ecx temp = buf[0]; 14 1c: 89 02 mov %eax,(%edx) buf[0]=buf[1]; 15 1e: a1 00 00 00 00 mov 0x0,%eax Get *bufp1=&buf[1] 16 1f: R_386_32 bufp1 Relocation entry 17 23: 89 08 mov %ecx,(%eax) buf[1]=temp; 18 25: 5d pop %ebp 19 26: c3 ret 
Figure 7.19 Code and relocation entries for Problem 7.12. 
Line # in Fig. 7.10 Address Value 
7.13 ¡ô¡ô¡ô 

Consider the C code and corresponding relocatable object module in Figure 7.20. 
A. Determine which instructions in .text will need to be modi.ed by the linker when the module is relocated. For each such instruction, list the information in its relocation entry: section offset, relocation type, and symbol name. 
B. Determine which data objects in .data will need to be modi.ed by the linker when the module is relocated. For each such instruction, list the information in its relocation entry: section offset, relocation type, and symbol name. 
Feel free to use tools such as objdump to help you solve this problem. 
7.14 ¡ô¡ô¡ô 

Consider the C code and corresponding relocatable object module in Figure 7.21. 
A. Determine which instructions in .text will need to be modi.ed by the linker when the module is relocated. For each such instruction, list the information in its relocation entry: section offset, relocation type, and symbol name. 
696  Chapter 7  Linking  
(a) C code  
1  extern  int  p3(void);  
2  intx=1;  
3  int *xp = &x;  
4  
5  void  p2(int  y)  {  
6  }  
7  
8  void  p1()  {  
9  p2(*xp  + p3());  
10  }  
(b) .text section of relocatable object .le  
1  00000000  <p2>:  
2  0:  55  push  %ebp  
3  1:  89 e5  mov  %esp,%ebp  
4  3:  89 ec  mov  %ebp,%esp  
5  5:  5d  pop  %ebp  
6  6:  c3  ret  
7  00000008  <p1>:  
8  8:  55  push  %ebp  
9  9:  89 e5  mov  %esp,%ebp  
10  b:  83 ec 08  sub  $0x8,%esp  
11  e:  83 c4 f4  add  $0xfffffff4,%esp  
12  11:  e8 fc ff ff ff  call  12 <p1+0xa>  
13  16:  89 c2  mov  %eax,%edx  
14  18:  a1 00 00 00 00  mov  0x0,%eax  
15  1d:  03 10  add  (%eax),%edx  
16  1f:  52  push  %edx  
17  20:  e8 fc ff ff ff  call  21 <p1+0x19>  
18  25:  89 ec  mov  %ebp,%esp  
19  27:  5d  pop  %ebp  
20  28:  c3  ret  
(c) .data section of relocatable object .le  
1  00000000  <x>:  
2  0:  01  00  00  00  
3  00000004  <xp>:  
4  4:  00  00  00  00  
Figure 7.20  Example code for Problem 7.13.  

(a) C code 1 int relo3(int val) { 2 switch (val) { 3 case 100: 4 return(val); 5 case 101: 6 return(val+1); 7 case 103: case 104: 8 return(val+3); 9 case 105: 10 return(val+5); 
11 default: 12 return(val+6); 
13 
} 

14 
} 



(b) .text section of relocatable object .le 1 00000000 <relo3>: 2 0: 55 push %ebp 3 1: 89 e5 mov %esp,%ebp 4 3: 8b 45 08 mov 0x8(%ebp),%eax 5 6: 8d 50 9c lea 0xffffff9c(%eax),%edx 6 9: 83 fa 05 cmp $0x5,%edx 7 c: 77 17 ja 25 <relo3+0x25> 8 e: ff 24 95 00 00 00 00 jmp *0x0(,%edx,4) 9 15: 40 inc %eax 10 16: eb 10 jmp 28 <relo3+0x28> 11 18: 83 c0 03 add $0x3,%eax 12 1b: eb 0b jmp 28 <relo3+0x28> 13 1d: 8d 76 00 lea 0x0(%esi),%esi 14 20: 83 c0 05 add $0x5,%eax 15 23: eb 03 jmp 28 <relo3+0x28> 16 25: 83 c0 06 add $0x6,%eax 17 28: 89 ec mov %ebp,%esp 
18 2a: 5d pop %ebp 19 2b: c3 ret 
(c) .rodata section of relocatable object .le 
This is the jump table for the switch statement 1 0000 28000000 15000000 25000000 18000000 4 words at offsets 0x0,0x4,0x8, and 0xc 2 0010 18000000 20000000 2 words at offsets 0x10 and 0x14 
Figure 7.21 Example code for Problem 7.14. 
B. Determine which data objects in .rodata will need to be modi.ed by the linker when the module is relocated. For each such instruction, list the in-formation in its relocation entry: section offset, relocation type, and symbol name. 
Feel free to use tools such as objdump to help you solve this problem. 
7.15 ¡ô¡ô¡ô 
Performing the following tasks will help you become more familiar with the various tools for manipulating object .les. 
A. How many object .les are contained in the versions of libc.a and libm.a on your system? 
B. Does gcc -O2 produce different executable code than gcc -O2 -g? 
C. What shared libraries does the gcc driver on your system use? 
Solutions 
to 
Practice 
Problems 

Solution to Problem 7.1 (page 662) 
The purpose of this problem is to help you understand the relationship between linker symbols and C variables and functions. Notice that the C local variable temp does not have a symbol table entry. 
Symbol swap.o .symtab entry? Symbol type Module where de.ned Section 
buf  yes  extern  main.o  .data  
bufp0  yes  global  swap.o  .data  
bufp1  yes  global  swap.o  .bss  
swap  yes  global  swap.o  .text  
temp  no  ¡ª  ¡ª  ¡ª  

Solution to Problem 7.2 (page 666) 
This is a simple drill that checks your understanding of the rules that a Unix linker uses when it resolves global symbols that are de.ned in more than one module. Understanding these rules can help you avoid some nasty programming bugs. 
A. The linker chooses the strong symbol de.ned in module 1 over the weak symbol de.ned in module 2 (rule 2): 
(a) REF(main.1) --> DEF(main.1) (b) REF(main.2) --> DEF(main.1) 
B. This is an ERROR, because each module de.nes a strong symbol main (rule 1). 

C. The linker chooses the strong symbol de.ned in module 2 over the weak symbol de.ned in module 1 (rule 2): 
(a) REF(x.1) --> DEF(x.2) (b) REF(x.2) --> DEF(x.2) 
Solution to Problem 7.3 (page 672) 
Placing static libraries in the wrong order on the command line is a common source of linker errors that confuses many programmers. However, once you understand how linkers use static libraries to resolve references, it¡¯s pretty straightforward. This little drill checks your understanding of this idea: 
A. gcc p.o libx.a 
B. gcc p.o libx.a liby.a 
C. gcc p.o libx.a liby.a libx.a 
Solution to Problem 7.4 (page 676) 
This problem concerns the disassembly listing in Figure 7.10. Our purpose here is to give you some practice reading disassembly listings and to check your under-standing of PC-relative addressing. 
A. The hex address of the relocated reference in line 5 is 0x80483bb. 
B. The hex value of the relocated reference in line 5 is 0x9. Remember that the disassembly listing shows the value of the reference in little-endian byte order. 
C. The key observation here is that no matter where the linker locates the .text section, the distance between the reference and the swap function is always the same. Thus, because the reference is a PC-relative address, its value will be 0x9, regardless of where the linker locates the .text section. 
Solution to Problem 7.5 (page 681) 
How C programs actually start up is a mystery to most programmers. These questions check your understanding of this startup process. You can answer them by referring to the C startup code in Figure 7.14. 
A. Every program needs a main function, because the C startup code, which is common to every C program, jumps to a function called main. 
B. If main terminates with a return statement, then control passes back to the startup routine, which returns control to the operating system by calling _exit. The same behavior occurs if the user omits the return statement. If main terminates with a call to exit, then exit eventually returns control to the operating system by calling _exit. The net effect is the same in all three cases: when main has .nished, control passes back to the operating system. 
This page intentionally left blank 

CHAPTER 
8 
Exceptional 
Control 
Flow 
8.1 
Exceptions 
703 
8.2 
Processes 
712 
8.3 
System 
Call 
Error 
Handling 
717 
8.4 
Process 
Control 
718 
8.5 
Signals 
736 
8.6 
Nonlocal 
Jumps 
759 
8.7 
Tools 
for 
Manipulating 
Processes 
762 
8.8 
Summary 
763 
Bibliographic 
Notes 
763 
Homework 
Problems 
764 
Solutions 
to 
Practice 
Problems 
771 
From the time you .rst apply power to a processor until the time you shut it off, the program counter assumes a sequence of values 
a0,a1,...,a
n.1 
where each a is the address of some corresponding instruction Ik. Each transition 
k 
from ak to ak+1 is called a control transfer. A sequence of such control transfers is 
called the .ow of control,or control .ow of the processor. The simplest kind of control .ow is a ¡°smooth¡± sequence where each I and
k 
Ik+1 are adjacent in memory. Typically, abrupt changes to this smooth .ow, where Ik+1 is not adjacent to I , are caused by familiar program instructions such as jumps, 
k 
calls, and returns. Such instructions are necessary mechanisms that allow programs to react to changes in internal program state represented by program variables. 
But systems must also be able to react to changes in system state that are not captured by internal program variables and are not necessarily related to the execution of the program. For example, a hardware timer goes off at regular intervals and must be dealt with. Packets arrive at the network adapter and must be stored in memory. Programs request data from a disk and then sleep until they are noti.ed that the data are ready. Parent processes that create child processes must be noti.ed when their children terminate. 
Modern systems react to these situations by making abrupt changes in the control .ow. In general, we refer to these abrupt changes as exceptional control .ow (ECF). Exceptional control .ow occurs at all levels of a computer system. For example, at the hardware level, events detected by the hardware trigger abrupt control transfers to exception handlers. At the operating systems level, the kernel transfers control from one user process to another via context switches. At the application level, a process can send a signal to another process that abruptly transfers control to a signal handler in the recipient. An individual program can react to errors by sidestepping the usual stack discipline and making nonlocal jumps to arbitrary locations in other functions. 
As programmers, there are a number of reasons why it is important for you to understand ECF: 
. Understanding ECF will help you understand important systems concepts.ECF is the basic mechanism that operating systems use to implement I/O, processes, and virtual memory. Before you can really understand these important ideas, you need to understand ECF. 
. Understanding ECF will help you understand how applications interact with the operating system. Applications request services from the operating system by using a form of ECF known as a trap or system call. For example, writing data to a disk, reading data from a network, creating a new process, and terminating the current process are all accomplished by application programs invoking system calls. Understanding the basic system call mechanism will help you understand how these services are provided to applications. 
. Understanding ECF will help you write interesting new application programs. 
The operating system provides application programs with powerful ECF 

mechanisms for creating new processes, waiting for processes to terminate, notifying other processes of exceptional events in the system, and detecting and responding to these events. If you understand these ECF mechanisms, then you can use them to write interesting programs such as Unix shells and Web servers. 
. Understanding ECF will help you understand concurrency. ECF is a basic mechanism for implementing concurrency in computer systems. An exception handler that interrupts the execution of an application program, processes and threads whose execution overlap in time, and a signal handler that interrupts the execution of an application program are all examples of concurrency in action. Understanding ECF is a .rst step to understanding concurrency. We will return to study it in more detail in Chapter 12. 
. Understanding ECF will help you understand how software exceptions work. 
Languages such as C++ and Java provide software exception mechanisms via try, catch, and throw statements. Software exceptions allow the program to make nonlocal jumps (i.e., jumps that violate the usual call/return stack discipline) in response to error conditions. Nonlocal jumps are a form of application-level ECF, and are provided in C via the setjmp and longjmp functions. Understanding these low-level functions will help you understand how higher-level software exceptions can be implemented. 
Up to this point in your study of systems, you have learned how applications interact with the hardware. This chapter is pivotal in the sense that you will begin to learn how your applications interact with the operating system. Interestingly, these interactions all revolve around ECF. We describe the various forms of ECF that exist at all levels of a computer system. We start with exceptions, which lie at the intersection of the hardware and the operating system. We also discuss system calls, which are exceptions that provide applications with entry points into the operating system. We then move up a level of abstraction and describe processes and signals, which lie at the intersection of applications and the operating system. Finally, we discuss nonlocal jumps, which are an application-level form of ECF. 
8.1 
Exceptions 

Exceptions are a form of exceptional control .ow that are implemented partly by the hardware and partly by the operating system. Because they are partly implemented in hardware, the details vary from system to system. However, the basic ideas are the same for every system. Our aim in this section is to give you a general understanding of exceptions and exception handling, and to help demystify what is often a confusing aspect of modern computer systems. 
An exception is an abrupt change in the control .ow in response to some change in the processor¡¯s state. Figure 8.1 shows the basic idea. In the .gure, the processor is executing some current instruction I when a signi.cant change in 
curr 

the processor¡¯s state occurs. The state is encoded in various bits and signals inside the processor. The change in state is known as an event. The event might be directly 
704  Chapter 8  Exceptional Control Flow  
Figure 8.1 Anatomy of an exception.  Application program  Exception handler  

A change in the processor¡¯s  
state (event) triggers an  
abrupt control transfer (an exception) from the application program to an  Event occurs here  Icurr Inext  
exception handler. After  
it .nishes processing, the  
handler either returns  
control to the interrupted  
program or aborts.  


Exception 


Exception processing 
Exception return (optional) 

related to the execution of the current instruction. For example, a virtual memory page fault occurs, an arithmetic over.ow occurs, or an instruction attempts a divide by zero. On the other hand, the event might be unrelated to the execution of the current instruction. For example, a system timer goes off or an I/O request completes. 
In any case, when the processor detects that the event has occurred, it makes an indirect procedure call (the exception), through a jump table called an exception table, to an operating system subroutine (the exception handler) that is speci.cally designed to process this particular kind of event. 
When the exception handler .nishes processing, one of three things happens, depending on the type of event that caused the exception: 
1. The handler returns control to the current instruction I , the instruction 
curr 
that was executing when the event occurred. 
2. 
The handler returns control to Inext , the instruction that would have executed next had the exception not occurred. 

3. 
The handler aborts the interrupted program. 


Section 8.1.2 says more about these possibilities. 

Aside Hardware vs. software exceptions 
C++ and Java programmers will have noticed that the term ¡°exception¡± is also used to describe the application-level ECF mechanism provided by C++ and Java in the form of catch, throw, and try statements. If we wanted to be perfectly clear, we might distinguish between ¡°hardware¡± and ¡°software¡± exceptions, but this is usually unnecessary because the meaning is clear from the context. 
8.1.1 Exception Handling 
Exceptions can be dif.cult to understand because handling them involves close cooperation between hardware and software. It is easy to get confused about which component performs which task. Let¡¯s look at the division of labor between hardware and software in more detail. 
Figure 8.2 

Exception table. The 
exception table is a 
Exception tablejump table where entry k contains the address 01 of the handler code for 2 
exception k. 
. . . 
Section 8.1 Exceptions 705 

Code for exception handler 0  
Code for exception handler 1  
Code for exception handler 2  



Each type of possible exception in a system is assigned a unique nonnegative integer exception number. Some of these numbers are assigned by the designers of the processor. Other numbers are assigned by the designers of the operating system kernel (the memory-resident part of the operating system). Examples of the former include divide by zero, page faults, memory access violations, break-points, and arithmetic over.ows. Examples of the latter include system calls and signals from external I/O devices. 
At system boot time (when the computer is reset or powered on), the operat-ing system allocates and initializes a jump table called an exception table, so that entry k contains the address of the handler for exception k. Figure 8.2 shows the format of an exception table. 
At run time (when the system is executing some program), the processor detects that an event has occurred and determines the corresponding exception number k. The processor then triggers the exception by making an indirect pro-cedure call, through entry k of the exception table, to the corresponding handler. Figure 8.3 shows how the processor uses the exception table to form the address of the appropriate exception handler. The exception number is an index into the ex-ception table, whose starting address is contained in a special CPU register called the exception table base register. 
An exception is akin to a procedure call, but with some important differences. 
. As with a procedure call, the processor pushes a return address on the stack before branching to the handler. However, depending on the class of excep-tion, the return address is either the current instruction (the instruction that 
Figure 8.3 Exception number 
(x 4) Exception table

Generating the address 
0

of an exception handler. 
1 

Address of entry 2

The exception number is 
an index into the exception 
table. 

for exception # k 
. . . 
n 1 

was executing when the event occurred) or the next instruction (the instruc-tion that would have executed after the current instruction had the event not occurred). 
. The processor also pushes some additional processor state onto the stack that will be necessary to restart the interrupted program when the handler returns. For example, an IA32 system pushes the EFLAGS register containing, among other things, the current condition codes, onto the stack. 
. If control is being transferred from a user program to the kernel, all of these items are pushed onto the kernel¡¯s stack rather than onto the user¡¯s stack. . Exception handlers run in kernel mode (Section 8.2.4), which means they have complete access to all system resources. 
Once the hardware triggers the exception, the rest of the work is done in software by the exception handler. After the handler has processed the event, it optionally returns to the interrupted program by executing a special ¡°return from interrupt¡± instruction, which pops the appropriate state back into the processor¡¯s control and data registers, restores the state to user mode (Section 8.2.4) if the exception interrupted a user program, and then returns control to the interrupted program. 
8.1.2 Classes of Exceptions 
Exceptions can be divided into four classes: interrupts, traps, faults, and aborts. The table in Figure 8.4 summarizes the attributes of these classes. 
Interrupts 
Interrupts occur asynchronously as a result of signals from I/O devices that are external to the processor. Hardware interrupts are asynchronous in the sense that they are not caused by the execution of any particular instruction. Exception handlers for hardware interrupts are often called interrupt handlers. 
Figure 8.5 summarizes the processing for an interrupt. I/O devices such as network adapters, disk controllers, and timer chips trigger interrupts by signaling a pin on the processor chip and placing onto the system bus the exception number that identi.es the device that caused the interrupt. 

Class  Cause  Async/Sync  Return behavior  
Interrupt  Signal from I/O device  Async  Always returns to next instruction  
Trap  Intentional exception  Sync  Always returns to next instruction  
Fault  Potentially recoverable error  Sync  Might return to current instruction  
Abort  Nonrecoverable error  Sync  Never returns  
Figure 8.4 Classes of exceptions. Asynchronous exceptions occur as a result of events in I/O devices that are external to the processor. Synchronous exceptions occur as a direct result of executing an instruction. 


Figure 8.5 
Interrupt handling. 

The interrupt handler returns control to the next instruction in the application program¡¯s control .ow. 
(1) Interrupt pin goes high during Icurr 
execution of Inext current instruction 

(2) Control passes 
to handler after current instruction finishes 


(3) Interrupt handler runs 
(4) Handler 
returns to next instruction 


After the current instruction .nishes executing, the processor notices that the interrupt pin has gone high, reads the exception number from the system bus, and then calls the appropriate interrupt handler. When the handler returns, it returns control to the next instruction (i.e., the instruction that would have followed the current instruction in the control .ow had the interrupt not occurred). The effect is that the program continues executing as though the interrupt had never happened. 
The remaining classes of exceptions (traps, faults, and aborts) occur syn-chronously as a result of executing the current instruction. We refer to this in-struction as the faulting instruction. 
Traps and System Calls 
Traps are intentional exceptions that occur as a result of executing an instruction. Like interrupt handlers, trap handlers return control to the next instruction. The most important use of traps is to provide a procedure-like interface between user programs and the kernel known as a system call. 
User programs often need to request services from the kernel such as reading a .le (read), creating a new process (fork), loading a new program (execve), or terminating the current process (exit). To allow controlled access to such kernel services, processors provide a special ¡°syscall n¡± instruction that user programs can execute when they want to request service n. Executing the syscall instruction causes a trap to an exception handler that decodes the argument and calls the appropriate kernel routine. Figure 8.6 summarizes the processing for a system call. From a programmer¡¯s perspective, a system call is identical to a 
Figure 8.6 

Trap handling. The trap 
(2) Control passes

handler returns control 
to handler(1) Application

to the next instruction in 
syscall
makes a 



the application program¡¯s Inext 

(3) Trap system call handler runs
control .ow. 
(4) Handler returns 
to instruction following the syscall 


Figure 8.7  
Fault handling. Depend- 
ing on whether the fault can be repaired or not, the fault handler either reexecutes the faulting  (1) Current instruction causes a fault  Icurr  

instruction or aborts. 
(2) Control passes to handler 
(3) Fault handler runs 

abort 


(4) Handler either reexecutes current instruction or aborts 
regular function call. However, their implementations are quite different. Regular functions run in user mode, which restricts the types of instructions they can execute, and they access the same stack as the calling function. A system call runs in kernel mode, which allows it to execute instructions, and accesses a stack de.ned in the kernel. Section 8.2.4 discusses user and kernel modes in more detail. 
Faults 
Faults result from error conditions that a handler might be able to correct. When a fault occurs, the processor transfers control to the fault handler. If the handler is able to correct the error condition, it returns control to the faulting instruction, thereby reexecuting it. Otherwise, the handler returns to an abort routine in the kernel that terminates the application program that caused the fault. Figure 8.7 summarizes the processing for a fault. 
A classic example of a fault is the page fault exception, which occurs when an instruction references a virtual address whose corresponding physical page is not resident in memory and must therefore be retrieved from disk. As we will see in Chapter 9, a page is a contiguous block (typically 4 KB) of virtual memory. The page fault handler loads the appropriate page from disk and then returns control to the instruction that caused the fault. When the instruction executes again, the appropriate physical page is resident in memory and the instruction is able to run to completion without faulting. 
Aborts 
Aborts result from unrecoverable fatal errors, typically hardware errors such as parity errors that occur when DRAM or SRAM bits are corrupted. Abort handlers never return control to the application program. As shown in Figure 8.8, the handler returns control to an abort routine that terminates the application program. 
8.1.3 Exceptions in Linux/IA32 Systems 
To help make things more concrete, let¡¯s look at some of the exceptions de.ned for IA32 systems. There are up to 256 different exception types [27]. Numbers in the range from 0 to 31 correspond to exceptions that are de.ned by the Intel architects, and thus are identical for any IA32 system. Numbers in the range from 32 to 255 correspond to interrupts and traps that are de.ned by the operating system. Figure 8.9 shows a few examples. 

Section 8.1  Exceptions  709  
Figure 8.8 Abort handling. The abort handler passes control to a kernel abort routine that terminates the application program.  (1) Fatal hardware error occurs  Icurr  (2) Control passes to handler  (3) Abort handler runs (4) Handler returns to abort routine  abort  

Exception number  Description  Exception class  
0  Divide error  Fault  
13  General protection fault  Fault  
14  Page fault  Fault  
18  Machine check  Abort  
32¨C127  OS-de.ned exceptions  Interrupt or trap  
128 (0x80)  System call  Trap  
129¨C255  OS-de.ned exceptions  Interrupt or trap  
Figure 8.9 Examples of exceptions in IA32 systems. 


Linux/IA32 Faults and Aborts 
Divide error. A divide error (exception 0) occurs when an application attempts to divide by zero, or when the result of a divide instruction is too big for the destina-tion operand. Unix does not attempt to recover from divide errors, opting instead to abort the program. Linux shells typically report divide errors as ¡°Floating ex-ceptions.¡± 
General protection fault.The infamous general protection fault (exception 13) occurs for many reasons, usually because a program references an unde.ned area of virtual memory, or because the program attempts to write to a read-only text segment. Linux does not attempt to recover from this fault. Linux shells typically report general protection faults as ¡°Segmentation faults.¡± 
Page fault. A page fault (exception 14) is an example of an exception where the faulting instruction is restarted. The handler maps the appropriate page of physical memory on disk into a page of virtual memory, and then restarts the faulting instruction. We will see how page faults work in detail in Chapter 9. 
Machine check. A machine check (exception 18) occurs as a result of a fatal hardware error that is detected during the execution of the faulting instruction. Machine check handlers never return control to the application program. 
Number  Name  Description  Number  Name  Description  
1  exit  Terminate process  27  alarm  Set signal delivery alarm clock  
2  fork  Create new process  29  pause  Suspend process until signal arrives  
3  read  Read .le  37  kill  Send signal to another process  
4  write  Write .le  48  signal  Install signal handler  
5  open  Open .le  63  dup2  Copy .le descriptor  
6  close  Close .le  64  getppid  Get parent¡¯s process ID  
7  waitpid  Wait for child to terminate  65  getpgrp  Get process group  
11  execve  Load and run program  67  sigaction  Install portable signal handler  
19  lseek  Go to .le offset  90  mmap  Map memory page to .le  
20  getpid  Get process ID  106  stat  Get information about .le  
Figure 8.10 Examples of popular system calls in Linux/IA32 systems. Linux provides hundreds of system calls. Source: /usr/include/sys/syscall.h. 


Linux/IA32 System Calls 
Linux provides hundreds of system calls that application programs use when they want to request services from the kernel, such as reading a .le, writing a .le, or creating a new process. Figure 8.10 shows some of the more popular Linux system calls. Each system call has a unique integer number that corresponds to an offset in a jump table in the kernel. 
System calls are provided on IA32 systems via a trapping instruction called int n, where n can be the index of any of the 256 entries in the IA32 exception table. Historically, system calls are provided through exception 128 (0x80). 
C programs can invoke any system call directly by using the syscall function. However, this is rarely necessary in practice. The standard C library provides a set of convenient wrapper functions for most system calls. The wrapper functions package up the arguments, trap to the kernel with the appropriate system call number, and then pass the return status of the system call back to the calling program. Throughout this text, we will refer to system calls and their associated wrapper functions interchangeably as system-level functions. 
It is quite interesting to study how programs can use the int instruction to invoke Linux system calls directly. All parameters to Linux system calls are passed through general purpose registers rather than the stack. By convention, register %eax contains the syscall number, and registers %ebx, %ecx, %edx, %esi, %edi, and %ebp contain up to six arbitrary arguments. The stack pointer %esp cannot be used because it is overwritten by the kernel when it enters kernel mode. 
For example, consider the following version of the familiar hello program, written using the write system-level function: 

1  int main()  
2  {  
3  write(1, "hello, world\n", 13);  
4  exit(0);  
5  }  

code/ecf/hello-asm.sa 

1 .section .data 2 string: 3 .ascii "hello, world\n" 4 string_end: 5 .equ len, string_end -string 
6 .section .text 7 .globl main 8 main: 
First, call write(1, "hello, world\n", 13) 
9 movl $4, %eax System call number 4 10 movl $1, %ebx stdout has descriptor 1 11 movl $string, %ecx Hello world string 12 movl $len, %edx String length 13 int $0x80 System call code 
Next, call exit(0) 14 movl $1, %eax System call number 0 15 movl $0, %ebx Argument is 0 16 int $0x80 System call code 
code/ecf/hello-asm.sa 

Figure 8.11 Implementing the hello program directly with Linux system calls. 
The .rst argument to write sends the output to stdout. The second argument is the sequence of bytes to write, and the third argument gives the number of bytes to write. 
Figure 8.11 shows an assembly language version of hello that uses the int instruction to invoke the write and exit system calls directly. Lines 9¨C13 invoke the write function. First, line 9 stores the number for the write system call in %eax, and lines 10¨C12 set up the argument list. Then line 13 uses the int instruction to invoke the system call. Similarly, lines 14¨C16 invoke the exit system call. 
Aside A note on terminology 
The terminology for the various classes of exceptions varies from system to system. Processor macroar-chitecture speci.cations often distinguish between asynchronous ¡°interrupts¡± and synchronous ¡°excep-tions,¡± yet provide no umbrella term to refer to these very similar concepts. To avoid having to constantly refer to ¡°exceptions and interrupts¡± and ¡°exceptions or interrupts,¡± we use the word ¡°exception¡± as the general term and distinguish between asynchronous exceptions (interrupts) and synchronous ex-ceptions (traps, faults, and aborts) only when it is appropriate. As we have noted, the basic ideas are the same for every system, but you should be aware that some manufacturers¡¯ manuals use the word ¡°exception¡± to refer only to those changes in control .ow caused by synchronous events. 
8.2 
Processes 

Exceptions are the basic building blocks that allow the operating system to provide the notion of a process, one of the most profound and successful ideas in computer science. 
When we run a program on a modern system, we are presented with the illusion that our program is the only one currently running in the system. Our program appears to have exclusive use of both the processor and the memory. The processor appears to execute the instructions in our program, one after the other, without interruption. Finally, the code and data of our program appear to be the only objects in the system¡¯s memory. These illusions are provided to us by the notion of a process. 
The classic de.nition of a process is an instance of a program in execution. Each program in the system runs in the context of some process. The context consists of the state that the program needs to run correctly. This state includes the program¡¯s code and data stored in memory, its stack, the contents of its general-purpose registers, its program counter, environment variables, and the set of open .le descriptors. 
Each time a user runs a program by typing the name of an executable object .le to the shell, the shell creates a new process and then runs the executable object .le in the context of this new process. Application programs can also create new processes and run either their own code or other applications in the context of the new process. 
A detailed discussion of how operating systems implement processes is be-
yond our scope. Instead, we will focus on the key abstractions that a process 
provides to the application: 
. An independent logical control .ow that provides the illusion that our pro-gram has exclusive use of the processor. . A private address space that provides the illusion that our program has exclu-sive use of the memory system. 
Let¡¯s look more closely at these abstractions. 
8.2.1 Logical Control Flow 
A process provides each program with the illusion that it has exclusive use of the processor, even though many other programs are typically running concurrently on the system. If we were to use a debugger to single step the execution of our program, we would observe a series of program counter (PC) values that corresponded exclusively to instructions contained in our program¡¯s executable object .le or in shared objects linked into our program dynamically at run time. This sequence of PC values is known as a logical control .ow, or simply logical .ow. 
Consider a system that runs three processes, as shown in Figure 8.12. The 
single physical control .ow of the processor is partitioned into three logical .ows, 
one for each process. Each vertical line represents a portion of the logical .ow for 

Figure 8.12 Process A Process B Process C 
Logical control .ows. 
Processes provide each program with the illusion that it has exclusive use of the processor. Each vertical Time bar represents a portion of the logical control .ow for a process. 


a process. In the example, the execution of the three logical .ows is interleaved. Process A runs for a while, followed by B, which runs to completion. Process C then runs for awhile, followed by A, which runs to completion. Finally, C is able to run to completion. 
The key point in Figure 8.12 is that processes take turns using the processor. Each process executes a portion of its .ow and then is preempted (temporarily suspended) while other processes take their turns. To a program running in the context of one of these processes, it appears to have exclusive use of the proces-sor. The only evidence to the contrary is that if we were to precisely measure the elapsed time of each instruction, we would notice that the CPU appears to peri-odically stall between the execution of some of the instructions in our program. However, each time the processor stalls, it subsequently resumes execution of our program without any change to the contents of the program¡¯s memory locations or registers. 
8.2.2 Concurrent Flows 
Logical .ows take many different forms in computer systems. Exception handlers, processes, signal handlers, threads, and Java processes are all examples of logical .ows. 
A logical .ow whose execution overlaps in time with another .ow is called a concurrent .ow, and the two .ows are said to run concurrently. More precisely, .ows X and Y are concurrent with respect to each other if and only if X begins after Y begins and before Y .nishes, or Y begins after X begins and before X .nishes. For example, in Figure 8.12, processes A and B run concurrently, as do A and C. On the other hand, B and C do not run concurrently, because the last instruction of B executes before the .rst instruction of C. 
The general phenomenon of multiple .ows executing concurrently is known as concurrency. The notion of a process taking turns with other processes is also known as multitasking. Each time period that a process executes a portion of its .ow is called a time slice. Thus, multitasking is also referred to as time slicing.For example, in Figure 8.12, the .ow for Process A consists of two time slices. 
Notice that the idea of concurrent .ows is independent of the number of processor cores or computers that the .ows are running on. If two .ows overlap in time, then they are concurrent, even if they are running on the same processor. However, we will sometimes .nd it useful to identify a proper subset of concurrent .ows known as parallel .ows. If two .ows are running concurrently on different processor cores or computers, then we say that they are parallel .ows, that they are running in parallel, and have parallel execution. 
Practice Problem 8.1 
Consider three processes with the following starting and ending times: 
Process Start time End time 
A  0  2  
B  1  4  
C  3  5  

For each pair of processes, indicate whether they run concurrently (y) or not (n): 
Process pair Concurrent? AB 
AC 
BC 
8.2.3 Private Address Space 
A process provides each program with the illusion that it has exclusive use of the system¡¯s address space. On a machine with n-bit addresses, the address space is the set of 2n possible addresses, 0, 1,...,2n . 1. A process provides each program with its own private address space. This space is private in the sense that a byte of memory associated with a particular address in the space cannot in general be read or written by any other process. 
Although the contents of the memory associated with each private address space is different in general, each such space has the same general organization. For example, Figure 8.13 shows the organization of the address space for an x86 Linux process. The bottom portion of the address space is reserved for the user program, with the usual text, data, heap, and stack segments. Code segments begin at address 0x08048000 for 32-bit processes, and at address 0x00400000 for 64-bit processes. The top portion of the address space is reserved for the kernel. This part of the address space contains the code, data, and stack that the kernel uses when it executes instructions on behalf of the process (e.g., when the application program executes a system call). 
8.2.4 User and Kernel Modes 
In order for the operating system kernel to provide an airtight process abstraction, the processor must provide a mechanism that restricts the instructions that an application can execute, as well as the portions of the address space that it can access. 

Section 8.2 Processes 715 

Figure 8.13 Process address space.  
 Memory invisible to user code %esp (stack pointer)  

 brk  
0x08048000 (32) 0x00400000 (64)  Loaded from the executable file  



Processors typically provide this capability with a mode bit in some control register that characterizes the privileges that the process currently enjoys. When the mode bit is set, the process is running in kernel mode (sometimes called supervisor mode). A process running in kernel mode can execute any instruction in the instruction set and access any memory location in the system. 
When the mode bit is not set, the process is running in user mode. A process in user mode is not allowed to execute privileged instructions that do things such as halt the processor, change the mode bit, or initiate an I/O operation. Nor is it allowed to directly reference code or data in the kernel area of the address space. Any such attempt results in a fatal protection fault. User programs must instead access kernel code and data indirectly via the system call interface. 
A process running application code is initially in user mode. The only way for the process to change from user mode to kernel mode is via an exception such as an interrupt, a fault, or a trapping system call. When the exception occurs, and control passes to the exception handler, the processor changes the mode from user mode to kernel mode. The handler runs in kernel mode. When it returns to the application code, the processor changes the mode from kernel mode back to user mode. 
Linux provides a clever mechanism, called the /proc .lesystem, that allows user mode processes to access the contents of kernel data structures. The /proc .lesystem exports the contents of many kernel data structures as a hierarchy of text .les that can be read by user programs. For example, you can use the /proc .lesys-tem to .nd out general system attributes such as CPU type (/proc/cpuinfo), or the memory segments used by a particular process (/proc/<process id>/maps). 
The 2.6 version of the Linux kernel introduced a /sys .lesystem, which exports additional low-level information about system buses and devices. 
8.2.5 Context Switches 
The operating system kernel implements multitasking using a higher-level form of exceptional control .ow known as a context switch. The context switch mecha-nism is built on top of the lower-level exception mechanism that we discussed in Section 8.1. 
The kernel maintains a context for each process. The context is the state that the kernel needs to restart a preempted process. It consists of the values of objects such as the general purpose registers, the .oating-point registers, the program counter, user¡¯s stack, status registers, kernel¡¯s stack, and various kernel data structures such as a page table that characterizes the address space, a process table that contains information about the current process, and a .le table that contains information about the .les that the process has opened. 
At certain points during the execution of a process, the kernel can decide to preempt the current process and restart a previously preempted process. This decision is known as scheduling, and is handled by code in the kernel called the scheduler. When the kernel selects a new process to run, we say that the kernel has scheduled that process. After the kernel has scheduled a new process to run, it preempts the current process and transfers control to the new process using a mechanism called a context switch that (1) saves the context of the current process, (2) restores the saved context of some previously preempted process, and 
(3) passes control to this newly restored process. 
A context switch can occur while the kernel is executing a system call on behalf of the user. If the system call blocks because it is waiting for some event to occur, then the kernel can put the current process to sleep and switch to another process. For example, if a read system call requires a disk access, the kernel can opt to perform a context switch and run another process instead of waiting for the data to arrive from the disk. Another example is the sleep system call, which is an explicit request to put the calling process to sleep. In general, even if a system call does not block, the kernel can decide to perform a context switch rather than return control to the calling process. 
A context switch can also occur as a result of an interrupt. For example, all systems have some mechanism for generating periodic timer interrupts, typically every 1 ms or 10 ms. Each time a timer interrupt occurs, the kernel can decide that the current process has run long enough and switch to a new process. 
Figure 8.14 shows an example of context switching between a pair of processes A and B. In this example, initially process A is running in user mode until it traps to the kernel by executing a read system call. The trap handler in the kernel requests a DMA transfer from the disk controller and arranges for the disk to interrupt the processor after the disk controller has .nished transferring the data from disk to memory. 
The disk will take a relatively long time to fetch the data (on the order of tens of milliseconds), so instead of waiting and doing nothing in the interim, the kernel performs a context switch from process A to B. Note that before the switch, the kernel is executing instructions in user mode on behalf of process A. During the .rst part of the switch, the kernel is executing instructions in kernel mode on behalf of process A. Then at some point it begins executing instructions (still in kernel mode) on behalf of process B. And after the switch, the kernel is executing instructions in user mode on behalf of process B. 


Process B then runs for a while in user mode until the disk sends an interrupt to signal that data has been transferred from disk to memory. The kernel decides that process B has run long enough and performs a context switch from process B to A, returning control in process A to the instruction immediately following the read system call. Process A continues to run until the next exception occurs, and so on. 
Aside Cache pollution and exceptional control .ow 
In general, hardware cache memories do not interact well with exceptional control .ows such as interrupts and context switches. If the current process is interrupted brie.y by an interrupt, then the cache is cold for the interrupt handler. If the handler accesses enough items from main memory, then the cache will also be cold for the interrupted process when it resumes. In this case, we say that the handler has polluted the cache. A similar phenomenon occurs with context switches. When a process resumes after a context switch, the cache is cold for the application program and must be warmed up again. 
8.3 
System 
Call 
Error 
Handling 

When Unix system-level functions encounter an error, they typically return .1 and set the global integer variable errno to indicate what went wrong. Program-mers should always check for errors, but unfortunately, many skip error checking because it bloats the code and makes it harder to read. For example, here is how we might check for errors when we call the Linux fork function: 
1  if  ((pid  =  fork())  <  0)  {  
2  fprintf(stderr,  "fork  error:  %s\n",  strerror(errno));  
3  exit(0);  
4  }  

The strerror function returns a text string that describes the error associated with a particular value of errno. We can simplify this code somewhat by de.ning the following error-reporting function: 
1 void unix_error(char *msg) /* Unix-style error */ 2 { 3 fprintf(stderr, "%s: %s\n", msg, strerror(errno)); 4 exit(0); 
5 } 
Given this function, our call to fork reduces from four lines to two lines: 
1 if ((pid = fork()) < 0) 2 unix_error("fork error"); 
We can simplify our code even further by using error-handling wrappers. For a given base function foo, we de.ne a wrapper function Foo with identical arguments, but with the .rst letter of the name capitalized. The wrapper calls the base function, checks for errors, and terminates if there are any problems. For example, here is the error-handling wrapper for the fork function: 
1 pid_t Fork(void) 2 { 3 pid_t pid; 4 5 if ((pid = fork()) < 0) 6 unix_error("Fork error"); 7 return pid; 
8 } 
Given this wrapper, our call to fork shrinks to a single compact line: 
1 pid = Fork(); 
We will use error-handling wrappers throughout the remainder of this book. They allow us to keep our code examples concise, without giving you the mistaken impression that it is permissible to ignore error checking. Note that when we discuss system-level functions in the text, we will always refer to them by their lowercase base names, rather than by their uppercase wrapper names. 
See Appendix A for a discussion of Unix error handling and the error-handling wrappers used throughout this book. The wrappers are de.ned in a .le called csapp.c, and their prototypes are de.ned in a header .le called csapp.h; these are available online from the CS:APP Web site. 
8.4 
Process 
Control 

Unix provides a number of system calls for manipulating processes from C pro-grams. This section describes the important functions and gives examples of how they are used. 

8.4.1 Obtaining Process IDs 
Each process has a unique positive (nonzero) process ID (PID). The getpid function returns the PID of the calling process. The getppid function returns the PID of its parent (i.e., the process that created the calling process). 
#include <sys/types.h> #include <unistd.h> 
pid_t getpid(void); pid_t getppid(void); 
Returns: PID of either the caller or the parent 

The getpid and getppid routines return an integer value of type pid_t, which on Linux systems is de.ned in types.h as an int. 
8.4.2 Creating and Terminating Processes 
From a programmer¡¯s perspective, we can think of a process as being in one of three states: 
. Running. The process is either executing on the CPU or is waiting to be executed and will eventually be scheduled by the kernel. 
. Stopped.The execution of the process is suspended and will not be scheduled. A process stops as a result of receiving a SIGSTOP, SIGTSTP, SIGTTIN, or SIGTTOU signal, and it remains stopped until it receives a SIGCONT signal, at which point it can begin running again. (A signal is a form of software interrupt that is described in detail in Section 8.5.) 
. Terminated. The process is stopped permanently. A process becomes termi-nated for one of three reasons: (1) receiving a signal whose default action is to terminate the process, (2) returning from the main routine, or (3) calling the exit function: 
#include <stdlib.h> void exit(int status); 
This function does not return 

The exit function terminates the process with an exit status of status. (The other way to set the exit status is to return an integer value from the main routine.) 
A parent process creates a new running child process by calling the fork function. 
#include <sys/types.h> #include <unistd.h> 
pid_t fork(void); 
Returns: 0 to child, PID of child to parent, .1 on error 
The newly created child process is almost, but not quite, identical to the par-ent. The child gets an identical (but separate) copy of the parent¡¯s user-level virtual address space, including the text, data, and bss segments, heap, and user stack. The child also gets identical copies of any of the parent¡¯s open .le descriptors, which means the child can read and write any .les that were open in the parent when it called fork. The most signi.cant difference between the parent and the newly created child is that they have different PIDs. 
The fork function is interesting (and often confusing) because it is called once but it returns twice: once in the calling process (the parent), and once in the newly created child process. In the parent, fork returns the PID of the child. In the child, fork returns a value of 0. Since the PID of the child is always nonzero, the return value provides an unambiguous way to tell whether the program is executing in the parent or the child. 
Figure 8.15 shows a simple example of a parent process that uses fork to create a child process. When the fork call returns in line 8, x has a value of 1 in both the parent and child. The child increments and prints its copy of x in line 10. Similarly, the parent decrements and prints its copy of x in line 15. 
When we run the program on our Unix system, we get the following result: 
unix> ./fork parent: x=0 child : x=2 
There are some subtle aspects to this simple example. 
. Call once, return twice. The fork function is called once by the parent, but it returns twice: once to the parent and once to the newly created child. This is fairly straightforward for programs that create a single child. But programs with multiple instances of fork can be confusing and need to be reasoned about carefully. 
. Concurrent execution. The parent and the child are separate processes that run concurrently. The instructions in their logical control .ows can be inter-leaved by the kernel in an arbitrary way. When we run the program on our system, the parent process completes its printf statement .rst, followed by the child. However, on another system the reverse might be true. In general, as programmers we can never make assumptions about the interleaving of the instructions in different processes. 
code/ecf/fork.c 

1 #include "csapp.h" 2 3 int main() 4 { 5 pid_t pid; 6 intx=1; 7 8 pid = Fork(); 9 if (pid == 0) { /* Child */ 
10 printf("child : x=%d\n", ++x); 
11 exit(0); 
12 } 13 14 /* Parent */ 15 printf("parent: x=%d\n", --x); 16 exit(0); 
17 } 
code/ecf/fork.c 

Figure 8.15 Using fork to create a new process. 
. Duplicate but separate address spaces. If we could halt both the parent and the child immediately after the fork function returned in each process, we would see that the address space of each process is identical. Each process has the same user stack, the same local variable values, the same heap, the same global variable values, and the same code. Thus, in our example program, local variable x has a value of 1 in both the parent and the child when the fork function returns in line 8. However, since the parent and the child are separate processes, they each have their own private address spaces. Any subsequent changes that a parent or child makes to x are private and are not re.ected in the memory of the other process. This is why the variable x has different values in the parent and child when they call their respective printf statements. 
. Shared .les. When we run the example program, we notice that both parent and child print their output on the screen. The reason is that the child inherits all of the parent¡¯s open .les. When the parent calls fork, the stdout .le is open and directed to the screen. The child inherits this .le and thus its output is also directed to the screen. 
When you are .rst learning about the fork function, it is often helpful to 
sketch the process graph, where each horizontal arrow corresponds to a process 
that executes instructions from left to right, and each vertical arrow corresponds 
to the execution of a fork function. For example, how many lines of output would the program in Figure 8.16(a) generate? Figure 8.16(b) shows the corresponding process graph. The parent 
Exceptional Control Flow  
(a) Calls fork once  (b) Prints two output lines  
1 2 3 4 5  #include "csapp.h" int main() { Fork();  hello hello  

6 printf("hello\n"); fork 7 exit(0); 
8 
} 

(c) 
Calls fork twice (d) Prints four output lines 1 #include "csapp.h" 


2 hello 3 int main() 


hello 
4 { 5 Fork(); 6 Fork(); 7 printf("hello\n"); 

fork fork
8 exit(0); 
9 
} 

(e) 
Calls fork three times (f) Prints eight output lines 


1 #include "csapp.h" hello 2 


hello 
3 int main() 4 { 5 Fork(); 6 Fork(); 7 Fork(); 8 printf("hello\n"); 9 exit(0); 
10 } 

creates a child when it executes the .rst (and only) fork in the program. Each of these calls printf once, so the program prints two output lines. 
Now what if we were to call fork twice, as shown in Figure 8.16(c)? As we see from Figure 8.16(d), the parent calls fork to create a child, and then the parent and child each call fork, which results in two more processes. Thus, there are four processes, each of which calls printf, so the program generates four output lines. 

Continuing this line of thought, what would happen if we were to call fork three times, as in Figure 8.16(e)? As we see from the process graph in Fig-ure 8.16(f), there are a total of eight processes. Each process calls printf,so the program produces eight output lines. 
Practice Problem 8.2  
Consider the following program:  
code/ecf/forkprob0.c  
1  #include "csapp.h"  
2  
3  int main()  
4  {  
5  intx=1;  
6  
7  if (Fork() == 0)  
8  printf("printf1: x=%d\n", ++x);  
9  printf("printf2: x=%d\n", --x);  
10  exit(0);  
11  }  
code/ecf/forkprob0.c  
A. What is the output of the child process?  
B. What is the output of the parent process?  

8.4.3 Reaping Child Processes 
When a process terminates for any reason, the kernel does not remove it from the system immediately. Instead, the process is kept around in a terminated state until it is reaped by its parent. When the parent reaps the terminated child, the kernel passes the child¡¯s exit status to the parent, and then discards the terminated process, at which point it ceases to exist. A terminated process that has not yet been reaped is called a zombie. 
Aside Why are terminated children called zombies? 
In folklore, a zombie is a living corpse, an entity that is half alive and half dead. A zombie process is similar in the sense that while it has already terminated, the kernel maintains some of its state until it can be reaped by the parent. 
If the parent process terminates without reaping its zombie children, the kernel arranges for the init process to reap them. The init process has a PID of 1 and is created by the kernel during system initialization. Long-running programs 
such as shells or servers should always reap their zombie children. Even though 
zombies are not running, they still consume system memory resources. 
A process waits for its children to terminate or stop by calling the waitpid 
function. 
#include <sys/types.h> #include <sys/wait.h> 
pid_t waitpid(pid_t pid, int *status, int options); 
Returns: PID of child if OK, 0 (if WNOHANG) or .1 on error 
The waitpid function is complicated. By default (when options = 0), waitpid suspends execution of the calling process until a child process in its wait set terminates. If a process in the wait set has already terminated at the time of the call, then waitpid returns immediately. In either case, waitpid returns the PID of the terminated child that caused waitpid to return, and the terminated child is removed from the system. 
Determining the Members of the Wait Set 
The members of the wait set are determined by the pid argument: 
. If pid>0, then the wait set is the singleton child process whose process ID is equal to pid. . If pid=-1, then the wait set consists of all of the parent¡¯s child processes. 
The waitpid function also supports other kinds of wait sets, involving Unix process groups, that we will not discuss. 
Modifying the Default Behavior 
The default behavior can be modi.ed by setting options to various combinations of the WNOHANG and WUNTRACED constants: 
. WNOHANG: Return immediately (with a return value of 0) if none of the child processes in the wait set has terminated yet. The default behavior sus-pends the calling process until a child terminates. This option is useful in those cases where you want to continue doing useful work while waiting for a child to terminate. 
. WUNTRACED: Suspend execution of the calling process until a process in the wait set becomes either terminated or stopped. Return the PID of the terminated or stopped child that caused the return. The default behavior returns only for terminated children. This option is useful when you want to check for both terminated and stopped children. 
. WNOHANG|WUNTRACED: Return immediately, with a return value of 0, if none of the children in the wait set has stopped or terminated, or with a return value equal to the PID of one of the stopped or terminated children. 

Checking the Exit Status of a Reaped Child 
If the status argument is non-NULL, then waitpid encodes status information about the child that caused the return in the status argument. The wait.h include .le de.nes several macros for interpreting the status argument: 
. WIFEXITED(status): Returns true if the child terminated normally, via a call to exit or a return. . WEXITSTATUS(status): Returns the exit status of a normally terminated child. This status is only de.ned if WIFEXITED returned true. . WIFSIGNALED(status): Returns true if the child process terminated be-cause of a signal that was not caught. (Signals are explained in Section 8.5.) 
. WTERMSIG(status): Returns the number of the signal that caused the child process to terminate. This status is only de.ned if WIFSIGNALED(status) returned true. 
. WIFSTOPPED(status): Returns true if the child that caused the return is currently stopped. . WSTOPSIG(status): Returns the number of the signal that caused the child to stop. This status is only de.ned if WIFSTOPPED(status) returned true. 
Error Conditions 

If the calling process has no children, then waitpid returns .1 and sets errno to ECHILD. If the waitpid function was interrupted by a signal, then it returns .1 and sets errno to EINTR. 
Aside Constants associated with Unix functions 
Constants such as WNOHANG and WUNTRACED are de.ned by system header .les. For example, WNOHANG and WUNTRACED are de.ned (indirectly) by the wait.h header .le: 
/* Bits in the third argument to ¡®waitpid¡¯. */ #define WNOHANG 1 /* Don¡¯t block waiting. */ #define WUNTRACED 2 /* Report status of stopped children. */ 
In order to use these constants, you must include the wait.h header .le in your code: 
#include <sys/wait.h> 
The man page for each Unix function lists the header .les to include whenever you use that function in your code. Also, in order to check return codes such as ECHILD and EINTR, you must include errno.h. To simplify our code examples, we include a single header .le called csapp.h that includes the header .les for all of the functions used in the book. The csapp.h header .le is available online from the CS:APP Web site. 
Practice Problem 8.3 
List all of the possible output sequences for the following program: 
code/ecf/waitprob0.c  
1  int main()  
2  {  
3  if  (Fork()  == 0) {  
4  printf("a");  
5  }  
6  else {  
7  printf("b");  
8  waitpid(-1, NULL, 0);  
9  }  
10  printf("c");  
11  exit(0);  
12 }  
code/ecf/waitprob0.c  

The wait Function 
The wait function is a simpler version of waitpid: 
#include <sys/types.h> #include <sys/wait.h> 
pid_t wait(int *status); 
Returns: PID of child if OK or .1 on error 
Calling wait(&status) is equivalent to calling waitpid(-1, &status, 0). 
Examples of Using waitpid 
Because the waitpid function is somewhat complicated, it is helpful to look at a few examples. Figure 8.17 shows a program that uses waitpid to wait, in no particular order, for all of its N children to terminate. 
In line 11, the parent creates each of the N children, and in line 12, each child exits with a unique exit status. Before moving on, make sure you understand why line 12 is executed by each of the children, but not the parent. 
In line 15, the parent waits for all of its children to terminate by using waitpid as the test condition of a while loop. Because the .rst argument is .1, the call to waitpid blocks until an arbitrary child has terminated. As each child terminates, the call to waitpid returns with the nonzero PID of that child. Line 16 checks the exit status of the child. If the child terminated normally, in this case by calling the exit function, then the parent extracts the exit status and prints it on stdout. 

Section 8.4 Process Control 727 

code/ecf/waitpid1.c  
1  #include "csapp.h"  
2  #define N 2  
3  
4  int main()  
5  {  
6  int status, i;  
7  pid_t pid;  
8  
9  /* Parent creates N children */  
10  for (i=0;i<N; i++)  
11  if ((pid = Fork()) == 0) /* Child */  
12  exit(100+i);  
13  
14  /* Parent reaps N children in no particular order  */  
15  while ((pid = waitpid(-1, &status, 0)) > 0) {  
16  if (WIFEXITED(status))  
17  printf("child %d terminated normally with  exit  status=%d\n",  
18  pid, WEXITSTATUS(status));  
19  else  
20  printf("child %d terminated abnormally\n", pid);  
21  }  
22  
23  /* The only normal termination is if there are no  more  children */  
24  if (errno != ECHILD)  
25  unix_error("waitpid error");  
26  
27  exit(0);  
28  }  
code/ecf/waitpid1.c  

Figure 8.17 Using the waitpid function to reap zombie children in no particular order. 
When all of the children have been reaped, the next call to waitpid returns .1 and sets errno to ECHILD. Line 24 checks that the waitpid function terminated normally, and prints an error message otherwise. When we run the program on our Unix system, it produces the following output: 
unix> ./waitpid1 child 22966 terminated normally with exit status=100 child 22967 terminated normally with exit status=101 
Notice that the program reaps its children in no particular order. The order that they were reaped is a property of this speci.c computer system. On another 
code/ecf/waitpid2.c 

1 #include "csapp.h" 2 #define N 2 3 4 int main() 5 { 6 int status, i; 7 pid_t pid[N], retpid; 8 9 /* Parent creates N children */ 
10 for (i=0;i<N; i++) 
11 if ((pid[i] = Fork()) == 0) /* Child */ 
12 exit(100+i); 
13 

14 /* Parent reaps N children in order */ 
15 i=0; 

16 while ((retpid = waitpid(pid[i++], &status, 0)) > 0) { 
17 if (WIFEXITED(status)) 
18 printf("child %d terminated normally with exit status=%d\n", 
19 retpid, WEXITSTATUS(status)); 
20 else 

21 printf("child %d terminated abnormally\n", retpid); 
22 } 23 24 /* The only normal termination is if there are no more children */ 25 if (errno != ECHILD) 26 unix_error("waitpid error"); 27 28 exit(0); 
29 } 
code/ecf/waitpid2.c 

Figure 8.18 Using waitpid to reap zombie children in the order they were created. 
system, or even another execution on the same system, the two children might have been reaped in the opposite order. This is an example of the nondeterministic behavior that can make reasoning about concurrency so dif.cult. Either of the two possible outcomes is equally correct, and as a programmer you may never assume that one outcome will always occur, no matter how unlikely the other outcome appears to be. The only correct assumption is that each possible outcome is equally likely. 
Figure 8.18 shows a simple change that eliminates this nondeterminism in the output order by reaping the children in the same order that they were created by the parent. In line 11, the parent stores the PIDs of its children in order, and then waits for each child in this same order by calling waitpid with the appropriate PID in the .rst argument. 

Practice Problem 8.4 
Consider the following program: 
code/ecf/waitprob1.c  
1  int  main()  
2  {  
3  int  status;  
4  pid_t  pid;  
5  
6  printf("Hello\n");  
7  pid  =  Fork();  
8  printf("%d\n",  !pid);  
9  if  (pid  !=  0)  {  
10  if  (waitpid(-1,  &status,  0)  >  0)  {  
11  if  (WIFEXITED(status)  !=  0)  
12  printf("%d\n",  WEXITSTATUS(status));  
13  }  
14  }  
15  printf("Bye\n");  
16  exit(2);  
17  }  
code/ecf/waitprob1.c  

A. How many output lines does this program generate? 
B. What is one possible ordering of these output lines? 
8.4.4 Putting Processes to Sleep 
The sleep function suspends a process for a speci.ed period of time. 
#include  <unistd.h>  
unsigned  int  sleep(unsigned  int  secs);  
Returns: seconds left to sleep  

Sleep returns zero if the requested amount of time has elapsed, and the number of seconds still left to sleep otherwise. The latter case is possible if the sleep function returns prematurely because it was interrupted by a signal. We will discuss signals in detail in Section 8.5. 
Another function that we will .nd useful is the pause function, which puts the calling function to sleep until a signal is received by the process. 
#include <unistd.h> int pause(void); 
Always returns .1 
Practice Problem 8.5 
Write a wrapper function for sleep, called snooze, with the following interface: 
unsigned int snooze(unsigned int secs); 
The snooze function behaves exactly as the sleep function, except that it prints a message describing how long the process actually slept: 
Slept for 4 of 5 secs. 
8.4.5 Loading and Running Programs 
The execve function loads and runs a new program in the context of the current process. 
#include  <unistd.h>  
int  execve(const  char  *filename,  const  char  *argv[],  
const  char  *envp[]);  

Does not return if OK, returns .1 on error 
The execve function loads and runs the executable object .le filename with the argument list argv and the environment variable list envp. Execve returns to the calling program only if there is an error such as not being able to .nd filename. So unlike fork, which is called once but returns twice, execve is called once and never returns. 
The argument list is represented by the data structure shown in Figure 8.19. The argv variable points to a null-terminated array of pointers, each of which 
argv[]
Figure 8.19 



points to an argument string. By convention, argv[0] is the name of the executable object .le. The list of environment variables is represented by a similar data structure, shown in Figure 8.20. The envp variable points to a null-terminated array of pointers to environment variable strings, each of which is a name-value pair of the form ¡°NAME=VALUE¡±. 
After execve loads filename, it calls the startup code described in Section 7.9. The startup code sets up the stack and passes control to the main routine of the new program, which has a prototype of the form 
int  main(int  argc,  char  **argv, char **envp);  
or equivalently,  
int  main(int  argc,  char  *argv[],  char  *envp[]);  

When main begins executing in a 32-bit Linux process, the user stack has the organization shown in Figure 8.21. Let¡¯s work our way from the bottom of the stack (the highest address) to the top (the lowest address). First are the argument 
0xbfffffff 
Bottom of stack
Figure 8.21 

Typical organization of the user stack when a new program starts. 



Null-terminated environment variable strings  
Null-terminated command-line arg strings  
Unused  
envp[n] NULL  
envp[n 1]  
¡­  
envp[0]  
argv[argc] NULL  
argv[argc 1]  
¡­  
argv[0]  
(Dynamic linker variables)  
envp  
argv  
argc  
Stack frame for main  


0xbffffa7c 
Top of stack 

and environment strings, which are stored contiguously on the stack, one after the other without any gaps. These are followed further up the stack by a null-terminated array of pointers, each of which points to an environment variable string on the stack. The global variable environ points to the .rst of these pointers, envp[0]. The environment array is followed immediately by the null-terminated argv[] array, with each element pointing to an argument string on the stack. At the top of the stack are the three arguments to the main routine: (1) envp, which points to the envp[] array, (2) argv, which points to the argv[] array, and (3) argc, which gives the number of non-null pointers in the argv[] array. 
Unix provides several functions for manipulating the environment array: 
#include <stdlib.h> 
char *getenv(const char *name); Returns: ptr to name if exists, NULL if no match 
The getenv function searches the environment array for a string ¡°name=value¡±. If found, it returns a pointer to value, otherwise it returns NULL. 
#include <stdlib.h> int setenv(const char *name, const char *newvalue, int overwrite); 
Returns: 0 on success, .1 on error 
void unsetenv(const char *name); 
Returns: nothing 
If the environment array contains a string of the form ¡°name=oldvalue¡±, then unsetenv deletes it and setenv replaces oldvalue with newvalue, but only if overwrite is nonzero. If name does not exist, then setenv adds ¡°name=newvalue¡± to the array. 

Aside Programs vs. processes 
This is a good place to pause and make sure you understand the distinction between a program and a process. A program is a collection of code and data; programs can exist as object modules on disk or as segments in an address space. A process is a speci.c instance of a program in execution; a program always runs in the context of some process. Understanding this distinction is important if you want to understand the fork and execve functions. The fork function runs the same program in a new child process that is a duplicate of the parent. The execve function loads and runs a new program in the context of the current process. While it overwrites the address space of the current process, it does not create a new process. The new program still has the same PID, and it inherits all of the .le descriptors that were open at the time of the call to the execve function. 
Practice Problem 8.6 
Write a program called myecho that prints its command line arguments and envi-ronment variables. For example: 
unix> ./myecho arg1 arg2 
Command line arguments: argv[ 0]: myecho argv[ 1]: arg1 argv[ 2]: arg2 
Environment variables: envp[ 0]: PWD=/usr0/droh/ics/code/ecf envp[ 1]: TERM=emacs 
... 

envp[25]: USER=droh envp[26]: SHELL=/usr/local/bin/tcsh envp[27]: HOME=/usr0/droh 
8.4.6 Using fork and execve to Run Programs 
Programs such as Unix shells and Web servers (Chapter 11) make heavy use of the fork and execve functions. A shell is an interactive application-level program that runs other programs on behalf of the user. The original shell was the sh program, which was followed by variants such as csh, tcsh, ksh, and bash.A shell performs a sequence of read/evaluate steps, and then terminates. The read step reads a command line from the user. The evaluate step parses the command line and runs programs on behalf of the user. 
Figure 8.22 shows the main routine of a simple shell. The shell prints a command-line prompt, waits for the user to type a command line on stdin, and then evaluates the command line. 
Figure 8.23 shows the code that evaluates the command line. Its .rst task is to call the parseline function (Figure 8.24), which parses the space-separated command-line arguments and builds the argv vector that will eventually be passed to execve. The .rst argument is assumed to be either the name of a built-in shell command that is interpreted immediately, or an executable object .le that will be loaded and run in the context of a new child process. 
If the last argument is an ¡°&¡± character, then parseline returns 1, indicating that the program should be executed in the background (the shell does not wait 
code/ecf/shellex.c 
1  #include "csapp.h"  
2  #define MAXARGS  128  
3  
4  /* Function prototypes */  
5  void eval(char *cmdline);  
6  int parseline(char *buf, char **argv);  
7  int builtin_command(char **argv);  
8  
9  int main()  
10  {  
11  char cmdline[MAXLINE]; /* Command line */  
12  
13  while (1) {  
14  /* Read */  
15  printf("> ");  
16  Fgets(cmdline, MAXLINE, stdin);  
17  if (feof(stdin))  
18  exit(0);  
19  
20  /* Evaluate */  
21  eval(cmdline);  
22  }  
23  }  
code/ecf/shellex.c  
Figure 8.22 The main routine for a simple shell program. 


for it to complete). Otherwise it returns 0, indicating that the program should be run in the foreground (the shell waits for it to complete). 
After parsing the command line, the eval function calls the builtin_command function, which checks whether the .rst command line argument is a built-in shell command. If so, it interprets the command immediately and returns 1. Otherwise, it returns 0. Our simple shell has just one built-in command, the quit command, which terminates the shell. Real shells have numerous commands, such as pwd, jobs, and fg. 
If builtin_command returns 0, then the shell creates a child process and executes the requested program inside the child. If the user has asked for the program to run in the background, then the shell returns to the top of the loop and waits for the next command line. Otherwise the shell uses the waitpid function to wait for the job to terminate. When the job terminates, the shell goes on to the next iteration. 
Notice that this simple shell is .awed because it does not reap any of its background children. Correcting this .aw requires the use of signals, which we describe in the next section. 

Section 8.4 Process Control 735 

code/ecf/shellex.c  
1  /*  eval  -Evaluate  a  command  line  */  
2  void  eval(char  *cmdline)  
3  {  
4  char  *argv[MAXARGS];  /*  Argument  list  execve()  */  
char  buf[MAXLINE];  /*  Holds  modified  command  line  */  
6  int  bg;  /*  Should  the  job  run  in  bg  or  fg?  */  
7  pid_t  pid;  /*  Process  id  */  
8  
9  strcpy(buf,  cmdline);  
bg  =  parseline(buf,  argv);  
11  if  (argv[0]  ==  NULL)  
12  return;  /*  Ignore  empty  lines  */  
13  
14  if  (!builtin_command(argv))  {  
if  ((pid  =  Fork())  ==  0)  {  /*  Child  runs  user  job  */  
16  if  (execve(argv[0],  argv,  environ)  <  0)  {  
17  printf("%s:  Command  not  found.\n",  argv[0]);  
18  exit(0);  
19  }  
}  
21  
22  /*  Parent  waits  for  foreground  job  to  terminate  */  
23  if  (!bg)  {  
24  int  status;  
if  (waitpid(pid,  &status,  0)  <  0)  
26  unix_error("waitfg:  waitpid  error");  
27  }  
28  else  
29  printf("%d  %s",  pid,  cmdline);  
}  
31  return;  
32  }  
33  
34  /*  If  first  arg  is  a  builtin  command,  run  it  and  return  true  */  
int  builtin_command(char  **argv)  
36  {  
37  if  (!strcmp(argv[0],  "quit"))  /*  quit  command  */  
38  exit(0);  
39  if  (!strcmp(argv[0],  "&"))  /*  Ignore  singleton  &  */  
return 1;  
41  return  0;  /*  Not  a  builtin  command  */  
42  }  
code/ecf/shellex.c  

Figure 8.23 eval: Evaluates the shell command line. 
code/ecf/shellex.c 

1 /* parseline -Parse the command line and build the argv array */ 2 int parseline(char *buf, char **argv) 3 { 4 char *delim; /* Points to first space delimiter */ 5 int argc; /* Number of args */ 6 int bg; /* Background job? */ 7 8 buf[strlen(buf)-1]=¡¯¡¯; /* Replace trailing ¡¯\n¡¯ with space */ 9 while (*buf && (*buf == ¡¯ ¡¯)) /* Ignore leading spaces */ 
10 buf++; 
11 

12 /* Build the argv list */ 
13 argc = 0; 
14 while ((delim = strchr(buf, ¡¯ ¡¯))) { 
15 argv[argc++] = buf; 
16 *delim = ¡¯\0¡¯; 
17 buf = delim + 1; 
18 while (*buf && (*buf == ¡¯ ¡¯)) /* Ignore spaces */ 
19 buf++; 
20 } 21 argv[argc] = NULL; 22 23 if (argc == 0) /* Ignore blank line */ 24 return 1; 25 26 /* Should the job run in the background? */ 27 if ((bg = (*argv[argc-1] == ¡¯&¡¯)) != 0) 28 argv[--argc] = NULL; 29 30 return bg; 
31 } 
code/ecf/shellex.c 

Figure 8.24 parseline: Parses a line of input for the shell. 
8.5 
Signals 

To this point in our study of exceptional control .ow, we have seen how hardware and software cooperate to provide the fundamental low-level exception mecha-nism. We have also seen how the operating system uses exceptions to support a form of exceptional control .ow known as the process context switch. In this sec-tion, we will study a higher-level software form of exceptional control .ow, known as a Unix signal, that allows processes and the kernel to interrupt other processes. 

Section 8.5 Signals 737 

Number  Name  Default action  Corresponding event  
1  SIGHUP  Terminate  Terminal line hangup  
2  SIGINT  Terminate  Interrupt from keyboard  
3  SIGQUIT  Terminate  Quit from keyboard  
4  SIGILL  Terminate  Illegal instruction  
5  SIGTRAP  Terminate and dump core (1)  Trace trap  
6  SIGABRT  Terminate and dump core (1)  Abort signal from abort function  
7  SIGBUS  Terminate  Bus error  
8  SIGFPE  Terminate and dump core (1)  Floating point exception  
9  SIGKILL  Terminate (2)  Kill program  
10  SIGUSR1  Terminate  User-de.ned signal 1  
11  SIGSEGV  Terminate and dump core (1)  Invalid memory reference (seg fault)  
12  SIGUSR2  Terminate  User-de.ned signal 2  
13  SIGPIPE  Terminate  Wrote to a pipe with no reader  
14  SIGALRM  Terminate  Timer signal from alarm function  
15  SIGTERM  Terminate  Software termination signal  
16  SIGSTKFLT  Terminate  Stack fault on coprocessor  
17  SIGCHLD  Ignore  A child process has stopped or terminated  
18  SIGCONT  Ignore  Continue process if stopped  
19  SIGSTOP  Stop until next SIGCONT (2)  Stop signal not from terminal  
20  SIGTSTP  Stop until next SIGCONT  Stop signal from terminal  
21  SIGTTIN  Stop until next SIGCONT  Background process read from terminal  
22  SIGTTOU  Stop until next SIGCONT  Background process wrote to terminal  
23  SIGURG  Ignore  Urgent condition on socket  
24  SIGXCPU  Terminate  CPU time limit exceeded  
25  SIGXFSZ  Terminate  File size limit exceeded  
26  SIGVTALRM  Terminate  Virtual timer expired  
27  SIGPROF  Terminate  Pro.ling timer expired  
28  SIGWINCH  Ignore  Window size changed  
29  SIGIO  Terminate  I/O now possible on a descriptor  
30  SIGPWR  Terminate  Power failure  

Figure 8.25 Linux signals. Notes: (1) Years ago, main memory was implemented with a technology known as core memory. ¡°Dumping core¡± is a historical term that means writing an image of the code and data memory segments to disk. (2) This signal can neither be caught nor ignored. 
A signal is a small message that noti.es a process that an event of some type has occurred in the system. For example, Figure 8.25 shows the 30 different types of signals that are supported on Linux systems. Typing ¡°man 7 signal¡± on the shell command line gives the list. 
Each signal type corresponds to some kind of system event. Low-level hard-ware exceptions are processed by the kernel¡¯s exception handlers and would not normally be visible to user processes. Signals provide a mechanism for exposing the occurrence of such exceptions to user processes. For example, if a process at-tempts to divide by zero, then the kernel sends it a SIGFPE signal (number 8). If a process executes an illegal instruction, the kernel sends it a SIGILL signal (number 4). If a process makes an illegal memory reference, the kernel sends it a SIGSEGV signal (number 11). Other signals correspond to higher-level soft-ware events in the kernel or in other user processes. For example, if you type a ctrl-c (i.e., press the ctrl key and the c key at the same time) while a process is running in the foreground, then the kernel sends a SIGINT (number 2) to the foreground process. A process can forcibly terminate another process by sending it a SIGKILL signal (number 9). When a child process terminates or stops, the kernel sends a SIGCHLD signal (number 17) to the parent. 
8.5.1 Signal Terminology 
The transfer of a signal to a destination process occurs in two distinct steps: 
. Sending a signal. The kernel sends (delivers) a signal to a destination process by updating some state in the context of the destination process. The signal is delivered for one of two reasons: (1) The kernel has detected a system event such as a divide-by-zero error or the termination of a child process. 
(2) A process has invoked the kill function (discussed in the next section) to explicitly request the kernel to send a signal to the destination process. A process can send a signal to itself. 
. Receiving a signal. A destination process receives a signal when it is forced by the kernel to react in some way to the delivery of the signal. The process can either ignore the signal, terminate, or catch the signal by executing a user-level function called a signal handler. Figure 8.26 shows the basic idea of a handler catching a signal. 
A signal that has been sent but not yet received is called a pending signal.At any point in time, there can be at most one pending signal of a particular type. If a process has a pending signal of type k, then any subsequent signals of type k sent to that process are not queued; they are simply discarded. A process can selectively block the receipt of certain signals. When a signal is blocked, it can be delivered, but the resulting pending signal will not be received until the process unblocks the signal. 
Figure 8.26 

Signal handling. Receipt of a signal triggers a control transfer to a signal handler. After it .nishes processing, the handler returns control  (1) Signal received by process  Icurr Inext  (2) Control passes to signal handler  (3) Signal handler runs  
to the interrupted program.  (4) Signal handler  
returns to  

 next instruction  

A pending signal is received at most once. For each process, the kernel main-tains the set of pending signals in the pending bit vector, and the set of blocked signals in the blocked bit vector. The kernel sets bit k in pending whenever a sig-nal of type k is delivered and clears bit k in pending whenever a signal of type k is received. 
8.5.2 Sending Signals 
Unix systems provide a number of mechanisms for sending signals to processes. All of the mechanisms rely on the notion of a process group. 
Process Groups 

Every process belongs to exactly one process group, which is identi.ed by a positive integer process group ID.The getpgrp function returns the process group ID of the current process. 
#include <unistd.h> pid_t getpgrp(void); 
Returns: process group ID of calling process 

By default, a child process belongs to the same process group as its parent. A process can change the process group of itself or another process by using the setpgid function: 
#include <unistd.h> int setpgid(pid_t pid, pid_t pgid); 
Returns: 0 on success, .1 on error 

The setpgid function changes the process group of process pid to pgid.If pid is zero, the PID of the current process is used. If pgid is zero, the PID of the process speci.ed by pid is used for the process group ID. For example, if process 15213 is the calling process, then 
setpgid(0, 0); 
creates a new process group whose process group ID is 15213, and adds process 15213 to this new group. 
Sending Signals with the /bin/kill Program 
The /bin/kill program sends an arbitrary signal to another process. For example, the command 
unix> /bin/kill -9 15213 

40 pgid 40 
Foreground process group 20 
sends signal 9 (SIGKILL) to process 15213. A negative PID causes the signal to be sent to every process in process group PID. For example, the command 
unix> /bin/kill -9 -15213 
sends a SIGKILL signal to every process in process group 15213. Note that we use the complete path /bin/kill here because some Unix shells have their own built-in kill command. 
Sending Signals from the Keyboard 
Unix shells use the abstraction of a job to represent the processes that are created as a result of evaluating a single command line. At any point in time, there is at most one foreground job and zero or more background jobs. For example, typing 
unix> ls | sort 
creates a foreground job consisting of two processes connected by a Unix pipe: one running the ls program, the other running the sort program. 
The shell creates a separate process group for each job. Typically, the process group ID is taken from one of the parent processes in the job. For example, Figure 8.27 shows a shell with one foreground job and two background jobs. The parent process in the foreground job has a PID of 20 and a process group ID of 
20. The parent process has created two children, each of which are also members of process group 20. 
Typing ctrl-c at the keyboard causes a SIGINT signal to be sent to the shell. The shell catches the signal (see Section 8.5.3) and then sends a SIGINT to every process in the foreground process group. In the default case, the result is to terminate the foreground job. Similarly, typing crtl-z sends a SIGTSTP signal to the shell, which catches it and sends a SIGTSTP signal to every process in the foreground process group. In the default case, the result is to stop (suspend) the foreground job. 

Sending Signals with the kill Function 
Processes send signals to other processes (including themselves) by calling the kill function. 
#include <sys/types.h> #include <signal.h> 
int kill(pid_t pid, int sig); 
Returns: 0 if OK, .1 on error 

If pid is greater than zero, then the kill function sends signal number sig to process pid.If pid is less than zero, then kill sends signal sig to every process in process group abs(pid). Figure 8.28 shows an example of a parent that uses the kill function to send a SIGKILL signal to its child. 
code/ecf/kill.c  
1  #include  "csapp.h"  
2  
3  int  main()  
4  {  
5  pid_t  pid;  
6  
7  /*  Child  sleeps  until  SIGKILL  signal  received,  then  dies  */  
8  if  ((pid  =  Fork())  ==  0)  {  
9  Pause();  /*  Wait  for  a  signal  to  arrive  */  
10  printf("control  should  never  reach  here!\n");  
11  exit(0);  
12  }  
13  
14  /*  Parent  sends  a  SIGKILL  signal  to  a  child  */  
15  Kill(pid,  SIGKILL);  
16  exit(0);  
17  }  
code/ecf/kill.c  
Figure 8.28 Using the kill function to send a signal to a child. 


Sending Signals with the alarm Function 
A process can send SIGALRM signals to itself by calling the alarm function. 
#include <unistd.h> unsigned int alarm(unsigned int secs); 
Returns: remaining secs of previous alarm, or 0 if no previous alarm 
The alarm function arranges for the kernel to send a SIGALRM signal to the calling process in secs seconds. If secs is zero, then no new alarm is scheduled. In any event, the call to alarm cancels any pending alarms, and returns the number of seconds remaining until any pending alarm was due to be delivered (had not this call to alarm canceled it), or 0 if there were no pending alarms. 
Figure 8.29 shows a program called alarm that arranges to be interrupted by a SIGALRM signal every second for .ve seconds. When the sixth SIGALRM is delivered it terminates. When we run the program in Figure 8.29, we get the following output: a ¡°BEEP¡± every second for .ve seconds, followed by a ¡°BOOM¡± when the program terminates. 
unix> ./alarm BEEP BEEP BEEP BEEP BEEP BOOM! 
Notice that the program in Figure 8.29 uses the signal function to install a signal handler function (handler) that is called asynchronously, interrupting the in.nite while loop in main, whenever the process receives a SIGALRM signal. When the handler function returns, control passes back to main, which picks up where it was interrupted by the arrival of the signal. Installing and using signal handlers can be quite subtle, and is the topic of the next few sections. 
8.5.3 Receiving Signals 
When the kernel is returning from an exception handler and is ready to pass control to process p, it checks the set of unblocked pending signals (pending & ~blocked) for process p. If this set is empty (the usual case), then the kernel passes control to the next instruction (Inext ) in the logical control .ow of p. 
However, if the set is nonempty, then the kernel chooses some signal k in the set (typically the smallest k) and forces p to receive signal k. The receipt of the signal triggers some action by the process. Once the process completes the action, then control passes back to the next instruction (Inext ) in the logical control .ow of 
p. Each signal type has a prede.ned default action, which is one of the following: 
. The process terminates. 
. The process terminates and dumps core. 

Section 8.5 Signals 743 

code/ecf/alarm.c  
1  #include  "csapp.h"  
2  
3  void  handler(int  sig)  
4  {  
5  static  int  beeps  =  0;  
6  
7  printf("BEEP\n");  
8  if  (++beeps  <  5)  
9  Alarm(1);  /*  Next  SIGALRM  will  be  delivered  in  1  second  */  
10  else  {  
11  printf("BOOM!\n");  
12  exit(0);  
13  }  
14  }  
15  
16  int  main()  
17  {  
18  Signal(SIGALRM,  handler);  /*  Install  SIGALRM  handler  */  
19  Alarm(1);  /*  Next  SIGALRM  will  be  delivered  in  1s  */  
20  
21  while  (1)  {  
22  ;  /*  Signal  handler  returns  control  here  each  time  */  
23  }  
24  exit(0);  
25  }  
code/ecf/alarm.c  

Figure 8.29 Using the alarm function to schedule periodic events. 
. The process stops until restarted by a SIGCONT signal. 
. The process ignores the signal. 
Figure 8.25 shows the default actions associated with each type of signal. For ex-ample, the default action for the receipt of a SIGKILL is to terminate the receiving process. On the other hand, the default action for the receipt of a SIGCHLD is to ignore the signal. A process can modify the default action associated with a signal by using the signal function. The only exceptions are SIGSTOP and SIGKILL, whose default actions cannot be changed. 
#include <signal.h> typedef void (*sighandler_t)(int); 
sighandler_t signal(int signum, sighandler_t handler); 
Returns: ptr to previous handler if OK, SIG_ERR on error (does not set errno) 
The signal function can change the action associated with a signal signum in one of three ways: 
. If handler is SIG_IGN, then signals of type signum are ignored. . If handler is SIG_DFL, then the action for signals of type signum reverts to the default action. . Otherwise, handler is the address of a user-de.ned function, called a signal handler, that will be called whenever the process receives a signal of type signum. Changing the default action by passing the address of a handler to the signal function is known as installing the handler. The invocation of the handler is called catching the signal. The execution of the handler is referred to as handling the signal. 
When a process catches a signal of type k, the handler installed for signal k is invoked with a single integer argument set to k. This argument allows the same handler function to catch different types of signals. 
When the handler executes its return statement, control (usually) passes back to the instruction in the control .ow where the process was interrupted by the receipt of the signal. We say ¡°usually¡± because in some systems, interrupted system calls return immediately with an error. 
Figure 8.30 shows a program that catches the SIGINT signal sent by the shell whenever the user types ctrl-c at the keyboard. The default action for SIGINT is to immediately terminate the process. In this example, we modify the default behavior to catch the signal, print a message, and then terminate the process. 
The handler function is de.ned in lines 3¨C7. The main routine installs the handler in lines 12¨C 13, and then goes to sleep until a signal is received (line 15). When the SIGINT signal is received, the handler runs, prints a message (line 5), and then terminates the process (line 6). 
Signal handlers are yet another example of concurrency in a computer system. The execution of the signal handler interrupts the execution of the main C routine, akin to the way that a low-level exception handler interrupts the control .ow of the current application program. Since the logical control .ow of the signal handler overlaps the logical control .ow of the main routine, the signal handler and the main routine run concurrently. 
Practice Problem 8.7 
Write a program, called snooze, that takes a single command line argument, calls the snooze function from Problem 8.5 with this argument, and then terminates. Write your program so that the user can interrupt the snooze function by typing ctrl-c at the keyboard. For example: 
unix> ./snooze 5 Slept for 3 of 5 secs. User hits crtl-c after 3 seconds unix> 
code/ecf/sigint1.c 

1 #include "csapp.h" 2 3 void handler(int sig) /* SIGINT handler */ 4 { 5 printf("Caught SIGINT\n"); 6 exit(0); 
7 } 8 9 int main() 

10 { 11 /* Install the SIGINT handler */ 12 if (signal(SIGINT, handler) == SIG_ERR) 13 unix_error("signal error"); 14 15 pause(); /* Wait for the receipt of a signal */ 16 17 exit(0); 
18 } 
code/ecf/sigint1.c 

Figure 8.30 A program that uses a signal handler to catch a SIGINT signal. 
8.5.4 Signal Handling Issues 
Signal handling is straightforward for programs that catch a single signal and then terminate. However, subtle issues arise when a program catches multiple signals. 
. Pending signals are blocked. Unix signal handlers typically block pending signals of the type currently being processed by the handler. For example, suppose a process has caught a SIGINT signal and is currently running its SIGINT handler. If another SIGINT signal is sent to the process, then the SIGINT will become pending, but will not be received until after the handler returns. 
. Pending signals are not queued. There can be at most one pending signal of any particular type. Thus, if two signals of type k are sent to a destination process while signal k is blocked because the destination process is currently executing a handler for signal k, then the second signal is simply discarded; it is not queued. The key idea is that the existence of a pending signal merely indicates that at least one signal has arrived. 
. System calls can be interrupted. System calls such as read, wait, and accept that can potentially block the process for a long period of time are called slow system calls. On some systems, slow system calls that are interrupted when a handler catches a signal do not resume when the signal handler returns, but instead return immediately to the user with an error condition and errno set to EINTR. 
Let¡¯s look more closely at the subtleties of signal handling, using a simple application that is similar in nature to real programs such as shells and Web servers. The basic structure is that a parent process creates some children that run independently for a while and then terminate. The parent must reap the children to avoid leaving zombies in the system. But we also want the parent to be free to do other work while the children are running. So we decide to reap the children with a SIGCHLD handler, instead of explicitly waiting for the children to terminate. (Recall that the kernel sends a SIGCHLD signal to the parent whenever one of its children terminates or stops.) 
Figure 8.31 shows our .rst attempt. The parent installs a SIGCHLD handler, and then creates three children, each of which runs for 1 second and then ter-minates. In the meantime, the parent waits for a line of input from the terminal and then processes it. This processing is modeled by an in.nite loop. When each child terminates, the kernel noti.es the parent by sending it a SIGCHLD signal. The parent catches the SIGCHLD, reaps one child, does some additional cleanup work (modeled by the sleep(2) statement), and then returns. 
The signal1 program in Figure 8.31 seems fairly straightforward. When we run it on our Linux system, however, we get the following output: 
linux> ./signal1 Hello from child 10320 Hello from child 10321 Hello from child 10322 Handler reaped child 10320 Handler reaped child 10322 
<cr> 
Parent processing input 
From the output, we note that although three SIGCHLD signals were sent to the parent, only two of these signals were received, and thus the parent only reaped two children. If we suspend the parent process, we see that, indeed, child process 10321 was never reaped and remains a zombie (indicated by the string ¡°defunct¡± in the output of the ps command): 
<ctrl-z> 
Suspended 
linux> ps 
PID TTY STAT TIME COMMAND 
... 
10319 p5 T 0:03 signal1 
10321 p5 Z 0:00 signal1 <defunct> 
10323 p5 R 0:00 ps 
What went wrong? The problem is that our code failed to account for the facts that signals can block and that signals are not queued. Here¡¯s what happened: The .rst signal is received and caught by the parent. While the handler is still processing the .rst signal, the second signal is delivered and added to the set of pending signals. However, since SIGCHLD signals are blocked by the SIGCHLD handler, 
code/ecf/signal1.c 

1 #include "csapp.h" 2 3 void handler1(int sig) 4 { 
pid_t pid; 
6 7 if ((pid = waitpid(-1, NULL, 0)) < 0) 8 unix_error("waitpid error"); 9 printf("Handler reaped child %d\n", (int)pid); 
Sleep(2); 11 return; 
12 } 13 14 int main() 

{ 16 int i, n; 17 char buf[MAXBUF]; 18 19 if (signal(SIGCHLD, handler1) == SIG_ERR) 
unix_error("signal error"); 
21 22 /* Parent creates children */ 23 for (i=0;i<3; i++) { 24 if (Fork() == 0) { 
printf("Hello from child %d\n", (int)getpid()); 26 Sleep(1); 27 exit(0); 
28 
} 

29 
} 



31 /* Parent waits for terminal input and then processes it */ 32 if ((n = read(STDIN_FILENO, buf, sizeof(buf))) < 0) 33 unix_error("read"); 34 
printf("Parent processing input\n"); 36 while (1) 37 ; 38 39 exit(0); 
} 
code/ecf/signal1.c 

Figure 8.31 signal1: This program is .awed because it fails to deal with the facts that signals can block, signals are not queued, and system calls can be interrupted. 
the second signal is not received. Shortly thereafter, while the handler is still processing the .rst signal, the third signal arrives. Since there is already a pending SIGCHLD, this third SIGCHLD signal is discarded. Sometime later, after the handler has returned, the kernel notices that there is a pending SIGCHLD signal and forces the parent to receive the signal. The parent catches the signal and executes the handler a second time. After the handler .nishes processing the second signal, there are no more pending SIGCHLD signals, and there never will be, because all knowledge of the third SIGCHLD has been lost. The crucial lesson is that signals cannot be used to count the occurrence of events in other processes. 
To .x the problem, we must recall that the existence of a pending signal only implies that at least one signal has been delivered since the last time the process received a signal of that type. So we must modify the SIGCHLD handler to reap as many zombie children as possible each time it is invoked. Figure 8.32 shows the modi.ed SIGCHLD handler. When we run signal2 on our Linux system, it now correctly reaps all of the zombie children: 
linux> ./signal2 Hello from child 10378 Hello from child 10379 Hello from child 10380 Handler reaped child 10379 Handler reaped child 10378 Handler reaped child 10380 
<cr> 
Parent processing input 
However, we are not .nished yet. If we run the signal2 program on an older version of the Solaris operating system, it correctly reaps all of the zombie children. However, now the blocked read system call returns prematurely with an error, before we are able to type in our input on the keyboard: 
solaris> ./signal2 Hello from child 18906 Hello from child 18907 Hello from child 18908 Handler reaped child 18906 Handler reaped child 18908 Handler reaped child 18907 read: Interrupted system call 
What went wrong? The problem arises because on this particular Solaris system, slow system calls such as read are not restarted automatically after they are interrupted by the delivery of a signal. Instead, they return prematurely to the calling application with an error condition, unlike Linux systems, which restart interrupted system calls automatically. 
In order to write portable signal handling code, we must allow for the pos-sibility that system calls will return prematurely and then restart them manually 
code/ecf/signal2.c 

1 #include "csapp.h" 2 3 void handler2(int sig) 4 { 
pid_t pid; 
6 7 while ((pid = waitpid(-1, NULL, 0)) > 0) 8 printf("Handler reaped child %d\n", (int)pid); 9 if (errno != ECHILD) 
unix_error("waitpid error"); 11 Sleep(2); 12 return; 
13 } 14 

int main() 16 { 17 int i, n; 18 char buf[MAXBUF]; 19 
if (signal(SIGCHLD, handler2) == SIG_ERR) 21 unix_error("signal error"); 22 23 /* Parent creates children */ 24 for (i=0;i<3; i++) { 
if (Fork() == 0) { 26 printf("Hello from child %d\n", (int)getpid()); 27 Sleep(1); 28 exit(0); 
29 } } 

31 32 /* Parent waits for terminal input and then processes it */ 33 if ((n = read(STDIN_FILENO, buf, sizeof(buf))) < 0) 34 unix_error("read error"); 
36 printf("Parent processing input\n"); 37 while (1) 38 ; 39 
exit(0); 
41 } 
code/ecf/signal2.c 

Figure 8.32 signal2: An improved version of Figure 8.31 that correctly accounts for the facts that signals can block and are not queued. However, it does not allow for the possibility that system calls can be interrupted. 
when this occurs. Figure 8.33 shows the modi.cation to signal2 that manually restarts aborted read calls. The EINTR return code in errno indicates that the read system call returned prematurely after it was interrupted. 
When we run our new signal3 program on a Solaris system, the program runs correctly: 
solaris> ./signal3 Hello from child 19571 Hello from child 19572 Hello from child 19573 Handler reaped child 19571 Handler reaped child 19572 Handler reaped child 19573 
<cr> 
Parent processing input 
Practice Problem 8.8 
What is the output of the following program? 
code/ecf/signalprob0.c 
1 pid_t pid; 2 int counter = 2; 3 4 void handler1(int sig) { 5 counter = counter -1; 6 printf("%d", counter); 7 fflush(stdout); 8 exit(0); 
9 
} 10 11 int main() { 12 signal(SIGUSR1, handler1); 13 14 printf("%d", counter); 15 fflush(stdout); 16 17 if ((pid = fork()) == 0) { 18 while(1) {}; 

19 
} 20 kill(pid, SIGUSR1); 21 waitpid(-1, NULL, 0); 22 counter = counter + 1; 23 printf("%d", counter); 24 exit(0); 


25 } 
code/ecf/signalprob0.c 
code/ecf/signal3.c 

1  #include "csapp.h"  
2  
3  void handler2(int sig)  
4  {  
pid_t pid;  
6  
7  while ((pid = waitpid(-1, NULL, 0)) > 0)  
8  printf("Handler reaped child %d\n", (int)pid);  
9  if (errno != ECHILD)  
unix_error("waitpid error");  
11  Sleep(2);  
12  return;  
13  }  
14  
int  main() {  
16  int i, n;  
17  char buf[MAXBUF];  
18  pid_t pid;  
19  
if (signal(SIGCHLD, handler2) == SIG_ERR)  
21  unix_error("signal error");  
22  
23  /* Parent creates children */  
24  for (i=0;i<3; i++) {  
pid = Fork();  
26  if (pid == 0) {  
27  printf("Hello from child %d\n", (int)getpid());  
28  Sleep(1);  
29  exit(0);  
}  
31  }  
32  
33  /* Manually restart the read call if it is interrupted */  
34  while ((n = read(STDIN_FILENO, buf, sizeof(buf))) <  0)  
if (errno != EINTR)  
36  unix_error("read error");  
37  
38  printf("Parent processing input\n");  
39  while (1)  
;  
41  
42  exit(0);  
43  }  
Figure 8.33 signal3: An improved version of Figure 8.32 that correctly accounts for the fact that system calls can be interrupted. 


code/ecf/signal3.c 

8.5.5 Portable Signal Handling 
The differences in signal handling semantics from system to system¡ªsuch as whether or not an interrupted slow system call is restarted or aborted pre-maturely¡ªis an ugly aspect of Unix signal handling. To deal with this problem, the Posix standard de.nes the sigaction function, which allows users on Posix-compliant systems such as Linux and Solaris to clearly specify the signal handling semantics they want. 
#include <signal.h> 
int sigaction(int signum, struct sigaction *act, struct sigaction *oldact); 
Returns: 0 if OK, .1 on error 
The sigaction function is unwieldy because it requires the user to set the entries of a structure. A cleaner approach, originally proposed by W. Richard Stevens [109], is to de.ne a wrapper function, called Signal, that calls sigaction for us. Figure 8.34 shows the de.nition of Signal, which is invoked in the same way as the signal function. The Signal wrapper installs a signal handler with the following signal handling semantics: 
. Only signals of the type currently being processed by the handler are blocked. . As with all signal implementations, signals are not queued. . Interrupted system calls are automatically restarted whenever possible. 

code/src/csapp.c  
1  handler_t  *Signal(int  signum,  handler_t  *handler)  
2  {  
3  struct  sigaction  action,  old_action;  
4  
5  action.sa_handler  =  handler;  
6  sigemptyset(&action.sa_mask);  /*  Block  sigs  of  type  being  handled  */  
7  action.sa_flags  =  SA_RESTART;  /*  Restart  syscal ls  if  possible  */  
8  
9  if  (sigaction(signum,  &action,  &old_action)  <  0)  
10  unix_error("Signal  error");  
11  return  (old_action.sa_handler);  
12  }  
code/src/csapp.c  
Figure 8.34 Signal: A wrapper for sigaction that provides portable signal handling on Posix-compliant systems. 


. Once the signal handler is installed, it remains installed until Signal is called with a handler argument of either SIG_IGN or SIG_DFL. (Some older Unix systems restore the signal action to its default action after a signal has been processed by a handler.) 
Figure 8.35 shows a version of the signal2 program from Figure 8.32 that uses our Signal wrapper to get predictable signal handling semantics on different computer systems. The only difference is that we have installed the handler with a call to Signal rather than a call to signal. The program now runs correctly on both our Solaris and Linux systems, and we no longer need to manually restart interrupted read system calls. 
8.5.6 Explicitly Blocking and Unblocking Signals 
Applications can explicitly block and unblock selected signals using the sigproc-mask function: 
#include <signal.h> 
int sigprocmask(int how, const sigset_t *set, sigset_t *oldset); int sigemptyset(sigset_t *set); int sigfillset(sigset_t *set); int sigaddset(sigset_t *set, int signum); int sigdelset(sigset_t *set, int signum); 
Returns: 0 if OK, .1 on error 

int sigismember(const sigset_t *set, int signum); 
Returns: 1 if member, 0 if not, .1 on error 

The sigprocmask function changes the set of currently blocked signals (the blocked bit vector described in Section 8.5.1). The speci.c behavior depends on the value of how: 
. SIG_BLOCK: Add the signals in set to blocked (blocked = blocked | set). . SIG_UNBLOCK: Remove the signals in set from blocked (blocked = blocked & ~set). . SIG_SETMASK: blocked = set. 
If oldset is non-NULL, the previous value of the blocked bit vector is stored in oldset. 
Signal sets such as set are manipulated using the following functions. The sigemptyset initializes set to the empty set. The sigfillset function adds every signal to set.The sigaddset function adds signum to set, sigdelset deletes signum from set, and sigismember returns 1 if signum is a member of set, and 0 if not. 
code/ecf/signal4.c 

1 #include "csapp.h" 2 3 void handler2(int sig) 4 { 
pid_t pid; 
6 7 while ((pid = waitpid(-1, NULL, 0)) > 0) 8 printf("Handler reaped child %d\n", (int)pid); 9 if (errno != ECHILD) 
unix_error("waitpid error"); 11 Sleep(2); 12 return; 
13 } 14 

int main() 16 { 17 int i, n; 18 char buf[MAXBUF]; 19 pid_t pid; 
21  Signal(SIGCHLD, handler2); /* sigaction  error-handling wrapper */  
22  
23  /* Parent creates children */  
24  for (i=0;i<3; i++) {  
pid = Fork();  
26  if (pid == 0) {  
27  printf("Hello from child  %d\n",  (int)ge tp id());  
28  Sleep(1);  
29  exit(0);  
}  
31  }  
32  
33  /* Parent waits for terminal input and then  proce sses it */  
34  if ((n = read(STDIN_FILENO, buf,  sizeof (buf)))  <  0)  
unix_error("read error");  
36  
37  printf("Parent processing input\n");  
38  while (1)  
39  ;  
exit(0);  
41 }  
code/ecf/signal4.c  
Figure 8.35 signal4: A version of Figure 8.32 that uses our Signal wrapper to get portable signal handling semantics. 


8.5.7 Synchronizing Flows to Avoid Nasty Concurrency Bugs 
The problem of how to program concurrent .ows that read and write the same storage locations has challenged generations of computer scientists. In general, the number of potential interleavings of the .ows is exponential in the number of instructions. Some of those interleavings will produce correct answers, and others will not. The fundamental problem is to somehow synchronize the concurrent .ows so as to allow the largest set of feasible interleavings such that each of the feasible interleavings produces a correct answer. 
Concurrent programming is a deep and important problem that we will discuss in more detail in Chapter 12. However, we can use what you¡¯ve learned about exceptional control .ow in this chapter to give you a sense of the interesting intellectual challenges associated with concurrency. For example, consider the program in Figure 8.36, which captures the structure of a typical Unix shell. The parent keeps track of its current children using entries in a job list, with one entry per job. The addjob and deletejob functions add and remove entries from the job list, respectively. 
After the parent creates a new child process, it adds the child to the job list. When the parent reaps a terminated (zombie) child in the SIGCHLD signal handler, it deletes the child from the job list. At .rst glance, this code appears to be correct. Unfortunately, the following sequence of events is possible: 
1. 
The parent executes the fork function and the kernel schedules the newly created child to run instead of the parent. 

2. 
Before the parent is able to run again, the child terminates and becomes a zombie, causing the kernel to deliver a SIGCHLD signal to the parent. 

3. 
Later, when the parent becomes runnable again but before it is executed, the kernel notices the pending SIGCHLD and causes it to be received by running the signal handler in the parent. 

4. 
The signal handler reaps the terminated child and calls deletejob, which does nothing because the parent has not added the child to the list yet. 

5. 
After the handler completes, the kernel then runs the parent, which returns from fork and incorrectly adds the (nonexistent) child to the job list by calling addjob. 


Thus, for some interleavings of the parent¡¯s main routine and signal handling .ows, it is possible for deletejob to be called before addjob. This results in an incorrect entry on the job list, for a job that no longer exists and that will never be removed. On the other hand, there are also interleavings where events occur in the correct order. For example, if the kernel happens to schedule the parent to run when the fork call returns instead of the child, then the parent will correctly add the child to the job list before the child terminates and the signal handler removes the job from the list. 
This is an example of a classic synchronization error known as a race. In this case, the race is between the call to addjob in the main routine and the call to deletejob in the handler. If addjob wins the race, then the answer is correct. If 
code/ecf/procmask1.c  
1  void  handler(int  sig)  
2  {  
3  pid_t  pid;  
4  while  ((pid  =  waitpid(-1,  NULL,  0))  >  0)  /*  Reap  a  zombie  child  */  
5  deletejob(pid);  /*  Delete  the  child  from  the  job  list  */  
6  if  (errno  !=  ECHILD)  
7  unix_error("waitpid  error");  
8  }  
9  
10  int  main(int  argc,  char  **argv)  
11  {  
12  int pid;  
13  
14  Signal(SIGCHLD,  handler);  
15  initjobs();  /*  Initialize  the  job  list  */  
16  
17  while  (1)  {  
18  /*  Child  process  */  
19  if  ((pid  =  Fork())  ==  0)  {  
20  Execve("/bin/date",  argv,  NULL);  
21  }  
22  
23  /*  Parent  process  */  
24  addjob(pid);  /*  Add  the  child  to  the  job  list  */  
25  }  
26  exit(0);  
27  }  
code/ecf/procmask1.c  
Figure 8.36  A shell program with a subtle synchronization error. If the child  
terminates before the parent is able to run, then addjob and deletejob will be called  
in the wrong order.  
not, the answer is incorrect. Such errors are enormously dif.cult to debug because  
it is often impossible to test every interleaving. You may run the code a billion  
times without a problem, but then the next test results in an interleaving that  
triggers the race.  
Figure 8.37 shows one way to eliminate the race in Figure 8.36. By blocking  
SIGCHLD signals before the call to fork and then unblocking them only after we  
have called addjob, we guarantee that the child will be reaped after it is added to  
the job list. Notice that children inherit the blocked set of their parents, so we must  
be careful to unblock the SIGCHLD signal in the child before calling execve.  

code/ecf/procmask2.c 

1 void handler(int sig) 2 { 3 pid_t pid; 4 while ((pid = waitpid(-1, NULL, 0)) > 0) /* Reap a zombie child */ 
deletejob(pid); /* Delete the child from the job list */ 6 if (errno != ECHILD) 7 unix_error("waitpid error"); 
8 } 9 

int main(int argc, char **argv) 11 { 12 int pid; 13 sigset_t mask; 14 
Signal(SIGCHLD, handler); 16 initjobs(); /* Initialize the job list */ 17 18 while (1) { 19 Sigemptyset(&mask); 
Sigaddset(&mask, SIGCHLD); 21 Sigprocmask(SIG_BLOCK, &mask, NULL); /* Block SIGCHLD */ 22 23 /* Child process */ 24 if ((pid = Fork()) == 0) { 
Sigprocmask(SIG_UNBLOCK, &mask, NULL); /* Unblock SIGCHLD */ 26 Execve("/bin/date", argv, NULL); 
27 
} 28 29 /* Parent process */ 

addjob(pid); /* Add the child to the job list */ 31 Sigprocmask(SIG_UNBLOCK, &mask, NULL); /* Unblock SIGCHLD */ 

32 
} 33 exit(0); 



34 } 
code/ecf/procmask2.c 

Figure 8.37 Using sigprocmask to synchronize processes. In this example, the parent ensures that addjob executes before the corresponding deletejob. 
1  #include  <stdio.h>  
2  #include  <stdlib.h>  
3  #include  <unistd.h>  
4  #include  <sys/time.h>  
#include  <sys/types.h>  
6  
7  /*  Sleep  for  a  random  period  between  [0,  MAX_SLEEP]  us.  */  
8  #define  MAX_SLEEP  100000  
9  
/*  Macro  that  maps  val  into  the  range  [0,  RAND_MAX]  */  
11  #define  CONVERT(val)  (((double)val)/(double)RAND_MAX)  
12  
13  pid_t  Fork(void)  
14  {  
static  struct  timeval  time;  
16  unsigned  bool,  secs;  
17  pid_t  pid;  
18  
19  /*  Generate  a  different  seed  each  time  the  function  is  called  */  
gettimeofday(&time,  NULL);  
21  srand(time.tv_usec);  
22  
23  /*  Determine  whether  to  sleep  in  parent  of  child  and  for  how  long  */  
24  bool  =  (unsigned)(CONVERT(rand())  + 0.5);  
secs  =  (unsigned)(CONVERT(rand())  *  MAX_SLEEP);  
26  
27  /*  Call  the  real  fork  function  */  
28  if  ((pid  =  fork())  <  0)  
29  return  pid;  
31  /*  Randomly  decide  to  sleep  in  the  parent  or  the  child  */  
32  if  (pid  ==  0)  {  /*  Child  */  
33  if(bool)  {  
34  usleep(secs);  
}  
36  }  
37  else { /* Parent */  
38  if (!bool) {  
39  usleep(secs);  
}  
41  }  
42  
43  /*  Return  the  PID  like  a  normal  fork  call  */  
44  return  pid;  
}  
code/ecf/rfork.c  
Figure 8.38 A wrapper for fork that randomly determines the order in which the parent and child execute. The parent and child .ip a coin to determine which will sleep, thus giving the other process a chance to be scheduled. 


Aside A handy trick for exposing races in your code 
Races such as those in Figure 8.36 are dif.cult to detect because they depend on kernel-speci.c scheduling decisions. After a call to fork, some kernels schedule the child to run .rst, while other kernels schedule the parent to run .rst. If you were to run the code in Figure 8.36 on one of the latter systems, it would never fail, no matter how many times you tested it. But as soon as you ran it on one of the former systems, then the race would be exposed and the code would fail. Figure 8.38 shows a wrapper function that can help expose such hidden assumptions about the execution ordering of parent and child processes. The basic idea is that after each call to fork, the parent and child .ip a coin to determine which of them will sleep for a bit, thus giving the other process the opportunity to run .rst. If we were to run the code multiple times, then with high probability we would exercise both orderings of child and parent executions, regardless of the particular kernel¡¯s scheduling policy. 
8.6 
Nonlocal 
Jumps 

C provides a form of user-level exceptional control .ow, called a nonlocal jump, that transfers control directly from one function to another currently executing function without having to go through the normal call-and-return sequence. Non-local jumps are provided by the setjmp and longjmp functions. 
#include <setjmp.h> 
int setjmp(jmp_buf env); int sigsetjmp(sigjmp_buf env, int savesigs); 
Returns: 0 from setjmp, nonzero from longjmps 

The setjmp function saves the current calling environment in the env buffer, for later use by longjmp, and returns a 0. The calling environment includes the program counter, stack pointer, and general purpose registers. 
#include <setjmp.h> 
void longjmp(jmp_buf env, int retval); void siglongjmp(sigjmp_buf env, int retval); 
Never returns 

The longjmp function restores the calling environment from the env buffer and then triggers a return from the most recent setjmp call that initialized env. The setjmp then returns with the nonzero return value retval. 
The interactions between setjmp and longjmp can be confusing at .rst glance. The setjmp function is called once, but returns multiple times: once when the setjmp is .rst called and the calling environment is stored in the env buffer, and once for each corresponding longjmp call. On the other hand, the longjmp function is called once, but never returns. 
An important application of nonlocal jumps is to permit an immediate return from a deeply nested function call, usually as a result of detecting some error condition. If an error condition is detected deep in a nested function call, we can use a nonlocal jump to return directly to a common localized error handler instead of laboriously unwinding the call stack. 
Figure 8.39 shows an example of how this might work. The main routine .rst calls setjmp to save the current calling environment, and then calls function foo, which in turn calls function bar.If foo or bar encounter an error, they return immediately from the setjmp via a longjmp call. The nonzero return value of the setjmp indicates the error type, which can then be decoded and handled in one place in the code. 
Another important application of nonlocal jumps is to branch out of a signal handler to a speci.c code location, rather than returning to the instruction that was interrupted by the arrival of the signal. Figure 8.40 shows a simple program that illustrates this basic technique. The program uses signals and nonlocal jumps to do a soft restart whenever the user types ctrl-c at the keyboard. The sigsetjmp and siglongjmp functions are versions of setjmp and longjmp that can be used by signal handlers. 
The initial call to the sigsetjmp function saves the calling environment and signal context (including the pending and blocked signal vectors) when the pro-gram .rst starts. The main routine then enters an in.nite processing loop. When the user types ctrl-c, the shell sends a SIGINT signal to the process, which catches it. Instead of returning from the signal handler, which would pass control back to the interrupted processing loop, the handler performs a nonlocal jump back to the beginning of the main program. When we ran the program on our system, we got the following output: 
unix> ./restart starting processing... processing... restarting User hits ctrl-c processing... restarting User hits ctrl-c processing... 

Aside Software exceptions in C++ and Java 
The exception mechanisms provided by C++ and Java are higher-level, more-structured versions of the C setjmp and longjmp functions. You can think of a catch clause inside a try statement as being akin to a setjmp function. Similarly, a throw statement is similar to a longjmp function. 
Section 8.6  Nonlocal Jumps  761  
code/ecf/setjmp.c  
1  #include "csapp.h"  
2  
3  jmp_buf buf;  
4  
5  int error1 = 0;  
6  int error2 = 1;  
7  
8  void foo(void), bar(void);  
9  
10  int main()  
11  {  
12  int rc;  
13  
14  rc = setjmp(buf);  
15  if (rc == 0)  
16  foo();  
17  else if (rc == 1)  
18  printf("Detected an error1 condition in foo\n");  
19  else if (rc == 2)  
20  printf("Detected an error2 condition in foo\n");  
21  else  
22  printf("Unknown error condition in foo\n");  
23  exit(0);  
24  }  
25  
26  /* Deeply nested function foo */  
27  void foo(void)  
28  {  
29  if (error1)  
30  longjmp(buf, 1);  
31  bar();  
32  }  
33  
34  void bar(void)  
35  {  
36  if (error2)  
37  longjmp(buf, 2);  
38  }  
code/ecf/setjmp.c  
Figure 8.39 Nonlocal jump example. This example shows the framework for using nonlocal jumps to recover from error conditions in deeply nested functions without having to unwind the entire stack. 


code/ecf/restart.c  
1  #include "csapp.h"  
2  
3  sigjmp_buf buf;  
4  
5  void handler(int sig)  
6  {  
7  siglongjmp(buf, 1);  
8  }  
9  
10  int main()  
11  {  
12  Signal(SIGINT, handler);  
13  
14  if (!sigsetjmp(buf,  1))  
15  printf("starting\n");  
16  else  
17  printf("restarting\n");  
18  
19  while(1) {  
20  Sleep(1);  
21  printf("processing...\n");  
22  }  
23  exit(0);  
24  }  
code/ecf/restart.c  
Figure 8.40 A program that uses nonlocal jumps to restart itself when the user types ctrl-c. 


8.7 
Tools 
for 
Manipulating 
Processes 

Linux systems provide a number of useful tools for monitoring and manipulating processes: 
strace: Prints a trace of each system call invoked by a running program and its children. A fascinating tool for the curious student. Compile your program with -static to get a cleaner trace without a lot of output related to shared libraries. 
ps: Lists processes (including zombies) currently in the system. 
top: Prints information about the resource usage of current processes. 
pmap: Displays the memory map of a process. 
/proc: A virtual .lesystem that exports the contents of numerous kernel data 
structures in an ASCII text form that can be read by user programs. For example, type ¡°cat /proc/loadavg¡± to see the current load average on your Linux system. 

8.8 
Summary 


Exceptional control .ow (ECF) occurs at all levels of a computer system and is a basic mechanism for providing concurrency in a computer system. 
At the hardware level, exceptions are abrupt changes in the control .ow that are triggered by events in the processor. The control .ow passes to a software handler, which does some processing and then returns control to the interrupted control .ow. 
There are four different types of exceptions: interrupts, faults, aborts, and traps. Interrupts occur asynchronously (with respect to any instructions) when an external I/O device such as a timer chip or a disk controller sets the interrupt pin on the processor chip. Control returns to the instruction following the faulting instruction. Faults and aborts occur synchronously as the result of the execution of an instruction. Fault handlers restart the faulting instruction, while abort han-dlers never return control to the interrupted .ow. Finally, traps are like function calls that are used to implement the system calls that provide applications with controlled entry points into the operating system code. 
At the operating system level, the kernel uses ECF to provide the funda-mental notion of a process. A process provides applications with two important abstractions: (1) logical control .ows that give each program the illusion that it has exclusive use of the processor, and (2) private address spaces that provide the illusion that each program has exclusive use of the main memory. 
At the interface between the operating system and applications, applications can create child processes, wait for their child processes to stop or terminate, run new programs, and catch signals from other processes. The semantics of signal handling is subtle and can vary from system to system. However, mechanisms exist on Posix-compliant systems that allow programs to clearly specify the expected signal handling semantics. 
Finally, at the application level, C programs can use nonlocal jumps to bypass the normal call/return stack discipline and branch directly from one function to another. 
Bibliographic 
Notes 

The Intel macroarchitecture speci.cation contains a detailed discussion of excep-tions and interrupts on Intel processors [27]. Operating systems texts [98, 104, 112] contain additional information on exceptions, processes, and signals. The classic work by W. Richard Stevens [110] is a valuable and highly readable description of how to work with processes and signals from application programs. Bovet and Cesati [11] give a wonderfully clear description of the Linux kernel, including de-tails of the process and signal implementations. Blum [9] is an excellent reference for x86 assembly language, and describes in detail the x86 syscall interface. 
Homework 
Problems 

8.9 ¡ô 
Consider four processes with the following starting and ending times: 
Process Start time End time 
A5 7 B2 4 C3 6 D1 8 
For each pair of processes, indicate whether they run concurrently (y) or not (n): 
Process pair Concurrent? AB AC AD BC BD CD 
8.10 ¡ô 
In this chapter, we have introduced some functions with unusual call and return behaviors: setjmp, longjmp, execve, and fork. Match each function with one of the following behaviors: 
A. Called once, returns twice. 
B. Called once, never returns. 
C. Called once, returns one or more times. 
8.11 ¡ô 
How many ¡°hello¡± output lines does this program print? 
code/ecf/forkprob1.c 
1 #include "csapp.h" 2 3 int main() 4 { 5 int i; 6 7 for (i=0;i<2; i++) 8 Fork(); 9 printf("hello\n"); 
10 exit(0); 
11 } 
code/ecf/forkprob1.c 
8.12 ¡ô 

How many ¡°hello¡± output lines does this program print? 
code/ecf/forkprob4.c 

1 #include "csapp.h" 2 3 void doit() 4 { 5 Fork(); 6 Fork(); 7 printf("hello\n"); 8 return; 
9 } 10 11 int main() 12 { 13 doit(); 14 printf("hello\n"); 15 exit(0); 
16 } 
code/ecf/forkprob4.c 
8.13 ¡ô 

What is one possible output of the following program? 
code/ecf/forkprob3.c 

1 #include "csapp.h" 2 3 int main() 4 { 5 intx=3; 6 7 if (Fork() != 0) 8 printf("x=%d\n", ++x); 9 
10 printf("x=%d\n", --x); 
11 exit(0); 
12 } 
code/ecf/forkprob3.c 
8.14 ¡ô 

How many ¡°hello¡± output lines does this program print? 
code/ecf/forkprob5.c 

1 #include "csapp.h" 2 3 void doit() 
766  Chapter 8  Exceptional Control Flow  
4 {  
5 if (Fork() == 0) {  
6 Fork();  
7 printf("hello\n");  
8 exit(0);  
9 }  
10 return;  
11 }  
12  
13 int main()  
14 {  
15 doit();  
16 printf("hello\n");  
17 exit(0);  
18 }  
code/ecf/forkprob5.c  
8.15 ¡ô  
How many ¡°hello¡± lines does this program print?  
code/ecf/forkprob6.c  
1 #include "csapp.h"  
2  
3 void doit()  
4 {  
5 if (Fork() == 0) {  
6 Fork();  
7 printf("hello\n");  
8 return;  
9 }  
10 return;  
11 }  
12  
13 int main()  
14 {  
15 doit();  
16 printf("hello\n");  
17 exit(0);  
18 }  
code/ecf/forkprob6.c  
8.16 ¡ô  
What is the output of the following program?  
code/ecf/forkprob7.c  
1 #include "csapp.h"  
2 int counter = 1;  

3 4 int main() 5 { 6 if (fork() == 0) { 7 counter--; 8 exit(0); 
9 
} 10 else { 11 Wait(NULL); 12 printf("counter = %d\n", ++counter); 

13 
} 14 exit(0); 



15 } 
code/ecf/forkprob7.c 
8.17 ¡ô 

Enumerate all of the possible outputs of the program in Problem 8.4. 
8.18 ¡ô¡ô 

Consider the following program: 
code/ecf/forkprob2.c 

1 #include "csapp.h" 2 3 void end(void) 4 { 5 printf("2"); 
6 } 7 8 int main() 9 { 

10 if (Fork() == 0) 11 atexit(end); 12 if (Fork() == 0) 13 printf("0"); 14 else 15 printf("1"); 16 exit(0); 
17 } 
code/ecf/forkprob2.c 

Determine which of the following outputs are possible. Note: The atexit function takes a pointer to a function and adds it to a list of functions (initially empty) that will be called when the exit function is called. 
A. 112002 
B. 211020 

C. 102120 D. 122001 E. 100212 
8.19 ¡ô¡ô 
How many lines of output does the following function print? Give your answer as a function of n. Assume n ¡Ý 1. 
code/ecf/forkprob8.c 
1 void foo(int n) 2 { 3 int i; 4 5 for (i=0;i<n; i++) 6 Fork(); 7 printf("hello\n"); 8 exit(0); 
9 } 
code/ecf/forkprob8.c 
8.20 ¡ô¡ô 
Use execve to write a program called myls whose behavior is identical to the /bin/ls program. Your program should accept the same command line argu-ments, interpret the identical environment variables, and produce the identical output. 
The ls program gets the width of the screen from the COLUMNS environ-ment variable. If COLUMNS is unset, then ls assumes that the screen is 80 columns wide. Thus, you can check your handling of the environment variables by setting the COLUMNS environment to something smaller than 80: 
unix> setenv COLUMNS 40 unix> ./myls 
...output is 40 columns wide unix> unsetenv COLUMNS unix> ./myls 
...output is now 80 columns wide 
8.21 ¡ô¡ô 
What are the possible output sequences from the following program? 
code/ecf/waitprob3.c 
1 int main() 2 { 3 if (fork() == 0) { 4 printf("a"); 5 exit(0); 
6 } 
7 else { 

8 printf("b"); 
9 waitpid(-1, NULL, 0); 
10 } 11 printf("c"); 12 exit(0); 
13 } 
code/ecf/waitprob3.c 
8.22 ¡ô¡ô¡ô 

Write your own version of the Unix system function 
int mysystem(char *command); 
The mysystem function executes command by calling ¡°/bin/sh -c command¡±, and then returns after command has completed. If command exits normally (by calling the exit function or executing a return statement), then mysystem returns the command exit status. For example, if command terminates by calling exit(8), then system returns the value 8. Otherwise, if command terminates abnormally, then mysystem returns the status returned by the shell. 
8.23 ¡ô¡ô 

One of your colleagues is thinking of using signals to allow a parent process to count events that occur in a child process. The idea is to notify the parent each time an event occurs by sending it a signal, and letting the parent¡¯s signal handler increment a global counter variable, which the parent can then inspect after the child has terminated. However, when he runs the test program in Figure 8.41 on his system, he discovers that when the parent calls printf, counter always has a value of 2, even though the child has sent .ve signals to the parent. Perplexed, he comes to you for help. Can you explain the bug? 
8.24 ¡ô¡ô¡ô 

Modify the program in Figure 8.17 so that the following two conditions are met: 
1. 
Each child terminates abnormally after attempting to write to a location in the read-only text segment. 

2. 
The parent prints output that is identical (except for the PIDs) to the follow-ing: 


child 12255 terminated by signal 11: Segmentation fault child 12254 terminated by signal 11: Segmentation fault 
Hint: Read the man page for psignal(3). 
8.25 ¡ô¡ô¡ô 

Write a version of the fgets function, called tfgets, that times out after 5 seconds. The tfgets function accepts the same inputs as fgets. If the user doesn¡¯t type an input line within 5 seconds, tfgets returns NULL. Otherwise, it returns a pointer to the input line. 
code/ecf/counterprob.c  
1  #include  "csapp.h"  
2  
3  int  counter  =  0;  
4  
5  void  handler(int  sig)  
6  {  
7  counter++;  
8  sleep(1); /* Do some work in the handler  */  
9  return;  
10  }  
11  
12  int  main()  
13  {  
14  int i;  
15  
16  Signal(SIGUSR2,  handler);  
17  
18  if  (Fork()  ==  0)  {  /*  Child  */  
19  for  (i=0;i<5;  i++)  {  
20  Kill(getppid(),  SIGUSR2);  
21  printf("sent  SIGUSR2  to  parent\n");  
22  }  
23  exit(0);  
24  }  
25  
26  Wait(NULL);  
27  printf("counter=%d\n", counter);  
28  exit(0);  
29  }  
code/ecf/counterprob.c  
Figure 8.41 Counter program referenced in Problem 8.23. 


8.26 ¡ô¡ô¡ô¡ô 
Using the example in Figure 8.22 as a starting point, write a shell program that supports job control. Your shell should have the following features: 
. The command line typed by the user consists of a name and zero or more argu-ments, all separated by one or more spaces. If name is a built-in command, the shell handles it immediately and waits for the next command line. Otherwise, the shell assumes that name is an executable .le, which it loads and runs in the context of an initial child process (job). The process group ID for the job is identical to the PID of the child. 
. Each job is identi.ed by either a process ID (PID) or a job ID (JID), which is a small arbitrary positive integer assigned by the shell. JIDs are denoted on 

the command line by the pre.x ¡®%¡¯. For example, ¡°%5¡± denotes JID 5, and ¡°5¡± denotes PID 5. 
. If the command line ends with an ampersand, then the shell runs the job in the background. Otherwise, the shell runs the job in the foreground. . Typing ctrl-c (ctrl-z) causes the shell to send a SIGINT (SIGTSTP) signal to every process in the foreground process group. . The jobs built-in command lists all background jobs. 
. The bg <job> built-in command restarts <job> by sending it a SIGCONT signal, and then runs it in the background. The <job> argument can be either a PID or a JID. 
. The fg <job> built-in command restarts <job> by sending it a SIGCONT signal, and then runs it in the foreground. 
. The shell reaps all of its zombie children. If any job terminates because it receives a signal that was not caught, then the shell prints a message to the terminal with the job¡¯s PID and a description of the offending signal. 
Figure 8.42 shows an example shell session. 
Solutions 
to 
Practice 
Problems 

Solution to Problem 8.1 (page 714) 
Processes A and B are concurrent with respect to each other, as are B and C, because their respective executions overlap, that is, one process starts before the other .nishes. Processes A and C are not concurrent, because their executions do not overlap; A .nishes before C begins. 
Solution to Problem 8.2 (page 723) 
In our example program in Figure 8.15, the parent and child execute disjoint sets of instructions. However, in this program, the parent and child execute non-disjoint sets of instructions, which is possible because the parent and child have identical code segments. This can be a dif.cult conceptual hurdle, so be sure you understand the solution to this problem. 
A. The key idea here is that the child executes both printf statements. After the fork returns, it executes the printf in line 8. Then it falls out of the if statement and executes the printf in line 9. Here is the output produced by the child: 
printf1: x=2 printf2: x=1 

B. The parent executes only the printf in line 9: 
printf2: x=0 

unix> ./shell Run your shell program > bogus bogus: Command not found. Execve can¡¯t find executable > foo 10 Job 5035 terminated by signal: Interrupt User types ctrl-c > foo 100 & [1] 5036 foo 100 & > foo 200 & [2] 5037 foo 200 & > jobs 
[1] 5036 Running foo 100 & 
[2] 5037 Running foo 200 & > fg %1 Job [1] 5036 stopped by signal: Stopped User types ctrl-z > jobs 
[1] 5036 Stopped foo 100 & 
[2] 5037 Running foo 200 & > bg 5035 5035: No such process > bg 5036 [1] 5036 foo 100 & > /bin/kill 5036 Job 5036 terminated by signal: Terminated >fg %2 Wait for fg job to finish. > quit unix> Back to the Unix shell 
Figure 8.42 Sample shell session for Problem 8.26. 
Solution to Problem 8.3 (page 726) 
The parent prints b and then c. The child prints a and then c. It¡¯s very important to realize that you cannot make any assumption about how the execution of the parent and child are interleaved. Thus, any topological sort of b ¡ú c and a ¡ú c is a possible output sequence. There are four such sequences: acbc, bcac, abcc, and bacc. 
Solution to Problem 8.4 (page 729) 
A. Each time we run this program, it generates six output lines. 
B. The ordering of the output lines will vary from system to system, depending on the how the kernel interleaves the instructions of the parent and the child. In general, any topological sort of the following graph is a valid ordering: 
--> ¡®¡®0¡¯¡¯ --> ¡®¡®2¡¯¡¯ --> ¡®¡®Bye¡¯¡¯ Parent process / ¡®¡®Hello¡¯¡¯ \ --> ¡®¡®1¡¯¡¯ --> ¡®¡®Bye¡¯¡¯ Child process 

For example, when we run the program on our system, we get the following output: 
unix> ./waitprob1 Hello 0 1 Bye 2 Bye 
In this case, the parent runs .rst, printing ¡°Hello¡± in line 6 and ¡°0¡± in line 8. The call to wait blocks because the child has not yet terminated, so the kernel does a context switch and passes control to the child, which prints ¡°1¡± in line 8 and ¡°Bye¡± in line 15, and then terminates with an exit status of 2 in line 16. After the child terminates, the parent resumes, printing the child¡¯s exit status in line 12 and ¡°Bye¡± in line 15. 
Solution to Problem 8.5 (page 730) 
code/ecf/snooze.c 

1 unsigned int snooze(unsigned int secs) { 2 unsigned int rc = sleep(secs); 3 printf("Slept for %u of %u secs.\n", secs -rc, secs); 4 return rc; 
5 } 
code/ecf/snooze.c 

Solution to Problem 8.6 (page 733) 
code/ecf/myecho.c 

1 #include "csapp.h" 2 3 int main(int argc, char *argv[], char *envp[]) 
code/ecf/myecho.c 

4 { 5 int i; 6 7 printf("Command line arguments:\n"); 8 for (i=0; argv[i] != NULL; i++) 9 printf(" argv[%2d]: %s\n", i, 
10 11 printf("\n"); 12 printf("Environment variables:\n"); 13 for (i=0; envp[i] != NULL; i++) 14 printf(" envp[%2d]: %s\n", i, 15 16 exit(0); 
17 } 
argv[i]); 
envp[i]); 

774  Chapter 8  Exceptional Control Flow  
Solution to Problem 8.7 (page 744)  
The sleep function returns prematurely whenever the sleeping process receives a  
signal that is not ignored. But since the default action upon receipt of a SIGINT is  
to terminate the process (Figure 8.25), we must install a SIGINT handler to allow  
the sleep function to return. The handler simply catches the SIGNAL and returns  
control to the sleep function, which returns immediately.  
code/ecf/snooze.c  
1  #include "csapp.h"  
2  
3  /* SIGINT handler */  
4  void handler(int sig)  
5  {  
6  return; /* Catch the signal and return */  
7  }  
8  
9  unsigned int snooze(unsigned int secs) {  
10  unsigned int rc = sleep(secs);  
11  printf("Slept for %u of %u secs.\n", secs -rc, secs);  
12  return rc;  
13  }  
14  
15  int main(int argc, char **argv) {  
16  
17  if (argc != 2) {  
18  fprintf(stderr, "usage: %s <secs>\n", argv[0]);  
19  exit(0);  
20  }  
21  
22  if (signal(SIGINT, handler) == SIG_ERR) /* Install SIGINT handler */  
23  unix_error("signal error\n");  
24  (void)snooze(atoi(argv[1]));  
25  exit(0);  
26  }  
code/ecf/snooze.c  
Solution to Problem 8.8 (page 750)  
This program prints the string ¡°213¡±, which is the shorthand name of the CS:APP  
course at Carnegie Mellon. The parent starts by printing ¡°2¡±, then forks the child,  
which spins in an in.nite loop. The parent then sends a signal to the child, and  
waits for it to terminate. The child catches the signal (interrupting the in.nite  
loop), decrements the counter (from an initial value of 2), prints ¡°1¡±, and then  
terminates. After the parent reaps the child, it increments the counter (from an  
initial value of 2), prints ¡°3¡±, and terminates.  

CHAPTER 
9 
Virtual 
Memory 
9.1 
Physical 
and 
Virtual 
Addressing 
777 
9.2 
Address 
Spaces 
778 
9.3 
VM 
as 
a 
Tool 
for 
Caching 
779 
9.4 
VM 
as 
a 
Tool 
for 
Memory 
Management 
785 
9.5 
VM 
as 
a 
Tool 
for 
Memory 
Protection 
786 
9.6 
Address 
Translation 
787 
9.7 
Case 
Study: 
The 
Intel 
Core 
i7/Linux 
Memory 
System 
799 
9.8 
Memory 
Mapping 
807 
9.9 
Dynamic 
Memory 
Allocation 
812 
9.10 
Garbage 
Collection 
838 
9.11 
Common 
Memory-Related 
Bugs 
in 
C 
Programs 
843 
9.12 
Summary 
848 
Bibliographic 
Notes 
848 
Homework 
Problems 
849 
Solutions 
to 
Practice 
Problems 
853 
Processes in a system share the CPU and main memory with other processes. However, sharing the main memory poses some special challenges. As demand on the CPU increases, processes slow down in some reasonably smooth way. But if too many processes need too much memory, then some of them will simply not be able to run. When a program is out of space, it is out of luck. Memory is also vulnerable to corruption. If some process inadvertently writes to the memory used by another process, that process might fail in some bewildering fashion totally unrelated to the program logic. 
In order to manage memory more ef.ciently and with fewer errors, modern systems provide an abstraction of main memory known as virtual memory (VM). Virtual memory is an elegant interaction of hardware exceptions, hardware ad-dress translation, main memory, disk .les, and kernel software that provides each process with a large, uniform, and private address space. With one clean mech-anism, virtual memory provides three important capabilities. (1) It uses main memory ef.ciently by treating it as a cache for an address space stored on disk, keeping only the active areas in main memory, and transferring data back and forth between disk and memory as needed. (2) It simpli.es memory management by providing each process with a uniform address space. (3) It protects the address space of each process from corruption by other processes. 
Virtual memory is one of the great ideas in computer systems. A major reason for its success is that it works silently and automatically, without any intervention from the application programmer. Since virtual memory works so well behind the scenes, why would a programmer need to understand it? There are several reasons. 
. Virtual memory is central. Virtual memory pervades all levels of computer systems, playing key roles in the design of hardware exceptions, assemblers, linkers, loaders, shared objects, .les, and processes. Understanding virtual memory will help you better understand how systems work in general. 
. Virtual memory is powerful. Virtual memory gives applications powerful ca-pabilities to create and destroy chunks of memory, map chunks of memory to portions of disk .les, and share memory with other processes. For example, did you know that you can read or modify the contents of a disk .le by reading and writing memory locations? Or that you can load the contents of a .le into memory without doing any explicit copying? Understanding virtual memory will help you harness its powerful capabilities in your applications. 
. Virtual memory is dangerous. Applications interact with virtual memory ev-ery time they reference a variable, dereference a pointer, or make a call to a dynamic allocation package such as malloc. If virtual memory is used improp-erly, applications can suffer from perplexing and insidious memory-related bugs. For example, a program with a bad pointer can crash immediately with a ¡°Segmentation fault¡± or a ¡°Protection fault,¡± run silently for hours before crashing, or scariest of all, run to completion with incorrect results. Under-standing virtual memory, and the allocation packages such as malloc that manage it, can help you avoid these errors. 
This chapter looks at virtual memory from two angles. The .rst half of the chapter describes how virtual memory works. The second half describes how 

virtual memory is used and managed by applications. There is no avoiding the fact that VM is complicated, and the discussion re.ects this in places. The good news is that if you work through the details, you will be able to simulate the virtual memory mechanism of a small system by hand, and the virtual memory idea will be forever demysti.ed. 
The second half builds on this understanding, showing you how to use and manage virtual memory in your programs. You will learn how to manage virtual memory via explicit memory mapping and calls to dynamic storage allocators such as the malloc package. You will also learn about a host of common memory-related errors in C programs and how to avoid them. 
9.1 
Physical 
and 
Virtual 
Addressing 

The main memory of a computer system is organized as an array of M contiguous byte-sized cells. Each byte has a unique physical address (PA). The .rst byte has an address of 0, the next byte an address of 1, the next byte an address of 2, and so on. Given this simple organization, the most natural way for a CPU to access memory would be to use physical addresses. We call this approach physical addressing. Figure 9.1 shows an example of physical addressing in the context of a load instruction that reads the word starting at physical address 4. 
When the CPU executes the load instruction, it generates an effective physical address and passes it to main memory over the memory bus. The main memory fetches the 4-byte word starting at physical address 4 and returns it to the CPU, which stores it in a register. 
Early PCs used physical addressing, and systems such as digital signal pro-cessors, embedded microcontrollers, and Cray supercomputers continue to do so. However, modern processors use a form of addressing known as virtual address-ing, as shown in Figure 9.2. 
With virtual addressing, the CPU accesses main memory by generating a vir-tual address (VA), which is converted to the appropriate physical address before being sent to the memory. The task of converting a virtual address to a physical one is known as address translation. Like exception handling, address translation 
Figure 9.1 Main memory 
A system that uses 0: Physical
physical addressing. 1: 
address 

2: 3: 4: 5: 6: 7: 8: 

M 1: Data word 


. . . 

CPU chip 


Data word 
requires close cooperation between the CPU hardware and the operating sys-tem. Dedicated hardware on the CPU chip called the memory management unit (MMU) translates virtual addresses on the .y, using a look-up table stored in main memory whose contents are managed by the operating system. 
9.2 
Address 
Spaces 

An address space is an ordered set of nonnegative integer addresses 
{0,1,2,...} 
If the integers in the address space are consecutive, then we say that it is a linear address space. To simplify our discussion, we will always assume linear address spaces. In a system with virtual memory, the CPU generates virtual addresses from an address space of N = 2n addresses called the virtual address space: 
{0,1,2,...,N . 1} 
The size of an address space is characterized by the number of bits that are needed to represent the largest address. For example, a virtual address space with N = 2n addresses is called an n-bit address space. Modern systems typically support either 32-bit or 64-bit virtual address spaces. 
A system also has a physical address space that corresponds to the Mbytes of physical memory in the system: 
{0,1,2,...,M. 1} 
M is not required to be a power of two, but to simplify the discussion we will assume that M= 2m . 
The concept of an address space is important because it makes a clean dis-tinction between data objects (bytes) and their attributes (addresses). Once we recognize this distinction, then we can generalize and allow each data object to have multiple independent addresses, each chosen from a different address space. 

Figure 9.3 Virtual memory Physical memory 
0

How a VM system uses VP 0 0 

PP 0 PP 1 

main memory as a cache. VP 1 
p
PP 2m 
1
M 1p
VP 2n 1 
N 1 
Virtual pages (VPs) Physical pages (PPs) stored on disk cached in DRAM 

This is the basic idea of virtual memory. Each byte of main memory has a virtual address chosen from the virtual address space, and a physical address chosen from the physical address space. 
Practice Problem 9.1 
Complete the following table, .lling in the missing entries and replacing each question mark with the appropriate integer. Use the following units: K = 210 (Kilo), M = 220 (Mega), G = 230 (Giga), T = 240 (Tera), P = 250 (Peta), or E = 260 
(Exa).  
No. virtual address bits (n)  No. virtual addresses (N)  Largest possible virtual address  
8  2? = 64K 2? = 256T  232 . 1 =?G . 1  
64  

9.3 
VM 
as 
a 
Tool 
for 
Caching 

Conceptually, a virtual memory is organized as an array of N contiguous byte-sized cells stored on disk. Each byte has a unique virtual address that serves as an index into the array. The contents of the array on disk are cached in main memory. As with any other cache in the memory hierarchy, the data on disk (the lower level) is partitioned into blocks that serve as the transfer units between the disk and the main memory (the upper level). VM systems handle this by partitioning the virtual memory into .xed-sized blocks called virtual pages (VPs). Each virtual page is P = 2p bytes in size. Similarly, physical memory is partitioned into physical pages (PPs), also P bytes in size. (Physical pages are also referred to as page frames.) 
At any point in time, the set of virtual pages is partitioned into three disjoint subsets: 
. Unallocated: Pages that have not yet been allocated (or created) by the VM system. Unallocated blocks do not have any data associated with them, and thus do not occupy any space on disk. 
. Cached: Allocated pages that are currently cached in physical memory. 
. Uncached: Allocated pages that are not cached in physical memory. 
The example in Figure 9.3 shows a small virtual memory with eight virtual pages. Virtual pages 0 and 3 have not been allocated yet, and thus do not yet exist on disk. Virtual pages 1, 4, and 6 are cached in physical memory. Pages 2, 5, and 7 are allocated, but are not currently cached in main memory. 
9.3.1 DRAM Cache Organization 
To help us keep the different caches in the memory hierarchy straight, we will use the term SRAM cache to denote the L1, L2, and L3 cache memories between the CPU and main memory, and the term DRAM cache to denote the VM system¡¯s cache that caches virtual pages in main memory. 
The position of the DRAM cache in the memory hierarchy has a big impact on the way that it is organized. Recall that a DRAM is at least 10 times slower than an SRAM and that disk is about 100,000 times slower than a DRAM. Thus, misses in DRAM caches are very expensive compared to misses in SRAM caches because DRAM cache misses are served from disk, while SRAM cache misses are usually served from DRAM-based main memory. Further, the cost of reading the .rst byte from a disk sector is about 100,000 times slower than reading successive bytes in the sector. The bottom line is that the organization of the DRAM cache is driven entirely by the enormous cost of misses. 
Because of the large miss penalty and the expense of accessing the .rst byte, virtual pages tend to be large, typically 4 KB to 2 MB. Due to the large miss penalty, DRAM caches are fully associative, that is, any virtual page can be placed in any physical page. The replacement policy on misses also assumes greater importance, because the penalty associated with replacing the wrong virtual page is so high. Thus, operating systems use much more sophisticated replacement algorithms for DRAM caches than the hardware does for SRAM caches. (These replacement algorithms are beyond our scope here.) Finally, because of the large access time of disk, DRAM caches always use write-back instead of write-through. 
9.3.2 Page Tables 
As with any cache, the VM system must have some way to determine if a virtual page is cached somewhere in DRAM. If so, the system must determine which physical page it is cached in. If there is a miss, the system must determine where the virtual page is stored on disk, select a victim page in physical memory, and copy the virtual page from disk to DRAM, replacing the victim page. 
These capabilities are provided by a combination of operating system soft-ware, address translation hardware in the MMU (memory management unit), and a data structure stored in physical memory known as a page table that maps vir-tual pages to physical pages. The address translation hardware reads the page table each time it converts a virtual address to a physical address. The operating system 
Page table. 
disk address 
Valid 
PTE 0 
PTE 7 Memory resident page table (DRAM) VP 1 


VP 2 
VP 3 
VP 4 
Physical memory (DRAM) 

Virtual memory (disk) 

VP 6 
VP 7 
PP 0 PP 3 

is responsible for maintaining the contents of the page table and transferring pages back and forth between disk and DRAM. 
Figure 9.4 shows the basic organization of a page table. A page table is an array of page table entries (PTEs). Each page in the virtual address space has a PTE at a .xed offset in the page table. For our purposes, we will assume that each PTE consists of a valid bit and an n-bit address .eld. The valid bit indicates whether the virtual page is currently cached in DRAM. If the valid bit is set, the address .eld indicates the start of the corresponding physical page in DRAM where the virtual page is cached. If the valid bit is not set, then a null address indicates that the virtual page has not yet been allocated. Otherwise, the address points to the start of the virtual page on disk. 
The example in Figure 9.4 shows a page table for a system with eight virtual pages and four physical pages. Four virtual pages (VP 1, VP 2, VP 4, and VP 7) are currently cached in DRAM. Two pages (VP 0 and VP 5) have not yet been allocated, and the rest (VP 3 and VP 6) have been allocated, but are not currently cached. An important point to notice about Figure 9.4 is that because the DRAM cache is fully associative, any physical page can contain any virtual page. 
Practice Problem 9.2 
Determine the number of page table entries (PTEs) that are needed for the following combinations of virtual address size (n) and page size (P ): 
n  P  = 2p  No. PTEs  
16 16 32 32  4K 8K 4K 8K  

782  Chapter 9  Virtual Memory  
Figure 9.5  Physical page  Physical memory  
VM page hit. The reference  Virtual address  number or  (DRAM)  

disk address

to a word in VP 2 is a hit. PP 0 
Valid 
PTE 0 

PP 3 
Virtual memory (disk) 
PTE 7 
Memory resident page table (DRAM) 

VP 1  
VP 2  
VP 3  
VP 4  
VP 6  
VP 7  

9.3.3 Page Hits 
Consider what happens when the CPU reads a word of virtual memory contained in VP 2, which is cached in DRAM (Figure 9.5). Using a technique we will describe in detail in Section 9.6, the address translation hardware uses the virtual address as an index to locate PTE 2 and read it from memory. Since the valid bit is set, the address translation hardware knows that VP 2 is cached in memory. So it uses the physical memory address in the PTE (which points to the start of the cached page in PP 1) to construct the physical address of the word. 
9.3.4 Page Faults 
In virtual memory parlance, a DRAM cache miss is known as a page fault. Fig-ure 9.6 shows the state of our example page table before the fault. The CPU has referenced a word in VP 3, which is not cached in DRAM. The address transla-tion hardware reads PTE 3 from memory, infers from the valid bit that VP 3 is not cached, and triggers a page fault exception. 
The page fault exception invokes a page fault exception handler in the kernel, which selects a victim page, in this case VP 4 stored in PP 3. If VP 4 has been modi.ed, then the kernel copies it back to disk. In either case, the kernel modi.es the page table entry for VP 4 to re.ect the fact that VP 4 is no longer cached in main memory. 
Next, the kernel copies VP 3 from disk to PP 3 in memory, updates PTE 3, and then returns. When the handler returns, it restarts the faulting instruction, which resends the faulting virtual address to the address translation hardware. But now, VP 3 is cached in main memory, and the page hit is handled normally by the address translation hardware. Figure 9.7 shows the state of our example page table after the page fault. 
Virtual memory was invented in the early 1960s, long before the widening CPU-memory gap spawned SRAM caches. As a result, virtual memory systems 

Figure 9.6 Physical page Physical memory Virtual address number or (DRAM)
VM page fault (before). 
disk address

The reference to a word in 
Valid 

VP 3 is a miss and triggers 
PTE 0 a page fault. 
PP 3 
Virtual memory (disk) PTE 7 
Memory resident page table (DRAM) 
VP 1  
VP 2  
VP 3  
VP 4  
VP 6  
VP 7  

Figure 9.7 

VM page fault (after). The page fault handler selects VP 4 as the victim and replaces it with a copy of VP 3 from disk. After the page fault handler restarts the faulting instruction, it will read the word from memory normally, without generating an exception. 
Physical page Physical memory Virtual address number or (DRAM) disk address 


PP 0 

PP 0 
Valid 
PTE 0 
PP 3 
Virtual memory (disk) 
PTE 7 
Memory resident page table (DRAM) 
VP 1  
VP 2  
VP 3  
VP 4  
VP 6  
VP 7  


use a different terminology from SRAM caches, even though many of the ideas are similar. In virtual memory parlance, blocks are known as pages. The activity of transferring a page between disk and memory is known as swapping or paging. Pages are swapped in (paged in) from disk to DRAM, and swapped out (paged out) from DRAM to disk. The strategy of waiting until the last moment to swap in a page, when a miss occurs, is known as demand paging. Other approaches, such as trying to predict misses and swap pages in before they are actually referenced, are possible. However, all modern systems use demand paging. 
9.3.5 Allocating Pages 
Figure 9.8 shows the effect on our example page table when the operating system allocates a new page of virtual memory, for example, as a result of calling malloc. 
784  Chapter 9  Virtual Memory  
Figure 9.8  Physical page  Physical memory  
Allocating a new virtual  number or  (DRAM)  

disk address

page. The kernel allocates 

PP 0 
Valid 

VP 5 on disk and points PTE 0 PTE 5 to this new location. 
PP 3 
Virtual memory (disk) PTE 7 
Memory resident page table (DRAM) 
VP 1  
VP 2  
VP 3  
VP 4  
VP 5  
VP 6  
VP 7  

In the example, VP 5 is allocated by creating room on disk and updating PTE 5 to point to the newly created page on disk. 
9.3.6 Locality to the Rescue Again 
When many of us learn about the idea of virtual memory, our .rst impression is often that it must be terribly inef.cient. Given the large miss penalties, we worry that paging will destroy program performance. In practice, virtual memory works well, mainly because of our old friend locality. 
Although the total number of distinct pages that programs reference during an entire run might exceed the total size of physical memory, the principle of locality promises that at any point in time they will tend to work on a smaller set of active pages known as the working set or resident set. After an initial overhead where the working set is paged into memory, subsequent references to the working set result in hits, with no additional disk traf.c. 
As long as our programs have good temporal locality, virtual memory systems work quite well. But of course, not all programs exhibit good temporal locality. If the working set size exceeds the size of physical memory, then the program can produce an unfortunate situation known as thrashing, where pages are swapped in and out continuously. Although virtual memory is usually ef.cient, if a program¡¯s performance slows to a crawl, the wise programmer will consider the possibility that it is thrashing. 

Aside Counting page faults 
You can monitor the number of page faults (and lots of other information) with the Unix getrusage function. 
9.4 
VM 
as 
a 
Tool 
for 
Memory 
Management 

In the last section, we saw how virtual memory provides a mechanism for using the DRAM to cache pages from a typically larger virtual address space. Interestingly, some early systems such as the DEC PDP-11/70 supported a virtual address space that was smaller than the available physical memory. Yet virtual memory was still a useful mechanism because it greatly simpli.ed memory management and provided a natural way to protect memory. 
Thus far, we have assumed a single page table that maps a single virtual address space to the physical address space. In fact, operating systems provide a separate page table, and thus a separate virtual address space, for each process. Figure 9.9 shows the basic idea. In the example, the page table for process i maps VP 1 to PP 2 and VP 2 to PP 7. Similarly, the page table for process j maps VP 1 to PP 7 and VP 2 to PP 10. Notice that multiple virtual pages can be mapped to the same shared physical page. 
The combination of demand paging and separate virtual address spaces has a profound impact on the way that memory is used and managed in a system. In particular, VM simpli.es linking and loading, the sharing of code and data, and allocating memory to applications. 
. Simplifying linking. A separate address space allows each process to use the same basic format for its memory image, regardless of where the code and data actually reside in physical memory. For example, as we saw in Figure 8.13, every process on a given Linux system has a similar memory format. The text section always starts at virtual address 0x08048000 (for 32-bit address spaces), or at address 0x400000 (for 64-bit address spaces). The data and bss sections follow immediately after the text section. The stack occupies the highest portion of the process address space and grows downward. Such uniformity greatly simpli.es the design and implementation of linkers, allowing them to produce fully linked executables that are independent of the ultimate location of the code and data in physical memory. 
. Simplifying loading. Virtual memory also makes it easy to load executable and shared object .les into memory. Recall from Chapter 7 that the .text 
Figure 9.9 

How VM provides processes with separate address spaces. The operating system maintains a separate page table for each process in the system. 
Physical memory 
Process i: 
N 1 

Shared page
0 
Process j: 

and .data sections in ELF executables are contiguous. To load these sections into a newly created process, the Linux loader allocates a contiguous chunk of virtual pages starting at address 0x08048000 (32-bit address spaces) or 0x400000 (64-bit address spaces), marks them as invalid (i.e., not cached), and points their page table entries to the appropriate locations in the object .le. The interesting point is that the loader never actually copies any data from disk into memory. The data is paged in automatically and on demand by the virtual memory system the .rst time each page is referenced, either by the CPU when it fetches an instruction, or by an executing instruction when it references a memory location. 
This notion of mapping a set of contiguous virtual pages to an arbitrary location in an arbitrary .le is known as memory mapping. Unix provides a system call called mmap that allows application programs to do their own memory mapping. We will describe application-level memory mapping in more detail in Section 9.8. 
. Simplifying sharing. Separate address spaces provide the operating system with a consistent mechanism for managing sharing between user processes and the operating system itself. In general, each process has its own private code, data, heap, and stack areas that are not shared with any other process. In this case, the operating system creates page tables that map the corresponding virtual pages to disjoint physical pages. 
However, in some instances it is desirable for processes to share code and data. For example, every process must call the same operating system kernel code, and every C program makes calls to routines in the standard C library such as printf. Rather than including separate copies of the kernel and standard C library in each process, the operating system can arrange for multiple processes to share a single copy of this code by mapping the appropriate virtual pages in different processes to the same physical pages, as we saw in Figure 9.9. 
. Simplifying memory allocation.Virtual memory provides a simple mechanism for allocating additional memory to user processes. When a program running in a user process requests additional heap space (e.g., as a result of calling malloc), the operating system allocates an appropriate number, say, k,of contiguous virtual memory pages, and maps them to k arbitrary physical pages located anywhere in physical memory. Because of the way page tables work, there is no need for the operating system to locate k contiguous pages of physical memory. The pages can be scattered randomly in physical memory. 
9.5 
VM 
as 
a 
Tool 
for 
Memory 
Protection 

Any modern computer system must provide the means for the operating system to control access to the memory system. A user process should not be allowed to modify its read-only text section. Nor should it be allowed to read or modify any of the code and data structures in the kernel. It should not be allowed to read or write the private memory of other processes, and it should not be allowed to 
Section 9.6 Address Translation 787 

Page tables with permission bits SUP READ WRITE Address 
VP 0: 
Process i: VP 1: VP 2: 
VP 0: 
Process j: VP 1: 
VP 2: PP 0 



PP 2 PP 4 PP 6 
PP 9 
PP 11 
. . .
. . . 

modify any virtual pages that are shared with other processes, unless all parties explicitly allow it (via calls to explicit interprocess communication system calls). 
As we have seen, providing separate virtual address spaces makes it easy to isolate the private memories of different processes. But the address translation mechanism can be extended in a natural way to provide even .ner access control. Since the address translation hardware reads a PTE each time the CPU generates an address, it is straightforward to control access to the contents of a virtual page by adding some additional permission bits to the PTE. Figure 9.10 shows the general idea. 
In this example, we have added three permission bits to each PTE. The SUP bit indicates whether processes must be running in kernel (supervisor) mode to access the page. Processes running in kernel mode can access any page, but processes running in user mode are only allowed to access pages for which SUP is 0. The READ and WRITE bits control read and write access to the page. For example, if process i is running in user mode, then it has permission to read VP 0 and to read or write VP 1. However, it is not allowed to access VP 2. 
If an instruction violates these permissions, then the CPU triggers a general protection fault that transfers control to an exception handler in the kernel. Unix shells typically report this exception as a ¡°segmentation fault.¡± 
9.6 
Address 
Translation 

This section covers the basics of address translation. Our aim is to give you an appreciation of the hardware¡¯s role in supporting virtual memory, with enough detail so that you can work through some concrete examples by hand. However, keep in mind that we are omitting a number of details, especially related to timing, that are important to hardware designers but are beyond our scope. For your 
Basic parameters 
Symbol  Description  
N =2n  Number of addresses in virtual address space  
M =2m  Number of addresses in physical address space  
P =2p  Page size (bytes)  
Components of a virtual address (VA)  
Symbol  Description  

VPO  Virtual page offset (bytes)  
VPN  Virtual page number  
TLBI  TLB index  
TLBT  TLB tag  
Components of a physical address (PA)  
Symbol  Description  

PPO  Physical page offset (bytes)  
PPN  Physical page number  
CO  Byte offset within cache block  
CI  Cache index  
CT  Cache tag  
Figure 9.11 Summary of address translation symbols. 


reference, Figure 9.11 summarizes the symbols that we will be using throughout this section. 
Formally, address translation is a mapping between the elements of an N-element virtual address space (VAS) and an M-element physical address space (PAS), 
MAP: VAS ¡úPAS ¡È. 
where 
 
A if data at virtual addr A is present at physical addr A in PAS MAP(A) = . if data at virtual addr A is not present in physical memory 
Figure 9.12 shows how the MMU uses the page table to perform this mapping. A control register in the CPU, the page table base register (PTBR) points to the current page table. The n-bit virtual address has two components: a p-bit virtual page offset (VPO) and an (n .p)-bit virtual page number (VPN). The MMU uses the VPN to select the appropriate PTE. For example, VPN 0 selects PTE 0, VPN 1 selects PTE 1, and so on. The corresponding physical address is the concatenation of the physical page number (PPN) from the page table entry and the VPO from 
Virtual address 


n1 pp10 Virtual page number (VPN) Virtual page offset (VPO) 
Valid Physical page number (PPN) 

Page The VPN acts 
table as index into the page table 
If valid 0 then page not in memory m1 
pp1 

0 (page fault) 

Physical address 

the virtual address. Notice that since the physical and virtual pages are both P bytes, the physical page offset (PPO) is identical to the VPO. 
Figure 9.13(a) shows the steps that the CPU hardware performs when there is a page hit. 
.  Step 1: The processor generates a virtual address and sends it to the MMU.  
.  Step 2: The MMU generates the PTE address and requests it from the  
cache/main memory.  
.  Step 3: The cache/main memory returns the PTE to the MMU.  
.  Step 3: The MMU constructs the physical address and sends it to cache/main  
memory.  
.  Step 4: The cache/main memory returns the requested data word to the pro- 
cessor.  
Unlike a page hit, which is handled entirely by hardware, handling a page  

fault requires cooperation between hardware and the operating system kernel (Figure 9.13(b)). 
. Steps 1 to 3: The same as Steps 1 to 3 in Figure 9.13(a). . Step 4: The valid bit in the PTE is zero, so the MMU triggers an exception, which transfers control in the CPU to a page fault exception handler in the operating system kernel. . Step 5: The fault handler identi.es a victim page in physical memory, and if that page has been modi.ed, pages it out to disk. . Step 6: The fault handler pages in the new page and updates the PTE in memory. 


(a) Page hit 


(b) Page fault 
. Step 7: The fault handler returns to the original process, causing the faulting instruction to be restarted. The CPU resends the offending virtual address to the MMU. Because the virtual page is now cached in physical memory, there is a hit, and after the MMU performs the steps in Figure 9.13(b), the main memory returns the requested word to the processor. 
Practice Problem 9.3 
Given a 32-bit virtual address space and a 24-bit physical address, determine the number of bits in the VPN, VPO, PPN, and PPO for the following page sizes P : 
P No. VPN bits No. VPO bits No. PPN bits No. PPO bits 
1KB 
2KB 4KB 8KB 


Figure 9.14 Integrating VM with a physically addressed cache. VA: virtual address. PTEA: page table entry address. PTE: page table entry. PA: physical address. 
9.6.1 Integrating Caches and VM 
In any system that uses both virtual memory and SRAM caches, there is the issue of whether to use virtual or physical addresses to access the SRAM cache. Although a detailed discussion of the trade-offs is beyond our scope here, most systems opt for physical addressing. With physical addressing, it is straightforward for multiple processes to have blocks in the cache at the same time and to share blocks from the same virtual pages. Further, the cache does not have to deal with protection issues because access rights are checked as part of the address translation process. 
Figure 9.14 shows how a physically addressed cache might be integrated with virtual memory. The main idea is that the address translation occurs before the cache lookup. Notice that page table entries can be cached, just like any other data words. 
9.6.2 Speeding up Address Translation with a TLB 
As we have seen, every time the CPU generates a virtual address, the MMU must refer to a PTE in order to translate the virtual address into a physical address. In the worst case, this requires an additional fetch from memory, at a cost of tens to hundreds of cycles. If the PTE happens to be cached in L1, then the cost goes down to one or two cycles. However, many systems try to eliminate even this cost by including a small cache of PTEs in the MMU called a translation lookaside buffer (TLB). 
A TLB is a small, virtually addressed cache where each line holds a block consisting of a single PTE. A TLB usually has a high degree of associativity. As shown in Figure 9.15, the index and tag .elds that are used for set selection and line matching are extracted from the virtual page number in the virtual address. If the TLB has T = 2t sets, then the TLB index (TLBI) consists of the t least signi.cant bits of the VPN, and the TLB tag (TLBT) consists of the remaining bits in the VPN. 
Figure 9.15 

Components of a virtual address that are used to 

access the TLB. 
Figure 9.16(a) shows the steps involved when there is a TLB hit (the usual case). The key point here is that all of the address translation steps are performed inside the on-chip MMU, and thus are fast. 
. Step 1: The CPU generates a virtual address. . Steps 2 and 3: The MMU fetches the appropriate PTE from the TLB. . Step 4: The MMU translates the virtual address to a physical address and sends 
it to the cache/main memory. . Step 5: The cache/main memory returns the requested data word to the CPU. 
When there is a TLB miss, then the MMU must fetch the PTE from the L1 cache, as shown in Figure 9.16(b). The newly fetched PTE is stored in the TLB, possibly overwriting an existing entry. 
9.6.3 Multi-Level Page Tables 
To this point we have assumed that the system uses a single page table to do address translation. But if we had a 32-bit address space, 4 KB pages, and a 4-byte PTE, then we would needa4MB page table resident in memory at all times, even if the application referenced only a small chunk of the virtual address space. The problem is compounded for systems with 64-bit address spaces. 
The common approach for compacting the page table is to use a hierarchy of page tables instead. The idea is easiest to understand with a concrete example. Consider a 32-bit virtual address space partitioned into 4 KB pages, with page table entries that are 4 bytes each. Suppose also that at this point in time the virtual address space has the following form: The .rst 2K pages of memory are allocated for code and data, the next 6K pages are unallocated, the next 1023 pages are also unallocated, and the next page is allocated for the user stack. Figure 9.17 shows how we might construct a two-level page table hierarchy for this virtual address space. 
Each PTE in the level-1 table is responsible for mappinga4MB chunk of the virtual address space, where each chunk consists of 1024 contiguous pages. For example, PTE 0 maps the .rst chunk, PTE 1 the next chunk, and so on. Given that the address space is 4 GB, 1024 PTEs are suf.cient to cover the entire space. 
If every page in chunk i is unallocated, then level 1 PTE i is null. For example, in Figure 9.17, chunks 2¨C7 are unallocated. However, if at least one page in chunk i is allocated, then level 1 PTE i points to the base of a level 2 page table. For example, in Figure 9.17, all or portions of chunks 0, 1, and 8 are allocated, so their level 1 PTEs point to level 2 page tables. 
CPU chip 



Data 

(a) TLB hit CPU chip 



Each PTE in a level 2 page table is responsible for mappinga4KB page of virtual memory, just as before when we looked at single-level page tables. Notice that with 4-byte PTEs, each level 1 and level 2 page table is 4K bytes, which conveniently is the same size as a page. 
This scheme reduces memory requirements in two ways. First, if a PTE in the level 1 table is null, then the corresponding level 2 page table does not even have to exist. This represents a signi.cant potential savings, since most of the 4 GB virtual address space for a typical program is unallocated. Second, only the level 1 table needs to be in main memory at all times. The level 2 page tables can be created and 
Level 1 Level 2 Virtual page table page tables memory 

2K allocated VM pages for code and data 
6K unallocated VM pages 
1023 unallocated pages 
1 allocated VM page for the stack 
paged in and out by the VM system as they are needed, which reduces pressure on main memory. Only the most heavily used level 2 page tables need to be cached in main memory. 
Figure 9.18 summarizes address translation with a k-level page table hierarchy. The virtual address is partitioned into k VPNs and a VPO. Each VPN i,1 ¡Ü i ¡Ü k, is an index into a page table at level i. Each PTE in a level-j table, 1 ¡Ü j ¡Ü k . 1, points to the base of some page table at level j + 1. Each PTE in a level-k table contains either the PPN of some physical page or the address of a disk block. To construct the physical address, the MMU must access k PTEs before it can determine the PPN. As with a single-level hierarchy, the PPO is identical to the VPO. 
Accessing k PTEs may seem expensive and impractical at .rst glance. How-ever, the TLB comes to the rescue here by caching PTEs from the page tables at the different levels. In practice, address translation with multi-level page tables is not signi.cantly slower than with single-level page tables. 
9.6.4 Putting It Together: End-to-end Address Translation 
In this section, we put it all together with a concrete example of end-to-end address translation on a small system with a TLB and L1 d-cache. To keep things manageable, we make the following assumptions: 
. The memory is byte addressable. 
. Memory accesses are to 1-byte words (not 4-byte words). 
. . . 

.  Virtual addresses are 14 bits wide (n = 14).  
.  Physical addresses are 12 bits wide (m = 12).  
.  The page size is 64 bytes (P  = 64).  
.  The TLB is four-way set associative with 16 total entries.  
.  The L1 d-cache is physically addressed and direct mapped, with a 4-byte line  
size and 16 total sets.  
Figure 9.19 shows the formats of the virtual and physical addresses. Since each  

page is 26 = 64 bytes, the low-order 6 bits of the virtual and physical addresses serve as the VPO and PPO respectively. The high-order 8 bits of the virtual address serve as the VPN. The high-order 6 bits of the physical address serve as the PPN. 
Figure 9.20 shows a snapshot of our little memory system, including the TLB (Figure 9.20(a)), a portion of the page table (Figure 9.20(b)), and the L1 cache (Figure 9.20(c)). Above the .gures of the TLB and cache, we have also shown how the bits of the virtual and physical addresses are partitioned by the hardware as it accesses these devices. 
131211 10987 6543210 
VPN VPO 


Physical address 

(Virtual page number) (Virtual page offset) 
11 10 987 6543210 



PPN 
PPO (Physical page number) (Physical page offset) 


Figure 9.19 Addressing for small memory system. Assume 14-bit virtual addresses (n = 14), 12-bit physical addresses (m = 12), and 64-byte pages (P = 64). 
13  12  TLBT 11 10  9  8  TLBI 7 6  5  4  3  2  1  0  
Set  Tag  PPN  VPN Valid Tag  PPN  Valid  Tag  PPN  VPO Valid  Tag  PPN  Valid  


0 1 2 3 
03  0  09  0D  1  00  0  07  02  1  
03  2D  1  02  0  04  0  0A  0  
02  0  08  0  06  0  03  0  
07  0  03  0D  1  0A  34  1  02  0  

(a) TLB: Four sets, 16 entries, four-way set associative VPN PPN Valid VPN PPN Valid 
00 01 02 03 04 05 06 07 08 09 0A 0B 0C 0D 0E 0F 
28  1  
¡ª  0  
33  1  
02  1  
¡ª  0  
16  1  
¡ª  0  
¡ª  0  

13  1  
17  1  
09  1  
0  
0  
2D  1  
11  1  
0D  1  


(b) Page table: Only the first 16 PTEs are shown CT 
CI
CO 


Idx Tag Valid Blk 0 Blk 1 Blk 2 Blk 3 
0 1 2 3 4 5 6 7 8 9 A B C D E F 
19  1  99  11  23  11  
15  0  ¡ª  ¡ª  ¡ª  ¡ª  
1B  1  00  02  04  08  
36  0  ¡ª  ¡ª  ¡ª  ¡ª  
32  1  43  6D  8F  09  
0D  1  36  72  F0  1D  
31  0  ¡ª  ¡ª  ¡ª  ¡ª  
16  1  11  C2  DF  03  
24  1  3A  00  51  89  
2D  0  ¡ª  ¡ª  ¡ª  ¡ª  
2D  1  93  15  DA  3B  
0B  0  ¡ª  ¡ª  ¡ª  ¡ª  
12  0  ¡ª  ¡ª  ¡ª  ¡ª  
16  1  04  96  34  15  
13  1  83  77  1B  D3  
14  0  ¡ª  ¡ª  ¡ª  ¡ª  
Figure 9.20 TLB, page table, and cache for small memory system. All values in the TLB, page table, and cache are in hexadecimal notation. 


(c) Cache: Sixteen sets, 4-byte blocks, direct mapped 

. TLB: The TLB is virtually addressed using the bits of the VPN. Since the TLB has four sets, the 2 low-order bits of the VPN serve as the set index (TLBI). The remaining 6 high-order bits serve as the tag (TLBT) that distinguishes the different VPNs that might map to the same TLB set. 
. Page table. The page table is a single-level design with a total of 28 = 256 page table entries (PTEs). However, we are only interested in the .rst sixteen of these. For convenience, we have labeled each PTE with the VPN that indexes it; but keep in mind that these VPNs are not part of the page table and not stored in memory. Also, notice that the PPN of each invalid PTE is denoted with a dash to reinforce the idea that whatever bit values might happen to be stored there are not meaningful. 
. Cache. The direct-mapped cache is addressed by the .elds in the physical address. Since each block is 4 bytes, the low-order 2 bits of the physical address serve as the block offset (CO). Since there are 16 sets, the next 4 bits serve as the set index (CI). The remaining 6 bits serve as the tag (CT). 
Given this initial setup, let¡¯s see what happens when the CPU executes a load instruction that reads the byte at address 0x03d4. (Recall that our hypothetical CPU reads one-byte words rather than four-byte words.) To begin this kind of manual simulation, we .nd it helpful to write down the bits in the virtual address, identify the various .elds we will need, and determine their hex values. The hardware performs a similar task when it decodes the address. 
TLBT TLBI 
0x03 0x03 

bitposition 13121110 9876543210 
VA = 0x03d4  0  0  0  0  1  1  1  1  0  1  0  1  0  0  
VPN  VPO  
0x0f  0x14  

To begin, the MMU extracts the VPN (0x0F) from the virtual address and checks with the TLB to see if it has cached a copy of PTE 0x0F from some previous memory reference. The TLB extracts the TLB index (0x03) and the TLB tag (0x3) from the VPN, hits on a valid match in the second entry of Set 0x3, and returns the cached PPN (0x0D) to the MMU. 
If the TLB had missed, then the MMU would need to fetch the PTE from main memory. However, in this case we got lucky and had a TLB hit. The MMU now has everything it needs to form the physical address. It does this by concatenating the PPN (0x0D) from the PTE with the VPO (0x14) from the virtual address, which forms the physical address (0x354). 
Next, the MMU sends the physical address to the cache, which extracts the cache offset CO (0x0), the cache set index CI (0x5), and the cache tag CT (0x0D) from the physical address. 
798  Chapter 9  Virtual Memory  
CT  CI  CO  
bit position  11  10  0x0d 9 8  7  6  5  0x05 4 3  2  0x0 1 0  

PA = 0x354  0  0  1  1  0  1  0  1  0  1  0  0  
PPN  PPO  
0x0d  0x14  

Since the tag in Set 0x5 matches CT, the cache detects a hit, reads out the data byte (0x36) at offset CO, and returns it to the MMU, which then passes it back to the CPU. 
Other paths through the translation process are also possible. For example, if the TLB misses, then the MMU must fetch the PPN from a PTE in the page table. If the resulting PTE is invalid, then there is a page fault and the kernel must page in the appropriate page and rerun the load instruction. Another possibility is that the PTE is valid, but the necessary memory block misses in the cache. 
Practice Problem 9.4 
Show how the example memory system in Section 9.6.4 translates a virtual address into a physical address and accesses the cache. For the given virtual address, indicate the TLB entry accessed, physical address, and cache byte value returned. Indicate whether the TLB misses, whether a page fault occurs, and whether a cache miss occurs. If there is a cache miss, enter ¡°¨C¡± for ¡°Cache byte returned.¡± If there is a page fault, enter ¡°¨C¡± for ¡°PPN¡± and leave parts C and D blank. 
Virtual address: 0x03d7 
A. Virtual address format 
13 12 11 10 9 8 7 6 5 4 3 2 1 0 

B. Address translation 
Parameter Value VPN 
TLB index TLB tag TLB hit? (Y/N) Page fault? (Y/N) PPN 
C. Physical address format 
11 10 9 8 7 6 5 4 3 2 1 0 


D. Physical memory reference 
Parameter Value Byte offset 
Cache index Cache tag Cache hit? (Y/N) Cache byte returned 
9.7 
Case 
Study: 
The 
Intel 
Core 
i7/Linux 
Memory 
System 

We conclude our discussion of virtual memory mechanisms with a case study of a real system: an Intel Core i7 running Linux. The Core i7 is based on the Nehalem microarchitecture. Although the Nehalem design allows for full 64-bit virtual and physical address spaces, the current Core i7 implementations (and those for the foreseeable future) support a 48-bit (256 TB) virtual address space and a 52-bit (4 PB) physical address space, along with a compatability mode that supports 32-bit (4 GB) virtual and physical address spaces. 
Figure 9.21 gives the highlights of the Core i7 memory system. The processor package includes four cores, a large L3 cache shared by all of the cores, and a 

Processor package 


To other cores 
To I/O bridge 

DDR3 memory controller. Each core contains a hierarchy of TLBs, a hierarchy of data and instruction caches, and a set of fast point-to-point links, based on the Intel QuickPath technology, for communicating directly with the other cores and the external I/O bridge. The TLBs are virtually addressed, and four-way set associative. The L1, L2, and L3 caches are physically addressed, and eight-way set associative, with a block size of 64 bytes. The page size can be con.gured at start-up time as either 4 KB or 4 MB. Linux uses 4-KB pages. 
9.7.1 Core i7 Address Translation 
Figure 9.22 summarizes the entire Core i7 address translation process, from the time the CPU generates a virtual address until a data word arrives from memory. The Core i7 uses a four-level page table hierarchy. Each process has its own private page table hierarchy. When a Linux process is running, the page tables associated with allocated pages are all memory-resident, although the Core i7 architecture allows these page tables to be swapped in and out. The CR3 control register points to the beginning of the level 1 (L1) page table. The value of CR3 is part of each process context, and is restored during each context switch. 
CR3 


Page tables 
6362 5251 1211 9876543210 
XD  Unused  Page table physical base addr  Unused  G  PS  A  CD  WT  U/S  R/W  P=1  


Field  Description  
P  Child page table present in physical memory (1) or not (0).  
R/W  Read-only or read-write access permission for all reachable pages.  
U/S  User or supervisor (kernel) mode access permission for all reachable pages.  
WT  Write-through or write-back cache policy for the child page table.  
CD  Caching disabled or enabled for the child page table.  
A  Reference bit (set by MMU on reads and writes, cleared by software).  
PS  Page size either 4 KB or 4 MB (de.ned for Level 1 PTEs only).  
Base addr  40 most signi.cant bits of physical base address of child page table.  
XD  Disable or enable instruction fetches from all pages reachable from this PTE.  
Figure 9.23 Format of level 1, level 2, and level 3 page table entries. Each entry referencesa4KB child page table. 


Figure 9.23 shows the format of an entry in a level 1, level 2, or level 3 page table. When P = 1 (which is always the case with Linux), the address .eld contains a 40-bit physical page number (PPN) that points to the beginning of the appropriate page table. Notice that this imposesa4KB alignment requirement on page tables. 
Figure 9.24 shows the format of an entry in a level 4 page table. When P = 1, the address .eld contains a 40-bit PPN that points to the base of some page in physical memory. Again, this imposesa4KB alignment requirement on physical pages. 
The PTE has three permission bits that control access to the page. The R/W bit determines whether the contents of a page are read/write or read/only. The U/S bit, which determines whether the page can be accessed in user mode, protects code and data in the operating system kernel from user programs. The XD (exe-cute disable) bit, which was introduced in 64-bit systems, can be used to disable instruction fetches from individual memory pages. This is an important new fea-ture that allows the operating system kernel to reduce the risk of buffer over.ow attacks by restricting execution to the read-only text segment. 
As the MMU translates each virtual address, it also updates two other bits that can be used by the kernel¡¯s page fault handler. The MMU sets the A bit, which is known as a reference bit, each time a page is accessed. The kernel can use the reference bit to implement its page replacement algorithm. The MMU sets the D bit, or dirty bit, each time the page is written to. A page that has been modi.ed is sometimes called a dirty page. The dirty bit tells the kernel whether or not it must write-back a victim page before it copies in a replacement page. The kernel can call a special kernel-mode instruction to clear the reference or dirty bits. 
6362 5251 1211 9876543210 
XD  Unused  Page physical base addr  Unused  G  0  D  A  CD  WT  U/S  R/W  P=1  


Field  Description  
P  Child page present in physical memory (1) or not (0).  
R/W  Read-only or read/write access permission for child page.  
U/S  User or supervisor mode (kernel mode) access permission for child page.  
WT  Write-through or write-back cache policy for the child page.  
CD  Cache disabled or enabled.  
A  Reference bit (set by MMU on reads and writes, cleared by software).  
D  Dirty bit (set by MMU on writes, cleared by software).  
G  Global page (don¡¯t evict from TLB on task switch).  
Base addr  40 most signi.cant bits of physical base address of child page.  
XD  Disable or enable instruction fetches from the child page.  
Figure 9.24 Format of level 4 page table entries. Each entry referencesa4KB child page. 


Figure 9.25 shows how the Core i7 MMU uses the four levels of page tables to translate a virtual address to a physical address. The 36-bit VPN is partitioned into four 9-bit chunks, each of which is used as an offset into a page table. The CR3 register contains the physical address of the L1 page table. VPN 1 provides an offset to an L1 PTE, which contains the base address of the L2 page table. VPN 2 provides an offset to an L2 PTE, and so on. 

Aside Optimizing address translation 
In our discussion of address translation, we have described a sequential two-step process where the MMU (1) translates the virtual address to a physical address, and then (2) passes the physical address to the L1 cache. However, real hardware implementations use a neat trick that allows these steps to be partially overlapped, thus speeding up accesses to the L1 cache. For example, a virtual address on a Core i7 with 4 KB pages has 12 bits of VPO, and these bits are identical to the 12 bits of PPO in the corresponding physical address. Since the eight-way set-associative physically addressed L1 caches have 64 sets and 64-byte cache blocks, each physical address has 6 (log2 64) cache offset bits and 6 (log2 64) index bits. These 12 bits .t exactly in the 12-bit VPO of a virtual address, which is no accident! When the CPU needs a virtual address translated, it sends the VPN to the MMU and the VPO to the L1 cache. While the MMU is requesting a page table entry from the TLB, the L1 cache is busy using the VPO bits to .nd the appropriate set and read out the eight tags and corresponding data words in that set. When the MMU gets the PPN back from the TLB, the cache is ready to try to match the PPN to one of these eight tags. 

12 
VPO Virtual address 

L1 PT L2 PT L3 PT L4 PT 
Page global Page upper Page middle Page 
40 directory 40 directory 40 directory 40 table 
CR3 Physical 
address of L1 PT 9 
9 
9 
9 
Offset into

L1 PTE L2 PTE L3 PTE L4 PTE 
12 physical and virtual page 

512 GB 1 GB 2 MB 4 KB 
Physical region region region region 
address per entry per entry per entry per entry 
of page 
40 

40 12 
Physical address 

Figure 9.25 Core i7 page table translation. Legend: PT: page table, PTE: page table entry, VPN: virtual page number, VPO: virtual page offset, PPN: physical page number, PPO: physical page offset. The Linux names for the four levels of page tables are also shown. 
9.7.2 Linux Virtual Memory System 
A virtual memory system requires close cooperation between the hardware and the kernel. Details vary from version to version, and a complete description is beyond our scope. Nonetheless, our aim in this section is to describe enough of the Linux virtual memory system to give you a sense of how a real operating system organizes virtual memory and how it handles page faults. 
Linux maintains a separate virtual address space for each process of the form shown in Figure 9.26. We have seen this picture a number of times already, with its familiar code, data, heap, shared library, and stack segments. Now that we understand address translation, we can .ll in some more details about the kernel virtual memory that lies above the user stack. 
The kernel virtual memory contains the code and data structures in the kernel. Some regions of the kernel virtual memory are mapped to physical pages that are shared by all processes. For example, each process shares the kernel¡¯s code and global data structures. Interestingly, Linux also maps a set of contiguous virtual pages (equal in size to the total amount of DRAM in the system) to the corresponding set of contiguous physical pages. This provides the kernel with a convenient way to access any speci.c location in physical memory, for example, 
Figure 9.26 

The virtual memory of a Linux process. 
0x08048000 (32) 0x40000000 (64) 

Different for each process 
Identical for each process 
%esp 
brk 


Kernel virtual memory 
Process virtual memory 
when it needs to access page tables, or to perform memory-mapped I/O operations on devices that are mapped to particular physical memory locations. 
Other regions of kernel virtual memory contain data that differs for each process. Examples include page tables, the stack that the kernel uses when it is executing code in the context of the process, and various data structures that keep track of the current organization of the virtual address space. 
Linux Virtual Memory Areas 
Linux organizes the virtual memory as a collection of areas (also called segments). An area is a contiguous chunk of existing (allocated) virtual memory whose pages are related in some way. For example, the code segment, data segment, heap, shared library segment, and user stack are all distinct areas. Each existing virtual page is contained in some area, and any virtual page that is not part of some area does not exist and cannot be referenced by the process. The notion of an area is important because it allows the virtual address space to have gaps. The kernel does not keep track of virtual pages that do not exist, and such pages do not consume any additional resources in memory, on disk, or in the kernel itself. 
Figure 9.27 highlights the kernel data structures that keep track of the virtual memory areas in a process. The kernel maintains a distinct task structure (task_ struct in the source code) for each process in the system. The elements of the task 
Process virtual memory 
vm_area_struct 

task_struct mm_struct 




Shared libraries  

Data  
Text  


0 


structure either contain or point to all of the information that the kernel needs to run the process (e.g., the PID, pointer to the user stack, name of the executable object .le, and program counter). 
One of the entries in the task structure points to an mm_struct that charac-terizes the current state of the virtual memory. The two .elds of interest to us are pgd, which points to the base of the level 1 table (the page global directory), and mmap, which points to a list of vm_area_structs (area structs), each of which characterizes an area of the current virtual address space. When the kernel runs this process, it stores pgd in the CR3 control register. 
For our purposes, the area struct for a particular area contains the following .elds: 
.  vm_start: Points to the beginning of the area  
.  vm_end: Points to the end of the area  
.  vm_prot: Describes the read/write permissions for all of the pages contained  
in the area  
.  vm_flags: Describes (among other things) whether the pages in the area are  
shared with other processes or private to this process  
.  vm_next: Points to the next area struct in the list  

Linux Page Fault Exception Handling 
Suppose the MMU triggers a page fault while trying to translate some virtual address A. The exception results in a transfer of control to the kernel¡¯s page fault handler, which then performs the following steps: 
1. 
Is virtual address A legal? In other words, does A lie within an area de.ned by some area struct? To answer this question, the fault handler searches the list of area structs, comparing A with the vm_start and vm_end in each area struct. If the instruction is not legal, then the fault handler triggers a segmentation fault, which terminates the process. This situation is labeled ¡°1¡± in Figure 9.28. 

Because a process can create an arbitrary number of new virtual memory areas (using the mmap function described in the next section), a sequential search of the list of area structs might be very costly. So in practice, Linux superimposes a tree on the list, using some .elds that we have not shown, and performs the search on this tree. 

2. 
Is the attempted memory access legal? In other words, does the process have permission to read, write, or execute the pages in this area? For example, was the page fault the result of a store instruction trying to write to a read-only page in the text segment? Is the page fault the result of a process running in user mode that is attempting to read a word from kernel virtual memory? If the attempted access is not legal, then the fault handler triggers a protec-tion exception, which terminates the process. This situation is labeled ¡°2¡± in Figure 9.28. 


Process virtual memory
vm_area_struct 

0 
Segmentation fault: accessing a non-existing page 
Normal page fault 
Protection exception: e.g., violating permission by writing to a read-only page 

3. At this point, the kernel knows that the page fault resulted from a legal operation on a legal virtual address. It handles the fault by selecting a victim page, swapping out the victim page if it is dirty, swapping in the new page, and updating the page table. When the page fault handler returns, the CPU restarts the faulting instruction, which sends A to the MMU again. This time, the MMU translates A normally, without generating a page fault. 
9.8 
Memory 
Mapping 

Linux (along with other forms of Unix) initializes the contents of a virtual memory area by associating it with an object on disk, a process known as memory mapping. Areas can be mapped to one of two types of objects: 
1. 
Regular .le in the Unix .le system: An area can be mapped to a contiguous section of a regular disk .le, such as an executable object .le. The .le section is divided into page-sized pieces, with each piece containing the initial contents of a virtual page. Because of demand paging, none of these virtual pages is actually swapped into physical memory until the CPU .rst touches the page (i.e., issues a virtual address that falls within that page¡¯s region of the address space). If the area is larger than the .le section, then the area is padded with zeros. 

2. 
Anonymous .le: An area can also be mapped to an anonymous .le, created by the kernel, that contains all binary zeros. The .rst time the CPU touches a virtual page in such an area, the kernel .nds an appropriate victim page in physical memory, swaps out the victim page if it is dirty, overwrites the victim page with binary zeros, and updates the page table to mark the page as resident. Notice that no data is actually transferred between disk and memory. For this reason, pages in areas that are mapped to anonymous .les are sometimes called demand-zero pages. 


In either case, once a virtual page is initialized, it is swapped back and forth between a special swap .le maintained by the kernel. The swap .le is also known as the swap space or the swap area. An important point to realize is that at any point in time, the swap space bounds the total amount of virtual pages that can be allocated by the currently running processes. 
9.8.1 Shared Objects Revisited 
The idea of memory mapping resulted from a clever insight that if the virtual memory system could be integrated into the conventional .le system, then it could provide a simple and ef.cient way to load programs and data into memory. 
As we have seen, the process abstraction promises to provide each process with its own private virtual address space that is protected from errant writes or reads by other processes. However, many processes have identical read-only text areas. For example, each process that runs the Unix shell program tcsh has the same text area. Further, many programs need to access identical copies of 
808  Chapter 9  Virtual Memory  
Process 1  Physical  Process 2  Process 1  Physical  Process 2  
virtual memory  memory  virtual memory  virtual memory  memory  virtual memory  




Shared object 
(a) 

Figure 9.29 A shared object. (a) After process 1 maps the shared object. (b) After process 2 maps the same shared object. (Note that the physical pages are not necessarily contiguous.) 
read-only run-time library code. For example, every C program requires functions from the standard C library such as printf. It would be extremely wasteful for each process to keep duplicate copies of these commonly used codes in physical memory. Fortunately, memory mapping provides us with a clean mechanism for controlling how objects are shared by multiple processes. 
An object can be mapped into an area of virtual memory as either a shared object or a private object. If a process maps a shared object into an area of its virtual address space, then any writes that the process makes to that area are visible to any other processes that have also mapped the shared object into their virtual memory. Further, the changes are also re.ected in the original object on disk. 
Changes made to an area mapped to a private object, on the other hand, are not visible to other processes, and any writes that the process makes to the area are not re.ected back to the object on disk. A virtual memory area into which a shared object is mapped is often called a shared area. Similarly for a private area. 
Suppose that process 1 maps a shared object into an area of its virtual memory, as shown in Figure 9.29(a). Now suppose that process 2 maps the same shared ob-ject into its address space (not necessarily at the same virtual address as process 1), as shown in Figure 9.29(b). 
Since each object has a unique .le name, the kernel can quickly determine that process 1 has already mapped this object and can point the page table entries in process 2 to the appropriate physical pages. The key point is that only a single copy of the shared object needs to be stored in physical memory, even though the object is mapped into multiple shared areas. For convenience, we have shown the physical pages as being contiguous, but of course this is not true in general. 
Private objects are mapped into virtual memory using a clever technique known as copy-on-write. A private object begins life in exactly the same way as a 
Shared object 
(b) 

Process 1 Physical Process 2 Process 1 Physical Process 2 virtual memory memory virtual memory virtual memory memory virtual memory 


Write to private copy-on-write page 
copy-on-write object 
(b) 

Figure 9.30 A private copy-on-write object. (a) After both processes have mapped the private copy-on-write object. (b) After process 2 writes to a page in the private area. 
shared object, with only one copy of the private object stored in physical memory. For example, Figure 9.30(a) shows a case where two processes have mapped a private object into different areas of their virtual memories but share the same physical copy of the object. For each process that maps the private object, the page table entries for the corresponding private area are .agged as read-only, and the area struct is .agged as private copy-on-write. So long as neither process attempts to write to its respective private area, they continue to share a single copy of the object in physical memory. However, as soon as a process attempts to write to some page in the private area, the write triggers a protection fault. 
When the fault handler notices that the protection exception was caused by the process trying to write to a page in a private copy-on-write area, it creates a new copy of the page in physical memory, updates the page table entry to point to the new copy, and then restores write permissions to the page, as shown in Figure 9.30(b). When the fault handler returns, the CPU reexecutes the write, which now proceeds normally on the newly created page. 
By deferring the copying of the pages in private objects until the last possible moment, copy-on-write makes the most ef.cient use of scarce physical memory. 
9.8.2 The fork Function Revisited 
Now that we understand virtual memory and memory mapping, we can get a clear idea of how the fork function creates a new process with its own independent virtual address space. 
When the fork function is called by the current process, the kernel creates various data structures for the new process and assigns it a unique PID. To create the virtual memory for the new process, it creates exact copies of the current process¡¯s mm_struct, area structs, and page tables. It .ags each page in both processes as read-only, and .ags each area struct in both processes as private copy-on-write. 
When the fork returns in the new process, the new process now has an exact copy of the virtual memory as it existed when the fork was called. When either of the processes performs any subsequent writes, the copy-on-write mechanism creates new pages, thus preserving the abstraction of a private address space for each process. 
9.8.3 The execve Function Revisited 
Virtual memory and memory mapping also play key roles in the process of loading programs into memory. Now that we understand these concepts, we can under-stand how the execve function really loads and executes programs. Suppose that the program running in the current process makes the following call: 
Execve("a.out", NULL, NULL); 
As you learned in Chapter 8, the execve function loads and runs the program contained in the executable object .le a.out within the current process, effectively replacing the current program with the a.out program. Loading and running a.out requires the following steps: 
. Delete existing user areas. Delete the existing area structs in the user portion of the current process¡¯s virtual address. 
. Map private areas. Create new area structs for the text, data, bss, and stack areas of the new program. All of these new areas are private copy-on-write. The text and data areas are mapped to the text and data sections of the a.out .le. The bss area is demand-zero, mapped to an anonymous .le whose size is contained in a.out. The stack and heap area are also demand-zero, initially of zero-length. Figure 9.31 summarizes the different mappings of the private areas. 
. Map shared areas. If the a.out program was linked with shared objects, such as the standard C library libc.so, then these objects are dynamically linked into the program, and then mapped into the shared region of the user¡¯s virtual address space. 
. Set the program counter (PC). The last thing that execve does is to set the program counter in the current process¡¯s context to point to the entry point in the text area. 
The next time this process is scheduled, it will begin execution from the entry point. Linux will swap in code and data pages as needed. 
9.8.4 User-level Memory Mapping with the mmap Function 
Unix processes can use the mmap function to create new areas of virtual memory and to map objects into these areas. 
Figure 9.31 

How the loader maps the areas of the user address space. 
libc.so 

a.out 

0 

Private, demand-zero 
Shared, file-backed 
Private, demand-zero Private, demand-zero Private, file-backed 

#include <unistd.h> #include <sys/mman.h> 
void *mmap(void *start, size_t length, int prot, int flags, int fd, off_t offset); 
Returns: pointer to mapped area if OK, MAP_FAILED (.1) on error 
The mmap function asks the kernel to create a new virtual memory area, preferably one that starts at address start, and to map a contiguous chunk of the object speci.ed by .le descriptor fd to the new area. The contiguous object chunk has a size of length bytes and starts at an offset of offset bytes from the beginning of the .le. The start address is merely a hint, and is usually speci.ed as NULL. For our purposes, we will always assume a NULL start address. Figure 9.32 depicts the meaning of these arguments. 
The prot argument contains bits that describe the access permissions of the newly mapped virtual memory area (i.e., the vm_prot bits in the corresponding area struct). 
.  PROT_EXEC: Pages in the area consist of instructions that may be executed  
by the CPU.  
.  PROT_READ: Pages in the area may be read.  
.  PROT_WRITE: Pages in the area may be written.  
.  PROT_NONE: Pages in the area cannot be accessed.  

Figure 9.32 

Visual interpretation of mmap arguments. 

length (bytes) 
start length (bytes) 
(or address chosen by the
offset 
kernel)(bytes) 
0 
0 
Disk file specified by Process file descriptor fd virtual memory 
The flags argument consists of bits that describe the type of the mapped object. If the MAP_ANON .ag bit is set, then the backing store is an anonymous object and the corresponding virtual pages are demand-zero. MAP_PRIVATE indicates a private copy-on-write object, and MAP_SHARED indicates a shared object. For example, 
bufp = Mmap(-1, size, PROT_READ, MAP_PRIVATE|MAP_ANON, 0, 0); 
asks the kernel to create a new read-only, private, demand-zero area of virtual memory containing size bytes. If the call is successful, then bufp contains the address of the new area. 
The munmap function deletes regions of virtual memory: 
#include <unistd.h> #include <sys/mman.h> 
int munmap(void *start, size_t length); 
Returns: 0 if OK, .1 on error 
The munmap function deletes the area starting at virtual address start and consist-ing of the next length bytes. Subsequent references to the deleted region result in segmentation faults. 
Practice Problem 9.5 
Write a C program mmapcopy.c that uses mmap to copy an arbitrary-sized disk .le to stdout. The name of the input .le should be passed as a command line argument. 
9.9 
Dynamic 
Memory 
Allocation 

While it is certainly possible to use the low-level mmap and munmap functions to create and delete areas of virtual memory, C programmers typically .nd it more 
Figure 9.33 
The heap. 

Top of the heap (brk ptr) 
0 

convenient and more portable to use a dynamic memory allocator when they need to acquire additional virtual memory at run time. 
A dynamic memory allocator maintains an area of a process¡¯s virtual memory known as the heap (Figure 9.33). Details vary from system to system, but without loss of generality, we will assume that the heap is an area of demand-zero mem-ory that begins immediately after the uninitialized bss area and grows upward (toward higher addresses). For each process, the kernel maintains a variable brk (pronounced ¡°break¡±) that points to the top of the heap. 
An allocator maintains the heap as a collection of various-sized blocks. Each block is a contiguous chunk of virtual memory that is either allocated or free.An allocated block has been explicitly reserved for use by the application. A free block is available to be allocated. A free block remains free until it is explicitly allocated by the application. An allocated block remains allocated until it is freed, either explicitly by the application, or implicitly by the memory allocator itself. 
Allocators come in two basic styles. Both styles require the application to explicitly allocate blocks. They differ about which entity is responsible for freeing allocated blocks. 
. Explicit allocators require the application to explicitly free any allocated blocks. For example, the C standard library provides an explicit allocator called the malloc package. C programs allocate a block by calling the malloc function, and free a block by calling the free function. The new and delete calls in C++ are comparable. 
. Implicit allocators, on the other hand, require the allocator to detect when an allocated block is no longer being used by the program and then free the block. Implicit allocators are also known as garbage collectors, and the process of automatically freeing unused allocated blocks is known as garbage collection. For example, higher-level languages such as Lisp, ML, and Java rely on garbage collection to free allocated blocks. 
The remainder of this section discusses the design and implementation of explicit allocators. We will discuss implicit allocators in Section 9.10. For concrete-ness, our discussion focuses on allocators that manage heap memory. However, you should be aware that memory allocation is a general idea that arises in a vari-ety of contexts. For example, applications that do intensive manipulation of graphs will often use the standard allocator to acquire a large block of virtual memory, and then use an application-speci.c allocator to manage the memory within that block as the nodes of the graph are created and destroyed. 
9.9.1 The malloc and free Functions 
The C standard library provides an explicit allocator known as the malloc package. Programs allocate blocks from the heap by calling the malloc function. 
#include <stdlib.h> void *malloc(size_t size); 
Returns: ptr to allocated block if OK, NULL on error 
The malloc function returns a pointer to a block of memory of at least size bytes that is suitably aligned for any kind of data object that might be contained in the block. On the Unix systems that we are familiar with, malloc returns a block that is aligned to an 8-byte (double word) boundary. 

Aside How big is a word? 
Recall from our discussion of machine code in Chapter 3 that Intel refers to 4-byte objects as double words. However, throughout this section, we will assume that words are 4-byte objects and that double words are 8-byte objects, which is consistent with conventional terminology. 
If malloc encounters a problem (e.g., the program requests a block of memory that is larger than the available virtual memory), then it returns NULL and sets errno. Malloc does not initialize the memory it returns. Applications that want initialized dynamic memory can use calloc, a thin wrapper around the malloc function that initializes the allocated memory to zero. Applications that want to change the size of a previously allocated block can use the realloc function. 
Dynamic memory allocators such as malloc can allocate or deallocate heap 
memory explicitly by using the mmap and munmap functions, or they can use the 
sbrk function: 

#include <unistd.h> void *sbrk(intptr_t incr); 
Returns: old brk pointer on success, .1 on error 

The sbrk function grows or shrinks the heap by adding incr to the kernel¡¯s brk pointer. If successful, it returns the old value of brk, otherwise it returns .1 and sets errno to ENOMEM. If incr is zero, then sbrk returns the current value of brk. Calling sbrk with a negative incr is legal but tricky because the return value (the old value of brk) points to abs(incr) bytes past the new top of the heap. 
Programs free allocated heap blocks by calling the free function. 
#include <stdlib.h> void free(void *ptr); 
Returns: nothing 

The ptr argument must point to the beginning of an allocated block that was obtained from malloc, calloc,or realloc. If not, then the behavior of free is unde.ned. Even worse, since it returns nothing, free gives no indication to the application that something is wrong. As we shall see in Section 9.11, this can produce some baf.ing run-time errors. 
Figure 9.34 shows how an implementation of malloc and free might manage a (very) small heap of 16 words for a C program. Each box represents a 4-byte word. The heavy-lined rectangles correspond to allocated blocks (shaded) and free blocks (unshaded). Initially, the heap consists of a single 16-word double-word aligned free block. 
. Figure 9.34(a): The program asks for a four-word block. Malloc responds by carving out a four-word block from the front of the free block and returning a pointer to the .rst word of the block. 
. Figure 9.34(b): The program requests a .ve-word block. Malloc responds by allocating a six-word block from the front of the free block. In this example, malloc pads the block with an extra word in order to keep the free block aligned on a double-word boundary. 
. Figure 9.34(c): The program requests a six-word block and malloc responds by carving out a six-word block from the free block. 
. Figure 9.34(d): The program frees the six-word block that was allocated in Figure 9.34(b). Notice that after the call to free returns, the pointer p2 still points to the freed block. It is the responsibility of the application not to use p2 again until it is reinitialized by a new call to malloc. 

(a) p1 = malloc(4*sizeof(int)) 

(b) p2 = malloc(5*sizeof(int)) 

(c) p3 = malloc(6*sizeof(int)) 

(d) free(p2) 

(e) p4 = malloc(2*sizeof(int)) 
Figure 9.34 Allocating and freeing blocks with malloc and free. Each square corresponds to a word. Each heavy rectangle corresponds to a block. Allocated blocks are shaded. Padded regions of allocated blocks are shaded with stripes. Free blocks are unshaded. Heap addresses increase from left to right. 
. Figure 9.34(e): The program requests a two-word block. In this case, malloc allocates a portion of the block that was freed in the previous step and returns a pointer to this new block. 
9.9.2 Why Dynamic Memory Allocation? 
The most important reason that programs use dynamic memory allocation is that often they do not know the sizes of certain data structures until the program actually runs. For example, suppose we are asked to write a C program that reads a list of n ASCII integers, one integer per line, from stdin into a C array. The input consists of the integer n, followed by the n integers to be read and stored into the array. The simplest approach is to de.ne the array statically with some hard-coded maximum array size: 
1  #include "csapp.h"  
2  #define MAXN 15213  
3  
4  int array[MAXN];  


Section 9.9 Dynamic Memory Allocation 817 

5  
6  int  main()  
7  {  
8  int  i,  n;  
9  
10  scanf("%d",  &n);  
11  if  (n  >  MAXN)  
12  app_error("Input  file  too  big");  
13  for  (i=0;i<n;  i++)  
14  scanf("%d",  &array[i]);  
15  exit(0);  
16  }  

Allocating arrays with hard-coded sizes like this is often a bad idea. The value of MAXN is arbitrary and has no relation to the actual amount of available virtual memory on the machine. Further, if the user of this program wanted to read a .le that was larger than MAXN, the only recourse would be to recompile the program with a larger value of MAXN. While not a problem for this simple example, the presence of hard-coded array bounds can become a maintenance nightmare for large software products with millions of lines of code and numerous users. 
A better approach is to allocate the array dynamically, at run time, after the value of n becomes known. With this approach, the maximum size of the array is limited only by the amount of available virtual memory. 
1 #include "csapp.h" 2 3 int main() 4 { 5 int *array, i, n; 6 7 scanf("%d", &n); 8 array = (int *)Malloc(n * sizeof(int)); 9 for (i=0;i<n; i++) 
10 scanf("%d", &array[i]); 11 exit(0); 
12 } 

Dynamic memory allocation is a useful and important programming tech-nique. However, in order to use allocators correctly and ef.ciently, programmers need to have an understanding of how they work. We will discuss some of the grue-some errors that can result from the improper use of allocators in Section 9.11. 
9.9.3 Allocator Requirements and Goals 
Explicit allocators must operate within some rather stringent constraints. 
. Handling arbitrary request sequences. An application can make an arbitrary sequence of allocate and free requests, subject to the constraint that each free request must correspond to a currently allocated block obtained from a previous allocate request. Thus, the allocator cannot make any assumptions about the ordering of allocate and free requests. For example, the allocator cannot assume that all allocate requests are accompanied by a matching free request, or that matching allocate and free requests are nested. 
. Making immediate responses to requests. The allocator must respond imme-diately to allocate requests. Thus, the allocator is not allowed to reorder or buffer requests in order to improve performance. 
. Using only the heap. In order for the allocator to be scalable, any non-scalar data structures used by the allocator must be stored in the heap itself. 
. Aligning blocks (alignment requirement). The allocator must align blocks in such a way that they can hold any type of data object. On most systems, this means that the block returned by the allocator is aligned on an 8-byte (double-word) boundary. 
. Not modifying allocated blocks.Allocators can only manipulate or change free blocks. In particular, they are not allowed to modify or move blocks once they are allocated. Thus, techniques such as compaction of allocated blocks are not permitted. 
Working within these constraints, the author of an allocator attempts to meet 
the often con.icting performance goals of maximizing throughput and memory 
utilization. 
. Goal 1: Maximizing throughput. Given some sequence of nallocate and free requests 
R0,R1,...,Rk,...,R
n.1 
we would like to maximize an allocator¡¯s throughput, which is de.ned as the number of requests that it completes per unit time. For example, if an allo-cator completes 500 allocate requests and 500 free requests in 1 second, then its throughput is 1,000 operations per second. In general, we can maximize throughput by minimizing the average time to satisfy allocate and free re-quests. As we¡¯ll see, it is not too dif.cult to develop allocators with reasonably good performance where the worst-case running time of an allocate request is linear in the number of free blocks and the running time of a free request is constant. 
. Goal 2: Maximizing memory utilization.Naive programmers often incorrectly assume that virtual memory is an unlimited resource. In fact, the total amount of virtual memory allocated by all of the processes in a system is limited by the amount of swap space on disk. Good programmers know that virtual memory is a .nite resource that must be used ef.ciently. This is especially true for a dynamic memory allocator that might be asked to allocate and free large blocks of memory. 
There are a number of ways to characterize how ef.ciently an allocator uses the heap. In our experience, the most useful metric is peak utilization.As 

before, we are given some sequence of nallocate and free requests R0,R1,...,Rk,...,R
n.1 

If an application requests a block of pbytes, then the resulting allocated block has a payload of p bytes. After request R has completed, let the aggregate
k 

payload, denoted Pk, be the sum of the payloads of the currently allocated blocks, and let H denote the current (monotonically nondecreasing) size of 
k 

the heap. Then the peak utilization over the .rst k requests, denoted by U ,is 
k 
given by 
maxi¡ÜkP
i 
Uk = 
H
k 

The objective of the allocator then is to maximize the peak utilization Un.1 over the entire sequence. As we will see, there is a tension between maximiz-ing throughput and utilization. In particular, it is easy to write an allocator that maximizes throughput at the expense of heap utilization. One of the in-teresting challenges in any allocator design is .nding an appropriate balance between the two goals. 
Aside Relaxing the monotonicity assumption 
We could relax the monotonically nondecreasing assumption in our de.nition of U and allow the heap 
k to grow up and down by letting Hk be the highwater mark over the .rst krequests. 
9.9.4 Fragmentation 
The primary cause of poor heap utilization is a phenomenon known as fragmen-tation, which occurs when otherwise unused memory is not available to satisfy allocate requests. There are two forms of fragmentation: internal fragmentation and external fragmentation. 
Internal fragmentation occurs when an allocated block is larger than the pay-load. This might happen for a number of reasons. For example, the implementation of an allocator might impose a minimum size on allocated blocks that is greater than some requested payload. Or, as we saw in Figure 9.34(b), the allocator might increase the block size in order to satisfy alignment constraints. 
Internal fragmentation is straightforward to quantify. It is simply the sum of the differences between the sizes of the allocated blocks and their payloads. Thus, at any point in time, the amount of internal fragmentation depends only on the pattern of previous requests and the allocator implementation. 
External fragmentation occurs when there is enough aggregate free memory to satisfy an allocate request, but no single free block is large enough to handle the request. For example, if the request in Figure 9.34(e) were for six words rather than two words, then the request could not be satis.ed without requesting additional virtual memory from the kernel, even though there are six free words remaining in the heap. The problem arises because these six words are spread over two free blocks. 
External fragmentation is much more dif.cult to quantify than internal frag-mentation because it depends not only on the pattern of previous requests and the allocator implementation, but also on the pattern of future requests. For example, suppose that after k requests all of the free blocks are exactly four words in size. Does this heap suffer from external fragmentation? The answer depends on the pattern of future requests. If all of the future allocate requests are for blocks that are smaller than or equal to four words, then there is no external fragmentation. On the other hand, if one or more requests ask for blocks larger than four words, then the heap does suffer from external fragmentation. 
Since external fragmentation is dif.cult to quantify and impossible to predict, 
allocators typically employ heuristics that attempt to maintain small numbers of 
larger free blocks rather than large numbers of smaller free blocks. 
9.9.5 Implementation Issues 
The simplest imaginable allocator would organize the heap as a large array of bytes and a pointer p that initially points to the .rst byte of the array. To allocate size bytes, malloc would save the current value of p on the stack, increment p by size, and return the old value of p to the caller. Free would simply return to the caller without doing anything. 
This naive allocator is an extreme point in the design space. Since each malloc and free execute only a handful of instructions, throughput would be extremely good. However, since the allocator never reuses any blocks, memory utilization would be extremely bad. A practical allocator that strikes a better balance between throughput and utilization must consider the following issues: 
. Free block organization: How do we keep track of free blocks? . Placement: How do we choose an appropriate free block in which to place a newly allocated block? . Splitting: After we place a newly allocated block in some free block, what do we do with the remainder of the free block? . Coalescing: What do we do with a block that has just been freed? 
The rest of this section looks at these issues in more detail. Since the basic techniques of placement, splitting, and coalescing cut across many different free block organizations, we will introduce them in the context of a simple free block organization known as an implicit free list. 
9.9.6 Implicit Free Lists 
Any practical allocator needs some data structure that allows it to distinguish block boundaries and to distinguish between allocated and free blocks. Most allocators embed this information in the blocks themselves. One simple approach is shown in Figure 9.35. 
31 Header 3 210 

a = 1: Allocated 
malloc returns a a = 0: Free 
pointer to the beginning of the payload The block size includes the header, payload, and any padding 

Figure 9.35 Format of a simple heap block. 
In this case, a block consists of a one-word header, the payload, and possibly some additional padding.The header encodes the block size (including the header and any padding) as well as whether the block is allocated or free. If we impose a double-word alignment constraint, then the block size is always a multiple of eight and the 3 low-order bits of the block size are always zero. Thus, we need to store only the 29 high-order bits of the block size, freeing the remaining 3 bits to encode other information. In this case, we are using the least signi.cant of these bits (the allocated bit) to indicate whether the block is allocated or free. For example, suppose we have an allocated block with a block size of 24 (0x18) bytes. Then its header would be 
0x00000018 | 0x1 = 0x00000019 
Similarly, a free block with a block size of 40 (0x28) bytes would have a header of 
0x00000028 | 0x0 = 0x00000028 
The header is followed by the payload that the application requested when it called malloc. The payload is followed by a chunk of unused padding that can be any size. There are a number of reasons for the padding. For example, the padding might be part of an allocator¡¯s strategy for combating external fragmentation. Or it might be needed to satisfy the alignment requirement. 
Given the block format in Figure 9.35, we can organize the heap as a sequence of contiguous allocated and free blocks, as shown in Figure 9.36. 

We call this organization an implicit free list because the free blocks are linked implicitly by the size .elds in the headers. The allocator can indirectly traverse the entire set of free blocks by traversing all of the blocks in the heap. Notice that we need some kind of specially marked end block, in this example a terminating header with the allocated bit set and a size of zero. (As we will see in Section 9.9.12, setting the allocated bit simpli.es the coalescing of free blocks.) 
The advantage of an implicit free list is simplicity. A signi.cant disadvantage is that the cost of any operation, such as placing allocated blocks, that requires a search of the free list will be linear in the total number of allocated and free blocks in the heap. 
It is important to realize that the system¡¯s alignment requirement and the allocator¡¯s choice of block format impose a minimum block size on the allocator. No allocated or free block may be smaller than this minimum. For example, if we assume a double-word alignment requirement, then the size of each block must be a multiple of two words (8 bytes). Thus, the block format in Figure 9.35 induces a minimum block size of two words: one word for the header, and another to maintain the alignment requirement. Even if the application were to request a single byte, the allocator would still create a two-word block. 
Practice Problem 9.6 
Determine the block sizes and header values that would result from the following sequence of malloc requests. Assumptions: (1) The allocator maintains double-word alignment, and uses an implicit free list with the block format from Fig-ure 9.35. (2) Block sizes are rounded up to the nearest multiple of 8 bytes. 
Request Block size (decimal bytes) Block header (hex) 
malloc(1) 
malloc(5) malloc(12) malloc(13) 
9.9.7 Placing Allocated Blocks 
When an application requests a block of k bytes, the allocator searches the free list for a free block that is large enough to hold the requested block. The manner in which the allocator performs this search is determined by the placement policy. Some common policies are .rst .t, next .t, and best .t. 
First .t searches the free list from the beginning and chooses the .rst free block that .ts. Next .t is similar to .rst .t, but instead of starting each search at the beginning of the list, it starts each search where the previous search left off. Best .t examines every free block and chooses the free block with the smallest size that .ts. 
An advantage of .rst .t is that it tends to retain large free blocks at the end of the list. A disadvantage is that it tends to leave ¡°splinters¡± of small free blocks toward the beginning of the list, which will increase the search time for larger blocks. Next .t was .rst proposed by Donald Knuth as an alternative to .rst .t, motivated by the idea that if we found a .t in some free block the last time, there is a good chance that we will .nd a .t the next time in the remainder of the block. Next .t can run signi.cantly faster than .rst .t, especially if the front of the list becomes littered with many small splinters. However, some studies suggest that next .t suffers from worse memory utilization than .rst .t. Studies have found that best .t generally enjoys better memory utilization than either .rst .t or next .t. However, the disadvantage of using best .t with simple free list organizations such as the implicit free list, is that it requires an exhaustive search of the heap. Later, we will look at more sophisticated segregated free list organizations that approximate a best-.t policy without an exhaustive search of the heap. 


9.9.8 Splitting Free Blocks 
Once the allocator has located a free block that .ts, it must make another policy decision about how much of the free block to allocate. One option is to use the entire free block. Although simple and fast, the main disadvantage is that it introduces internal fragmentation. If the placement policy tends to produce good .ts, then some additional internal fragmentation might be acceptable. 
However, if the .t is not good, then the allocator will usually opt to split the free block into two parts. The .rst part becomes the allocated block, and the remainder becomes a new free block. Figure 9.37 shows how the allocator might split the eight-word free block in Figure 9.36 to satisfy an application¡¯s request for three words of heap memory. 
9.9.9 Getting Additional Heap Memory 
What happens if the allocator is unable to .nd a .t for the requested block? One option is to try to create some larger free blocks by merging (coalescing) free blocks that are physically adjacent in memory (next section). However, if this does not yield a suf.ciently large block, or if the free blocks are already maximally coalesced, then the allocator asks the kernel for additional heap memory by calling the sbrk function. The allocator transforms the additional memory into one large free block, inserts the block into the free list, and then places the requested block in this new free block. 

9.9.10 Coalescing Free Blocks 
When the allocator frees an allocated block, there might be other free blocks that are adjacent to the newly freed block. Such adjacent free blocks can cause a phenomenon known as false fragmentation, where there is a lot of available free memory chopped up into small, unusable free blocks. For example, Figure 9.38 shows the result of freeing the block that was allocated in Figure 9.37. The result is two adjacent free blocks with payloads of three words each. As a result, a subsequent request for a payload of four words would fail, even though the aggregate size of the two free blocks is large enough to satisfy the request. 
To combat false fragmentation, any practical allocator must merge adjacent free blocks in a process known as coalescing. This raises an important policy decision about when to perform coalescing. The allocator can opt for immediate coalescing by merging any adjacent blocks each time a block is freed. Or it can opt for deferred coalescing by waiting to coalesce free blocks at some later time. For example, the allocator might defer coalescing until some allocation request fails, and then scan the entire heap, coalescing all free blocks. 
Immediate coalescing is straightforward and can be performed in constant time, but with some request patterns it can introduce a form of thrashing where a block is repeatedly coalesced and then split soon thereafter. For example, in Fig-ure 9.38 a repeated pattern of allocating and freeing a three-word block would introduce a lot of unnecessary splitting and coalescing. In our discussion of allo-cators, we will assume immediate coalescing, but you should be aware that fast allocators often opt for some form of deferred coalescing. 
9.9.11 Coalescing with Boundary Tags 
How does an allocator implement coalescing? Let us refer to the block we want to free as the current block. Then coalescing the next free block (in memory) is straightforward and ef.cient. The header of the current block points to the header of the next block, which can be checked to determine if the next block is free. If so, its size is simply added to the size of the current header and the blocks are coalesced in constant time. 
But how would we coalesce the previous block? Given an implicit free list of blocks with headers, the only option would be to search the entire list, remember-ing the location of the previous block, until we reached the current block. With an 
31 3210
Figure 9.39 
Block size  a/f  
Payload (allocated block only)  
Padding (optional)  
Block size  a/f  

a = 001: Allocated

Format of heap block that 
Header 
a = 000: Free 

uses a boundary tag. 
Footer 

implicit free list, this means that each call to free would require time linear in the size of the heap. Even with more sophisticated free list organizations, the search time would not be constant. 
Knuth developed a clever and general technique, known as boundary tags, that allows for constant-time coalescing of the previous block. The idea, which is shown in Figure 9.39, is to add a footer (the boundary tag) at the end of each block, where the footer is a replica of the header. If each block includes such a footer, then the allocator can determine the starting location and status of the previous block by inspecting its footer, which is always one word away from the start of the current block. 
Consider all the cases that can exist when the allocator frees the current block: 
1. 
The previous and next blocks are both allocated. 

2. 
The previous block is allocated and the next block is free. 

3. 
The previous block is free and the next block is allocated. 

4. 
The previous and next blocks are both free. 


Figure 9.40 shows how we would coalesce each of the four cases. In case 1, both adjacent blocks are allocated and thus no coalescing is possible. So the status of the current block is simply changed from allocated to free. In case 2, the current block is merged with the next block. The header of the current block and the footer of the next block are updated with the combined sizes of the current and next blocks. In case 3, the previous block is merged with the current block. The header of the previous block and the footer of the current block are updated with the combined sizes of the two blocks. In case 4, all three blocks are merged to form a single free block, with the header of the previous block and the footer of the next block updated with the combined sizes of the three blocks. In each case, the coalescing is performed in constant time. 
The idea of boundary tags is a simple and elegant one that generalizes to many different types of allocators and free list organizations. However, there is a potential disadvantage. Requiring each block to contain both a header and a footer can introduce signi.cant memory overhead if an application manipulates 
m1  a  

m1  a  
n  a  

n  a  
m2  a  

m2  a  


m1  a  

m1  a  
n  f  

n  f  
m2  a  

m2  a  

m1  a  

m1  a  
n  a  

n  a  
m2  f  

m2  f  


m1  a  

m1  a  
nm2  f  


nm2  f  


Case 1 Case 2 



m1  f  

m1  f  
n  a  

n  a  
m2  a  

m2  a  
Figure 9.40 Coalescing with boundary tags. Case 1: prev and next allocated. Case 2: prev allocated, next free. Case 3: prev free, next allocated. Case 4: next and prev free. 


m1  f  

m1  f  
n  a  

n  a  
m2  f  

m2  f  

nm1m2  f  

nm1m2  f  

nm1  f  

nm1  f  
m2  a  

m2  a  


Case 3 Case 4 
many small blocks. For example, if a graph application dynamically creates and destroys graph nodes by making repeated calls to malloc and free, and each graph node requires only a couple of words of memory, then the header and the footer will consume half of each allocated block. 
Fortunately, there is a clever optimization of boundary tags that eliminates the need for a footer in allocated blocks. Recall that when we attempt to coalesce the current block with the previous and next blocks in memory, the size .eld in the footer of the previous block is only needed if the previous block is free.Ifwe were to store the allocated/free bit of the previous block in one of the excess low-order bits of the current block, then allocated blocks would not need footers, and we could use that extra space for payload. Note, however, that free blocks still need footers. 
Practice Problem 9.7 
Determine the minimum block size for each of the following combinations of alignment requirements and block formats. Assumptions: Implicit free list, zero-sized payloads are not allowed, and headers and footers are stored in 4-byte words. 

Section 9.9  Dynamic Memory Allocation  827  
Alignment Single word Single word Double word Double word  Allocated block Header and footer Header, but no footer Header and footer Header, but no footer  Free block Header and footer Header and footer Header and footer Header and footer  Minimum block size (bytes)  

9.9.12 Putting It Together: Implementing a Simple Allocator 
Building an allocator is a challenging task. The design space is large, with nu-merous alternatives for block format and free list format, as well as placement, splitting, and coalescing policies. Another challenge is that you are often forced to program outside the safe, familiar con.nes of the type system, relying on the error-prone pointer casting and pointer arithmetic that is typical of low-level sys-tems programming. 
While allocators do not require enormous amounts of code, they are subtle and unforgiving. Students familiar with higher-level languages such as C++ or Java often hit a conceptual wall when they .rst encounter this style of programming. To help you clear this hurdle, we will work through the implementation of a simple allocator based on an implicit free list with immediate boundary-tag coalescing. The maximum block size is 232 = 4 GB. The code is 64-bit clean, running without modi.cation in 32-bit (gcc -m32) or 64-bit (gcc -m64) processes. 
General Allocator Design 
Our allocator uses a model of the memory system provided by the memlib.c package shown in Figure 9.41. The purpose of the model is to allow us to run our allocator without interfering with the existing system-level malloc package. The mem_init function models the virtual memory available to the heap as a large, double-word aligned array of bytes. The bytes between mem_heap and mem_ brk represent allocated virtual memory. The bytes following mem_brk represent unallocated virtual memory. The allocator requests additional heap memory by calling the mem_sbrk function, which has the same interface as the system¡¯s sbrk function, as well as the same semantics, except that it rejects requests to shrink the heap. 
The allocator itself is contained in a source .le (mm.c) that users can compile and link into their applications. The allocator exports three functions to applica-tion programs: 
1 extern int mm_init(void); 
2 extern void *mm_malloc (size_t size); 
3 extern void mm_free (void *ptr); 
The mm_init function initializes the allocator, returning 0 if successful and .1 otherwise. The mm_malloc and mm_free functions have the same interfaces and semantics as their system counterparts. The allocator uses the block format 
code/vm/malloc/memlib.c 

1 /* Private global variables */ 2 static char *mem_heap; /* Points to first byte of heap */ 3 static char *mem_brk; /* Points to last byte of heap plus 1 */ 4 static char *mem_max_addr; /* Max legal heap addr plus 1*/ 5 6 /* 7 * mem_init -Initialize the memory system model 8 */ 9 void mem_init(void) 
10 { 

11 mem_heap = (char *)Malloc(MAX_HEAP); 
12 mem_brk = (char *)mem_heap; 
13 mem_max_addr = (char *)(mem_heap + MAX_HEAP); 
14 
} 15 16 /* 17 * mem_sbrk -Simple model of the sbrk function. Extends the heap 18 * by incr bytes and returns the start address of the new area. In 19 * this model, the heap cannot be shrunk. 20 */ 21 void *mem_sbrk(int incr) 22 { 23 char *old_brk = mem_brk; 24 25 if ( (incr < 0) || ((mem_brk + incr) > mem_max_addr)) { 26 errno = ENOMEM; 27 fprintf(stderr, "ERROR: mem_sbrk failed. Ran out of memory...\n"); 28 return (void *)-1; 

29 
} 30 mem_brk += incr; 31 return (void *)old_brk; 


32 } 
code/vm/malloc/memlib.c 
Figure 9.41 memlib.c: Memory system model. 
shown in Figure 9.39. The minimum block size is 16 bytes. The free list is organized as an implicit free list, with the invariant form shown in Figure 9.42. 
The .rst word is an unused padding word aligned to a double-word boundary. The padding is followed by a special prologue block, which is an 8-byte allocated block consisting of only a header and a footer. The prologue block is created during initialization and is never freed. Following the prologue block are zero or more regular blocks that are created by calls to malloc or free. The heap 

Prologue Regular Regular Regular Epilogue block block 1 block 2 block n block hdr 

always ends with a special epilogue block, which is a zero-sized allocated block that consists of only a header. The prologue and epilogue blocks are tricks that eliminate the edge conditions during coalescing. The allocator uses a single private (static) global variable (heap_listp) that always points to the prologue block. (As a minor optimization, we could make it point to the next block instead of the prologue block.) 
Basic Constants and Macros for Manipulating the Free List 
Figure 9.43 shows some basic constants and macros that we will use throughout the allocator code. Lines 2¨C4 de.ne some basic size constants: the sizes of words (WSIZE) and double words (DSIZE), and the size of the initial free block and the default size for expanding the heap (CHUNKSIZE). 
Manipulating the headers and footers in the free list can be troublesome because it demands extensive use of casting and pointer arithmetic. Thus, we .nd it helpful to de.ne a small set of macros for accessing and traversing the free list (lines 9¨C25). The PACK macro (line 9) combines a size and an allocate bit and returns a value that can be stored in a header or footer. 
The GET macro (line 12) reads and returns the word referenced by argu-ment p. The casting here is crucial. The argument p is typically a (void *) pointer, which cannot be dereferenced directly. Similarly, the PUT macro (line 13) stores val in the word pointed at by argument p. 
The GET_SIZE and GET_ALLOC macros (lines 16¨C17) return the size and allocated bit, respectively, from a header or footer at address p. The remaining macros operate on block pointers (denoted bp) that point to the .rst payload byte. Given a block pointer bp, the HDRP and FTRP macros (lines 20¨C21) return pointers to the block header and footer, respectively. The NEXT_BLKP and PREV_BLKP macros (lines 24¨C25) return the block pointers of the next and previous blocks, respectively. 
The macros can be composed in various ways to manipulate the free list. For example, given a pointer bp to the current block, we could use the following line of code to determine the size of the next block in memory: 
size_t size = GET_SIZE(HDRP(NEXT_BLKP(bp))); 
code/vm/malloc/mm.c 

1 /* Basic constants and macros */ 2 #define WSIZE 4 /* Word and header/footer size (bytes) */ 3 #define DSIZE 8 /* Double word size (bytes) */ 4 #define CHUNKSIZE (1<<12) /* Extend heap by this amount (bytes) */ 5 6 #define MAX(x, y) ((x) > (y)? (x) : (y)) 7 8 /* Pack a size and allocated bit into a word */ 9 #define PACK(size, alloc) ((size) | (alloc)) 
10 11 /* Read and write a word at address p */ 12 #define GET(p) (*(unsigned int *)(p)) 13 #define PUT(p, val) (*(unsigned int *)(p) = (val)) 14 15 /* Read the size and allocated fields from address p */ 16 #define GET_SIZE(p) (GET(p) & ~0x7) 17 #define GET_ALLOC(p) (GET(p) & 0x1) 18 19 /* Given block ptr bp, compute address of its header and footer */ 20 #define HDRP(bp) ((char *)(bp) -WSIZE) 21 #define FTRP(bp) ((char *)(bp) + GET_SIZE(HDRP(bp)) -DSIZE) 22 23 /* Given block ptr bp, compute address of next and previous blocks */ 24 #define NEXT_BLKP(bp) ((char *)(bp) + GET_SIZE(((char *)(bp) -WSIZE))) 25 #define PREV_BLKP(bp) ((char *)(bp) -GET_SIZE(((char *)(bp) -DSIZE))) 
code/vm/malloc/mm.c 
Figure 9.43 Basic constants and macros for manipulating the free list. 
Creating the Initial Free List 
Before calling mm_malloc or mm_free, the application must initialize the heap by calling the mm_init function (Figure 9.44). The mm_init function gets four words from the memory system and initializes them to create the empty free list (lines 4¨C10). It then calls the extend_heap function (Figure 9.45), which extends the heap by CHUNKSIZE bytes and creates the initial free block. At this point, the allocator is initialized and ready to accept allocate and free requests from the application. 
The extend_heap function is invoked in two different circumstances: (1) when the heap is initialized, and (2) when mm_malloc is unable to .nd a suitable .t. To maintain alignment, extend_heap rounds up the requested size to the nearest multiple of 2 words (8 bytes), and then requests the additional heap space from the memory system (lines 7¨C9). 
The remainder of the extend_heap function (lines 12¨C17) is somewhat subtle. The heap begins on a double-word aligned boundary, and every call to extend_ heap returns a block whose size is an integral number of double words. Thus, every 
code/vm/malloc/mm.c 

1 int mm_init(void) 2 { 3 /* Create the initial empty heap */ 4 if ((heap_listp = mem_sbrk(4*WSIZE)) == (void *)-1) 5 return -1; 6 PUT(heap_listp, 0); /* Alignment padding */ 7 PUT(heap_listp + (1*WSIZE), PACK(DSIZE, 1)); /* Prologue header */ 8 PUT(heap_listp + (2*WSIZE), PACK(DSIZE, 1)); /* Prologue footer */ 9 PUT(heap_listp + (3*WSIZE), PACK(0, 1)); /* Epilogue header */ 
10 heap_listp += (2*WSIZE); 11 12 /* Extend the empty heap with a free block of CHUNKSIZE bytes */ 13 if (extend_heap(CHUNKSIZE/WSIZE) == NULL) 14 return -1; 15 return 0; 
16 } 
code/vm/malloc/mm.c 

Figure 9.44 mm_init: Creates a heap with an initial free block. 
code/vm/malloc/mm.c 

1 static void *extend_heap(size_t words) 2 { 3 char *bp; 4 size_t size; 5 6 /* Allocate an even number of words to maintain alignment */ 7 size = (words % 2) ? (words+1) * WSIZE : words * WSIZE; 8 if ((long)(bp = mem_sbrk(size)) == -1) 9 return NULL; 
10 11 /* Initialize free block header/footer and the epilogue header */ 12 PUT(HDRP(bp), PACK(size, 0)); /* Free block header */ 13 PUT(FTRP(bp), PACK(size, 0)); /* Free block footer */ 14 PUT(HDRP(NEXT_BLKP(bp)), PACK(0, 1)); /* New epilogue header */ 15 16 /* Coalesce if the previous block was free */ 17 return coalesce(bp); 
18 } 
code/vm/malloc/mm.c 

Figure 9.45 extend_heap: Extends the heap with a new free block. 
call to mem_sbrk returns a double-word aligned chunk of memory immediately following the header of the epilogue block. This header becomes the header of the new free block (line 12), and the last word of the chunk becomes the new epilogue block header (line 14). Finally, in the likely case that the previous heap was terminated by a free block, we call the coalesce function to merge the two free blocks and return the block pointer of the merged blocks (line 17). 
Freeing and Coalescing Blocks 
An application frees a previously allocated block by calling the mm_free function (Figure 9.46), which frees the requested block (bp) and then merges adjacent free blocks using the boundary-tags coalescing technique described in Section 9.9.11. 
The code in the coalesce helper function is a straightforward implementation of the four cases outlined in Figure 9.40. There is one somewhat subtle aspect. The free list format we have chosen¡ªwith its prologue and epilogue blocks that are always marked as allocated¡ªallows us to ignore the potentially troublesome edge conditions where the requested block bp is at the beginning or end of the heap. Without these special blocks, the code would be messier, more error prone, and slower, because we would have to check for these rare edge conditions on each and every free request. 
Allocating Blocks 
An application requests a block of size bytes of memory by calling the mm_malloc function (Figure 9.47). After checking for spurious requests, the allocator must adjust the requested block size to allow room for the header and the footer, and to satisfy the double-word alignment requirement. Lines 12¨C13 enforce the minimum block size of 16 bytes: 8 bytes to satisfy the alignment requirement, and 8 more for the overhead of the header and footer. For requests over 8 bytes (line 15), the general rule is to add in the overhead bytes and then round up to the nearest multiple of 8. 
Once the allocator has adjusted the requested size, it searches the free list for a suitable free block (line 18). If there is a .t, then the allocator places the requested block and optionally splits the excess (line 19), and then returns the address of the newly allocated block. 
If the allocator cannot .nd a .t, it extends the heap with a new free block 
(lines 24¨C26), places the requested block in the new free block, optionally splitting 
the block (line 27), and then returns a pointer to the newly allocated block. 
Practice Problem 9.8 
Implement a find_fit function for the simple allocator described in Section 9.9.12. 
static void *find_fit(size_t asize) 
Your solution should perform a .rst-.t search of the implicit free list. 
code/vm/malloc/mm.c 

1 void mm_free(void *bp) 2 { 3 size_t size = GET_SIZE(HDRP(bp)); 4 
PUT(HDRP(bp), PACK(size, 0)); 6 PUT(FTRP(bp), PACK(size, 0)); 7 coalesce(bp); 
8 } 9 

static void *coalesce(void *bp) 11 { 12 size_t prev_alloc = GET_ALLOC(FTRP(PREV_BLKP(bp))); 13 size_t next_alloc = GET_ALLOC(HDRP(NEXT_BLKP(bp))); 14 size_t size = GET_SIZE(HDRP(bp)); 
16 if (prev_alloc && next_alloc) { /* Case 1 */ 17 return bp; 
18 } 19 

else if (prev_alloc && !next_alloc) { /* Case 2 */ 21 size += GET_SIZE(HDRP(NEXT_BLKP(bp))); 22 PUT(HDRP(bp), PACK(size, 0)); 23 PUT(FTRP(bp), PACK(size,0)); 
24 } 

26 else if (!prev_alloc && next_alloc) { /* Case 3 */ 27 size += GET_SIZE(HDRP(PREV_BLKP(bp))); 28 PUT(FTRP(bp), PACK(size, 0)); 29 PUT(HDRP(PREV_BLKP(bp)), PACK(size, 0)); 
bp = PREV_BLKP(bp); 
31 
} 32 33 else{ /*Case4*/ 34 size += GET_SIZE(HDRP(PREV_BLKP(bp))) + 

GET_SIZE(FTRP(NEXT_BLKP(bp))); 36 PUT(HDRP(PREV_BLKP(bp)), PACK(size, 0)); 37 PUT(FTRP(NEXT_BLKP(bp)), PACK(size, 0)); 38 bp = PREV_BLKP(bp); 

39 
} return bp; 


41 } 
code/vm/malloc/mm.c 

Figure 9.46 mm_free: Frees a block and uses boundary-tag coalescing to merge it with any adjacent free blocks in constant time. 
code/vm/malloc/mm.c 
1 void *mm_malloc(size_t size) 2 { 3 size_t asize; /* Adjusted block size */ 4 size_t extendsize; /* Amount to extend heap if no fit */ 5 char *bp; 6 7 /* Ignore spurious requests */ 8 if (size == 0) 9 return NULL; 
10 
11 /* Adjust block size to include overhead and alignment reqs. */ 
12 if (size <= DSIZE) 
13 asize = 2*DSIZE; 
14 else 
15 asize = DSIZE * ((size + (DSIZE) + (DSIZE-1)) / DSIZE); 
16 
17 /* Search the free list for a fit */ 
18 if ((bp = find_fit(asize)) != NULL) { 
19 place(bp, asize); 
20 return bp; 
21 } 22 23 /* No fit found. Get more memory and place the block */ 24 extendsize = MAX(asize,CHUNKSIZE); 25 if ((bp = extend_heap(extendsize/WSIZE)) == NULL) 26 return NULL; 27 place(bp, asize); 28 return bp; 
29 } 
code/vm/malloc/mm.c 
Figure 9.47 mm_malloc: Allocates a block from the free list. 
Practice Problem 9.9 
Implement a place function for the example allocator. 
static void place(void *bp, size_t asize) 
Your solution should place the requested block at the beginning of the free block, splitting only if the size of the remainder would equal or exceed the minimum block size. 

31 3210 31 3210 
Block size  a/f  
Payload  
Padding (optional)  
Block size  a/f  
Figure 9.48 Format of heap blocks that use doubly linked free lists. 


Block size  a/f  
pred (Predecessor)  
succ (Successor)  

Padding (optional)  
Block size  a/f  

Header
Header 

Old payload 
Footer
Footer 

(a) Allocated block (b) Free block 
9.9.13 Explicit Free Lists 
The implicit free list provides us with a simple way to introduce some basic allocator concepts. However, because block allocation time is linear in the total number of heap blocks, the implicit free list is not appropriate for a general-purpose allocator (although it might be .ne for a special-purpose allocator where the number of heap blocks is known beforehand to be small). 
A better approach is to organize the free blocks into some form of explicit data structure. Since by de.nition the body of a free block is not needed by the program, the pointers that implement the data structure can be stored within the bodies of the free blocks. For example, the heap can be organized as a doubly linked free list by including a pred (predecessor) and succ (successor) pointer in each free block, as shown in Figure 9.48. 
Using a doubly linked list instead of an implicit free list reduces the .rst .t allocation time from linear in the total number of blocks to linear in the number of free blocks. However, the time to free a block can be either linear or constant, depending on the policy we choose for ordering the blocks in the free list. 
One approach is to maintain the list in last-in .rst-out (LIFO) order by insert-ing newly freed blocks at the beginning of the list. With a LIFO ordering and a .rst .t placement policy, the allocator inspects the most recently used blocks .rst. In this case, freeing a block can be performed in constant time. If boundary tags are used, then coalescing can also be performed in constant time. 
Another approach is to maintain the list in address order, where the address of each block in the list is less than the address of its successor. In this case, freeing a block requires a linear-time search to locate the appropriate predecessor. The trade-off is that address-ordered .rst .t enjoys better memory utilization than LIFO-ordered .rst .t, approaching the utilization of best .t. 
A disadvantage of explicit lists in general is that free blocks must be large enough to contain all of the necessary pointers, as well as the header and possibly a footer. This results in a larger minimum block size, and increases the potential for internal fragmentation. 
9.9.14 Segregated Free Lists 
As we have seen, an allocator that uses a single linked list of free blocks requires time linear in the number of free blocks to allocate a block. A popular approach for reducing the allocation time, known generally as segregated storage, is to maintain multiple free lists, where each list holds blocks that are roughly the same size. The general idea is to partition the set of all possible block sizes into equivalence classes called size classes. There are many ways to de.ne the size classes. For example, we might partition the block sizes by powers of two: 
...
{1},{2},{3,4},{5.8},,{1025.2048},{2049.4096},{4097.¡Þ} 
Or we might assign small blocks to their own size classes and partition large blocks by powers of two: 
...
{1},{2},{3},,{1023},{1024},{1025.2048},{2049 . 4096},{4097.¡Þ} 
The allocator maintains an array of free lists, with one free list per size class, ordered by increasing size. When the allocator needs a block of size n, it searches the appropriate free list. If it cannot .nd a block that .ts, it searches the next list, and so on. 
The dynamic storage allocation literature describes dozens of variants of seg-regated storage that differ in how they de.ne size classes, when they perform coalescing, when they request additional heap memory from the operating sys-tem, whether they allow splitting, and so forth. To give you a sense of what is possible, we will describe two of the basic approaches: simple segregated storage and segregated .ts. 
Simple Segregated Storage 
With simple segregated storage, the free list for each size class contains same-sized blocks, each the size of the largest element of the size class. For example, if some size class is de.ned as {17.32}, then the free list for that class consists entirely of blocks of size 32. 
To allocate a block of some given size, we check the appropriate free list. If the list is not empty, we simply allocate the .rst block in its entirety. Free blocks are never split to satisfy allocation requests. If the list is empty, the allocator requests a .xed-sized chunk of additional memory from the operating system (typically a multiple of the page size), divides the chunk into equal-sized blocks, and links the blocks together to form the new free list. To free a block, the allocator simply inserts the block at the front of the appropriate free list. 
There are a number of advantages to this simple scheme. Allocating and freeing blocks are both fast constant-time operations. Further, the combination of the same-sized blocks in each chunk, no splitting, and no coalescing means that there is very little per-block memory overhead. Since each chunk has only same-sized blocks, the size of an allocated block can be inferred from its address. Since there is no coalescing, allocated blocks do not need an allocated/free .ag in the header. Thus, allocated blocks require no headers, and since there is no coalescing, they do not require any footers either. Since allocate and free operations insert and delete blocks at the beginning of the free list, the list need only be singly linked instead of doubly linked. The bottom line is that the only required .eld in any block is a one-word succ pointer in each free block, and thus the minimum block size is only one word. 

A signi.cant disadvantage is that simple segregated storage is susceptible to internal and external fragmentation. Internal fragmentation is possible because free blocks are never split. Worse, certain reference patterns can cause extreme external fragmentation because free blocks are never coalesced (Problem 9.10). 
Practice Problem 9.10 
Describe a reference pattern that results in severe external fragmentation in an allocator based on simple segregated storage. 
Segregated Fits 

With this approach, the allocator maintains an array of free lists. Each free list is associated with a size class and is organized as some kind of explicit or implicit list. Each list contains potentially different-sized blocks whose sizes are members of the size class. There are many variants of segregated .ts allocators. Here we describe a simple version. 
To allocate a block, we determine the size class of the request and do a .rst-.t search of the appropriate free list for a block that .ts. If we .nd one, then we (optionally) split it and insert the fragment in the appropriate free list. If we cannot .nd a block that .ts, then we search the free list for the next larger size class. We repeat until we .nd a block that .ts. If none of the free lists yields a block that .ts, then we request additional heap memory from the operating system, allocate the block out of this new heap memory, and place the remainder in the appropriate size class. To free a block, we coalesce and place the result on the appropriate free list. 
The segregated .ts approach is a popular choice with production-quality allocators such as the GNU malloc package provided in the C standard library because it is both fast and memory ef.cient. Search times are reduced because searches are limited to particular parts of the heap instead of the entire heap. Memory utilization can improve because of the interesting fact that a simple .rst-.t search of a segregated free list approximates a best-.t search of the entire heap. 
Buddy Systems 

A buddy system is a special case of segregated .ts where each size class is a power of two. The basic idea is that given a heap of 2m words, we maintain a separate free list for each block size 2k, where 0 ¡Ü k ¡Ü m. Requested block sizes are rounded up to the nearest power of two. Originally, there is one free block of size 2m words. 
To allocate a block of size 2k , we .nd the .rst available block of size 2j , such that k ¡Ü j ¡Ü m.If j = k, then we are done. Otherwise, we recursively split the block in half until j = k. As we perform this splitting, each remaining half (known as a buddy) is placed on the appropriate free list. To free a block of size 2k,we continue coalescing with the free. When we encounter an allocated buddy, we stop the coalescing. 
A key fact about buddy systems is that given the address and size of a block, 
it is easy to compute the address of its buddy. For example, a block of size 32 byes 
with address 
xxx...x00000 
has its buddy at address 
xxx...x10000 
In other words, the addresses of a block and its buddy differ in exactly one bit position. 
The major advantage of a buddy system allocator is its fast searching and coalescing. The major disadvantage is that the power-of-two requirement on the block size can cause signi.cant internal fragmentation. For this reason, buddy system allocators are not appropriate for general-purpose workloads. However, for certain application-speci.c workloads, where the block sizes are known in advance to be powers of two, buddy system allocators have a certain appeal. 
9.10 
Garbage 
Collection 

With an explicit allocator such as the C malloc package, an application allocates and frees heap blocks by making calls to malloc and free. It is the application¡¯s responsibility to free any allocated blocks that it no longer needs. 
Failing to free allocated blocks is a common programming error. For example, 
consider the following C function that allocates a block of temporary storage as 
part of its processing: 
1 void garbage() 2 { 3 int *p = (int *)Malloc(15213); 4 5 return; /* Array p is garbage at this point */ 
6 } 
Since p is no longer needed by the program, it should have been freed before garbage returned. Unfortunately, the programmer has forgotten to free the block. It remains allocated for the lifetime of the program, needlessly occupying heap space that could be used to satisfy subsequent allocation requests. 
A garbage collector is a dynamic storage allocator that automatically frees al-located blocks that are no longer needed by the program. Such blocks are known as garbage (hence the term garbage collector). The process of automatically re-claiming heap storage is known as garbage collection. In a system that supports garbage collection, applications explicitly allocate heap blocks but never explic-itly free them. In the context of a C program, the application calls malloc, but never calls free. Instead, the garbage collector periodically identi.es the garbage blocks and makes the appropriate calls to free to place those blocks back on the free list. 

Garbage collection dates back to Lisp systems developed by John McCarthy at MIT in the early 1960s. It is an important part of modern language systems such as Java, ML, Perl, and Mathematica, and it remains an active and important area of research. The literature describes an amazing number of approaches for garbage collection. We will limit our discussion to McCarthy¡¯s original Mark&Sweep al-gorithm, which is interesting because it can be built on top of an existing malloc package to provide garbage collection for C and C++ programs. 
9.10.1 Garbage Collector Basics 
A garbage collector views memory as a directed reachability graph of the form shown in Figure 9.49. The nodes of the graph are partitioned into a set of root nodes and a set of heap nodes. Each heap node corresponds to an allocated block in the heap. A directed edge p ¡ú q means that some location in block p points to some location in block q. Root nodes correspond to locations not in the heap that contain pointers into the heap. These locations can be registers, variables on the stack, or global variables in the read-write data area of virtual memory. 
We say that a node p is reachable if there exists a directed path from any root node to p. At any point in time, the unreachable nodes correspond to garbage that can never be used again by the application. The role of a garbage collector is to maintain some representation of the reachability graph and periodically reclaim the unreachable nodes by freeing them and returning them to the free list. 
Garbage collectors for languages like ML and Java, which exert tight con-trol over how applications create and use pointers, can maintain an exact repre-sentation of the reachability graph, and thus can reclaim all garbage. However, collectors for languages like C and C++ cannot in general maintain exact repre-sentations of the reachability graph. Such collectors are known as conservative garbage collectors. They are conservative in the sense that each reachable block is correctly identi.ed as reachable, while some unreachable nodes might be incor-rectly identi.ed as reachable. 


Collectors can provide their service on demand, or they can run as separate threads in parallel with the application, continuously updating the reachability graph and reclaiming garbage. For example, consider how we might incorporate a conservative collector for C programs into an existing malloc package, as shown in Figure 9.50. 
The application calls malloc in the usual manner whenever it needs heap space. If malloc is unable to .nd a free block that .ts, then it calls the garbage col-lector in hopes of reclaiming some garbage to the free list. The collector identi.es the garbage blocks and returns them to the heap by calling the free function. The key idea is that the collector calls free instead of the application. When the call to the collector returns, malloc tries again to .nd a free block that .ts. If that fails, then it can ask the operating system for additional memory. Eventually malloc returns a pointer to the requested block (if successful) or the NULL pointer (if unsuccessful). 
9.10.2 Mark&Sweep Garbage Collectors 
A Mark&Sweep garbage collector consists of a mark phase, which marks all reachable and allocated descendants of the root nodes, followed by a sweep phase, which frees each unmarked allocated block. Typically, one of the spare low-order bits in the block header is used to indicate whether a block is marked or not. 
Our description of Mark&Sweep will assume the following functions, where ptr is de.ned as typedef void *ptr. 
. ptr isPtr(ptr p):If p points to some word in an allocated block, returns a pointer b to the beginning of that block. Returns NULL otherwise. . int blockMarked(ptr b): Returns true if block b is already marked. . int blockAllocated(ptr b): Returns true if block b is allocated. . void markBlock(ptr b): Marks block b. . int length(ptr b): Returns the length in words (excluding the header) of block b. . void unmarkBlock(ptr b): Changes the status of block b from marked to unmarked. . ptr nextBlock(ptr b): Returns the successor of block b in the heap. 

(a) mark function (b) sweep function void mark(ptr p) { void sweep(ptr b, ptr end) { if ((b = isPtr(p)) == NULL) while (b < end) { return; if (blockMarked(b)) if (blockMarked(b)) unmarkBlock(b); return; else if (blockAllocated(b)) markBlock(b); free(b); len = length(b); b = nextBlock(b); for (i=0; i < len; i++) } mark(b[i]); return; 
return; } } 
Figure 9.51 Pseudo-code for the mark and sweep functions. 
The mark phase calls the mark function shown in Figure 9.51(a) once for each root node. The mark function returns immediately if p does not point to an allocated and unmarked heap block. Otherwise, it marks the block and calls itself recursively on each word in block. Each call to the mark function marks any unmarked and reachable descendants of some root node. At the end of the mark phase, any allocated block that is not marked is guaranteed to be unreachable and, hence, garbage that can be reclaimed in the sweep phase. 
The sweep phase is a single call to the sweep function shown in Figure 9.51(b). The sweep function iterates over each block in the heap, freeing any unmarked allocated blocks (i.e., garbage) that it encounters. 
Figure 9.52 shows a graphical interpretation of Mark&Sweep for a small heap. Block boundaries are indicated by heavy lines. Each square corresponds to a word of memory. Each block has a one-word header, which is either marked or unmarked. 


Unmarked block header 
After mark: Marked block header 


Initially, the heap in Figure 9.52 consists of six allocated blocks, each of which is unmarked. Block 3 contains a pointer to block 1. Block 4 contains pointers to blocks 3 and 6. The root points to block 4. After the mark phase, blocks 1, 3, 4, and 6 are marked because they are reachable from the root. Blocks 2 and 5 are unmarked because they are unreachable. After the sweep phase, the two unreachable blocks are reclaimed to the free list. 
9.10.3 Conservative Mark&Sweep for C Programs 
Mark&Sweep is an appropriate approach for garbage collecting C programs be-cause it works in place without moving any blocks. However, the C language poses some interesting challenges for the implementation of the isPtr function. 
First, C does not tag memory locations with any type information. Thus, there is no obvious way for isPtr to determine if its input parameter p is a pointer or not. Second, even if we were to know that p was a pointer, there would be no obvious way for isPtr to determine whether p points to some location in the payload of an allocated block. 
One solution to the latter problem is to maintain the set of allocated blocks as a balanced binary tree that maintains the invariant that all blocks in the left subtree are located at smaller addresses and all blocks in the right subtree are located in larger addresses. As shown in Figure 9.53, this requires two additional .elds (left and right) in the header of each allocated block. Each .eld points to the header of some allocated block. 
The isPtr(ptr p) function uses the tree to perform a binary search of the 
allocated blocks. At each step, it relies on the size .eld in the block header to 
determine if p falls within the extent of the block. 
The balanced tree approach is correct in the sense that it is guaranteed to mark all of the nodes that are reachable from the roots. This is a necessary guarantee, as application users would certainly not appreciate having their allocated blocks prematurely returned to the free list. However, it is conservative in the sense that it may incorrectly mark blocks that are actually unreachable, and thus it may fail to free some garbage. While this does not affect the correctness of application programs, it can result in unnecessary external fragmentation. 
The fundamental reason that Mark&Sweep collectors for C programs must be conservative is that the C language does not tag memory locations with type information. Thus, scalars like intsor floats can masquerade as pointers. For example, suppose that some reachable allocated block contains an int in its payload whose value happens to correspond to an address in the payload of some other allocated block b. There is no way for the collector to infer that the data is really an int and not a pointer. Therefore, the allocator must conservatively mark block b as reachable, when in fact it might not be. 

Left and right pointers in a balanced tree of allocated blocks. 

9.11 
Common 
Memory-Related 
Bugs 
in 
C 
Programs 

Managing and using virtual memory can be a dif.cult and error-prone task for C programmers. Memory-related bugs are among the most frightening because they often manifest themselves at a distance, in both time and space, from the source of the bug. Write the wrong data to the wrong location, and your program can run for hours before it .nally fails in some distant part of the program. We conclude our discussion of virtual memory with a discussion of some of the common memory-related bugs. 
9.11.1 Dereferencing Bad Pointers 
As we learned in Section 9.7.2, there are large holes in the virtual address space of a process that are not mapped to any meaningful data. If we attempt to dereference a pointer into one of these holes, the operating system will terminate our program with a segmentation exception. Also, some areas of virtual memory are read-only. Attempting to write to one of these areas terminates the program with a protection exception. 
A common example of dereferencing a bad pointer is the classic scanf bug. Suppose we want to use scanf to read an integer from stdin into a variable. The correct way to do this is to pass scanf a format string and the address of the variable: 
scanf("%d", &val) 
However, it is easy for new C programmers (and experienced ones too!) to pass the contents of val instead of its address: 
scanf("%d", val) 
In this case, scanf will interpret the contents of val as an address and attempt to write a word to that location. In the best case, the program terminates immediately with an exception. In the worst case, the contents of val correspond to some valid read/write area of virtual memory, and we overwrite memory, usually with disastrous and baf.ing consequences much later. 
9.11.2 Reading Uninitialized Memory 
While bss memory locations (such as uninitialized global C variables) are always initialized to zeros by the loader, this is not true for heap memory. A common error is to assume that heap memory is initialized to zero: 
1 /* Returny=Ax*/ 
2 int *matvec(int **A, int *x, int n) 
3 { 

4 int i, j; 
5 

6 int *y = (int *)Malloc(n * sizeof(int)); 
7 

8 for (i=0;i<n; i++) 
9 for (j=0;j<n; j++) 10 y[i] += A[i][j] * x[j]; 11 return y; 
12 } 
In this example, the programmer has incorrectly assumed that vector y has been initialized to zero. A correct implementation would explicitly zero y[i], or use calloc. 
9.11.3 Allowing Stack Buffer Over.ows 
As we saw in Section 3.12, a program has a buffer over.ow bug if it writes to a target buffer on the stack without examining the size of the input string. For example, the following function has a buffer over.ow bug because the gets function copies an arbitrary length string to the buffer. To .x this, we would need to use the fgets function, which limits the size of the input string. 
1 void bufoverflow() 2 { 3 char buf[64]; 4 5 gets(buf); /* Here is the stack buffer overflow bug */ 6 return; 
7 } 
9.11.4 Assuming that Pointers and the Objects They Point to Are the Same Size 
One common mistake is to assume that pointers to objects are the same size as the objects they point to: 
1 /* Create an nxm array */ 2 int **makeArray1(int n, int m) 3 { 4 int i; 5 int **A = (int **)Malloc(n * sizeof(int)); 6 7 for (i=0;i<n; i++) 8 A[i] = (int *)Malloc(m * sizeof(int)); 9 return A; 
10 } 
The intent here is to create an array of n pointers, each of which points to an array of m ints. However, because the programmer has written sizeof(int) instead of sizeof(int *) in line 5, the code actually creates an array of ints. 
This code will run .ne on machines where ints and pointers to ints are the same size. But if we run this code on a machine like the Core i7, where a pointer is larger than an int, then the loop in lines 7¨C8 will write past the end of the A array. Since one of these words will likely be the boundary tag footer of the allocated block, we may not discover the error until we free the block much later in the program, at which point the coalescing code in the allocator will fail dramatically and for no apparent reason. This is an insidious example of the kind of ¡°action at a distance¡± that is so typical of memory-related programming bugs. 

9.11.5 Making Off-by-One Errors 
Off-by-one errors are another common source of overwriting bugs: 
1 /* Create an nxm array */ 2 int **makeArray2(int n, int m) 3 { 4 int i; 5 int **A = (int **)Malloc(n * sizeof(int *)); 6 7 for(i=0; i<= n; i++) 8 A[i] = (int *)Malloc(m * sizeof(int)); 9 return A; 
10 } 

This is another version of the program in the previous section. Here we have created an n-element array of pointers in line 5, but then tried to initialize n + 1of its elements in lines 7 and 8, in the process overwriting some memory that follows the A array. 
9.11.6 Referencing a Pointer Instead of the Object It Points to 
If we are not careful about the precedence and associativity of C operators, then we incorrectly manipulate a pointer instead of the object it points to. For example, consider the following function, whose purpose is to remove the .rst item in a binary heap of *size items, and then reheapify the remaining *size -1 items: 
1 int *binheapDelete(int **binheap, int *size) 2 { 3 int *packet = binheap[0]; 4 5 binheap[0] = binheap[*size -1]; 6 *size--; /* This should be (*size)--*/ 7 heapify(binheap, *size, 0); 8 return(packet); 
9 } 

In line 6, the intent is to decrement the integer value pointed to by the size pointer. However, because the unary --and * operators have the same precedence and associate from right to left, the code in line 6 actually decrements the pointer itself instead of the integer value that it points to. If we are lucky, the program will crash immediately; but more likely we will be left scratching our heads when the program produces an incorrect answer much later in its execution. The moral here is to use parentheses whenever in doubt about precedence and associativity. For example, in line 6 we should have clearly stated our intent by using the expression (*size)--. 
9.11.7 Misunderstanding Pointer Arithmetic 
Another common mistake is to forget that arithmetic operations on pointers are performed in units that are the size of the objects they point to, which are not necessarily bytes. For example, the intent of the following function is to scan an array of ints and return a pointer to the .rst occurrence of val: 
1 int *search(int *p, int val) 2 { 3 while (*p && *p != val) 4 p += sizeof(int); /* Should be p++ */ 5 return p; 
6 } 
However, because line 4 increments the pointer by 4 (the number of bytes in an integer) each time through the loop, the function incorrectly scans every fourth integer in the array. 
9.11.8 Referencing Nonexistent Variables 
Naive C programmers who do not understand the stack discipline will sometimes reference local variables that are no longer valid, as in the following example: 
1 int *stackref () 2 { 3 int val; 4 5 return &val; 
6 } 
This function returns a pointer (say, p) to a local variable on the stack and then pops its stack frame. Although p still points to a valid memory address, it no longer points to a valid variable. When other functions are called later in the program, the memory will be reused for their stack frames. Later, if the program assigns some value to *p, then it might actually be modifying an entry in another function¡¯s stack frame, with potentially disastrous and baf.ing consequences. 

9.11.9 Referencing Data in Free Heap Blocks 
A similar error is to reference data in heap blocks that have already been freed. For example, consider the following example, which allocates an integer array x in line 6, prematurely frees block x in line 10, and then later references it in line 14: 
1 int *heapref(int n, int m) 2 { 3 int i; 4 int *x, *y; 5 6 x = (int *)Malloc(n * sizeof(int)); 7 8 /* ... */ /* Other calls to malloc and free go here */ 9 
10 free(x); 11 12 y = (int *)Malloc(m * sizeof(int)); 13 for (i=0;i<m; i++) 14 y[i] = x[i]++; /* Oops! x[i] is a word in a free block */ 15 16 return y; 
17 } 

Depending on the pattern of malloc and free calls that occur between lines 6 and 10, when the program references x[i] in line 14, the array x might be part of some other allocated heap block and have been overwritten. As with many memory-related bugs, the error will only become evident later in the program when we notice that the values in y are corrupted. 
9.11.10 Introducing Memory Leaks 
Memory leaks are slow, silent killers that occur when programmers inadvertently create garbage in the heap by forgetting to free allocated blocks. For example, the following function allocates a heap block x and then returns without freeing it: 
1 void leak(int n) 2 { 3 int *x = (int *)Malloc(n * sizeof(int)); 4 5 return; /* x is garbage at this point */ 
6 } 

If leak is called frequently, then the heap will gradually .ll up with garbage, in the worst case consuming the entire virtual address space. Memory leaks are particularly serious for programs such as daemons and servers, which by de.nition never terminate. 
9.12 
Summary 

Virtual memory is an abstraction of main memory. Processors that support virtual memory reference main memory using a form of indirection known as virtual ad-dressing. The processor generates a virtual address, which is translated into a phys-ical address before being sent to the main memory. The translation of addresses from a virtual address space to a physical address space requires close cooperation between hardware and software. Dedicated hardware translates virtual addresses using page tables whose contents are supplied by the operating system. 
Virtual memory provides three important capabilities. First, it automatically caches recently used contents of the virtual address space stored on disk in main memory. The block in a virtual memory cache is known as a page. A reference to a page on disk triggers a page fault that transfers control to a fault handler in the operating system. The fault handler copies the page from disk to the main memory cache, writing back the evicted page if necessary. Second, virtual memory simpli.es memory management, which in turn simpli.es linking, sharing data between processes, the allocation of memory for processes, and program loading. Finally, virtual memory simpli.es memory protection by incorporating protection bits into every page table entry. 
The process of address translation must be integrated with the operation of any hardware caches in the system. Most page table entries are located in the L1 cache, but the cost of accessing page table entries from L1 is usually eliminated by an on-chip cache of page table entries called a TLB. 
Modern systems initialize chunks of virtual memory by associating them with chunks of .les on disk, a process known as memory mapping. Memory mapping provides an ef.cient mechanism for sharing data, creating new processes, and loading programs. Applications can manually create and delete areas of the virtual address space using the mmap function. However, most programs rely on a dynamic memory allocator such as malloc, which manages memory in an area of the virtual address space called the heap. Dynamic memory allocators are application-level programs with a system-level feel, directly manipulating memory without much help from the type system. Allocators come in two .avors. Explicit allocators require applications to explicitly free their memory blocks. Implicit allocators (garbage collectors) free any unused and unreachable blocks automatically. 
Managing and using memory is a dif.cult and error-prone task for C program-mers. Examples of common errors include dereferencing bad pointers, reading uninitialized memory, allowing stack buffer over.ows, assuming that pointers and the objects they point to are the same size, referencing a pointer instead of the object it points to, misunderstanding pointer arithmetic, referencing nonexistent variables, and introducing memory leaks. 
Bibliographic 
Notes 

Kilburn and his colleagues published the .rst description of virtual memory [60]. Architecture texts contain additional details about the hardware¡¯s role in virtual memory [49]. Operating systems texts contain additional information about the operating system¡¯s role [98, 104, 112]. Bovet and Cesati [11] give a detailed de-scription of the Linux virtual memory system. Intel Corporation provides detailed documentation on 32-bit and 64-bit address translation on IA processors [30]. 

Knuth wrote the classic work on storage allocation in 1968 [61]. Since that time there has been a tremendous amount of work in the area. Wilson, Johnstone, Neely, and Boles have written a beautiful survey and performance evaluation of explicit allocators [117]. The general comments in this book about the throughput and utilization of different allocator strategies are paraphrased from their sur-vey. Jones and Lins provide a comprehensive survey of garbage collection [54]. Kernighan and Ritchie [58] show the complete code for a simple allocator based on an explicit free list with a block size and successor pointer in each free block. The code is interesting in that it uses unions to eliminate a lot of the complicated pointer arithmetic, but at the expense of a linear-time (rather than constant-time) free operation. 
Homework 
Problems 

9.11 ¡ô 

In the following series of problems, you are to show how the example memory system in Section 9.6.4 translates a virtual address into a physical address and accesses the cache. For the given virtual address, indicate the TLB entry accessed, the physical address, and the cache byte value returned. Indicate whether the TLB misses, whether a page fault occurs, and whether a cache miss occurs. If there is a cache miss, enter ¡°¨C¡± for ¡°Cache Byte returned.¡± If there is a page fault, enter ¡°¨C¡± for ¡°PPN¡± and leave parts C and D blank. 
Virtual address: 0x027c 
A. Virtual address format 
13 12 11 10 9 8 7 6 5 4 3 2 1 0 

B. Address translation 
Parameter Value VPN 
TLB index TLB tag TLB hit? (Y/N) Page fault? (Y/N) PPN 
C. Physical address format 
11 10 9 8 7 6 5 4 3 2 1 0 

D. Physical memory reference Parameter Value 
Byte offset 
Cache index Cache tag Cache hit? (Y/N) Cache byte returned 
9.12 ¡ô 
Repeat Problem 9.11 for the following address: 
Virtual address: 0x03a9 
A. Virtual address format 
13 12 11 10 9 8 7 6 5 4 3 2 1 0 

B.  Address translation  
Parameter  Value  
VPN TLB index TLB tag TLB hit? (Y/N) Page fault? (Y/N) PPN  
C.  Physical address format  
11  10  9  8  7  6  5  4  3  2  1  0  


D. Physical memory reference Parameter Value 
Byte offset 
Cache index Cache tag Cache hit? (Y/N) Cache byte returned 
9.13 ¡ô 

Repeat Problem 9.11 for the following address: 
Virtual address: 0x0040 
A. Virtual address format 
13 12 11 10 9 8 7 6 5 4 3 2 1 0 

B.  Address translation  
Parameter  Value  
VPN TLB index TLB tag TLB hit? (Y/N) Page fault? (Y/N) PPN  
C.  Physical address format  
11  10  9  8  7  6  5  4  3  2  1  0  


D. Physical memory reference 
Parameter Value Byte offset 
Cache index Cache tag Cache hit? (Y/N) Cache byte returned 
9.14 ¡ô¡ô 

Given an input .le hello.txt that consists of the string ¡°Hello, world!\n¡±, write a C program that uses mmap to change the contents of hello.txt to ¡°Jello, world!\n¡±. 
9.15 ¡ô 

Determine the block sizes and header values that would result from the following sequence of malloc requests. Assumptions: (1) The allocator maintains double-word alignment, and uses an implicit free list with the block format from Fig-ure 9.35. (2) Block sizes are rounded up to the nearest multiple of 8 bytes. 
Request Block size (decimal bytes) Block header (hex) 
malloc(3) 
malloc(11) malloc(20) malloc(21) 
9.16 ¡ô 
Determine the minimum block size for each of the following combinations of alignment requirements and block formats. Assumptions: Explicit free list, 4-byte pred and succ pointers in each free block, zero-sized payloads are not allowed, and headers and footers are stored in 4-byte words. 

Alignment Allocated block Free block Minimum block size (bytes) 
Single word Single word Double word  Header and footer Header, but no footer Header and footer  Header and footer Header and footer Header and footer  
Double word  Header, but no footer  Header and footer  
9.17  ¡ô¡ô¡ô  

Develop a version of the allocator in Section 9.9.12 that performs a next-.t search instead of a .rst-.t search. 
9.18 ¡ô¡ô¡ô 
The allocator in Section 9.9.12 requires both a header and a footer for each block in order to perform constant-time coalescing. Modify the allocator so that free blocks require a header and footer, but allocated blocks require only a header. 
9.19 ¡ô 
You are given three groups of statements relating to memory management and garbage collection below. In each group, only one statement is true. Your task is to indicate which statement is true. 
1. (a) In a buddy system, up to 50% of the space can be wasted due to internal fragmentation. 
(b) 
The .rst-.t memory allocation algorithm is slower than the best-.t algo-rithm (on average). 

(c) 
Deallocation using boundary tags is fast only when the list of free blocks is ordered according to increasing memory addresses. 

(d) 
The buddy system suffers from internal fragmentation, but not from external fragmentation. 


2. (a) Using the .rst-.t algorithm on a free list that is ordered according to decreasing block sizes results in low performance for allocations, but avoids external fragmentation. 
(b) 
For the best-.t method, the list of free blocks should be ordered according to increasing memory addresses. 

(c) 
The best-.t method chooses the largest free block into which the re-quested segment .ts. 



(d) Using the .rst-.t algorithm on a free list that is ordered according to increasing block sizes is equivalent to using the best-.t algorithm. 
3. Mark & sweep garbage collectors are called conservative if: 
(a) 
They coalesce freed memory only when a memory request cannot be satis.ed. 

(b) 
They treat everything that looks like a pointer as a pointer. 

(c) 
They perform garbage collection only when they run out of memory. 

(d) 
They do not free memory blocks forming a cyclic list. 


9.20 ¡ô¡ô¡ô¡ô 

Write your own version of malloc and free, and compare its running time and space utilization to the version of malloc provided in the standard C library. 
Solutions 
to 
Practice 
Problems 

Solution to Problem 9.1 (page 779) 
This problem gives you some appreciation for the sizes of different address spaces. At one point in time, a 32-bit address space seemed impossibly large. But now there are database and scienti.c applications that need more, and you can expect this trend to continue. At some point in your lifetime, expect to .nd yourself complaining about the cramped 64-bit address space on your personal computer! 
No. virtual address bits (n) No. virtual addresses (N) Largest possible virtual address 
8 28 = 256 28 . 1 = 255 216
16 = 64K 216 . 1 = 64K . 1 232 232 . 1 = 4G . 1
32 = 4G 248 248
48 = 256T = 256T . 1 264
64 = 16, 384P 264 . 1 = 16, 384P . 1 
Solution to Problem 9.2 (page 781) 
Since each virtual page is P = 2p bytes, there are a total of 2n/2p = 2n.p possible pages in the system, each of which needs a page table entry (PTE). 
nP = 2p # PTEs 
164K 16 168K 8 324K 1M 32 8K 512K 
Solution to Problem 9.3 (page 790) 
You need to understand this kind of problem well in order to fully grasp address translation. Here is how to solve the .rst subproblem: We are given n = 32 virtual address bits and m = 24 physical address bits. A page size of P = 1 KB means we need log2(1K) = 10 bits for both the VPO and PPO. (Recall that the VPO and PPO are identical.) The remaining address bits are the VPN and PPN, respectively. 
P # VPN bits # VPO bits # PPN bits # PPO bits 
1KB 22 10 14 10 2KB 21 11 13 11 4KB 20 12 12 12 8KB 19 13 11 13 
Solution to Problem 9.4 (page 798) 
Doing a few of these manual simulations is a great way to .rm up your under-standing of address translation. You might .nd it helpful to write out all the bits in the addresses, and then draw boxes around the different bit .elds, such as VPN, TLBI, etc. In this particular problem, there are no misses of any kind: the TLB has a copy of the PTE and the cache has a copy of the requested data words. See Problems 9.11, 9.12, and 9.13 for some different combinations of hits and misses. 
A. 00 0011 1101 0111 
B. Parameter Value VPN 0xf TLB index 0x3 TLB tag 0x3 TLB hit? (Y/N) Y 
Page fault? (Y/N) N PPN 0xd 
C. 0011 0101 0111 
D. Parameter Value Byte offset 0x3 Cache index 0x5 Cache tag 0xd 
Cache hit? (Y/N) Y Cache byte returned 0x1d 
Solution to Problem 9.5 (page 812) 
Solving this problem will give you a good feel for the idea of memory mapping. Try it yourself. We haven¡¯t discussed the open, fstat,or write functions, so you¡¯ll need to read their man pages to see how they work. 
code/vm/mmapcopy.c 
1 #include "csapp.h" 2 3 /* 4 * mmapcopy -uses mmap to copy file fd to stdout 5 */ 6 void mmapcopy(int fd, int size) 7 { 8 char *bufp; /* Pointer to memory mapped VM area */ 

9  
10  bufp = Mmap(NULL, size, PROT_READ, MAP_PRIVATE,  fd,  0);  
11  Write(1, bufp, size);  
12  return;  
13  }  
14  
15  /* mmapcopy driver */  
16  int  main(int argc, char **argv)  
17  {  
18  struct stat stat;  
19  int fd;  
20  
21  /* Check for required command line argument */  
22  if (argc != 2) {  
23  printf("usage: %s <filename>\n", argv[0]);  
24  exit(0);  
25  }  
26  
27  /* Copy the input argument to stdout */  
28  fd = Open(argv[1], O_RDONLY, 0);  
29  fstat(fd, &stat);  
30  mmapcopy(fd, stat.st_size);  
31  exit(0);  
32  }  
code/vm/mmapcopy.c  

Solution to Problem 9.6 (page 822) 
This problem touches on some core ideas such as alignment requirements, min-imum block sizes, and header encodings. The general approach for determining the block size is to round the sum of the requested payload and the header size to the nearest multiple of the alignment requirement (in this case 8 bytes). For example, the block size for the malloc(1) request is 4 + 1 = 5 rounded up to 8. The block size for the malloc(13) request is 13 + 4 = 17 rounded up to 24. 
Request Block size (decimal bytes) Block header (hex) malloc(1) 8 0x9 malloc(5) 16 0x11 malloc(12) 16 0x11 malloc(13) 24 0x19 
Solution to Problem 9.7 (page 826) 
The minimum block size can have a signi.cant effect on internal fragmentation. Thus, it is good to understand the minimum block sizes associated with different allocator designs and alignment requirements. The tricky part is to realize that the same block can be allocated or free at different points in time. Thus, the minimum block size is the maximum of the minimum allocated block size and the minimum free block size. For example, in the last subproblem, the minimum allocated block size is a 4-byte header and a 1-byte payload rounded up to eight bytes. The minimum free block size is a 4-byte header and 4-byte footer, which is already a multiple of 8 and doesn¡¯t need to be rounded. So the minimum block size for this allocator is 8 bytes. 
Alignment Allocated block Free block Minimum block size (bytes) 
Single word  Header and footer  Header and footer  12  
Single word  Header, but no footer  Header and footer  8  
Double word  Header and footer  Header and footer  16  
Double word  Header, but no footer  Header and footer  8  

Solution to Problem 9.8 (page 832) 
There is nothing very tricky here. But the solution requires you to understand how the rest of our simple implicit-list allocator works and how to manipulate and traverse blocks. 
code/vm/malloc/mm.c 

1 static void *find_fit(size_t asize) 2 { 3 /* First fit search */ 4 void *bp; 5 6 for (bp = heap_listp; GET_SIZE(HDRP(bp)) > 0; bp = NEXT_BLKP(bp)) { 7 if (!GET_ALLOC(HDRP(bp)) && (asize <= GET_SIZE(HDRP(bp)))) { 8 return bp; 
9 } 
10 } 11 return NULL; /* No fit */ 
code/vm/malloc/mm.c 
Solution to Problem 9.9 (page 834) 
This is another warm-up exercise to help you become familiar with allocators. Notice that for this allocator the minimum block size is 16 bytes. If the remainder of the block after splitting would be greater than or equal to the minimum block size, then we go ahead and split the block (lines 6 to 10). The only tricky part here is to realize that you need to place the new allocated block (lines 6 and 7) before moving to the next block (line 8). 
code/vm/malloc/mm.c 
1 static void place(void *bp, size_t asize) 2 { 3 size_t csize = GET_SIZE(HDRP(bp)); 4 5 if ((csize -asize) >= (2*DSIZE)) { 6 PUT(HDRP(bp), PACK(asize, 1)); 

7  PUT(FTRP(bp), PACK(asize, 1));  
8  bp = NEXT_BLKP(bp);  
9  PUT(HDRP(bp), PACK(csize-asize, 0));  
10  PUT(FTRP(bp), PACK(csize-asize, 0));  
11  }  
12  else {  
13  PUT(HDRP(bp), PACK(csize, 1));  
14  PUT(FTRP(bp), PACK(csize, 1));  
15  }  
16  }  
code/vm/malloc/mm.c  

Solution to Problem 9.10 (page 837) 
Here is one pattern that will cause external fragmentation: The application makes numerous allocation and free requests to the .rst size class, followed by numer-ous allocation and free requests to the second size class, followed by numerous allocation and free requests to the third size class, and so on. For each size class, the allocator creates a lot of memory that is never reclaimed because the allocator doesn¡¯t coalesce, and because the application never requests blocks from that size class again. 
This page intentionally left blank 

Part 
III 
Interaction 
and 
Communication 
Between 
Programs 
T
o this point in our study of computer systems, we have assumed that programs run in isolation, with minimal input and output. How-ever, in the real world, application programs use services provided by the operating system to communicate with I/O devices and with other programs. 
This part of the book will give you an understanding of the basic I/O services provided by Unix operating systems, and how to use these services to build applications such as Web clients and servers that com-municate with each other over the Internet. You will learn techniques for writing concurrent programs such as Web servers that can service mul-tiple clients at the same time. Writing concurrent application programs can also allow them to execute faster on modern multi-core processors. When you .nish this part, you will be well on your way to becoming a power programmer with a mature understanding of computer systems and their impact on your programs. 
This page intentionally left blank 

CHAPTER 
10 
System-Level 
I/O 
10.1 
Unix 
I/O 
862 
10.2 
Opening 
and 
Closing 
Files 
863 
10.3 
Reading 
and 
Writing 
Files 
865 
10.4 
Robust 
Reading 
and 
Writing 
with 
the 
Rio 
Package 
867 
10.5 
Reading 
File 
Metadata 
873 
10.6 
Sharing 
Files 
875 
10.7 
I/O 
Redirection 
877 
10.8 
Standard 
I/O 
879 
10.9 
Putting 
It 
Together: 
Which 
I/O 
Functions 
Should 
I 
Use? 
880 
10.10 
Summary 
881 
Bibliographic 
Notes 
882 
Homework 
Problems 
882 
Solutions 
to 
Practice 
Problems 
883 
Input/output (I/O) is the process of copying data between main memory and external devices such as disk drives, terminals, and networks. An input operation copies data from an I/O device to main memory, and an output operation copies data from memory to a device. 
All language run-time systems provide higher-level facilities for performing I/O. For example, ANSI C provides the standard I/O library, with functions such as printf and scanf that perform buffered I/O. The C++ language provides similar functionality with its overloaded << (¡°put to¡±) and >> (¡°get from¡±) operators. On Unix systems, these higher-level I/O functions are implemented using system-level Unix I/O functions provided by the kernel. Most of the time, the higher-level I/O functions work quite well and there is no need to use Unix I/O directly. So why bother learning about Unix I/O? 
. Understanding Unix I/O will help you understand other systems concepts. I/O is integral to the operation of a system, and because of this we often encounter circular dependences between I/O and other systems ideas. For example, I/O plays a key role in process creation and execution. Conversely, process creation plays a key role in how .les are shared by different processes. Thus, to really understand I/O you need to understand processes, and vice versa. We have already touched on aspects of I/O in our discussions of the memory hierarchy, linking and loading, processes, and virtual memory. Now that you have a better understanding of these ideas, we can close the circle and delve into I/O in more detail. 
. Sometimes you have no choice but to use Unix I/O. There are some important cases where using higher-level I/O functions is either impossible or inappro-priate. For example, the standard I/O library provides no way to access .le metadata such as .le size or .le creation time. Further, there are problems with the standard I/O library that make it risky to use for network programming. 
This chapter introduces you to the general concepts of Unix I/O and standard I/O, and shows you how to use them reliably from your C programs. Besides serving as a general introduction, this chapter lays a .rm foundation for our subsequent study of network programming and concurrency. 
10.1 
Unix 
I/O 

A Unix .le is a sequence of mbytes: 
B0,B1,...,Bk,...,B .1.
m 
All I/O devices, such as networks, disks, and terminals, are modeled as .les, and all input and output is performed by reading and writing the appropriate .les. This elegant mapping of devices to .les allows the Unix kernel to export a simple, low-level application interface, known as Unix I/O, that enables all input and output to be performed in a uniform and consistent way: 
. Opening .les. An application announces its intention to access an I/O device by asking the kernel to open the corresponding .le. The kernel returns a small nonnegative integer, called a descriptor, that identi.es the .le in all subsequent operations on the .le. The kernel keeps track of all information about the open .le. The application only keeps track of the descriptor. 

Each process created by a Unix shell begins life with three open .les: standard input (descriptor 0), standard output (descriptor 1), and standard error (descriptor 2). The header .le <unistd.h> de.nes constants STDIN_ FILENO, STDOUT_FILENO, and STDERR_FILENO, which can be used instead of the explicit descriptor values. 
. Changing the current .le position. The kernel maintains a .le position k, ini-tially 0, for each open .le. The .le position is a byte offset from the beginning of a .le. An application can set the current .le position k explicitly by per-forming a seek operation. 
. Reading and writing .les.A read operation copies n>0 bytes from a .le to memory, starting at the current .le position k, and then incrementing kby n. Given a .le with a size of mbytes, performing a read operation when k¡Ý m triggers a condition known as end-of-.le (EOF), which can be detected by the application. There is no explicit ¡°EOF character¡± at the end of a .le. 
Similarly, a write operation copies n>0 bytes from memory to a .le, starting at the current .le position k, and then updating k. 
. Closing .les. When an application has .nished accessing a .le, it informs the kernel by asking it to close the .le. The kernel responds by freeing the data structures it created when the .le was opened and restoring the descriptor to a pool of available descriptors. When a process terminates for any reason, the kernel closes all open .les and frees their memory resources. 
10.2 
Opening 
and 
Closing 
Files 

A process opens an existing .le or creates a new .le by calling the open function: 
#include <sys/types.h> #include <sys/stat.h> #include <fcntl.h> 
int open(char *filename, int flags, mode_t mode); 
Returns: new .le descriptor if OK, .1 on error 

The open function converts a filename to a .le descriptor and returns the descriptor number. The descriptor returned is always the smallest descriptor that is not currently open in the process. The flags argument indicates how the process intends to access the .le: 
. O_RDONLY: Reading only 
. O_WRONLY: Writing only 
. O_RDWR: Reading and writing 
For example, here is how to open an existing .le for reading: 
Mask  Description  
S_IRUSR  User (owner) can read this .le  
S_IWUSR  User (owner) can write this .le  
S_IXUSR  User (owner) can execute this .le  
S_IRGRP  Members of the owner¡¯s group can read this .le  
S_IWGRP  Members of the owner¡¯s group can write this .le  
S_IXGRP  Members of the owner¡¯s group can execute this .le  
S_IROTH  Others (anyone) can read this .le  
S_IWOTH  Others (anyone) can write this .le  
S_IXOTH  Others (anyone) can execute this .le  
Figure 10.1 Access permission bits. De.ned in sys/stat.h. 


fd = Open("foo.txt", O_RDONLY, 0); 
The flags argument can also be or¡¯d with one or more bit masks that provide additional instructions for writing: 
. O_CREAT: If the .le doesn¡¯t exist, then create a truncated (empty) version of it. . O_TRUNC: If the .le already exists, then truncate it. . O_APPEND: Before each write operation, set the .le position to the end of the .le. 
For example, here is how you might open an existing .le with the intent of appending some data: 
fd = Open("foo.txt", O_WRONLY|O_APPEND, 0); 
The mode argument speci.es the access permission bits of new .les. The symbolic names for these bits are shown in Figure 10.1. As part of its context, each process has a umask that is set by calling the umask function. When a process creates a new .le by calling the open function with some mode argument, then the access permission bits of the .le are set to mode & ~umask. For example, suppose we are given the following default values for mode and umask: 
#define DEF_MODE S_IRUSR|S_IWUSR|S_IRGRP|S_IWGRP|S_IROTH|S_IWOTH #define DEF_UMASK S_IWGRP|S_IWOTH 
Then the following code fragment creates a new .le in which the owner of the .le has read and write permissions, and all other users have read permissions: 

umask(DEF_UMASK); fd = Open("foo.txt", O_CREAT|O_TRUNC|O_WRONLY, DEF_MODE); 
Finally, a process closes an open .le by calling the close function. 
#include <unistd.h> int close(int fd); 
Returns: zero if OK, .1 on error 

Closing a descriptor that is already closed is an error. 
Practice Problem 10.1 
What is the output of the following program? 
1 #include "csapp.h" 
2 
3 int main() 

4  {  
5  int fd1, fd2;  
6  
7  fd1 = Open("foo.txt",  O_RDONLY,  0);  
8  Close(fd1);  
9  fd2 = Open("baz.txt",  O_RDONLY,  0);  
10  printf("fd2 = %d\n", fd2);  
11  exit(0);  
12  }  

10.3 
Reading 
and 
Writing 
Files 

Applications perform input and output by calling the read and write functions, respectively. 
#include <unistd.h> 
ssize_t read(int fd, void *buf, size_t n); 
Returns: number of bytes read if OK, 0 on EOF, .1 on error 

ssize_t write(int fd, const void *buf, size_t n); 
Returns: number of bytes written if OK, .1 on error 

The read function copies at most n bytes from the current .le position of descriptor fd to memory location buf. A return value of .1 indicates an error, and a return value of 0 indicates EOF. Otherwise, the return value indicates the number of bytes that were actually transferred. 
code/io/cpstdin.c  
1  #include "csapp.h"  
2  
3  int main(void)  
4  {  
5  char c;  
6  
7  while(Read(STDIN_FILENO, &c, 1) != 0)  
8  Write(STDOUT_FILENO, &c, 1);  
9  exit(0);  
10  }  
code/io/cpstdin.c  
Figure 10.2 Copies standard input to standard output one byte at a time. 


The write function copies at most n bytes from memory location buf to the current .le position of descriptor fd. Figure 10.2 shows a program that uses read and write calls to copy the standard input to the standard output, 1 byte at a time. 
Applications can explicitly modify the current .le position by calling the lseek function, which is beyond our scope. 

Aside What¡¯s the difference between ssize_t and size_t? 
You might have noticed that the read function has a size_t input argument and an ssize_t return value. So what¡¯s the difference between these two types? A size_t is de.ned as an unsigned int, and an ssize_t (signed size) is de.ned as an int.The read function returns a signed size rather than an unsigned size because it must return a .1 on error. Interestingly, the possibility of returning a single .1 reduces the maximum size of a read by a factor of two, from 4 GB down to 2 GB. 
In some situations, read and write transfer fewer bytes than the application requests. Such short counts do not indicate an error. They occur for a number of reasons: 
. Encountering EOF on reads. Suppose that we are ready to read from a .le that contains only 20 more bytes from the current .le position and that we are reading the .le in 50-byte chunks. Then the next read will return a short count of 20, and the read after that will signal EOF by returning a short count of zero. 
. Reading text lines from a terminal.If the open .le is associated with a terminal (i.e., a keyboard and display), then each read function will transfer one text line at a time, returning a short count equal to the size of the text line. 
. Reading and writing network sockets.If the open .le corresponds to a network socket (Section 11.3.3), then internal buffering constraints and long network delays can cause read and write to return short counts. Short counts can also occur when you call read and write on a Unix pipe, an interprocess communication mechanism that is beyond our scope. 

In practice, you will never encounter short counts when you read from disk .les except on EOF, and you will never encounter short counts when you write to disk .les. However, if you want to build robust (reliable) network applications such as Web servers, then you must deal with short counts by repeatedly calling read and write until all requested bytes have been transferred. 
10.4 
Robust 
Reading 
and 
Writing 
with 
the 
Rio 
Package 

In this section, we will develop an I/O package, called the Rio (Robust I/O) package, that handles these short counts for you automatically. The Rio package provides convenient, robust, and ef.cient I/O in applications such as network programs that are subject to short counts. Rio provides two different kinds of functions: 
. Unbuffered input and output functions. These functions transfer data directly between memory and a .le, with no application-level buffering. They are especially useful for reading and writing binary data to and from networks. 
. Buffered input functions.These functions allow you to ef.ciently read text lines and binary data from a .le whose contents are cached in an application-level buffer, similar to the one provided for standard I/O functions such as printf. Unlike the buffered I/O routines presented in [109], the buffered Rio input functions are thread-safe (Section 12.7.1) and can be interleaved arbitrarily on the same descriptor. For example, you can read some text lines from a descriptor, then some binary data, and then some more text lines. 
We are presenting the Rio routines for two reasons. First, we will be using them in the network applications we develop in the next two chapters. Second, by studying the code for these routines, you will gain a deeper understanding of Unix I/O in general. 
10.4.1 Rio Unbuffered Input and Output Functions 
Applications can transfer data directly between memory and a .le by calling the rio_readn and rio_writen functions. 
#include "csapp.h" 
ssize_t rio_readn(int fd, void *usrbuf, size_t n); ssize_t rio_writen(int fd, void *usrbuf, size_t n); Returns: number of bytes transferred if OK, 0 on EOF (rio_readn only), .1 on error 
The rio_readn function transfers up to n bytes from the current .le position of descriptor fd to memory location usrbuf. Similarly, the rio_writen function transfers n bytes from location usrbuf to descriptor fd.The rio_readn function can only return a short count if it encounters EOF. The rio_writen function never returns a short count. Calls to rio_readn and rio_writen can be interleaved arbitrarily on the same descriptor. 
Figure 10.3 shows the code for rio_readn and rio_writen. Notice that each function manually restarts the read or write function if it is interrupted by the return from an application signal handler. To be as portable as possible, we allow for interrupted system calls and restart them when necessary. (See Section 8.5.4 for a discussion on interrupted system calls.) 
10.4.2 Rio Buffered Input Functions 
A text line is a sequence of ASCII characters terminated by a newline character. On Unix systems, the newline character (¡®\n¡¯) is the same as the ASCII line feed character (LF) and has a numeric value of 0x0a. Suppose we wanted to write a program that counts the number of text lines in a text .le. How might we do this? One approach is to use the read function to transfer 1 byte at a time from the .le to the user¡¯s memory, checking each byte for the newline character. The disadvantage of this approach is that it is inef.cient, requiring a trap to the kernel to read each byte in the .le. 
A better approach is to call a wrapper function (rio_readlineb) that copies the text line from an internal read buffer, automatically making a read call to re.ll the buffer whenever it becomes empty. For .les that contain both text lines and binary data (such as the HTTP responses described in Section 11.5.3) we also provide a buffered version of rio_readn, called rio_readnb, that transfers raw bytes from the same read buffer as rio_readlineb. 
#include "csapp.h" 
void rio_readinitb(rio_t *rp, int fd); 
Returns: nothing 
ssize_t rio_readlineb(rio_t *rp, void *usrbuf, size_t maxlen); ssize_t rio_readnb(rio_t *rp, void *usrbuf, size_t n); 
Returns: number of bytes read if OK, 0 on EOF, .1 on error 
The rio_readinitb function is called once per open descriptor. It associates the descriptor fd with a read buffer of type rio_t at address rp. 
The rio_readlineb function reads the next text line from .le rp (including the terminating newline character), copies it to memory location usrbuf, and ter-minates the text line with the null (zero) character. The rio_readlineb function reads at most maxlen-1 bytes, leaving room for the terminating null character. Text lines that exceed maxlen-1 bytes are truncated and terminated with a null character. 
The rio_readnb function reads up to n bytes from .le rp to memory location usrbuf. Calls to rio_readlineb and rio_readnb can be interleaved arbitrarily on the same descriptor. However, calls to these buffered functions should not be interleaved with calls to the unbuffered rio_readn function. 
You will encounter numerous examples of the Rio functions in the remainder of this text. Figure 10.4 shows how to use the Rio functions to copy a text .le from standard input to standard output, one line at a time. 
code/src/csapp.c 

1 ssize_t rio_readn(int fd, void *usrbuf, size_t n) 2 { 3 size_t nleft = n; 4 ssize_t nread; 5 char *bufp = usrbuf; 6 7 while (nleft > 0) { 8 if ((nread = read(fd, bufp, nleft)) < 0) { 9 if (errno == EINTR) /* Interrupted by sig handler return */ 
10 nread = 0; /* and call read() again */ 
11 else 
12 return -1; /* errno set by read() */ 
13 
} 14 else if (nread == 0) 15 break; /* EOF */ 16 nleft -= nread; 17 bufp += nread; 

18 
} 19 return (n -nleft); /* Return >= 0 */ 


20 } 
code/src/csapp.c 
code/src/csapp.c 

1 ssize_t rio_writen(int fd, void *usrbuf, size_t n) 2 { 3 size_t nleft = n; 4 ssize_t nwritten; 5 char *bufp = usrbuf; 6 7 while (nleft > 0) { 8 if ((nwritten = write(fd, bufp, nleft)) <= 0) { 9 if (errno == EINTR) /* Interrupted by sig handler return */ 
10 nwritten = 0; /* and call write() again */ 
11 else 
12 return -1; /* errno set by write() */ 
13 
} 14 nleft -= nwritten; 15 bufp += nwritten; 

16 
} 17 return n; 


18 } 
code/src/csapp.c 

Figure 10.3 The rio_readn and rio_writen functions. 
870  Chapter 10  System-Level I/O  
code/io/cp.le.c  
1 2  #include  "csapp.h"  
3 4 5 6 7 8  int {  main(int argc, char int n; rio_t rio; char buf[MAXLINE];  **argv)  
9 10 11 12  }  Rio_readinitb(&rio, STDIN_FILENO); while((n = Rio_readlineb(&rio, buf, MAXLINE)) Rio_writen(STDOUT_FILENO, buf, n);  !=  0)  
code/io/cp.le.c  
Figure 10.4  Copying a text .le from standard input to standard output.  
Figure 10.5 shows the format of a read buffer, along with the code for the rio_readinitb function that initializes it. The rio_readinitb function sets up an empty read buffer and associates an open .le descriptor with that buffer. The heart of the Rio read routines is the rio_read function shown in Fig-ure 10.6. The rio_read function is a buffered version of the Unix read function. When rio_read is called with a request to read n bytes, there are rp->rio_cnt  
code/include/csapp.h  
1 2 3 4 5 6 7  #define RIO_BUFSIZE 8192 typedef struct { int rio_fd; int rio_cnt; char *rio_bufptr; char rio_buf[RIO_BUFSIZE]; } rio_t;  /* /* /* /*  Descriptor for this internal Unread bytes in internal buf Next unread byte in internal Internal buffer */  buf */ buf  */ */  
code/include/csapp.h  
code/src/csapp.c  
1 2 3 4 5 6  void rio_readinitb(rio_t *rp, int { rp->rio_fd = fd; rp->rio_cnt = 0; rp->rio_bufptr = rp->rio_buf; }  fd)  
code/src/csapp.c  
Figure 10.5 A read buffer of type rio_t and the rio_readinitb function that initializes it.  

code/src/csapp.c 

1 static ssize_t rio_read(rio_t *rp, char *usrbuf, size_t n) 2 { 3 int cnt; 4 5 while (rp->rio_cnt <= 0) { /* Refill if buf is empty */ 6 rp->rio_cnt = read(rp->rio_fd, rp->rio_buf, 7 sizeof(rp->rio_buf)); 8 if (rp->rio_cnt < 0) { 9 if (errno != EINTR) /* Interrupted by sig handler return */ 
10 return -1; 
11 
} 12 else if (rp->rio_cnt == 0) /* EOF */ 13 return 0; 14 else 15 rp->rio_bufptr = rp->rio_buf; /* Reset buffer ptr */ 

16 
} 17 18 /* Copy min(n, rp->rio_cnt) bytes from internal buf to user buf */ 19 cnt=n; 20 if (rp->rio_cnt < n) 21 cnt = rp->rio_cnt; 22 memcpy(usrbuf, rp->rio_bufptr, cnt); 23 rp->rio_bufptr += cnt; 24 rp->rio_cnt -= cnt; 25 return cnt; 


26 } 
code/src/csapp.c 

Figure 10.6 The internal rio_read function. 
unread bytes in the read buffer. If the buffer is empty, then it is replenished with a call to read. Receiving a short count from this invocation of read is not an er-ror, and simply has the effect of partially .lling the read buffer. Once the buffer is nonempty, rio_read copies the minimum of n and rp->rio_cnt bytes from the read buffer to the user buffer and returns the number of bytes copied. 
To an application program, the rio_read function has the same semantics as the Unix read function. On error, it returns .1 and sets errno appropriately. On EOF, it returns 0. It returns a short count if the number of requested bytes exceeds the number of unread bytes in the read buffer. The similarity of the two functions makes it easy to build different kinds of buffered read functions by substituting rio_read for read. For example, the rio_readnb function in Figure 10.7 has the same structure as rio_readn, with rio_read substituted for read. Similarly, the rio_readlineb routine in Figure 10.7 calls rio_read at most maxlen-1 times. Each call returns 1 byte from the read buffer, which is then checked for being the terminating newline. 
code/src/csapp.c  
1  ssize_t  rio_readlineb(rio_t  *rp,  void  *usrbuf,  size_t  maxlen)  
2  {  
3  int  n,  rc;  
4  char  c,  *bufp  =  usrbuf;  
5  
6  for  (n  =  1;  n  <  maxlen;  n++)  {  
7  if  ((rc  =  rio_read(rp,  &c,  1))  ==  1)  {  
8  *bufp++  =  c;  
9  if  (c  ==  ¡¯\n¡¯)  
10  break;  
11  } else if (rc == 0) {  
12  if (n == 1)  
13  return  0;  /*  EOF,  no  data  read  */  
14  else  
15  break;  /*  EOF,  some  data  was  read  */  
16  } else  
17  return  -1;  /*  Error  */  
18  }  
19  *bufp  =  0;  
20  return  n;  
21  }  
code/src/csapp.c  
code/src/csapp.c  
1  ssize_t  rio_readnb(rio_t  *rp,  void  *usrbuf,  size_t  n)  
2  {  
3  size_t  nleft  =  n;  
4  ssize_t  nread;  
5  char  *bufp  =  usrbuf;  
6  
7  while  (nleft  >  0)  {  
8  if  ((nread  =  rio_read(rp,  bufp,  nleft))  <  0)  {  
9  if  (errno  ==  EINTR)  /*  Interrupted  by  sig  handler  return  */  
10  nread  =  0;  /*  Call  read()  again  */  
11  else  
12  return  -1;  /*  errno  set  by  read()  */  
13  }  
14  else  if  (nread  ==  0)  
15  break;  /*  EOF  */  
16  nleft -= nread;  
17  bufp += nread;  
18  }  
19  return  (n  -nleft);  /*  Return  >=  0  */  
20  }  
code/src/csapp.c  
Figure 10.7  The rio_readlineb and rio_readnb functions.  

Aside Origins of the Rio package 
The Rio functions are inspired by the readline, readn, and writen functions described by W. Richard Stevens in his classic network programming text [109]. The rio_readn and rio_writen functions are identical to the Stevens readn and writen functions. However, the Stevens readline function has some limitations that are corrected in Rio. First, because readline is buffered and readn is not, these two functions cannot be used together on the same descriptor. Second, because it uses a static buffer, the Stevens readline function is not thread-safe, which required Stevens to introduce a different thread-safe version called readline_r. We have corrected both of these .aws with the rio_readlineb and rio_readnb functions, which are mutually compatible and thread-safe. 
10.5 
Reading 
File 
Metadata 

An application can retrieve information about a .le (sometimes called the .le¡¯s metadata) by calling the stat and fstat functions. 
#include <unistd.h> #include <sys/stat.h> 
int stat(const char *filename, struct stat *buf); int fstat(int fd, struct stat *buf); 
Returns: 0 if OK, .1 on error 

The stat function takes as input a .le name and .lls in the members of a stat structure shown in Figure 10.8. The fstat function is similar, but takes a .le 
statbuf.h (included by sys/stat.h) 

/* Metadata returned by the stat and fstat functions */ 
struct stat { dev_t st_dev; /* Device */ ino_t st_ino; /* inode */ mode_t st_mode; /* Protection and file type */ nlink_t st_nlink; /* Number of hard links */ uid_t st_uid; /* User ID of owner */ gid_t st_gid; /* Group ID of owner */ dev_t st_rdev; /* Device type (if inode device) */ off_t st_size; /* Total size, in bytes */ unsigned long st_blksize; /* Blocksize for filesystem I/O */ unsigned long st_blocks; /* Number of blocks allocated */ time_t st_atime; /* Time of last access */ time_t st_mtime; /* Time of last modification */ time_t st_ctime; /* Time of last change */ 
}; 
statbuf.h (included by sys/stat.h) 

Figure 10.8 The stat structure. 
874  Chapter 10  System-Level I/O  
Macro  Description  
S_ISREG() S_ISDIR() S_ISSOCK()  Is this a regular .le? Is this a directory .le? Is this a network socket?  
Figure 10.9 Macros for determining .le type from the st_mode bits. De.ned in sys/stat.h. 


descriptor instead of a .le name. We will need the st_mode and st_size members of the stat structure when we discuss Web servers in Section 11.5. The other members are beyond our scope. 
The st_size member contains the .le size in bytes. The st_mode member en-codes both the .le permission bits (Figure 10.1) and the .le type. Unix recognizes a number of different .le types. A regular .le contains some sort of binary or text data. To the kernel there is no difference between text .les and binary .les. A directory .le contains information about other .les. A socket is a .le that is used to communicate with another process across a network (Section 11.4). 
Unix provides macro predicates for determining the .le type from the st_ mode member. Figure 10.9 lists a subset of these macros. 
code/io/statcheck.c  
1  #include  "csapp.h"  
2  
3  int  main  (int  argc,  char  **argv)  
4  {  
5  struct  stat  stat;  
6  char  *type,  *readok;  
7  
8  Stat(argv[1],  &stat);  
9  if  (S_ISREG(stat.st_mode))  /*  Determine  file  type  */  
10  type = "regular";  
11  else  if  (S_ISDIR(stat.st_mode))  
12  type  =  "directory";  
13  else  
14  type  =  "other";  
15  if  ((stat.st_mode  &  S_IRUSR))  /*  Check  read  access  */  
16  readok  =  "yes";  
17  else  
18  readok  =  "no";  
19  
20  printf("type:  %s,  read:  %s\n",  type,  readok);  
21  exit(0);  
22  }  
code/io/statcheck.c  
Figure 10.10 Querying and manipulating a .le¡¯s st_mode bits. 



Figure 10.10 shows how we might use these macros and the stat function to read and interpret a .le¡¯s st_mode bits. 
10.6 
Sharing 
Files 

Unix .les can be shared in a number of different ways. Unless you have a clear picture of how the kernel represents open .les, the idea of .le sharing can be quite confusing. The kernel represents open .les using three related data structures: 
. Descriptor table. Each process has its own separate descriptor table whose en-tries are indexed by the process¡¯s open .le descriptors. Each open descriptor entry points to an entry in the .le table. 
. File table.The set of open .les is represented by a .le table that is shared by all processes. Each .le table entry consists of (for our purposes) the current .le position, a reference count of the number of descriptor entries that currently point to it, and a pointer to an entry in the v-node table. Closing a descriptor decrements the reference count in the associated .le table entry. The kernel will not delete the .le table entry until its reference count is zero. 
. v-node table. Like the .le table, the v-node table is shared by all processes. Each entry contains most of the information in the stat structure, including the st_mode and st_size members. 
Figure 10.11 shows an example where descriptors 1 and 4 reference two different .les through distinct open .le table entries. This is the typical situation, where .les are not shared, and where each descriptor corresponds to a distinct .le. 
Multiple descriptors can also reference the same .le through different .le table entries, as shown in Figure 10.12. This might happen, for example, if you were to call the open function twice with the same filename. The key idea is that each descriptor has its own distinct .le position, so different reads on different descriptors can fetch data from different locations in the .le. 
Figure 10.11 Descriptor table Open file table v-node table (one table (shared by (shared by
Typical kernel data 
per process) all processes) all processes)
structures for open 
File A

.les. In this example, 
stdin fd 0

two descriptors reference 
stdout fd 1

distinct .les. There is no 
stderr fd 2 

sharing. fd 3 fd 4 

File access  
File size  
File type  
¡­  

File B 

File access  
File size  
File type  
¡­  


Figure 10.12 

File sharing. This example shows two descriptors sharing the same disk .le through two open .le table entries. 
Descriptor table  Open file table  v-node table  
(one table  (shared by  (shared by  
per process)  all processes)  all processes)  
File A  

fd 0 fd 1 fd 2 fd 3 fd 4 




File access  
File size  
File type  
¡­  


How a child process 
all processes) all processes)

inherits the parent¡¯s open 
Parent¡¯s table File A

.les. The initial situation is in Figure 10.11. fd 0 
fd 1 fd 2 fd 3 fd 4

File access  
File size  
File type  
¡­  

fd 0 fd 1 fd 2 fd 3 fd 4 
File access  
File size  
File type  
¡­  

We can also understand how parent and child processes share .les. Suppose that before a call to fork, the parent process has the open .les shown in Fig-ure 10.11. Then Figure 10.13 shows the situation after the call to fork. The child gets its own duplicate copy of the parent¡¯s descriptor table. Parent and child share the same set of open .le tables, and thus share the same .le position. An important consequence is that the parent and child must both close their descriptors before the kernel will delete the corresponding .le table entry. 
Practice Problem 10.2 
Suppose the disk .le foobar.txt consists of the six ASCII characters ¡°foobar¡±. Then what is the output of the following program? 
1  #include  "csapp.h"  
2  
3  int  main()  


4 { 5 int fd1, fd2; 6 char c; 7 8 fd1 = Open("foobar.txt", O_RDONLY, 0); 9 fd2 = Open("foobar.txt", O_RDONLY, 0); 
10 Read(fd1, &c, 1); 11 Read(fd2, &c, 1); 12 printf("c = %c\n", c); 13 exit(0); 
14 } 

Practice Problem 10.3 
As before, suppose the disk .le foobar.txt consists of the six ASCII characters ¡°foobar¡±. Then what is the output of the following program? 
1  #include "csapp.h"  
2  
3  int  main()  
4  {  
5  int fd;  
6  char c;  
7  
8  fd = Open("foobar.txt", O_RDONLY, 0);  
9  if (Fork() == 0) {  
10  Read(fd, &c, 1);  
11  exit(0);  
12  }  
13  Wait(NULL);  
14  Read(fd, &c, 1);  
15  printf("c = %c\n", c);  
16  exit(0);  
17  }  

10.7 
I/O 
Redirection 

Unix shells provide I/O redirection operators that allow users to associate standard input and output with disk .les. For example, typing 
unix> ls > foo.txt 
causes the shell to load and execute the ls program, with standard output redi-rected to disk .le foo.txt. As we will see in Section 11.5, a Web server performs 
Figure 10.14 

Kernel data structures after redirecting stan-dard output by calling dup2(4,1). The initial situation is shown in Fig-ure 10.11. 
Descriptor table  Open file table  v-node table  
(one table  (shared by  (shared by  
per process)  all processes)  all processes) 
File A  

fd 0 fd 1 fd 2 fd 3 fd 4 

File B 

File access  
File size  
File type  
¡­  


File access  
File size  
File type  
¡­  

a similar kind of redirection when it runs a CGI program on behalf of the client. So how does I/O redirection work? One way is to use the dup2 function. 
#include  <unistd.h>  
int  dup2(int  oldfd,  int  newfd);  
Returns: nonnegative descriptor if OK, .1 on error  

The dup2 function copies descriptor table entry oldfd to descriptor table entry 
newfd, overwriting the previous contents of descriptor table entry newfd.If newfd 
was already open, then dup2 closes newfd before it copies oldfd. 
Suppose that before calling dup2(4,1) we have the situation in Figure 10.11, where descriptor 1 (standard output) corresponds to .le A (say, a terminal), and descriptor 4 corresponds to .le B (say, a disk .le). The reference counts for A and B are both equal to 1. Figure 10.14 shows the situation after calling dup2(4,1). Both descriptors now point to .le B; .le A has been closed and its .le table and v-node table entries deleted; and the reference count for .le B has been incremented. From this point on, any data written to standard output is redirected to .le B. 

Aside Right and left hoinkies 
To avoid confusion with other bracket-type operators such as ¡®]¡¯ and ¡®[¡¯, we have always referred to the shell¡¯s ¡®>¡¯ operator as a ¡°right hoinky¡±, and the ¡®<¡¯ operator as a ¡°left hoinky¡±. 
Practice Problem 10.4 
How would you use dup2 to redirect standard input to descriptor 5? 

Practice Problem 10.5 
Assuming that the disk .le foobar.txt consists of the six ASCII characters ¡°foobar¡±, what is the output of the following program? 
1  #include "csapp.h"  
2  
3  int  main()  
4  {  
5  int fd1, fd2;  
6  char c;  
7  
8  fd1 = Open("foobar.txt",  O_RDONLY,  0);  
9  fd2 = Open("foobar.txt",  O_RDONLY,  0);  
10  Read(fd2, &c, 1);  
11  Dup2(fd2, fd1);  
12  Read(fd1, &c, 1);  
13  printf("c = %c\n", c);  
14  exit(0);  
15  }  

10.8 
Standard 
I/O 

ANSI C de.nes a set of higher level input and output functions, called the standard I/O library, that provides programmers with a higher-level alternative to Unix I/O. The library (libc) provides functions for opening and closing .les (fopen and fclose), reading and writing bytes (fread and fwrite), reading and writing strings (fgets and fputs), and sophisticated formatted I/O (scanf and printf). 
The standard I/O library models an open .le as a stream. To the programmer, a stream is a pointer to a structure of type FILE. Every ANSI C program begins with three open streams, stdin, stdout, and stderr, which correspond to standard input, standard output, and standard error, respectively: 
#include <stdio.h> extern FILE *stdin; /* Standard input (descriptor 0) */ extern FILE *stdout; /* Standard output (descriptor 1) */ extern FILE *stderr; /* Standard error (descriptor 2) */ 
A stream of type FILE is an abstraction for a .le descriptor and a stream buffer. The purpose of the stream buffer is the same as the Rio read buffer: to minimize the number of expensive Unix I/O system calls. For example, suppose we have a program that makes repeated calls to the standard I/O getc function, where each invocation returns the next character from a .le. When getc is called the .rst time, the library .lls the stream buffer with a single call to the read function, and then returns the .rst byte in the buffer to the application. As long as there are unread bytes in the buffer, subsequent calls to getc can be served directly from the stream buffer. 
10.9 
Putting 
It 
Together: 
Which 
I/O 
Functions 
Should 
I 
Use? 

Figure 10.15 summarizes the various I/O packages that we have discussed in this chapter. Unix I/O is implemented in the operating system kernel. It is available to applications through functions such as open, close, lseek, read, write, and stat functions. The higher-level Rio and standard I/O functions are implemented ¡°on top of¡± (using) the Unix I/O functions. The Rio functions are robust wrappers for read and write that were developed speci.cally for this textbook. They au-tomatically deal with short counts and provide an ef.cient buffered approach for reading text lines. The standard I/O functions provide a more complete buffered alternative to the Unix I/O functions, including formatted I/O routines. 
So which of these functions should you use in your programs? The standard I/O functions are the method of choice for I/O on disk and terminal devices. Most C programmers use standard I/O exclusively throughout their careers, never both-ering with the lower-level Unix I/O functions. Whenever possible, we recommend that you do likewise. 
Unfortunately, standard I/O poses some nasty problems when we attempt to use it for input and output on networks. As we will see in Section 11.4, the Unix abstraction for a network is a type of .le called a socket. Like any Unix .le, sockets are referenced by .le descriptors, known in this case as socket descriptors. Application processes communicate with processes running on other computers by reading and writing socket descriptors. 
Standard I/O streams are full duplex in the sense that programs can perform input and output on the same stream. However, there are poorly documented restrictions on streams that interact badly with restrictions on sockets: 
. Restriction 1: Input functions following output functions. An input function cannot follow an output function without an intervening call to fflush, fseek, fsetpos,or rewind.The fflush function empties the buffer associated with 
fopen fread fscanf sscanf fgets fflush fclose fdopen fwrite fprintf sprintf fputs fseek rio_readn rio_writen rio_readinitb rio_readlineb rio_readnbopen read write lseek stat close 


a stream. The latter three functions use the Unix I/O lseek function to reset the current .le position. 
. Restriction 2: Output functions following input functions. An output function cannot follow an input function without an intervening call to fseek, fsetpos, or rewind, unless the input function encounters an end-of-.le. 
These restrictions pose a problem for network applications because it is illegal to use the lseek function on a socket. The .rst restriction on stream I/O can be worked around by adopting a discipline of .ushing the buffer before every input operation. However, the only way to work around the second restriction is to open two streams on the same open socket descriptor, one for reading and one for writing: 
FILE *fpin, *fpout; 
fpin = fdopen(sockfd, "r"); 
fpout = fdopen(sockfd, "w"); 
But this approach has problems as well, because it requires the application to call fclose on both streams in order to free the memory resources associated with each stream and avoid a memory leak: 
fclose(fpin); 

fclose(fpout); 
Each of these operations attempts to close the same underlying socket descrip-tor, so the second close operation will fail. This is not a problem for sequential programs, but closing an already closed descriptor in a threaded program is a recipe for disaster (see Section 12.7.4). 
Thus, we recommend that you not use the standard I/O functions for input and output on network sockets. Use the robust Rio functions instead. If you need formatted output, use the sprintf function to format a string in memory, and then send it to the socket using rio_writen. If you need formatted input, use rio_ readlineb to read an entire text line, and then use sscanf to extract different .elds from the text line. 
10.10 
Summary 

Unix provides a small number of system-level functions that allow applications to open, close, read, and write .les; fetch .le metadata; and perform I/O redirection. Unix read and write operations are subject to short counts that applications must anticipate and handle correctly. Instead of calling the Unix I/O functions directly, applications should use the Rio package, which deals with short counts automatically by repeatedly performing read and write operations until all of the requested data have been transferred. 
The Unix kernel uses three related data structures to represent open .les. Entries in a descriptor table point to entries in the open .le table, which point to entries in the v-node table. Each process has its own distinct descriptor table, while all processes share the same open .le and v-node tables. Understanding the general organization of these structures clari.es our understanding of both .le sharing and I/O redirection. 
The standard I/O library is implemented on top of Unix I/O and provides a powerful set of higher-level I/O routines. For most applications, standard I/O is the simpler, preferred alternative to Unix I/O. However, because of some mutually incompatible restrictions on standard I/O and network .les, Unix I/O, rather than standard I/O, should be used for network applications. 
Bibliographic 
Notes 

Stevens wrote the standard reference text for Unix I/O [110]. Kernighan and Ritchie give a clear and complete discussion of the standard I/O functions [58]. 
Homework 
Problems 

10.6 ¡ô 
What is the output of the following program? 
1 #include "csapp.h" 2 3 int main() 4 { 5 int fd1, fd2; 6 7 fd1 = Open("foo.txt", O_RDONLY, 0); 8 fd2 = Open("bar.txt", O_RDONLY, 0); 9 Close(fd2); 
10 fd2 = Open("baz.txt", O_RDONLY, 0); 11 printf("fd2 = %d\n", fd2); 12 exit(0); 
13 } 
10.7 ¡ô 
Modify the cpfile program in Figure 10.4 so that it uses the Rio functions to copy standard input to standard output, MAXBUF bytes at a time. 
10.8 ¡ô¡ô 
Write a version of the statcheck program in Figure 10.10, called fstatcheck, that takes a descriptor number on the command line rather than a .le name. 
10.9 ¡ô¡ô 
Consider the following invocation of the fstatcheck program from Problem 10.8: 
unix> fstatcheck 3 < foo.txt 

You might expect that this invocation of fstatcheck would fetch and display metadata for .le foo.txt. However, when we run it on our system, it fails with a ¡°bad .le descriptor.¡± Given this behavior, .ll in the pseudo-code that the shell must be executing between the fork and execve calls: 
if (Fork() == 0) { /* Child */ 
/* What code is the shell executing right here? */ 
Execve("fstatcheck", argv, envp); 
} 
10.10 ¡ô¡ô 

Modify the cpfile program in Figure 10.4 so that it takes an optional command line argument infile.If infile is given, then copy infile to standard output; otherwise, copy standard input to standard output as before. The twist is that your solution must use the original copy loop (lines 9¨C11) for both cases. You are only allowed to insert code, and you are not allowed to change any of the existing code. 
Solutions 
to 
Practice 
Problems 

Solution to Problem 10.1 (page 865) 
Unix processes begin life with open descriptors assigned to stdin (descriptor 0), stdout (descriptor 1), and stderr (descriptor 2). The open function always re-turns the lowest unopened descriptor, so the .rst call to open returns descriptor 3. The call to the close function frees up descriptor 3. The .nal call to open returns descriptor 3, and thus the output of the program is ¡°fd2=3¡±. 
Solution to Problem 10.2 (page 876) 
The descriptors fd1 and fd2 each have their own open .le table entry, so each descriptor has its own .le position for foobar.txt. Thus, the read from fd2 reads the .rst byte of foobar.txt, and the output is 
c=f 
and not 
c=o 

as you might have thought initially. 
Solution to Problem 10.3 (page 877) 
Recall that the child inherits the parent¡¯s descriptor table and that all processes shared the same open .le table. Thus, the descriptor fd in both the parent and child points to the same open .le table entry. When the child reads the .rst byte of the .le, the .le position increases by one. Thus, the parent reads the second byte, and the output is 
c=o 

Solution to Problem 10.4 (page 878) 
To redirect standard input (descriptor 0) to descriptor 5, we would call dup2(5,0), or equivalently, dup2(5,STDIN_FILENO). 
Solution to Problem 10.5 (page 879) 
At .rst glance, you might think the output would be 
c=f but because we are redirecting fd1 to fd2, the output is really c=o 

CHAPTER 
11 
Network 
Programming 
11.1 
The 
Client-Server 
Programming 
Model 
886 
11.2 
Networks 
887 
11.3 
The 
Global 
IP 
Internet 
891 
11.4 
The 
Sockets 
Interface 
900 
11.5 
Web 
Servers 
911 
11.6 
Putting 
It 
Together: 
The 
Tiny 
Web 
Server 
919 
11.7 
Summary 
927 
Bibliographic 
Notes 
928 
Homework 
Problems 
928 
Solutions 
to 
Practice 
Problems 
929 
Network applications are everywhere. Any time you browse the Web, send an email message, or pop up an X window, you are using a network application. Interestingly, all network applications are based on the same basic programming model, have similar overall logical structures, and rely on the same programming interface. 
Network applications rely on many of the concepts that you have already learned in our study of systems. For example, processes, signals, byte ordering, memory mapping, and dynamic storage allocation all play important roles. There are new concepts to master as well. We will need to understand the basic client-server programming model and how to write client-server programs that use the services provided by the Internet. At the end, we will tie all of these ideas together by developing a small but functional Web server that can serve both static and dynamic content with text and graphics to real Web browsers. 
11.1 
The 
Client-Server 
Programming 
Model 

Every network application is based on the client-server model. With this model, an application consists of a server process and one or more client processes. A server manages some resource, and it provides some service for its clients by manipulating that resource. For example, a Web server manages a set of disk .les that it retrieves and executes on behalf of clients. An FTP server manages a set of disk .les that it stores and retrieves for clients. Similarly, an email server manages a spool .le that it reads and updates for clients. 
The fundamental operation in the client-server model is the transaction (Fig-ure 11.1). A client-server transaction consists of four steps: 
1. 
When a client needs service, it initiates a transaction by sending a request to the server. For example, when a Web browser needs a .le, it sends a request to a Web server. 

2. 
The server receives the request, interprets it, and manipulates its resources in the appropriate way. For example, when a Web server receives a request from a browser, it reads a disk .le. 

3. 
The server sends a response to the client, and then waits for the next request. For example, a Web server sends the .le back to a client. 

4. 
The client receives the response and manipulates it. For example, after a Web browser receives a page from the server, it displays it on the screen. 



processes processes response request 
Figure 11.1 A client-server transaction. 

It is important to realize that clients and servers are processes and not ma-chines, or hosts as they are often called in this context. A single host can run many different clients and servers concurrently, and a client and server transaction can be on the same or different hosts. The client-server model is the same, regardless of the mapping of clients and servers to hosts. 
Aside Client-server transactions vs. database transactions 
Client-server transactions are not database transactions and do not share any of their properties, such as atomicity. In our context, a transaction is simply a sequence of steps carried out by a client and a server. 
11.2 
Networks 

Clients and servers often run on separate hosts and communicate using the hard-ware and software resources of a computer network. Networks are sophisticated systems, and we can only hope to scratch the surface here. Our aim is to give you a workable mental model from a programmer¡¯s perspective. 
To a host, a network is just another I/O device that serves as a source and sink for data, as shown in Figure 11.2. An adapter plugged into an expansion slot on the I/O bus provides the physical interface to the network. Data received from the network is copied from the adapter across the I/O and memory buses into memory, typically by a DMA transfer. Similarly, data can also be copied from memory to the network. 

Figure 11.3 
Ethernet segment. 

Physically, a network is a hierarchical system that is organized by geographical proximity. At the lowest level is a LAN (Local Area Network) that spans a building or a campus. The most popular LAN technology by far is Ethernet, which was developed in the mid-1970s at Xerox PARC. Ethernet has proven to be remarkably resilient, evolving from 3 Mb/s to 10 Gb/s. 
An Ethernet segment consists of some wires (usually twisted pairs of wires) and a small box called a hub, as shown in Figure 11.3. Ethernet segments typically span small areas, such as a room or a .oor in a building. Each wire has the same maximum bit bandwidth, typically 100 Mb/s or 1 Gb/s. One end is attached to an adapter on a host, and the other end is attached to a port on the hub. A hub slavishly copies every bit that it receives on each port to every other port. Thus, every host sees every bit. 
Each Ethernet adapter has a globally unique 48-bit address that is stored in a non-volatile memory on the adapter. A host can send a chunk of bits called a frame to any other host on the segment. Each frame includes some .xed number of header bits that identify the source and destination of the frame and the frame length, followed by a payload of data bits. Every host adapter sees the frame, but only the destination host actually reads it. 
Multiple Ethernet segments can be connected into larger LANs, called bridged Ethernets, using a set of wires and small boxes called bridges, as shown in Figure 11.4. Bridged Ethernets can span entire buildings or campuses. In a bridged Ethernet, some wires connect bridges to bridges, and others connect bridges to hubs. The bandwidths of the wires can be different. In our example, the bridge¨Cbridge wire has a 1 Gb/s bandwidth, while the four hub¨Cbridge wires have bandwidths of 100 Mb/s. 
Bridges make better use of the available wire bandwidth than hubs. Using a clever distributed algorithm, they automatically learn over time which hosts are reachable from which ports, and then selectively copy frames from one port to another only when it is necessary. For example, if host A sends a frame to host B, which is on the segment, then bridge X will throw away the frame when it arrives at its input port, thus saving bandwidth on the other segments. However, if host A sends a frame to host C on a different segment, then bridge X will copy the frame only to the port connected to bridge Y, which will copy the frame only to the port connected to bridge C¡¯s segment. 
To simplify our pictures of LANs, we will draw the hubs and bridges and the wires that connect them as a single horizontal line, as shown in Figure 11.5. 
At a higher level in the hierarchy, multiple incompatible LANs can be con-nected by specialized computers called routers to form an internet (interconnected network). 

AB 




100 Mb/s 

100 Mb/s 
1 Gb/s 
100 Mb/s 

100 Mb/s 

X 
Y 






Figure 11.4 Bridged Ethernet segments. 
Aside Internet vs. internet 
We will always use lowercase internet to denote the general concept, and uppercase Internet to denote a speci.c implementation, namely the global IP Internet. 
Each router has an adapter (port) for each network that it is connected to. Routers can also connect high-speed point-to-point phone connections, which are examples of networks known as WANs (Wide-Area Networks), so called because they span larger geographical areas than LANs. In general, routers can be used to build internets from arbitrary collections of LANs and WANs. For example, Figure 11.6 shows an example internet with a pair of LANs and WANs connected by three routers. 
The crucial property of an internet is that it can consist of different LANs and WANs with radically different and incompatible technologies. Each host is physically connected to every other host, but how is it possible for some source host to send data bits to another destination host across all of these incompatible networks? 
The solution is a layer of protocol software running on each host and router that smoothes out the differences between the different networks. This software 
Figure 11.5 
. . .

Conceptual view of a LAN. 



implements a protocol that governs how hosts and routers cooperate in order to transfer data. The protocol must provide two basic capabilities: 
. Naming scheme.Different LAN technologies have different and incompatible ways of assigning addresses to hosts. The internet protocol smoothes these differences by de.ning a uniform format for host addresses. Each host is then assigned at least one of these internet addresses that uniquely identi.es it. 
. Delivery mechanism. Different networking technologies have different and incompatible ways of encoding bits on wires and of packaging these bits into frames. The internet protocol smoothes these differences by de.ning a uniform way to bundle up data bits into discrete chunks called packets.A packet consists of a header, which contains the packet size and addresses of the source and destination hosts, and a payload, which contains data bits sent from the source host. 
Figure 11.7 shows an example of how hosts and routers use the internet protocol to transfer data across incompatible LANs. The example internet consists of two LANs connected by a router. A client running on host A, which is attached to LAN1, sends a sequence of data bytes to a server running on host B, which is attached to LAN2. There are eight basic steps: 
1. 
The client on host A invokes a system call that copies the data from the client¡¯s virtual address space into a kernel buffer. 

2. 
The protocol software on host A creates a LAN1 frame by appending an internet header and a LAN1 frame header to the data. The internet header is addressed to internet host B. The LAN1 frame header is addressed to the router. It then passes the frame to the adapter. Notice that the payload of the LAN1 frame is an internet packet, whose payload is the actual user data. This kind of encapsulation is one of the fundamental insights of internetworking. 

3. 
The LAN1 adapter copies the frame to the network. 

4. 
When the frame reaches the router, the router¡¯s LAN1 adapter reads it from the wire and passes it to the protocol software. 

5. 
The router fetches the destination internet address from the internet packet header and uses this as an index into a routing table to determine where to forward the packet, which in this case is LAN2. The router then strips off the 


Host A Host B 

Router 

(6) 
(1) Data 
internet packet 
(2) Data PH FH1 LAN1 frame 

(3)  
LAN1  
(4)  


(8) Data 

(7) 


LAN1 adapter  LAN2 adapter  

Protocol software  
Figure 11.7 How data travels from one host to another on an internet. Key: PH: internet packet header; FH1: frame header for LAN1; FH2: frame header for LAN2. 


Lan 2 frame 
LAN2 

FH1 

old LAN1 frame header, prepends a new LAN2 frame header addressed to 
host B, and passes the resulting frame to the adapter. 
6. 
The router¡¯s LAN2 adapter copies the frame to the network. 

7. 
When the frame reaches host B, its adapter reads the frame from the wire and passes it to the protocol software. 

8. 
Finally, the protocol software on host B strips off the packet header and frame header. The protocol software will eventually copy the resulting data into the server¡¯s virtual address space when the server invokes a system call that reads the data. 


Of course, we are glossing over many dif.cult issues here. What if different net-works have different maximum frame sizes? How do routers know where to for-ward frames? How are routers informed when the network topology changes? What if a packet gets lost? Nonetheless, our example captures the essence of the internet idea, and encapsulation is the key. 
11.3 
The 
Global 
IP 
Internet 

The global IP Internet is the most famous and successful implementation of an internet. It has existed in one form or another since 1969. While the internal architecture of the Internet is complex and constantly changing, the organization of client-server applications has remained remarkably stable since the early 1980s. Figure 11.8 shows the basic hardware and software organization of an Internet 
(5) 

Figure 11.8 

Hardware and software organization of an Internet application.  Sockets interface (system calls)  
Hardware interface (interrupts)  

Internet client host Internet server host 

client-server application. Each Internet host runs software that implements the TCP/IP protocol (Transmission Control Protocol/Internet Protocol), which is supported by almost every modern computer system. Internet clients and servers communicate using a mix of sockets interface functions and Unix I/O functions. (We will describe the sockets interface in Section 11.4.) The sockets functions are typically implemented as system calls that trap into the kernel and call various kernel-mode functions in TCP/IP. 
TCP/IP is actually a family of protocols, each of which contributes different capabilities. For example, the IP protocol provides the basic naming scheme and a delivery mechanism that can send packets, known as datagrams, from one Internet host to any other host. The IP mechanism is unreliable in the sense that it makes no effort to recover if datagrams are lost or duplicated in the network. UDP (Unreliable Datagram Protocol) extends IP slightly, so that packets can be transferred from process to process, rather than host to host. TCP is a complex protocol that builds on IP to provide reliable full duplex (bidirectional) connections between processes. To simplify our discussion, we will treat TCP/IP as a single monolithic protocol. We will not discuss its inner workings, and we will only discuss some of the basic capabilities that TCP and IP provide to application programs. We will not discuss UDP. 
From a programmer¡¯s perspective, we can think of the Internet as a worldwide collection of hosts with the following properties: 
. . .  The set of hosts is mapped to a set of 32-bit IP addresses. The set of IP addresses is mapped to a set of identi.ers called Internet domain names. A process on one Internet host can communicate with a process on any other Internet host over a connection.  
The next three sections discuss these fundamental Internet ideas in more  

detail. 
netinet/in.h 

/* Internet address structure */ 
struct in_addr { 

unsigned int s_addr; /* Network byte order (big-endian) */ }; 
netinet/in.h 

Figure 11.9 IP address structure. 
11.3.1 IP Addresses 
An IP address is an unsigned 32-bit integer. Network programs store IP addresses in the IP address structure shown in Figure 11.9. 
Aside Why store the scalar IP address in a structure? 
Storing a scalar address in a structure is an unfortunate artifact from the early implementations of the sockets interface. It would make more sense to de.ne a scalar type for IP addresses, but it is too late to change now because of the enormous installed base of applications. 
Because Internet hosts can have different host byte orders, TCP/IP de.nes a uniform network byte order (big-endian byte order) for any integer data item, such as an IP address, that is carried across the network in a packet header. Addresses in IP address structures are always stored in (big-endian) network byte order, even if the host byte order is little-endian. Unix provides the following functions for converting between network and host byte order: 
#include <netinet/in.h> 
unsigned long int htonl(unsigned long int hostlong); unsigned short int htons(unsigned short int hostshort); 
Returns: value in network byte order 

unsigned long int ntohl(unsigned long int netlong); unsigned short int ntohs(unsigned short int netshort); 
Returns: value in host byte order 

The htonl function converts a 32-bit integer from host byte order to network byte order. The ntohl function converts a 32-bit integer from network byte or-der to host byte order. The htons and ntohs functions perform corresponding conversions for 16-bit integers. 
IP addresses are typically presented to humans in a form known as dotted-decimal notation, where each byte is represented by its decimal value and sep-arated from the other bytes by a period. For example, 128.2.194.242 is the dotted-decimal representation of the address 0x8002c2f2. On Linux systems, you can use the hostname command to determine the dotted-decimal address of your own host: 
linux> hostname -i 128.2.194.242 
Internet programs convert back and forth between IP addresses and dotted-decimal strings using the functions inet_aton and inet_ntoa: 
#include <arpa/inet.h> 
int inet_aton(const char *cp, struct in_addr *inp); 
Returns: 1 if OK, 0 on error 
char *inet_ntoa(struct in_addr in); 
Returns: pointer to a dotted-decimal string 
The inet_aton function converts a dotted-decimal string (cp) to an IP address in network byte order (inp). Similarly, the inet_ntoa function converts an IP address in network byte order to its corresponding dotted-decimal string. Notice that a call to inet_aton passes a pointer to a structure, while a call to inet_ntoa passes the structure itself. 

Aside What do ntoa and aton mean? 
The ¡°n¡± denotes network representation. The ¡°a¡± denotes application representation. The ¡°to¡± means to. 
Practice Problem 11.1 
Complete the following table: 
Hex address Dotted-decimal address 
0x0 
0xffffffff 0x7f000001 
205.188.160.121 
64.12.149.13 
205.188.146.23 
Practice Problem 11.2 
Write a program hex2dd.c that converts its hex argument to a dotted-decimal string and prints the result. For example, 
Section 11.3 The Global IP Internet 895 

unix> ./hex2dd 0x8002c2f2 128.2.194.242 
Practice Problem 11.3 
Write a program dd2hex.c that converts its dotted-decimal argument to a hex number and prints the result. For example, 
unix> ./dd2hex 128.2.194.242 0x8002c2f2 
11.3.2 Internet Domain Names 
Internet clients and servers use IP addresses when they communicate with each other. However, large integers are dif.cult for people to remember, so the Internet also de.nes a separate set of more human-friendly domain names, as well as a mechanism that maps the set of domain names to the set of IP addresses. A domain name is a sequence of words (letters, numbers, and dashes) separated by periods, such as 
kittyhawk.cmcl.cs.cmu.edu 
The set of domain names forms a hierarchy, and each domain name encodes its position in the hierarchy. An example is the easiest way to understand this. Figure 11.10 shows a portion of the domain name hierarchy. The hierarchy is 
unnamed root 

mil edu gov com First-level domain names 

mit cmu berkeley amazon Second-level domain names 

cs ece www Third-level domain names 208.216.181.15 


cmcl pdl 
kittyhawk imperial 128.2.194.242 128.2.189.40 
Figure 11.10 Subset of the Internet domain name hierarchy. 
netdb.h 

/* DNS host entry structure */ 
struct hostent { char *h_name; /* Official domain name of host */ char **h_aliases; /* Null-terminated array of domain names */ int h_addrtype; /* Host address type (AF_INET) */ int h_length; /* Length of an address, in bytes */ char **h_addr_list; /* Null-terminated array of in_addr structs */ 
}; 
netdb.h 
Figure 11.11 DNS host entry structure. 
represented as a tree. The nodes of the tree represent domain names that are formed by the path back to the root. Subtrees are referred to as subdomains.The .rst level in the hierarchy is an unnamed root node. The next level is a collection of .rst-level domain names that are de.ned by a nonpro.t organization called ICANN (Internet Corporation for Assigned Names and Numbers). Common .rst-level domains include com, edu, gov, org, and net. 
At the next level are second-level domain names such as cmu.edu, which are assigned on a .rst-come .rst-serve basis by various authorized agents of ICANN. Once an organization has received a second-level domain name, then it is free to create any other new domain name within its subdomain. 
The Internet de.nes a mapping between the set of domain names and the set of IP addresses. Until 1988, this mapping was maintained manually in a sin-gle text .le called HOSTS.TXT. Since then, the mapping has been maintained in a distributed world-wide database known as DNS (Domain Name System). Con-ceptually, the DNS database consists of millions of the host entry structures shown in Figure 11.11, each of which de.nes the mapping between a set of domain names (an of.cial name and a list of aliases) and a set of IP addresses. In a mathematical sense, you can think of each host entry as an equivalence class of domain names and IP addresses. 
Internet applications retrieve arbitrary host entries from the DNS database by calling the gethostbyname and gethostbyaddr functions. 
#include <netdb.h> 
struct hostent *gethostbyname(const char *name); Returns: non-NULL pointer if OK, NULL pointer on error with h_errno set 
struct hostent *gethostbyaddr(const char *addr, int len, 0); Returns: non-NULL pointer if OK, NULL pointer on error with h_errno set 
The gethostbyname function returns the host entry associated with the do-main name name.The gethostbyaddr function returns the host entry associated with the IP address addr. The second argument gives the length in bytes of an IP 
code/netp/hostinfo.c 

1 #include "csapp.h" 2 3 int main(int argc, char **argv) 4 { 5 char **pp; 6 struct in_addr addr; 7 struct hostent *hostp; 8 9 if (argc != 2) { 
10 fprintf(stderr, "usage: %s <domain name or dotted-decimal>\n", 11 argv[0]); 12 exit(0); 
13 
} 14 15 if (inet_aton(argv[1], &addr) != 0) 16 hostp = Gethostbyaddr((const char *)&addr, sizeof(addr), AF_INET); 17 else 18 hostp = Gethostbyname(argv[1]); 19 20 printf("official hostname: %s\n", hostp->h_name); 21 22 for (pp = hostp->h_aliases; *pp != NULL; pp++) 23 printf("alias: %s\n", *pp); 24 25 for (pp = hostp->h_addr_list; *pp != NULL; pp++) { 26 addr.s_addr = ((struct in_addr *)*pp)->s_addr; 27 printf("address: %s\n", inet_ntoa(addr)); 

28 
} 29 exit(0); 



30 } 
code/netp/hostinfo.c 

Figure 11.12 Retrieves and prints a DNS host entry. 
address, which for the current Internet is always 4 bytes. For our purposes, the third argument is always zero. 
We can explore some of the properties of the DNS mapping with the hostinfo program in Figure 11.12, which reads a domain name or dotted-decimal address from the command line and displays the corresponding host entry. Each Internet host has the locally de.ned domain name localhost, which always maps to the loopback address 127.0.0.1: 
unix> ./hostinfo localhost official hostname: localhost alias: localhost.localdomain address: 127.0.0.1 
The localhost name provides a convenient and portable way to reference clients and servers that are running on the same machine, which can be especially useful for debugging. We can use hostname to determine the real domain name of our local host: 
unix> hostname bluefish.ics.cs.cmu.edu 
In the simplest case, there is a one-to-one mapping between a domain name and an IP address: 
unix> ./hostinfo bluefish.ics.cs.cmu.edu official hostname: bluefish.ics.cs.cmu.edu alias: bluefish.alias.cs.cmu.edu address: 128.2.205.216 
However, in some cases, multiple domain names are mapped to the same IP address: 
unix> ./hostinfo cs.mit.edu official hostname: eecs.mit.edu alias: cs.mit.edu address: 18.62.1.6 
In the most general case, multiple domain names can be mapped to multiple IP addresses: 
unix> ./hostinfo google.com official hostname: google.com address: 74.125.45.100 address: 74.125.67.100 address: 74.125.127.100 
Finally, we notice that some valid domain names are not mapped to any IP address: 
unix> ./hostinfo edu Gethostbyname error: No address associated with name unix> ./hostinfo cmcl.cs.cmu.edu Gethostbyname error: No address associated with name 

Aside How many Internet hosts are there? 
Twice a year since 1987, the Internet Software Consortium conducts the Internet Domain Survey.The survey, which estimates the number of Internet hosts by counting the number of IP addresses that have been assigned a domain name, reveals an amazing trend. Since 1987, when there were about 20,000 Internet hosts, the number of hosts has roughly doubled each year. By June 2009, there were nearly 700,000,000 Internet hosts! 
Practice Problem 11.4 
Compile the hostinfo program from Figure 11.12. Then run hostinfo google.com three times in a row on your system. 
A. What do you notice about the ordering of the IP addresses in the three host entries? 
B. How might this ordering be useful? 
11.3.3 Internet Connections 
Internet clients and servers communicate by sending and receiving streams of bytes over connections. A connection is point-to-point in the sense that it connects a pair of processes. It is full-duplex in the sense that data can .ow in both directions at the same time. And it is reliable in the sense that¡ªbarring some catastrophic failure such as a cable cut by the proverbial careless backhoe operator¡ªthe stream of bytes sent by the source process is eventually received by the destination process in the same order it was sent. 
A socket is an end point of a connection. Each socket has a corresponding socket address that consists of an Internet address and a 16-bit integer port, and is denoted by address:port. The port in the client¡¯s socket address is assigned automatically by the kernel when the client makes a connection request, and is known as an ephemeral port. However, the port in the server¡¯s socket address is typically some well-known port that is associated with the service. For example, Web servers typically use port 80, and email servers use port 25. On Unix machines, the .le /etc/services contains a comprehensive list of the services provided on that machine, along with their well-known ports. 
A connection is uniquely identi.ed by the socket addresses of its two end-points. This pair of socket addresses is known as a socket pair and is denoted by the tuple 
(cliaddr:cliport, servaddr:servport) 
where cliaddr is the client¡¯s IP address, cliport is the client¡¯s port, servaddr is the server¡¯s IP address, and servport is the server¡¯s port. For example, Fig-ure 11.13 shows a connection between a Web client and a Web server. 

Client host address Server host address 128.2.194.242 208.216.181.15 
Figure 11.13 Anatomy of an Internet connection. 
In this example, the Web client¡¯s socket address is 
128.2.194.242:51213 
where port 51213 is an ephemeral port assigned by the kernel. The Web server¡¯s socket address is 
208.216.181.15:80 
where port 80 is the well-known port associated with Web services. Given these client and server socket addresses, the connection between the client and server is uniquely identi.ed by the socket pair 
(128.2.194.242:51213, 208.216.181.15:80) 

Aside Origins of the Internet 
The Internet is one of the most successful examples of government, university, and industry partnership. Many factors contributed to its success, but we think two are particularly important: a sustained 30-year investment by the United States government, and a commitment by passionate researchers to what Dave Clarke at MIT has dubbed ¡°rough consensus and working code.¡± 
The seeds of the Internet were sown in 1957, when, at the height of the Cold War, the Soviet Union shocked the world by launching Sputnik, the .rst arti.cial earth satellite. In response, the United States government created the Advanced Research Projects Agency (ARPA), whose charter was to reestablish the U.S. lead in science and technology. In 1967, Lawrence Roberts at ARPA published plans for a new network called the ARPANET. The .rst ARPANET nodes were up and running by 1969. By 1971, there were 13 ARPANET nodes, and email had emerged as the .rst important network application. 
In 1972, Robert Kahn outlined the general principles of internetworking: a collection of intercon-nected networks, with communication between the networks handled independently on a ¡°best-effort basis¡± by black boxes called ¡°routers.¡± In 1974, Kahn and Vinton Cerf published the .rst details of TCP/IP, which by 1982 had become the standard internetworking protocol for ARPANET. On January 1, 1983, every node on the ARPANET switched to TCP/IP, marking the birth of the global IP Internet. 
In 1985, Paul Mockapetris invented DNS, and there were over 1000 Internet hosts. The next year, the National Science Foundation (NSF) built the NSFNET backbone connecting 13 sites with 56 Kb/s phone lines. It was later upgraded to 1.5 Mb/s T1 links in 1988, and 45 Mb/s T3 links in 1991. By 1988, there were more than 50,000 hosts. In 1989, the original ARPANET was of.cially retired. In 1995, when there were almost 10,000,000 Internet hosts, NSF retired NSFNET and replaced it with the modern Internet architecture based on private commercial backbones connected by public network access points. 
11.4 
The 
Sockets 
Interface 

The sockets interface is a set of functions that are used in conjunction with the Unix I/O functions to build network applications. It has been implemented on most modern systems, including all Unix variants, Windows, and Macintosh systems. 
Client Server 


open_listenfd open_clientfd 
Connection 

Await connection request from next client 

Figure 11.14 Overview of the sockets interface. 
Figure 11.14 gives an overview of the sockets interface in the context of a typical client-server transaction. You should use this picture as a road map when we discuss the individual functions. 
Aside Origins of the sockets interface 
The sockets interface was developed by researchers at University of California, Berkeley, in the early 1980s. For this reason, it is often referred to as Berkeley sockets. The Berkeley researchers developed the sockets interface to work with any underlying protocol. The .rst implementation was for TCP/IP, which they included in the Unix 4.2BSD kernel and distributed to numerous universities and labs. This was an important event in Internet history. Almost overnight, thousands of people had access to TCP/IP and its source codes. It generated tremendous excitement and sparked a .urry of new research in networking and internetworking. 
11.4.1 Socket Address Structures 
From the perspective of the Unix kernel, a socket is an end point for communi-cation. From the perspective of a Unix program, a socket is an open .le with a corresponding descriptor. 
Internet socket addresses are stored in 16-byte structures of the type sockaddr_in, shown in Figure 11.15. For Internet applications, the sin_family member is AF_INET, the sin_port member is a 16-bit port number, and the sin_ addr member is a 32-bit IP address. The IP address and port number are always stored in network (big-endian) byte order. 
sockaddr: socketbits.h (included by socket.h), sockaddr_in: netinet/in.h 

/* Generic socket address structure (for connect, bind, and accept) */ 
struct sockaddr { unsigned short sa_family; /* Protocol family */ char sa_data[14]; /* Address data. */ 
}; 

/* Internet-style socket address structure */ 
struct sockaddr_in { unsigned short sin_family; /* Address family (always AF_INET) */ unsigned short sin_port; /* Port number in network byte order */ struct in_addr sin_addr; /* IP address in network byte order */ unsigned char sin_zero[8]; /* Pad to sizeof(struct sockaddr) */ 
}; 
sockaddr: socketbits.h (included by socket.h), sockaddr_in: netinet/in.h 
Figure 11.15 Socket address structures. The in_addr struct is shown in Figure 11.9. 

Aside What does the _in suf.x mean? The _in suf.x is short for internet, not input. 
The connect, bind, and accept functions require a pointer to a protocol-speci.c socket address structure. The problem faced by the designers of the sockets interface was how to de.ne these functions to accept any kind of socket address structure. Today we would use the generic void * pointer, which did not exist in C at that time. The solution was to de.ne sockets functions to expect a pointer to a generic sockaddr structure, and then require applications to cast pointers to protocol-speci.c structures to this generic structure. To simplify our code exam-ples, we follow Stevens¡¯s lead and de.ne the following type: 
typedef struct sockaddr SA; 
We then use this type whenever we need to cast a sockaddr_in structure to a generic sockaddr structure. (See line 20 of Figure 11.16 for an example.) 
11.4.2 The socket Function 
Clients and servers use the socket function to create a socket descriptor. 
#include <sys/types.h> #include <sys/socket.h> 
int socket(int domain, int type, int protocol); 
Returns: nonnegative descriptor if OK, .1 on error 

In our codes, we will always call the socket function with the arguments 
clientfd = Socket(AF_INET, SOCK_STREAM, 0); 
where AF_INET indicates that we are using the Internet, and SOCK_STREAM indicates that the socket will be an end point for an Internet connection. The clientfd descriptor returned by socket is only partially opened and cannot yet be used for reading and writing. How we .nish opening the socket depends on whether we are a client or a server. The next section describes how we .nish opening the socket if we are a client. 
11.4.3 The connect Function 
A client establishes a connection with a server by calling the connect function. 
#include  <sys/socket.h>  
int  connect(int  sockfd,  struct  sockaddr  *serv_addr,  int  addrlen);  
Returns: 0 if OK, .1 on error  

The connect function attempts to establish an Internet connection with the server at socket address serv_addr, where addrlen is sizeof(sockaddr_in). The connect function blocks until either the connection is successfully established or an error occurs. If successful, the sockfd descriptor is now ready for reading and writing, and the resulting connection is characterized by the socket pair 
(x:y, serv_addr.sin_addr:serv_addr.sin_port) 
where x is the client¡¯s IP address and y is the ephemeral port that uniquely identi.es the client process on the client host. 
11.4.4 The open_clientfd Function 
We .nd it convenient to wrap the socket and connect functions into a helper function called open_clientfd that a client can use to establish a connection with a server. 
#include "csapp.h" int open_clientfd(char *hostname, int port); 
Returns: descriptor if OK, .1 on Unix error, .2 on DNS error 

The open_clientfd function establishes a connection with a server running on host hostname and listening for connection requests on the well-known port port. It returns an open socket descriptor that is ready for input and output using the Unix I/O functions. Figure 11.16 shows the code for open_clientfd. 
After creating the socket descriptor (line 7), we retrieve the DNS host entry for the server and copy the .rst IP address in the host entry (which is already in 
code/src/csapp.c  
1  int  open_clientfd(char  *hostname,  int  port)  
2  {  
3  int clientfd;  
4  struct  hostent  *hp;  
5  struct  sockaddr_in  serveraddr;  
6  
7  if  ((clientfd  =  socket(AF_INET,  SOCK_STREAM,  0))  <  0)  
8  return  -1;  /*  Check  errno  for  cause  of  error  */  
9  
10  /*  Fill  in  the  server¡¯s  IP  address  and  port  */  
11  if  ((hp  =  gethostbyname(hostname))  ==  NULL)  
12  return  -2;  /*  Check  h_errno  for  cause  of  error  */  
13  bzero((char  *)  &serveraddr,  sizeof(serveraddr));  
14  serveraddr.sin_family  =  AF_INET;  
15  bcopy((char  *)hp->h_addr_list[0],  
16  (char  *)&serveraddr.sin_addr.s_addr,  hp->h_length);  
17  serveraddr.sin_port  =  htons(port);  
18  
19  /*  Establish  a  connection  with  the  server  */  
20  if  (connect(clientfd,  (SA  *)  &serveraddr,  sizeof(serveraddr))  <  0)  
21  return -1;  
22  return  clientfd;  
23  }  
code/src/csapp.c  
Figure 11.16  open_clientfd: helper function that establishes a connection with  
a server.  
network byte order) to the server¡¯s socket address structure (lines 11¨C16). After  
initializing the socket address structure with the server¡¯s well-known port number  
in network byte order (line 17), we initiate the connection request to the server  
(line 20). When the connect function returns, we return the socket descriptor to  
the client, which can immediately begin using Unix I/O to communicate with the  
server.  
11.4.5  The bind Function  
The remaining sockets functions¡ªbind, listen, and accept¡ªare used by servers  
to establish connections with clients.  

#include  <sys/socket.h>  
int  bind(int  sockfd,  struct  sockaddr  *my_addr,  int  addrlen);  
Returns: 0 if OK, .1 on error  


The bind function tells the kernel to associate the server¡¯s socket address in my_addr with the socket descriptor sockfd.The addrlen argument is sizeof(sockaddr_in). 
11.4.6 The listen Function 
Clients are active entities that initiate connection requests. Servers are passive entities that wait for connection requests from clients. By default, the kernel assumes that a descriptor created by the socket function corresponds to an active socket that will live on the client end of a connection. A server calls the listen function to tell the kernel that the descriptor will be used by a server instead of a client. 
#include <sys/socket.h> int listen(int sockfd, int backlog); 
Returns: 0 if OK, .1 on error 

The listen function converts sockfd from an active socket to a listening socket that can accept connection requests from clients. The backlog argument is a hint about the number of outstanding connection requests that the kernel should queue up before it starts to refuse requests. The exact meaning of the backlog argument requires an understanding of TCP/IP that is beyond our scope. We will typically set it to a large value, such as 1024. 
11.4.7 The open_listenfd Function 
We .nd it helpful to combine the socket, bind, and listen functions into a helper function called open_listenfd that a server can use to create a listening descriptor. 
#include "csapp.h" int open_listenfd(int port); 
Returns: descriptor if OK, .1 on Unix error 

The open_listenfd function opens and returns a listening descriptor that is ready to receive connection requests on the well-known port port. Figure 11.17 shows the code for open_listenfd. After we create the listenfd socket descrip-tor, we use the setsockopt function (not described here) to con.gure the server so that it can be terminated and restarted immediately. By default, a restarted 
code/src/csapp.c 

1  int  open_listenfd(int  port)  
2  {  
3  int  listenfd,  optval=1;  
4  struct  sockaddr_in  serveraddr;  
5  
6  /*  Create  a  socket  descriptor  */  
7  if  ((listenfd  =  socket(AF_INET,  SOCK_STREAM,  0))  <  0)  
8  return  -1;  
9  
10  /*  Eliminates  "Address  already  in  use"  error  from  bind  */  
11  if  (setsockopt(listenfd,  SOL_SOCKET,  SO_REUSEADDR,  
12  (const  void  *)&optval  ,  sizeof(int))  <  0)  
13  return  -1;  
14  
15  /*  Listenfd  will  be  an  end  point  for  all  requests  to  port  
16  on  any  IP  address  for  this  host  */  
17  bzero((char  *)  &serveraddr,  sizeof(serveraddr));  
18  serveraddr.sin_family  =  AF_INET;  
19  serveraddr.sin_addr.s_addr  =  htonl(INADDR_ANY);  
20  serveraddr.sin_port  =  htons((unsigned  short)port);  
21  if  (bind(listenfd,  (SA  *)&serveraddr,  sizeof(serveraddr))  <  0)  
22  return  -1;  
23  
24  /*  Make  it  a  listening  socket  ready  to  accept  connection  requests  */  
25  if  (listen(listenfd,  LISTENQ)  <  0)  
26  return  -1;  
27  return  listenfd;  
28  }  
code/src/csapp.c  
Figure 11.17  open_listenfd: helper function that opens and returns a listening  
socket.  
server will deny connection requests from clients for approximately 30 seconds,  
which seriously hinders debugging.  
Next, we initialize the server¡¯s socket address structure in preparation for  
calling the bind function. In this case, we have used the INADDR_ANY wild- 
card address to tell the kernel that this server will accept requests to any of the IP  
addresses for this host (line 19), and to well-known port port (line 20). Notice that  
we use the htonl and htons functions to convert the IP address and port number  
from host byte order to network byte order. Finally, we convert listenfd to a  
listening descriptor (line 25) and return it to the caller.  

11.4.8 The accept Function Servers wait for connection requests from clients by calling the accept function: 
#include <sys/socket.h> int accept(int listenfd, struct sockaddr *addr, int *addrlen); 
Returns: nonnegative connected descriptor if OK, .1 on error 

The accept function waits for a connection request from a client to arrive on the listening descriptor listenfd, then .lls in the client¡¯s socket address in addr, and returns a connected descriptor that can be used to communicate with the client using Unix I/O functions. 
The distinction between a listening descriptor and a connected descriptor confuses many students. The listening descriptor serves as an end point for client connection requests. It is typically created once and exists for the lifetime of the server. The connected descriptor is the end point of the connection that is established between the client and the server. It is created each time the server accepts a connection request and exists only as long as it takes the server to service a client. 
Figure 11.18 outlines the roles of the listening and connected descriptors. In Step 1, the server calls accept, which waits for a connection request to ar-rive on the listening descriptor, which for concreteness we will assume is de-scriptor 3. Recall that descriptors 0¨C2 are reserved for the standard .les. In Step 2, the client calls the connect function, which sends a connection re-quest to listenfd. In Step 3, the accept function opens a new connected 
listenfd(3) 


1.Server blocks in accept, waiting for connection request on listening descriptor listenfd. 

clientfd 
Connection 
listenfd(3)


2.Client makes connection request by calling and blocking in connect. 
listenfd(3) 
3.Server returns connfd from accept. 


Client returns from connect. Connection is now established between clientfd and connfd. 
clientfd connfd(4) 
Figure 11.18 The roles of the listening and connected descriptors. 
descriptor connfd (which we will assume is descriptor 4), establishes the connec-tion between clientfd and connfd, and then returns connfd to the application. The client also returns from the connect, and from this point, the client and server can pass data back and forth by reading and writing clientfd and connfd, respectively. 

Aside Why the distinction between listening and connected descriptors? 
You might wonder why the sockets interface makes a distinction between listening and connected descriptors. At .rst glance, it appears to be an unnecessary complication. However, distinguishing between the two turns out to be quite useful, because it allows us to build concurrent servers that can process many client connections simultaneously. For example, each time a connection request arrives on the listening descriptor, we might fork a new process that communicates with the client over its connected descriptor. You¡¯ll learn more about concurrent servers in Chapter 12. 
11.4.9 Example Echo Client and Server 
The best way to learn the sockets interface is to study example code. Figure 11.19 shows the code for an echo client. After establishing a connection with the server, the client enters a loop that repeatedly reads a text line from standard input, sends the text line to the server, reads the echo line from the server, and prints the result to standard output. The loop terminates when fgets encounters EOF on standard input, either because the user typed ctrl-d at the keyboard or because it has exhausted the text lines in a redirected input .le. 
After the loop terminates, the client closes the descriptor. This results in an EOF noti.cation being sent to the server, which it detects when it receives a return code of zero from its rio_readlineb function. After closing its descrip-tor, the client terminates. Since the client¡¯s kernel automatically closes all open descriptors when a process terminates, the close in line 24 is not necessary. How-ever, it is good programming practice to explicitly close any descriptors we have opened. 
Figure 11.20 shows the main routine for the echo server. After opening the listening descriptor, it enters an in.nite loop. Each iteration waits for a con-nection request from a client, prints the domain name and IP address of the connected client, and calls the echo function that services the client. After the echo routine returns, the main routine closes the connected descriptor. Once the client and server have closed their respective descriptors, the connection is terminated. 
Notice that our simple echo server can only handle one client at a time. A server of this type that iterates through clients, one at a time, is called an iterative server. In Chapter 12, we will learn how to build more sophisticated concurrent servers that can handle multiple clients simultaneously. 
code/netp/echoclient.c 

1 #include "csapp.h" 2 3 int main(int argc, char **argv) 4 { 5 int clientfd, port; 6 char *host, buf[MAXLINE]; 7 rio_t rio; 8 9 if (argc != 3) { 
10 fprintf(stderr, "usage: %s <host> <port>\n", argv[0]); 
11 exit(0); 
12 
} 13 host = argv[1]; 14 port = atoi(argv[2]); 15 16 clientfd = Open_clientfd(host, port); 17 Rio_readinitb(&rio, clientfd); 18 19 while (Fgets(buf, MAXLINE, stdin) != NULL) { 20 Rio_writen(clientfd, buf, strlen(buf)); 21 Rio_readlineb(&rio, buf, MAXLINE); 22 Fputs(buf, stdout); 

23 
} 24 Close(clientfd); 25 exit(0); 


26 } 
code/netp/echoclient.c 

Figure 11.19 Echo client main routine. 
Finally, Figure 11.21 shows the code for the echo routine, which repeatedly reads and writes lines of text until the rio_readlineb function encounters EOF in line 10. 
Aside What does EOF on a connection mean? 
The idea of EOF is often confusing to students, especially in the context of Internet connections. First, we need to understand that there is no such thing as an EOF character. Rather, EOF is a condition that is detected by the kernel. An application .nds out about the EOF condition when it receives a zero return code from the read function. For disk .les, EOF occurs when the current .le position exceeds the .le length. For Internet connections, EOF occurs when a process closes its end of the connection. The process at the other end of the connection detects the EOF when it attempts to read past the last byte in the stream. 
code/netp/echoserveri.c 
1 
#include "csapp.h" 
2 3 
void echo(int connfd); 
4 
int main(int argc, char **argv) 6 { 7 int listenfd, connfd, port, clientlen; 8 struct sockaddr_in clientaddr; 9 struct hostent *hp; 
char *haddrp; 11 if (argc != 2) { 12 fprintf(stderr, "usage: %s <port>\n", argv[0]); 13 exit(0); 
14 } port = atoi(argv[1]); 
16 17 listenfd = Open_listenfd(port); 18 while (1) { 19 clientlen = sizeof(clientaddr); 
connfd = Accept(listenfd, (SA *)&clientaddr, &clientlen); 
21 22 /* Determine the domain name and IP address of the client */ 23 hp = Gethostbyaddr((const char *)&clientaddr.sin_addr.s_addr, 24 sizeof(clientaddr.sin_addr.s_addr), AF_INET); 
haddrp = inet_ntoa(clientaddr.sin_addr); 26 printf("server connected to %s (%s)\n", hp->h_name, haddrp); 27 28 echo(connfd); 29 Close(connfd); 
} 31 exit(0); 
32 } 
code/netp/echoserveri.c 
Figure 11.20 Iterative echo server main routine. 
Section 11.5 
code/netp/echo.c 

1  #include  "csapp.h"  
2  
3  void  echo(int  connfd)  
4  {  
5  size_t  n;  
6  char  buf[MAXLINE];  
7  rio_t  rio;  
8  
9  Rio_readinitb(&rio,  connfd);  
10  while((n  =  Rio_readlineb(&rio,  buf,  MAXLINE))  !=  0)  {  
11  printf("server  received  %d  bytes\n",  n);  
12  Rio_writen(connfd,  buf,  n);  
13  }  
14  }  
code/netp/echo.c  
Figure 11.21 echo function that reads and echoes text lines. 


11.5 
Web 
Servers 

So far we have discussed network programming in the context of a simple echo server. In this section, we will show you how to use the basic ideas of network programming to build your own small, but quite functional, Web server. 
11.5.1 Web Basics 
Web clients and servers interact using a text-based application-level protocol known as HTTP (Hypertext Transfer Protocol). HTTP is a simple protocol. A Web client (known as a browser) opens an Internet connection to a server and requests some content. The server responds with the requested content and then closes the connection. The browser reads the content and displays it on the screen. 
What distinguishes Web services from conventional .le retrieval services such as FTP? The main difference is that Web content can be written in a language known as HTML (Hypertext Markup Language). An HTML program (page) contains instructions (tags) that tell the browser how to display various text and graphical objects in the page. For example, the code 
<b> Make me bold! </b> 
tells the browser to print the text between the <b> and </b> tags in boldface type. However, the real power of HTML is that a page can contain pointers (hyperlinks) to content stored on any Internet host. For example, an HTML line of the form 
<a href="http://www.cmu.edu/index.html">Carnegie Mellon</a> 
tells the browser to highlight the text object ¡°Carnegie Mellon¡± and to create a hyperlink to an HTML .le called index.html that is stored on the CMU Web server. If the user clicks on the highlighted text object, the browser requests the corresponding HTML .le from the CMU server and displays it. 

Aside Origins of the World Wide Web 
The World Wide Web was invented by Tim Berners-Lee, a software engineer working at CERN, a Swiss physics lab. In 1989, Berners-Lee wrote an internal memo proposing a distributed hypertext system that would connect a ¡°web of notes with links.¡± The intent of the proposed system was to help CERN scientists share and manage information. Over the next 2 years, after Berners-Lee implemented the .rst Web server and Web browser, the Web developed a small following within CERN and a few other sites. A pivotal event occurred in 1993, when Marc Andreesen (who later founded Netscape) and his colleagues at NCSA released a graphical browser called mosaic for all three major platforms: Unix, Windows, and Macintosh. After the release of mosaic, interest in the Web exploded, with the number of Web sites increasing by a factor of 10 or more each year. By 2009, there were over 225,000,000 Web sites worldwide (source: Netcraft Web Survey). 
11.5.2 Web Content 
To Web clients and servers, content is a sequence of bytes with an associated MIME (Multipurpose Internet Mail Extensions) type. Figure 11.22 shows some common MIME types. 
Web servers provide content to clients in two different ways: 
. Fetch a disk .le and return its contents to the client. The disk .le is known as static content and the process of returning the .le to the client is known as serving static content. 
. Run an executable .le and return its output to the client. The output produced by the executable at run time is known as dynamic content, and the process of running the program and returning its output to the client is known as serving dynamic content. 
MIME type  Description  
text/html text/plain application/postscript image/gif image/jpeg  HTML page Unformatted text Postscript document Binary image encoded in GIF format Binary image encoded in JPEG format  
Figure 11.22 Example MIME types. 



Every piece of content returned by a Web server is associated with some .le that it manages. Each of these .les has a unique name known as a URL (Universal Resource Locator). For example, the URL 
http://www.google.com:80/index.html 

identi.es an HTML .le called /index.html on Internet host www.google.com 
that is managed by a Web server listening on port 80. The port number is optional and defaults to the well-known HTTP port 80. URLs for executable .les can include program arguments after the .le name. A ¡®?¡¯ character separates the .le name from the arguments, and each argument is separated by an ¡®&¡¯ character. For example, the URL 
http://bluefish.ics.cs.cmu.edu:8000/cgi-bin/adder?15000&213 

identi.es an executable called /cgi-bin/adder that will be called with two argu-ment strings: 15000 and 213. Clients and servers use different parts of the URL during a transaction. For instance, a client uses the pre.x 
http://www.google.com:80 

to determine what kind of server to contact, where the server is, and what port it is listening on. The server uses the suf.x 
/index.html 

to .nd the .le on its .le system and to determine whether the request is for static or dynamic content. There are several points to understand about how servers interpret the suf.x of a URL: 
. There are no standard rules for determining whether a URL refers to static or dynamic content. Each server has its own rules for the .les it manages. A common approach is to identify a set of directories, such as cgi-bin, where all executables must reside. 
. The initial ¡®/¡¯ in the suf.x does not denote the Unix root directory. Rather, it denotes the home directory for whatever kind of content is being requested. For example, a server might be con.gured so that all static content is stored in directory /usr/httpd/html and all dynamic content is stored in directory /usr/httpd/cgi-bin. 
. The minimal URL suf.x is the ¡®/¡¯ character, which all servers expand to some default home page such as /index.html. This explains why it is possible to fetch the home page of a site by simply typing a domain name into the browser. The browser appends the missing ¡®/¡¯ to the URL and passes it to the server, which expands the ¡®/¡¯ to some default .le name. 
914  Chapter 11  Network Programming  
1 2 3  unix> telnet www.aol.com Trying 205.188.146.23... Connected to aol.com.  80  Client: open connection to server Telnet prints 3 lines to the terminal  
4 5 6 7 8 9 10 11 12 13 14 15 16  Escape character is ¡¯^]¡¯. GET / HTTP/1.1 Host: www.aol.com HTTP/1.0 200 OK MIME-Version: 1.0 Date: Mon, 8 Jan 2010 4:59:42 Server: Apache-Coyote/1.1 Content-Type: text/html Content-Length: 42092 <html> ...  GMT  Client: Client: Client: Server: Server: Server: Server: Server: Server: Server:  request line required HTTP/1.1 header empty line terminates headers response line followed by five response headers expect HTML in the response body expect 42,092 bytes in the response body empty line terminates response headers first HTML line in response body 766 lines of HTML not shown  
17 18 19  </html> Connection unix>  closed  by  foreign  host.  Server: Server: Client:  last HTML line in closes connection closes connection  response body and terminates  
Figure 11.23  Example of an HTTP transaction that serves static content.  
11.5.3  HTTP Transactions  
Since HTTP is based on text lines transmitted over Internet connections, we can use the Unix telnet program to conduct transactions with any Web server on the Internet. The telnet program is very handy for debugging servers that talk to clients with text lines over connections. For example, Figure 11.23 uses telnet to request the home page from the AOL Web server. In line 1, we run telnet from a Unix shell and ask it to open a connection to the AOL Web server. Telnet prints three lines of output to the terminal, opens the connection, and then waits for us to enter text (line 5). Each time we enter a text line and hit the enter key, telnet reads the line, appends carriage return and line feed characters (¡°\r\n¡± in C notation), and sends the line to the server. This is consistent with the HTTP standard, which requires every text line to be terminated by a carriage return and line feed pair. To initiate the transaction, we enter an HTTP request (lines 5¨C7). The server replies with an HTTP response (lines 8¨C17) and then closes the connection (line 18).  
HTTP Requests  
An HTTP request consists of a request line (line 5), followed by zero or more request headers (line 6), followed by an empty text line that terminates the list of headers (line 7). A request line has the form  
<method>  <uri>  <version>  

HTTP supports a number of different methods, including GET, POST, OP-TIONS, HEAD, PUT, DELETE, and TRACE. We will only discuss the workhorse GET method, which according to one study accounts for over 99% of HTTP re-quests [107]. The GET method instructs the server to generate and return the content identi.ed by the URI (Uniform Resource Identi.er). The URI is the suf-.x of the corresponding URL that includes the .le name and optional arguments.1 
The <version> .eld in the request line indicates the HTTP version to which the request conforms. The most recent HTTP version is HTTP/1.1 [41]. HTTP/1.0 is a previous version from 1996 that is still in use [6]. HTTP/1.1 de.nes additional headers that provide support for advanced features such as caching and security, as well as a mechanism that allows a client and server to perform multiple trans-actions over the same persistent connection. In practice, the two versions are com-patible because HTTP/1.0 clients and servers simply ignore unknown HTTP/1.1 headers. 
To summarize, the request line in line 5 asks the server to fetch and return the HTML .le /index.html. It also informs the server that the remainder of the request will be in HTTP/1.1 format. 
Request headers provide additional information to the server, such as the brand name of the browser or the MIME types that the browser understands. Request headers have the form 
<header name>: <header data> 
For our purposes, the only header to be concerned with is the Host header (line 6), which is required in HTTP/1.1 requests, but not in HTTP/1.0 requests. The Host header is used by proxy caches, which sometimes serve as intermediaries between a browser and the origin server that manages the requested .le. Multiple proxies can exist between a client and an origin server in a so-called proxy chain. The data in the Host header, which identi.es the domain name of the origin server, allows a proxy in the middle of a proxy chain to determine if it might have a locally cached copy of the requested content. 
Continuing with our example in Figure 11.23, the empty text line in line 7 (generated by hitting enter on our keyboard) terminates the headers and instructs the server to send the requested HTML .le. 
HTTP Responses 

HTTP responses are similar to HTTP requests. An HTTP response consists of a response line (line 8), followed by zero or more response headers (lines 9¨C13), followed by an empty line that terminates the headers (line 14), followed by the response body (lines 15¨C17). A response line has the form 
<version> <status code> <status message> 
1. Actually, this is only true when a browser requests content. If a proxy server requests content, then the URI must be the complete URL. 
Status code Status message Description 
200 OK Request was handled without error. 301 Moved permanently Content has moved to the hostname in the Location header. 400 Bad request Request could not be understood by the server. 403 Forbidden Server lacks permission to access the requested .le. 404 Not found Server could not .nd the requested .le. 501 Not implemented Server does not support the request method. 505 HTTP version not supported Server does not support version in request. 
Figure 11.24 Some HTTP status codes. 
The version .eld describes the HTTP version that the response conforms to. The status code is a three-digit positive integer that indicates the disposition of the request. The status message gives the English equivalent of the error code. Figure 11.24 lists some common status codes and their corresponding messages. The response headers in lines 9¨C13 provide additional information about the response. For our purposes, the two most important headers are Content-Type (line 12), which tells the client the MIME type of the content in the response body, and Content-Length (line 13), which indicates its size in bytes. 
The empty text line in line 14 that terminates the response headers is followed by the response body, which contains the requested content. 
11.5.4 Serving Dynamic Content 
If we stop to think for a moment how a server might provide dynamic content to a client, certain questions arise. For example, how does the client pass any program arguments to the server? How does the server pass these arguments to the child process that it creates? How does the server pass other information to the child that it might need to generate the content? Where does the child send its output? These questions are addressed by a de facto standard called CGI (Common Gateway Interface). 
How Does the Client Pass Program Arguments to the Server? 
Arguments for GET requests are passed in the URI. As we have seen, a ¡®?¡¯ character separates the .le name from the arguments, and each argument is separated by an ¡®&¡¯ character. Spaces are not allowed in arguments and must be represented with the ¡°%20¡± string. Similar encodings exist for other special characters. 

Aside Passing arguments in HTTP POST requests 
Arguments for HTTP POST requests are passed in the request body rather than in the URI. 
Environment variable  Description  
QUERY_STRING SERVER_PORT REQUEST_METHOD REMOTE_HOST  Program arguments Port that the parent is listening on GET or POST Domain name of client  
REMOTE_ADDR  Dotted-decimal IP address of client  
CONTENT_TYPE CONTENT_LENGTH  POST only: MIME type of the request body POST only: Size in bytes of the request body  
Figure 11.25 Examples of CGI environment variables. 


How Does the Server Pass Arguments to the Child? 
After a server receives a request such as 
GET /cgi-bin/adder?15000&213 HTTP/1.1 
it calls fork to create a child process and calls execve to run the /cgi-bin/adder program in the context of the child. Programs like the adder program are often referred to as CGI programs because they obey the rules of the CGI standard. And since many CGI programs are written as Perl scripts, CGI programs are often called CGI scripts. Before the call to execve, the child process sets the CGI environment variable QUERY_STRING to ¡°15000&213¡±, which the adder program can reference at run time using the Unix getenv function. 
How Does the Server Pass Other Information to the Child? 
CGI de.nes a number of other environment variables that a CGI program can expect to be set when it runs. Figure 11.25 shows a subset. 
Where Does the Child Send Its Output? 
A CGI program sends its dynamic content to the standard output. Before the child process loads and runs the CGI program, it uses the Unix dup2 function to redirect standard output to the connected descriptor that is associated with the client. Thus, anything that the CGI program writes to standard output goes directly to the client. 
Notice that since the parent does not know the type or size of the content that the child generates, the child is responsible for generating the Content-type and Content-length response headers, as well as the empty line that terminates the headers. 
code/netp/tiny/cgi-bin/adder.c  
1  #include  "csapp.h"  
2  
3  int  main(void)  {  
4  char  *buf,  *p;  
5  char  arg1[MAXLINE],  arg2[MAXLINE],  content[MAXLINE];  
6  int  n1=0,  n2=0;  
7  
8  /*  Extract  the  two  arguments  */  
9  if  ((buf  =  getenv("QUERY_STRING"))  !=  NULL)  {  
10  p  =  strchr(buf,  ¡¯&¡¯);  
11  *p  =  ¡¯\0¡¯;  
12  strcpy(arg1,  buf);  
13  strcpy(arg2,  p+1);  
14  n1  =  atoi(arg1);  
15  n2  =  atoi(arg2);  
16  }  
17  
18  /*  Make  the  response  body  */  
19  sprintf(content,  "Welcome  to  add.com:  ");  
20  sprintf(content,  "%sTHE  Internet  addition  portal.\r\n<p>",  content);  
21  sprintf(content,  "%sThe  answer  is:  %d  + %d  =  %d\r\n<p>",  
22  content,  n1,  n2,  n1  + n2);  
23  sprintf(content,  "%sThanks  for  visiting!\r\n",  content);  
24  
25  /*  Generate  the  HTTP  response  */  
26  printf("Content-length:  %d\r\n",  (int)strlen(content));  
27  printf("Content-type:  text/html\r\n\r\n");  
28  printf("%s",  content);  
29  fflush(stdout);  
30  exit(0);  
31  }  
code/netp/tiny/cgi-bin/adder.c  
Figure 11.26  CGI program that sums two integers.  
Figure 11.26 shows a simple CGI program that sums its two arguments and  
returns an HTML .le with the result to the client. Figure 11.27 shows an HTTP  
transaction that serves dynamic content from the adder program.  

Aside Passing arguments in HTTP POST requests to CGI programs 
For POST requests, the child would also need to redirect standard input to the connected descriptor. The CGI program would then read the arguments in the request body from standard input. 
1 unix> telnet kittyhawk.cmcl.cs.cmu.edu 8000 Client: open connection 2 Trying 128.2.194.242... 3 Connected to kittyhawk.cmcl.cs.cmu.edu. 4 Escape character is ¡¯^]¡¯. 5 GET /cgi-bin/adder?15000&213 HTTP/1.0 Client: request line 6 Client: empty line terminates headers 7 HTTP/1.0 200 OK Server: response line 8 Server: Tiny Web Server Server: identify server 9 Content-length: 115 Adder: expect 115 bytes in response body 
10 Content-type: text/html Adder: expect HTML in response body 
11 Adder: empty line terminates headers 
12 Welcome to add.com: THE Internet addition portal. Adder: first HTML line 
13 <p>The answer is: 15000 + 213 = 15213 Adder: second HTML line in response body 
14 <p>Thanks for visiting! Adder: third HTML line in response body 
15 Connection closed by foreign host. Server: closes connection 
16 unix> Client: closes connection and terminates 
Figure 11.27 An HTTP transaction that serves dynamic HTML content. 
Practice Problem 11.5 
In Section 10.9, we warned you about the dangers of using the C standard I/O functions in network applications. Yet the CGI program in Figure 11.26 is able to use standard I/O without any problems. Why? 
11.6 
Putting 
It 
Together: 
The 
Tiny 
Web 
Server 

We conclude our discussion of network programming by developing a small but functioning Web server called Tiny. Tiny is an interesting program. It combines many of the ideas that we have learned about, such as process control, Unix I/O, the sockets interface, and HTTP, in only 250 lines of code. While it lacks the functionality, robustness, and security of a real server, it is powerful enough to serve both static and dynamic content to real Web browsers. We encourage you to study it and implement it yourself. It is quite exciting (even for the authors!) to point a real browser at your own server and watch it display a complicated Web page with text and graphics. 
The Tiny main Routine 
Figure 11.28 shows Tiny¡¯s main routine. Tiny is an iterative server that listens for connection requests on the port that is passed in the command line. After opening a listening socket by calling the open_listenfd function, Tiny executes the typical in.nite server loop, repeatedly accepting a connection request (line 31), performing a transaction (line 32), and closing its end of the connection (line 33). 
code/netp/tiny/tiny.c 
1 /* 2 * tiny.c -A simple, iterative HTTP/1.0 Web server that uses the 3 * GET method to serve static and dynamic content. 4 */ 5 #include "csapp.h" 6 7 void doit(int fd); 8 void read_requesthdrs(rio_t *rp); 9 int parse_uri(char *uri, char *filename, char *cgiargs); 
10 void serve_static(int fd, char *filename, int filesize); 
11 void get_filetype(char *filename, char *filetype); 
12 void serve_dynamic(int fd, char *filename, char *cgiargs); 
13 void clienterror(int fd, char *cause, char *errnum, 
14 char *shortmsg, char *longmsg); 
15 
16 int main(int argc, char **argv) 
17 { 
18 int listenfd, connfd, port, clientlen; 
19 struct sockaddr_in clientaddr; 
20 
21 /* Check command line args */ 
22 if (argc != 2) { 
23 fprintf(stderr, "usage: %s <port>\n", argv[0]); 
24 exit(1); 
25 } 26 port = atoi(argv[1]); 27 28 listenfd = Open_listenfd(port); 29 while (1) { 30 clientlen = sizeof(clientaddr); 31 connfd = Accept(listenfd, (SA *)&clientaddr, &clientlen); 32 doit(connfd); 33 Close(connfd); 
34 
} 

35 
} 


code/netp/tiny/tiny.c 
Figure 11.28 The Tiny Web server. 
The doit Function 
The doit function in Figure 11.29 handles one HTTP transaction. First, we read and parse the request line (lines 11¨C12). Notice that we are using the rio_ readlineb function from Figure 10.7 to read the request line. 

1  void  doit(int  fd)  
2  {  
3  int  is_static;  
4  struct  stat  sbuf;  
char  buf[MAXLINE],  method[MAXLINE],  uri[MAXLINE],  version[MAXLINE];  
6  char  filename[MAXLINE],  cgiargs[MAXLINE];  
7  rio_t  rio;  
8  
9  /*  Read  request  line  and  headers  */  
Rio_readinitb(&rio,  fd);  
11  Rio_readlineb(&rio,  buf,  MAXLINE);  
12  sscanf(buf,  "%s  %s  %s",  method,  uri,  version);  
13  if  (strcasecmp(method,  "GET"))  {  
14  clienterror(fd,  method,  "501",  "Not  Implemented",  
"Tiny  does  not  implement  this  method");  
16  return;  
17  }  
18  read_requesthdrs(&rio);  
19  
/*  Parse  URI  from  GET  request  */  
21  is_static  =  parse_uri(uri,  filename,  cgiargs);  
22  if  (stat(filename,  &sbuf)  <  0)  {  
23  clienterror(fd,  filename,  "404",  "Not  found",  
24  "Tiny  couldn¡¯t  find  this  file");  
return;  
26  }  
27  
28  if  (is_static)  {  /*  Serve  static  content  */  
29  if  (!(S_ISREG(sbuf.st_mode))  ||  !(S_IRUSR  &  sbuf.st_mode))  {  
clienterror(fd,  filename,  "403",  "Forbidden",  
31  "Tiny  couldn¡¯t  read  the  file");  
32  return;  
33  }  
34  serve_static(fd,  filename,  sbuf.st_size);  
}  
36  else  {  /*  Serve  dynamic  content  */  
37  if  (!(S_ISREG(sbuf.st_mode))  ||  !(S_IXUSR  &  sbuf.st_mode))  {  
38  clienterror(fd,  filename,  "403",  "Forbidden",  
39  "Tiny  couldn¡¯t  run  the  CGI  program");  
return;  
41  }  
42  serve_dynamic(fd, filename, cgiargs);  
43  }  
44  }  
Figure 11.29 Tiny doit: Handles one HTTP transaction. 


code/netp/tiny/tiny.c 

Tiny only supports the GET method. If the client requests another method (such as POST), we send it an error message and return to the main routine (lines 13¨C17), which then closes the connection and awaits the next connection request. Otherwise, we read and (as we shall see) ignore any request headers (line 18). 
Next, we parse the URI into a .le name and a possibly empty CGI argument string, and we set a .ag that indicates whether the request is for static or dynamic content (line 21). If the .le does not exist on disk, we immediately send an error message to the client and return. 
Finally, if the request is for static content, we verify that the .le is a regular .le and that we have read permission (line 29). If so, we serve the static content (line 34) to the client. Similarly, if the request is for dynamic content, we verify that the .le is executable (line 37), and if so we go ahead and serve the dynamic content (line 42). 
The clienterror Function 
Tiny lacks many of the error handling features of a real server. However, it does check for some obvious errors and reports them to the client. The clienterror function in Figure 11.30 sends an HTTP response to the client with the appropriate 
code/netp/tiny/tiny.c 
1 void clienterror(int fd, char *cause, char *errnum, 2 char *shortmsg, char *longmsg) 3 { 4 char buf[MAXLINE], body[MAXBUF]; 5 6 /* Build the HTTP response body */ 7 sprintf(body, "<html><title>Tiny Error</title>"); 8 sprintf(body, "%s<body bgcolor=""ffffff"">\r\n", body); 9 sprintf(body, "%s%s: %s\r\n", body, errnum, shortmsg); 
10 sprintf(body, "%s<p>%s: %s\r\n", body, longmsg, cause); 11 sprintf(body, "%s<hr><em>The Tiny Web server</em>\r\n", body); 12 13 /* Print the HTTP response */ 14 sprintf(buf, "HTTP/1.0 %s %s\r\n", errnum, shortmsg); 15 Rio_writen(fd, buf, strlen(buf)); 16 sprintf(buf, "Content-type: text/html\r\n"); 17 Rio_writen(fd, buf, strlen(buf)); 18 sprintf(buf, "Content-length: %d\r\n\r\n", (int)strlen(body)); 19 Rio_writen(fd, buf, strlen(buf)); 20 Rio_writen(fd, body, strlen(body)); 
21 } 
code/netp/tiny/tiny.c 
Figure 11.30 Tiny clienterror: Sends an error message to the client. 
code/netp/tiny/tiny.c 

1 void read_requesthdrs(rio_t *rp) 
2 { 

3 char buf[MAXLINE]; 
4 

5 Rio_readlineb(rp, buf, MAXLINE); 
6 while(strcmp(buf, "\r\n")) { 
7 Rio_readlineb(rp, buf, MAXLINE); 
8 printf("%s", buf); 
9 } 10 return; 
11 } 
code/netp/tiny/tiny.c 

Figure 11.31 Tiny read_requesthdrs: Reads and ignores request headers. 
status code and status message in the response line, along with an HTML .le in the response body that explains the error to the browser¡¯s user. Recall that an HTML response should indicate the size and type of the content in the body. Thus, we have opted to build the HTML content as a single string so that we can easily determine its size. Also, notice that we are using the robust rio_writen function from Figure 10.3 for all output. 
The read_requesthdrs Function 
Tiny does not use any of the information in the request headers. It simply reads and ignores them by calling the read_requesthdrs function in Figure 11.31. Notice that the empty text line that terminates the request headers consists of a carriage return and line feed pair, which we check for in line 6. 
The parse_uri Function 
Tiny assumes that the home directory for static content is its current directory, and that the home directory for executables is ./cgi-bin. Any URI that contains the string cgi-bin is assumed to denote a request for dynamic content. The default .le name is ./home.html. 
The parse_uri function in Figure 11.32 implements these policies. It parses the URI into a .le name and an optional CGI argument string. If the request is for static content (line 5), we clear the CGI argument string (line 6) and then convert the URI into a relative Unix pathname such as ./index.html (lines 7¨C 8). If the URI ends with a ¡®/¡¯ character (line 9), then we append the default .le name (line 10). On the other hand, if the request is for dynamic content (line 13), we extract any CGI arguments (lines 14¨C20) and convert the remaining portion of the URI to a relative Unix .le name (lines 21¨C22). 
code/netp/tiny/tiny.c  
1  int parse_uri(char *uri, char *filename,  char  *cgiargs)  
2  {  
3  char *ptr;  
4  
5  if (!strstr(uri, "cgi-bin")) { /*  Static  content */  
6  strcpy(cgiargs, "");  
7  strcpy(filename, ".");  
8  strcat(filename, uri);  
9  if (uri[strlen(uri)-1] == ¡¯/¡¯)  
10  strcat(filename, "home.html");  
11  return 1;  
12  }  
13  else { /* Dynamic content */  
14  ptr = index(uri, ¡¯?¡¯);  
15  if (ptr) {  
16  strcpy(cgiargs, ptr+1);  
17  *ptr = ¡¯\0¡¯;  
18  }  
19  else  
20  strcpy(cgiargs, "");  
21  strcpy(filename, ".");  
22  strcat(filename, uri);  
23  return 0;  
24  }  
25  }  
code/netp/tiny/tiny.c  
Figure 11.32 Tiny parse_uri: Parses an HTTP URI. 


The serve_static Function 
Tiny serves four different types of static content: HTML .les, unformatted text .les, and images encoded in GIF and JPEG formats. These .le types account for the majority of static content served over the Web. 
The serve_static function in Figure 11.33 sends an HTTP response whose body contains the contents of a local .le. First, we determine the .le type by inspecting the suf.x in the .le name (line 7) and then send the response line and response headers to the client (lines 8¨C12). Notice that a blank line terminates the headers. 
Next, we send the response body by copying the contents of the requested .le to the connected descriptor fd. The code here is somewhat subtle and needs to be studied carefully. Line 15 opens filename for reading and gets its descriptor. In line 16, the Unix mmap function maps the requested .le to a virtual memory area. Recall from our discussion of mmap in Section 9.8 that the call to mmap maps the 
code/netp/tiny/tiny.c 

1 void serve_static(int fd, char *filename, int filesize) 2 { 3 int srcfd; 4 char *srcp, filetype[MAXLINE], buf[MAXBUF]; 5 6 /* Send response headers to client */ 7 get_filetype(filename, filetype); 8 sprintf(buf, "HTTP/1.0 200 OK\r\n"); 9 sprintf(buf, "%sServer: Tiny Web Server\r\n", buf); 
10 sprintf(buf, "%sContent-length: %d\r\n", buf, filesize); 11 sprintf(buf, "%sContent-type: %s\r\n\r\n", buf, filetype); 12 Rio_writen(fd, buf, strlen(buf)); 13 14 /* Send response body to client */ 15 srcfd = Open(filename, O_RDONLY, 0); 16 srcp = Mmap(0, filesize, PROT_READ, MAP_PRIVATE, srcfd, 0); 17 Close(srcfd); 18 Rio_writen(fd, srcp, filesize); 19 Munmap(srcp, filesize); 
20 } 21 22 /* 23 * get_filetype -derive file type from file name 24 */ 25 void get_filetype(char *filename, char *filetype) 26 { 27 if (strstr(filename, ".html")) 28 strcpy(filetype, "text/html"); 29 else if (strstr(filename, ".gif")) 30 strcpy(filetype, "image/gif"); 31 else if (strstr(filename, ".jpg")) 32 strcpy(filetype, "image/jpeg"); 33 else 34 strcpy(filetype, "text/plain"); 
35 } 
code/netp/tiny/tiny.c 

Figure 11.33 Tiny serve_static: Serves static content to a client. 
.rst filesize bytes of .le srcfd to a private read-only area of virtual memory that starts at address srcp. 
Once we have mapped the .le to memory, we no longer need its descriptor, so we close the .le (line 17). Failing to do this would introduce a potentially fatal memory leak. Line 18 performs the actual transfer of the .le to the client. The 
code/netp/tiny/tiny.c 

1  void  serve_dynamic(int  fd,  char  *filename,  char  *cgiargs)  
2  {  
3  char  buf[MAXLINE],  *emptylist[]  =  {  NULL  };  
4  
5  /*  Return  first  part  of  HTTP  response  */  
6  sprintf(buf,  "HTTP/1.0  200 OK\r\n");  
7  Rio_writen(fd,  buf,  strlen(buf));  
8  sprintf(buf,  "Server:  Tiny  Web  Server\r\n");  
9  Rio_writen(fd,  buf,  strlen(buf));  
10  
11  if  (Fork()  ==  0)  {  /*  child  */  
12  /*  Real  server  would  set  all  CGI  vars  here  */  
13  setenv("QUERY_STRING",  cgiargs,  1);  
14  Dup2(fd,  STDOUT_FILENO);  /*  Redirect  stdout  to  client  */  
15  Execve(filename,  emptylist,  environ);  /*  Run  CGI  program  */  
16  }  
17  Wait(NULL);  /*  Parent  waits  for  and  reaps  child  */  
18  }  
code/netp/tiny/tiny.c  
Figure 11.34  Tiny serve_dynamic: Serves dynamic content to a client.  
rio_writen function copies the filesize bytes starting at location srcp (which  
of course is mapped to the requested .le) to the client¡¯s connected descriptor.  
Finally, line 19 frees the mapped virtual memory area. This is important to avoid  
a potentially fatal memory leak.  
The serve_dynamic Function  
Tiny serves any type of dynamic content by forking a child process, and then  
running a CGI program in the context of the child.  
The serve_dynamic function in Figure 11.34 begins by sending a response line  
indicating success to the client, along with an informational Server header. The  
CGI program is responsible for sending the rest of the response. Notice that this  
is not as robust as we might wish, since it doesn¡¯t allow for the possibility that the  
CGI program might encounter some error.  
After sending the .rst part of the response, we fork a new child process  
(line 11). The child initializes the QUERY_STRING environment variable with  
the CGI arguments from the request URI (line 13). Notice that a real server would  
set the other CGI environment variables here as well. For brevity, we have omitted  
this step. Also, we note that Solaris systems use the putenv function instead of the  
setenv function.  

Next, the child redirects the child¡¯s standard output to the connected .le descriptor (line 14), and then loads and runs the CGI program (line 15). Since the CGI program runs in the context of the child, it has access to the same open .les and environment variables that existed before the call to the execve function. Thus, everything that the CGI program writes to standard output goes directly to the client process, without any intervention from the parent process. 
Meanwhile, the parent blocks in a call to wait, waiting to reap the child when it terminates (line 17). 
Aside Dealing with prematurely closed connections 
Although the basic functions of a Web server are quite simple, we don¡¯t want to give you the false impression that writing a real Web server is easy. Building a robust Web server that runs for extended periods without crashing is a dif.cult task that requires a deeper understanding of Unix systems programming than we¡¯ve learned here. For example, if a server writes to a connection that has already been closed by the client (say, because you clicked the ¡°Stop¡± button on your browser), then the .rst such write returns normally, but the second write causes the delivery of a SIGPIPE signal whose default behavior is to terminate the process. If the SIGPIPE signal is caught or ignored, then the second write operation returns .1 with errno set to EPIPE. The strerr and perror functions report the EPIPE error as a ¡°Broken pipe¡±, a non-intuitive message that has confused generations of students. The bottom line is that a robust server must catch these SIGPIPE signals and check write function calls for EPIPE errors. 
11.7 
Summary 

Every network application is based on the client-server model. With this model, an application consists of a server and one or more clients. The server manages resources, providing a service for its clients by manipulating the resources in some way. The basic operation in the client-server model is a client-server transaction, which consists of a request from a client, followed by a response from the server. 
Clients and servers communicate over a global network known as the Internet. From a programmer¡¯s point of view, we can think of the Internet as a worldwide collection of hosts with the following properties: (1) Each Internet host has a unique 32-bit name called its IP address. (2) The set of IP addresses is mapped to a set of Internet domain names. (3) Processes on different Internet hosts can communicate with each other over connections. 
Clients and servers establish connections by using the sockets interface. A socket is an end point of a connection that is presented to applications in the form of a .le descriptor. The sockets interface provides functions for opening and closing socket descriptors. Clients and servers communicate with each other by reading and writing these descriptors. 
Web servers and their clients (such as browsers) communicate with each other using the HTTP protocol. A browser requests either static or dynamic content from the server. A request for static content is served by fetching a .le from the server¡¯s disk and returning it to the client. A request for dynamic content is served by running a program in the context of a child process on the server and returning its output to the client. The CGI standard provides a set of rules that govern how the client passes program arguments to the server, how the server passes these arguments and other information to the child process, and how the child sends its output back to the client. 
A simple but functioning Web server that serves both static and dynamic content can be implemented in a few hundred lines of C code. 
Bibliographic 
Notes 

The of.cial source of information for the Internet is contained in a set of freely available numbered documents known as RFCs (Requests for Comments). A searchable index of RFCs is available on the Web at 
http://rfc-editor.org 

RFCs are typically written for developers of Internet infrastructure, and thus are usually too detailed for the casual reader. However, for authoritative infor-mation, there is no better source. The HTTP/1.1 protocol is documented in RFC 2616. The authoritative list of MIME types is maintained at 
http://www.iana.org/assignments/media-types 

There are a number of good general texts on computer networking [62, 80, 113]. The great technical writer W. Richard Stevens developed a series of classic texts on such topics as advanced Unix programming [110], the Internet proto-cols [105, 106, 107], and Unix network programming [108, 109]. Serious students of Unix systems programming will want to study all of them. Tragically, Stevens died on September 1, 1999. His contributions will be greatly missed. 
Homework 
Problems 

11.6 ¡ô¡ô 
A. Modify Tiny so that it echoes every request line and request header. 
B. Use your favorite browser to make a request to Tiny for static content. Capture the output from Tiny in a .le. 
C. Inspect the output from Tiny to determine the version of HTTP your browser uses. 
D. Consult the HTTP/1.1 standard in RFC 2616 to determine the meaning of each header in the HTTP request from your browser. You can obtain RFC 2616 from www.rfc-editor.org/rfc.html. 

11.7 ¡ô¡ô 

Extend Tiny so that it serves MPG video .les. Check your work using a real browser. 
11.8 ¡ô¡ô 

Modify Tiny so that it reaps CGI children inside a SIGCHLD handler instead of explicitly waiting for them to terminate. 
11.9 ¡ô¡ô 

Modify Tiny so that when it serves static content, it copies the requested .le to the connected descriptor using malloc, rio_readn, and rio_writen, instead of mmap and rio_writen. 
11.10 ¡ô¡ô 

A. Write an HTML form for the CGI adder function in Figure 11.26. Your form should include two text boxes that users .ll in with the two numbers to be added together. Your form should request content using the GET method. 
B. Check your work by using a real browser to request the form from Tiny, submit the .lled-in form to Tiny, and then display the dynamic content generated by adder. 
11.11 ¡ô¡ô 

Extend Tiny to support the HTTP HEAD method. Check your work using telnet as a Web client. 
11.12 ¡ô¡ô¡ô 

Extend Tiny so that it serves dynamic content requested by the HTTP POST method. Check your work using your favorite Web browser. 
11.13 ¡ô¡ô¡ô 

Modify Tiny so that it deals cleanly (without terminating) with the SIGPIPE signals and EPIPE errors that occur when the write function attempts to write to a prematurely closed connection. 
Solutions 
to 
Practice 
Problems 

Solution to Problem 11.1 (page 894) 
Hex address  Dotted-decimal address  
0x0  0.0.0.0  
0xffffffff  255.255.255.255  
0x7f000001  127.0.0.1  
0xcdbca079  205.188.160.121  
0x400c950d  64.12.149.13  
0xcdbc9217  205.188.146.23  

Solution to Problem 11.2 (page 894) 

code/netp/hex2dd.c  
1  #include  "csapp.h"  
2  
3  int  main(int  argc,  char  **argv)  
4  {  
5  struct  in_addr  inaddr;  /*  addr  in  network  byte  order  */  
6  unsigned  int  addr;  /*  addr  in  host  byte  order  */  
7  
8  if  (argc  !=  2)  {  
9  fprintf(stderr,  "usage:  %s  <hex  number>\n",  argv[0]);  
10  exit(0);  
11  }  
12  sscanf(argv[1],  "%x",  &addr);  
13  inaddr.s_addr  =  htonl(addr);  
14  printf("%s\n",  inet_ntoa(inaddr));  
15  
16  exit(0);  
17  }  
code/netp/hex2dd.c  

Solution to Problem 11.3 (page 895) 
code/netp/dd2hex.c 
1 #include "csapp.h" 
2 
3 int main(int argc, char **argv) 
4 { 
5 struct in_addr inaddr; /* addr in network byte order */ 
6 unsigned int addr; /* addr in host byte order */ 
7 
8 if (argc != 2) { 
9 fprintf(stderr, "usage: %s <dotted-decimal>\n", argv[0]); 
10 exit(0); 
11 } 12 13 if (inet_aton(argv[1], &inaddr) == 0) 14 app_error("inet_aton error"); 15 addr = ntohl(inaddr.s_addr); 16 printf("0x%x\n", addr); 17 18 exit(0); 
19 } 
code/netp/dd2hex.c 

Solution to Problem 11.4 (page 899) 
Each time we request the host entry for google.com, the list of corresponding Internet addresses is returned in a different round-robin order. 
unix> ./hostinfo google.com official hostname: google.com address: 74.125.127.100 address: 74.125.45.100 address: 74.125.67.100 
unix> ./hostinfo google.com official hostname: google.com address: 74.125.67.100 address: 74.125.127.100 address: 74.125.45.100 
unix> ./hostinfo google.com official hostname: google.com address: 74.125.45.100 address: 74.125.67.100 address: 74.125.127.100 
The different ordering of the addresses in different DNS queries is known as DNS round-robin. It can be used to load-balance requests to a heavily used domain name. 
Solution to Problem 11.5 (page 919) 
The reason that standard I/O works in CGI programs is that the CGI program running in the child process does not need to explicitly close any of its input or output streams. When the child terminates, the kernel closes all descriptors automatically. 
This page intentionally left blank 

CHAPTER 
12 
Concurrent 
Programming 
12.1 
Concurrent 
Programming 
with 
Processes 
935 
12.2 
Concurrent 
Programming 
with 
I/O 
Multiplexing 
939 
12.3 
Concurrent 
Programming 
with 
Threads 
947 
12.4 
Shared 
Variables 
in 
Threaded 
Programs 
954 
12.5 
Synchronizing 
Threads 
with 
Semaphores 
957 
12.6 
Using 
Threads 
for 
Parallelism 
974 
12.7 
Other 
Concurrency 
Issues 
979 
12.8 
Summary 
988 
Bibliographic 
Notes 
989 
Homework 
Problems 
989 
Solutions 
to 
Practice 
Problems 
994 
As we learned in Chapter 8, logical control .ows are concurrent if they overlap in time. This general phenomenon, known as concurrency, shows up at many different levels of a computer system. Hardware exception handlers, processes, and Unix signal handlers are all familiar examples. 
Thus far, we have treated concurrency mainly as a mechanism that the oper-ating system kernel uses to run multiple application programs. But concurrency is not just limited to the kernel. It can play an important role in application programs as well. For example, we have seen how Unix signal handlers allow applications to respond to asynchronous events such as the user typing ctrl-c or the program accessing an unde.ned area of virtual memory. Application-level concurrency is useful in other ways as well: 
. Accessing slow I/O devices. When an application is waiting for data to arrive from a slow I/O device such as a disk, the kernel keeps the CPU busy by running other processes. Individual applications can exploit concurrency in a similar way by overlapping useful work with I/O requests. 
. Interacting with humans.People who interact with computers demand the abil-ity to perform multiple tasks at the same time. For example, they might want to resize a window while they are printing a document. Modern windowing systems use concurrency to provide this capability. Each time the user requests some action (say, by clicking the mouse), a separate concurrent logical .ow is created to perform the action. 
. Reducing latency by deferring work. Sometimes, applications can use concur-rency to reduce the latency of certain operations by deferring other operations and performing them concurrently. For example, a dynamic storage allocator might reduce the latency of individual free operations by deferring coalesc-ing to a concurrent ¡°coalescing¡± .ow that runs at a lower priority, soaking up spare CPU cycles as they become available. 
. Servicing multiple network clients. The iterative network servers that we stud-ied in Chapter 11 are unrealistic because they can only service one client at a time. Thus, a single slow client can deny service to every other client. For a real server that might be expected to service hundreds or thousands of clients per second, it is not acceptable to allow one slow client to deny service to the others. A better approach is to build a concurrent server that creates a separate logical .ow for each client. This allows the server to service multiple clients concurrently, and precludes slow clients from monopolizing the server. 
. Computing in parallel on multi-core machines. Many modern systems are equipped with multi-core processors that contain multiple CPUs. Applica-tions that are partitioned into concurrent .ows often run faster on multi-core machines than on uniprocessor machines because the .ows execute in parallel rather than being interleaved. 
Applications that use application-level concurrency are known as concurrent 
programs. Modern operating systems provide three basic approaches for building 
concurrent programs: 

. Processes. With this approach, each logical control .ow is a process that is scheduled and maintained by the kernel. Since processes have separate virtual address spaces, .ows that want to communicate with each other must use some kind of explicit interprocess communication (IPC) mechanism. 
. I/O multiplexing.This is a form of concurrent programming where applications explicitly schedule their own logical .ows in the context of a single process. Logical .ows are modeled as state machines that the main program explicitly transitions from state to state as a result of data arriving on .le descriptors. Since the program is a single process, all .ows share the same address space. 
. Threads. Threads are logical .ows that run in the context of a single process and are scheduled by the kernel. You can think of threads as a hybrid of the other two approaches, scheduled by the kernel like process .ows, and sharing the same virtual address space like I/O multiplexing .ows. 
This chapter investigates these three different concurrent programming tech-niques. To keep our discussion concrete, we will work with the same motivating application throughout¡ªa concurrent version of the iterative echo server from Section 11.4.9. 
12.1 
Concurrent 
Programming 
with 
Processes 

The simplest way to build a concurrent program is with processes, using familiar functions such as fork, exec, and waitpid. For example, a natural approach for building a concurrent server is to accept client connection requests in the parent, and then create a new child process to service each new client. 
To see how this might work, suppose we have two clients and a server that is listening for connection requests on a listening descriptor (say, 3). Now suppose that the server accepts a connection request from client 1 and returns a connected descriptor (say, 4), as shown in Figure 12.1. 
After accepting the connection request, the server forks a child, which gets a complete copy of the server¡¯s descriptor table. The child closes its copy of listening descriptor 3, and the parent closes its copy of connected descriptor 4, since they are no longer needed. This gives us the situation in Figure 12.2, where the child process is busy servicing the client. Since the connected descriptors in the parent and child each point to the same .le table entry, it is crucial for the parent to close 
Figure 12.1 
Connection 

request

Step 1: Server accepts 
connection request from 
listenfd(3)
clientfd
client. 
connfd(4) 

clientfd 

Figure 12.2 

Step 2: Server forks a Data 
transfers

child process to service the client. 
listenfd(3) clientfd 



Figure 12.3 

Step 3: Server accepts another connection request. 
clientfd 
Data transfers 
listenfd(3) clientfd connfd(5) 

Connection request

clientfd 
its copy of the connected descriptor. Otherwise, the .le table entry for connected descriptor 4 will never be released, and the resulting memory leak will eventually consume the available memory and crash the system. 
Now suppose that after the parent creates the child for client 1, it accepts a new connection request from client 2 and returns a new connected descriptor (say, 5), as shown in Figure 12.3. The parent then forks another child, which begins servicing its client using connected descriptor 5, as shown in Figure 12.4. At this point, the parent is waiting for the next connection request and the two children are servicing their respective clients concurrently. 
12.1.1 A Concurrent Server Based on Processes 
Figure 12.5 shows the code for a concurrent echo server based on processes. The echo function called in line 29 comes from Figure 11.21. There are several important points to make about this server: 
. First, servers typically run for long periods of time, so we must include a SIGCHLD handler that reaps zombie children (lines 4¨C9). Since SIGCHLD signals are blocked while the SIGCHLD handler is executing, and since Unix signals are not queued, the SIGCHLD handler must be prepared to reap multiple zombie children. 
Figure 12.4 

Step 4: Server forks Data transfersanother child to service the new client. 
listenfd(3) clientfd connfd(5) 




. Second, the parent and the child must close their respective copies of connfd (lines 33 and 30, respectively). As we have mentioned, this is especially im-portant for the parent, which must close its copy of the connected descriptor to avoid a memory leak. 
. Finally, because of the reference count in the socket¡¯s .le table entry, the connection to the client will not be terminated until both the parent¡¯s and child¡¯s copies of connfd are closed. 
12.1.2 Pros and Cons of Processes 
Processes have a clean model for sharing state information between parents and children: .le tables are shared and user address spaces are not. Having separate address spaces for processes is both an advantage and a disadvantage. It is im-possible for one process to accidentally overwrite the virtual memory of another process, which eliminates a lot of confusing failures¡ªan obvious advantage. 
On the other hand, separate address spaces make it more dif.cult for pro-cesses to share state information. To share information, they must use explicit IPC (interprocess communications) mechanisms. (See Aside.) Another disadvan-tage of process-based designs is that they tend to be slower because the overhead for process control and IPC is high. 
Aside Unix IPC 
You have already encountered several examples of IPC in this text. The waitpid function and Unix signals from Chapter 8 are primitive IPC mechanisms that allow processes to send tiny messages to processes running on the same host. The sockets interface from Chapter 11 is an important form of IPC that allows processes on different hosts to exchange arbitrary byte streams. However, the term Unix IPC is typically reserved for a hodge-podge of techniques that allow processes to communicate with other processes that are running on the same host. Examples include pipes, FIFOs, System V shared memory, and System V semaphores. These mechanisms are beyond our scope. The book by Stevens [108] is a good reference. 
code/conc/echoserverp.c 

1 #include "csapp.h" 2 void echo(int connfd); 3 4 void sigchld_handler(int sig) 
{ 6 while (waitpid(-1, 0, WNOHANG) > 0) 7 ; 8 return; 
9 } 

11 int main(int argc, char **argv) 12 { 13 int listenfd, connfd, port; 14 socklen_t clientlen=sizeof(struct sockaddr_in); 
struct sockaddr_in clientaddr; 

16 17 if (argc != 2) { 18 fprintf(stderr, "usage: %s <port>\n", argv[0]); 19 exit(0); 
} 21 port = atoi(argv[1]); 22 23 Signal(SIGCHLD, sigchld_handler); 24 listenfd = Open_listenfd(port); 
while (1) { 26 connfd = Accept(listenfd, (SA *) &clientaddr, &clientlen); 27 if (Fork() == 0) { 28 Close(listenfd); /* Child closes its listening socket */ 29 echo(connfd); /* Child services client */ 
Close(connfd); /* Child closes connection with client */ 31 exit(0); /* Child exits */ 
32 } 33 Close(connfd); /* Parent closes connected socket (important!) */ 
34 } } 
code/conc/echoserverp.c 
Figure 12.5 Concurrent echo server based on processes. The parent forks a child to handle each new connection request. 

Practice Problem 12.1 

After the parent closes the connected descriptor in line 33 of the concurrent server in Figure 12.5, the child is still able to communicate with the client using its copy of the descriptor. Why? 
Practice Problem 12.2 
If we were to delete line 30 of Figure 12.5, which closes the connected descriptor, the code would still be correct, in the sense that there would be no memory leak. Why? 
12.2 
Concurrent 
Programming 
with 
I/O 
Multiplexing 

Suppose you are asked to write an echo server that can also respond to interactive commands that the user types to standard input. In this case, the server must respond to two independent I/O events: (1) a network client making a connection request, and (2) a user typing a command line at the keyboard. Which event do we wait for .rst? Neither option is ideal. If we are waiting for a connection request in accept, then we cannot respond to input commands. Similarly, if we are waiting for an input command in read, then we cannot respond to any connection requests. 
One solution to this dilemma is a technique called I/O multiplexing. The basic idea is to use the select function to ask the kernel to suspend the process, return-ing control to the application only after one or more I/O events have occurred, as in the following examples: 
. Return when any descriptor in the set {0, 4} is ready for reading. 
. Return when any descriptor in the set {1, 2, 7} is ready for writing. 
. Timeout if 152.13 seconds have elapsed waiting for an I/O event to occur. 
Select is a complicated function with many different usage scenarios. We will only discuss the .rst scenario: waiting for a set of descriptors to be ready for reading. See [109, 110] for a complete discussion. 
#include <unistd.h> #include <sys/types.h> 
int select(int n, fd_set *fdset, NULL, NULL, NULL); 
Returns nonzero count of ready descriptors, .1 on error 

FD_ZERO(fd_set *fdset); /* Clear all bits in fdset */ FD_CLR(int fd, fd_set *fdset); /* Clear bit fd in fdset */ FD_SET(int fd, fd_set *fdset); /* Turn on bit fd in fdset */ FD_ISSET(int fd, fd_set *fdset); /* Is bit fd in fdset on? */ 
Macros for manipulating descriptor sets 

The select function manipulates sets of type fd_set, which are known as 
descriptor sets. Logically, we think of a descriptor set as a bit vector (introduced 
in Section 2.1) of size n: 
b .1,...,b1,b0
n 
Each bit bk corresponds to descriptor k. Descriptor kis a member of the descriptor set if and only if bk = 1. You are only allowed to do three things with descriptor sets: (1) allocate them, (2) assign one variable of this type to another, and (3) mod-ify and inspect them using the FD_ZERO, FD_SET, FD_CLR, and FD_ISSET macros. 
For our purposes, the select function takes two inputs: a descriptor set (fdset) called the read set, and the cardinality (n) of the read set (actually the maximum cardinality of any descriptor set). The select function blocks until at least one descriptor in the read set is ready for reading. A descriptor k is ready for reading if and only if a request to read 1 byte from that descriptor would not block. As a side effect, select modi.es the fd_set pointed to by argument fdset to indicate a subset of the read set called the ready set, consisting of the descriptors in the read set that are ready for reading. The value returned by the function indicates the cardinality of the ready set. Note that because of the side effect, we must update the read set every time select is called. 
The best way to understand select is to study a concrete example. Figure 12.6 shows how we might use select to implement an iterative echo server that also accepts user commands on the standard input. We begin by using the open_ listenfd function from Figure 11.17 to open a listening descriptor (line 17), and then using FD_ZERO to create an empty read set (line 19): 
listenfd stdin 
3210 read_set (.): 
0  0  0  0  

Next, in lines 20 and 21, we de.ne the read set to consist of descriptor 0 (standard input) and descriptor 3 (the listening descriptor), respectively: 
listenfd stdin 
3210 read_set ({0,3}) : 
1  0  0  1  

At this point, we begin the typical server loop. But instead of waiting for a connection request by calling the accept function, we call the select function, which blocks until either the listening descriptor or standard input is ready for reading (line 25). For example, here is the value of ready_set that select would return if the user hit the enter key, thus causing the standard input descriptor to become ready for reading: 
listenfd stdin 
3210 read_set ({0}) : 
0  0  0  1  


code/conc/select.c  
1  #include "csapp.h"  
2  void echo(int connfd);  
3  void command(void);  
4  
int  main(int argc, char **argv)  
6  {  
7  int listenfd, connfd, port;  
8  socklen_t clientlen = sizeof(struct sockaddr_in);  
9  struct sockaddr_in clientaddr;  
fd_set read_set, ready_set;  
11  
12  if (argc != 2) {  
13  fprintf(stderr, "usage: %s <port>\n", argv[0]);  
14  exit(0);  
}  
16  port = atoi(argv[1]);  
17  listenfd = Open_listenfd(port);  
18  
19  FD_ZERO(&read_set); /* Clear read set */  
FD_SET(STDIN_FILENO, &read_set); /* Add stdin to read set  */  
21  FD_SET(listenfd, &read_set); /* Add listenfd to read set */  
22  
23  while (1) {  
24  ready_set = read_set;  
Select(listenfd+1, &ready_set, NULL, NULL, NULL);  
26  if (FD_ISSET(STDIN_FILENO, &ready_set))  
27  command(); /* Read command line from stdin */  
28  if (FD_ISSET(listenfd, &ready_set)) {  
29  connfd = Accept(listenfd, (SA *)&clientaddr, &clientlen);  
echo(connfd); /* Echo client input until EOF */  
31  Close(connfd);  
32  }  
33  }  
34  }  

36 void command(void) { 37 char buf[MAXLINE]; 38 if (!Fgets(buf, MAXLINE, stdin)) 39 exit(0); /* EOF */ 
printf("%s", buf); /* Process the input command */ 
41 } 
code/conc/select.c 

Figure 12.6 An iterative echo server that uses I/O multiplexing. The server uses select to wait for connection requests on a listening descriptor and commands on standard input. 
Once select returns, we use the FD_ISSET macro to determine which de-scriptors are ready for reading. If standard input is ready (line 26), we call the command function, which reads, parses, and responds to the command before re-turning to the main routine. If the listening descriptor is ready (line 28), we call accept to get a connected descriptor, and then call the echo function from Fig-ure 11.21, which echoes each line from the client until the client closes its end of the connection. 
While this program is a good example of using select, it still leaves something to be desired. The problem is that once it connects to a client, it continues echoing input lines until the client closes its end of the connection. Thus, if you type a command to standard input, you will not get a response until the server is .nished with the client. A better approach would be to multiplex at a .ner granularity, echoing (at most) one text line each time through the server loop. 
Practice Problem 12.3 
In most Unix systems, typing ctrl-d indicates EOF on standard input. What happens if you type ctrl-d to the program in Figure 12.6 while it is blocked in the call to select? 
12.2.1 A Concurrent Event-Driven Server Based on I/O Multiplexing 
I/O multiplexing can be used as the basis for concurrent event-driven programs, where .ows make progress as a result of certain events. The general idea is to model logical .ows as state machines. Informally, a state machine is a collection of states, input events, and transitions that map states and input events to states. Each transition maps an (input state, input event) pair to an output state. A self-loop is a transition between the same input and output state. State machines are typically drawn as directed graphs, where nodes represent states, directed arcs represent transitions, and arc labels represent input events. A state machine begins execution in some initial state. Each input event triggers a transition from the current state to the next state. 
For each new client k, a concurrent server based on I/O multiplexing creates a new state machine sk and associates it with connected descriptor dk. As shown in Figure 12.7, each state machine sk has one state (¡°waiting for descriptor dk to be ready for reading¡±), one input event (¡°descriptor dk is ready for reading¡±), and one transition (¡°read a text line from descriptor dk¡±). 
The server uses the I/O multiplexing, courtesy of the select function, to detect the occurrence of input events. As each connected descriptor becomes ready for reading, the server executes the transition for the corresponding state machine, in this case reading and echoing a text line from the descriptor. 
Figure 12.8 shows the complete example code for a concurrent event-driven server based on I/O multiplexing. The set of active clients is maintained in a pool structure (lines 3¨C11). After initializing the pool by calling init_pool (line 29), the server enters an in.nite loop. During each iteration of this loop, the server calls 

Figure 12.7 Transition: 
State machine for ¡°read a text line from a logical .ow in a concurrent event-driven echo server. 


the select function to detect two different kinds of input events: (a) a connection request arriving from a new client, and (b) a connected descriptor for an existing client being ready for reading. When a connection request arrives (line 36), the server opens the connection (line 37) and calls the add_client function to add the client to the pool (line 38). Finally, the server calls the check_clients function to echo a single text line from each ready connected descriptor (line 42). 
The init_pool function (Figure 12.9) initializes the client pool. The clientfd array represents a set of connected descriptors, with the integer .1 denoting an available slot. Initially, the set of connected descriptors is empty (lines 5¨C7), and the listening descriptor is the only descriptor in the select read set (lines 10¨C12). 
The add_client function (Figure 12.10) adds a new client to the pool of active clients. After .nding an empty slot in the clientfd array, the server adds the connected descriptor to the array and initializes a corresponding Rio read buffer so that we can call rio_readlineb on the descriptor (lines 8¨C9). We then add the connected descriptor to the select read set (line 12), and we update some global properties of the pool. The maxfd variable (lines 15¨C16) keeps track of the largest .le descriptor for select.The maxi variable (lines 17¨C18) keeps track of the largest index into the clientfd array so that the check_clients functions does not have to search the entire array. 
The check_clients function echoes a text line from each ready connected descriptor (Figure 12.11). If we are successful in reading a text line from the descriptor, then we echo that line back to the client (lines 15¨C18). Notice that in line 15 we are maintaining a cumulative count of total bytes received from all clients. If we detect EOF because the client has closed its end of the connection, then we close our end of the connection (line 23) and remove the descriptor from the pool (lines 24¨C25). 
In terms of the .nite state model in Figure 12.7, the select function detects input events, and the add_client function creates a new logical .ow (state ma-chine). The check_clients function performs state transitions by echoing input lines, and it also deletes the state machine when the client has .nished sending text lines. 
944  Chapter 12  Concurrent Programming  
code/conc/echoservers.c  
1  #include  "csapp.h"  
2  
3  typedef  struct  {  /*  Represents  a  pool  of  connected  descriptors  */  
4  int  maxfd;  /*  Largest  descriptor  in  read_set  */  
5  fd_set  read_set;  /*  Set  of  all  active  descriptors  */  
6  fd_set  ready_set;  /*  Subset  of  descriptors  ready  for  reading  */  
7  int  nready;  /*  Number  of  ready  descriptors  from  select  */  
8  int  maxi;  /*  Highwater  index  into  client  array  */  
9  int  clientfd[FD_SETSIZE];  /*  Set  of  active  descriptors  */  
10  rio_t  clientrio[FD_SETSIZE];  /*  Set  of  active  read  buffers  */  
11  }  pool;  
12  
13  int  byte_cnt  =  0;  /*  Counts  total  bytes  received  by  server  */  
14  
15  int  main(int  argc,  char  **argv)  
16  {  
17  int  listenfd,  connfd,  port;  
18  socklen_t  clientlen  =  sizeof(struct  sockaddr_in);  
19  struct  sockaddr_in  clientaddr;  
20  static  pool  pool;  
21  
22  if  (argc  !=  2)  {  
23  fprintf(stderr,  "usage:  %s  <port>\n",  argv[0]);  
24  exit(0);  
25  }  
26  port  =  atoi(argv[1]);  
27  
28  listenfd  =  Open_listenfd(port);  
29  init_pool(listenfd,  &pool);  
30  while (1) {  
31  /*  Wait  for  listening/connected  descriptor(s)  to  become  ready  */  
32  pool.ready_set  =  pool.read_set;  
33  pool.nready  =  Select(pool.maxfd+1,  &pool.ready_set,  NULL,  NULL,  NULL);  
34  
35  /*  If  listening  descriptor  ready,  add  new  client  to  pool  */  
36  if  (FD_ISSET(listenfd,  &pool.ready_set))  {  
37  connfd  =  Accept(listenfd,  (SA  *)&clientaddr,  &clientlen);  
38  add_client(connfd,  &pool);  
39  }  
40  
41  /*  Echo  a  text  line  from  each  ready  connected  descriptor  */  
42  check_clients(&pool);  
43  }  
44  }  
code/conc/echoservers.c  
Figure 12.8  Concurrent echo server based on I/O multiplexing. Each server iteration  
echoes a text line from each ready descriptor.  

code/conc/echoservers.c 

1 void init_pool(int listenfd, pool *p) 2 { 3 /* Initially, there are no connected descriptors */ 4 int i; 5 p->maxi = -1; 6 for (i=0; i< FD_SETSIZE; i++) 7 p->clientfd[i] = -1; 8 9 /* Initially, listenfd is only member of select read set */ 
10 p->maxfd = listenfd; 
11 FD_ZERO(&p->read_set); 
12 FD_SET(listenfd, &p->read_set); 
13 } 
code/conc/echoservers.c 

Figure 12.9 init_pool: Initializes the pool of active clients. 
code/conc/echoservers.c 

1 void add_client(int connfd, pool *p) 2 { 3 int i; 4 p->nready--; 5 for (i = 0; i < FD_SETSIZE; i++) /* Find an available slot */ 6 if (p->clientfd[i] < 0) { 7 /* Add connected descriptor to the pool */ 8 p->clientfd[i] = connfd; 9 Rio_readinitb(&p->clientrio[i], connfd); 
10 

11 /* Add the descriptor to descriptor set */ 
12 FD_SET(connfd, &p->read_set); 
13 

14 /* Update max descriptor and pool highwater mark */ 
15 if (connfd > p->maxfd) 
16 p->maxfd = connfd; 
17 if (i > p->maxi) 
18 p->maxi = i; 
19 break; 
20 } 21 if (i == FD_SETSIZE) /* Couldn¡¯t find an empty slot */ 22 app_error("add_client error: Too many clients"); 
23 } 
code/conc/echoservers.c 

Figure 12.10 add_client: Adds a new client connection to the pool. 
946  Chapter 12  Concurrent Programming  
code/conc/echoservers.c  
1  void  check_clients(pool  *p)  
2  {  
3  int  i,  connfd,  n;  
4  char  buf[MAXLINE];  
5  rio_t  rio;  
6  
7  for  (i  =  0;  (i  <=  p->maxi)  &&  (p->nready  >  0);  i++)  {  
8  connfd  =  p->clientfd[i];  
9  rio  =  p->clientrio[i];  
10  
11  /*  If  the  descriptor  is  ready,  echo  a  text  line  from  it  */  
12  if  ((connfd  >  0)  &&  (FD_ISSET(connfd,  &p->ready_set)))  {  
13  p->nready--;  
14  if ((n = Rio_readlineb(&rio, buf, MAXLINE)) != 0) {  
15  byte_cnt  +=  n;  
16  printf("Server  received  %d  (%d  total)  bytes  on  fd  %d\n",  
17  n,  byte_cnt,  connfd);  
18  Rio_writen(connfd,  buf,  n);  
19  }  
20  
21  /*  EOF  detected,  remove  descriptor  from  pool  */  
22  else  {  
23  Close(connfd);  
24  FD_CLR(connfd, &p->read_set);  
25  p->clientfd[i] = -1;  
26  }  
27  }  
28  }  
29  }  
code/conc/echoservers.c  
Figure 12.11  check_clients: Services ready client connections.  
12.2.2  Pros and Cons of I/O Multiplexing  
The server in Figure 12.8 provides a nice example of the advantages and disad- 
vantages of event-driven programming based on I/O multiplexing. One advantage  
is that event-driven designs give programmers more control over the behavior of  
their programs than process-based designs. For example, we can imagine writ- 
ing an event-driven concurrent server that gives preferred service to some clients,  
which would be dif.cult for a concurrent server based on processes.  
Another advantage is that an event-driven server based on I/O multiplexing  
runs in the context of a single process, and thus every logical .ow has access to  
the entire address space of the process. This makes it easy to share data between  

.ows. A related advantage of running as a single process is that you can debug your concurrent server as you would any sequential program, using a familiar debugging tool such as gdb. Finally, event-driven designs are often signi.cantly more ef.cient than process-based designs because they do not require a process context switch to schedule a new .ow. 
A signi.cant disadvantage of event-driven designs is coding complexity. Our event-driven concurrent echo server requires three times more code than the process-based server. Unfortunately, the complexity increases as the granularity of the concurrency decreases. By granularity, we mean the number of instructions that each logical .ow executes per time slice. For instance, in our example concur-rent server, the granularity of concurrency is the number of instructions required to read an entire text line. As long as some logical .ow is busy reading a text line, no other logical .ow can make progress. This is .ne for our example, but it makes our event-driver server vulnerable to a malicious client that sends only a partial text line and then halts. Modifying an event-driven server to handle partial text lines is a nontrivial task, but it is handled cleanly and automatically by a process-based design. Another signi.cant disadvantage of event-based designs is that they cannot fully utilize multi-core processors. 
Practice Problem 12.4 
In the server in Figure 12.8, we are careful to reinitialize the pool.ready_set variable immediately before every call to select. Why? 
12.3 
Concurrent 
Programming 
with 
Threads 

To this point, we have looked at two approaches for creating concurrent logical .ows. With the .rst approach, we use a separate process for each .ow. The kernel schedules each process automatically. Each process has its own private address space, which makes it dif.cult for .ows to share data. With the second approach, we create our own logical .ows and use I/O multiplexing to explicitly schedule the .ows. Because there is only one process, .ows share the entire address space. This section introduces a third approach¡ªbased on threads¡ªthat is a hybrid of these two. 
A thread is a logical .ow that runs in the context of a process. Thus far in this book, our programs have consisted of a single thread per process. But modern systems also allow us to write programs that have multiple threads running concurrently in a single process. The threads are scheduled automatically by the kernel. Each thread has its own thread context, including a unique integer thread ID (TID), stack, stack pointer, program counter, general-purpose registers, and condition codes. All threads running in a process share the entire virtual address space of that process. 
Logical .ows based on threads combine qualities of .ows based on processes and I/O multiplexing. Like processes, threads are scheduled automatically by the kernel and are known to the kernel by an integer ID. Like .ows based on I/O 
Figure 12.12 
Concurrent thread execution. 
Time 

Thread context switch 
Thread context switch Thread context switch 
multiplexing, multiple threads run in the context of a single process, and thus share the entire contents of the process virtual address space, including its code, data, heap, shared libraries, and open .les. 
12.3.1 Thread Execution Model 
The execution model for multiple threads is similar in some ways to the execution model for multiple processes. Consider the example in Figure 12.12. Each process begins life as a single thread called the main thread. At some point, the main thread creates a peer thread, and from this point in time the two threads run concurrently. Eventually, control passes to the peer thread via a context switch, because the main thread executes a slow system call such as read or sleep, or because it is interrupted by the system¡¯s interval timer. The peer thread executes for a while before control passes back to the main thread, and so on. 
Thread execution differs from processes in some important ways. Because a thread context is much smaller than a process context, a thread context switch is faster than a process context switch. Another difference is that threads, unlike pro-cesses, are not organized in a rigid parent-child hierarchy. The threads associated with a process form a pool of peers, independent of which threads were created by which other threads. The main thread is distinguished from other threads only in the sense that it is always the .rst thread to run in the process. The main impact of this notion of a pool of peers is that a thread can kill any of its peers, or wait for any of its peers to terminate. Further, each peer can read and write the same shared data. 
12.3.2 Posix Threads 
Posix threads (Pthreads) is a standard interface for manipulating threads from C programs. It was adopted in 1995 and is available on most Unix systems. Pthreads de.nes about 60 functions that allow programs to create, kill, and reap threads, to share data safely with peer threads, and to notify peers about changes in the system state. 

Section 12.3 Concurrent Programming with Threads 949 

code/conc/hello.c  
1  #include  "csapp.h"  
2  void  *thread(void  *var gp);  
3  
4  int main()  
5  {  
6  pthread_t  tid;  
7  Pthread_create( &tid,  NULL,  thread,  NULL);  
8  Pthread_join(tid,  NULL);  
9  exit(0);  
10  }  
11  
12  void  *thread(void  * vargp)  /*  Thread  routine  */  
13  {  
14  printf("Hello,  world!\n");  
15  return  NULL;  
16  }  
code/conc/hello.c  

Figure 12.13 hello.c: The Pthreads ¡°Hello, world!¡± program. 
Figure 12.13 shows a simple Pthreads program. The main thread creates a peer thread and then waits for it to terminate. The peer thread prints ¡°Hello, world!\n¡± and terminates. When the main thread detects that the peer thread has terminated, it terminates the process by calling exit. 
This is the .rst threaded program we have seen, so let us dissect it carefully. The code and local data for a thread is encapsulated in a thread routine. As shown by the prototype in line 2, each thread routine takes as input a single generic pointer and returns a generic pointer. If you want to pass multiple arguments to a thread routine, then you should put the arguments into a structure and pass a pointer to the structure. Similarly, if you want the thread routine to return multiple arguments, you can return a pointer to a structure. 
Line 4 marks the beginning of the code for the main thread. The main thread declares a single local variable tid, which will be used to store the thread ID of the peer thread (line 6). The main thread creates a new peer thread by calling the pthread_create function (line 7). When the call to pthread_create returns, the main thread and the newly created peer thread are running concurrently, and tid contains the ID of the new thread. The main thread waits for the peer thread to terminate with the call to pthread_join in line 8. Finally, the main thread calls exit (line 9), which terminates all threads (in this case just the main thread) currently running in the process. 
Lines 12¨C16 de.ne the thread routine for the peer thread. It simply prints a string and then terminates the peer thread by executing the return statement in line 15. 
12.3.3 Creating Threads 
Threads create other threads by calling the pthread_create function. 
#include <pthread.h> typedef void *(func)(void *); 
int pthread_create(pthread_t *tid, pthread_attr_t *attr, func *f, void *arg); 
Returns: 0 if OK, nonzero on error 
The pthread_create function creates a new thread and runs the thread rou-tine f in the context of the new thread and with an input argument of arg.The attr argument can be used to change the default attributes of the newly created thread. Changing these attributes is beyond our scope, and in our examples, we will always call pthread_create with a NULL attr argument. 
When pthread_create returns, argument tid contains the ID of the newly created thread. The new thread can determine its own thread ID by calling the pthread_self function. 
#include <pthread.h> pthread_t pthread_self(void); 
Returns: thread ID of caller 
12.3.4 Terminating Threads 
A thread terminates in one of the following ways: 
. The thread terminates implicitly when its top-level thread routine returns. . The thread terminates explicitly by calling the pthread_exit function. If the main thread calls pthread_exit, it waits for all other peer threads to terminate, and then terminates the main thread and the entire process with a return value of thread_return. 
#include <pthread.h> void pthread_exit(void *thread_return); 
Returns: 0 if OK, nonzero on error 
. Some peer thread calls the Unix exit function, which terminates the process and all threads associated with the process. 
. Another peer thread terminates the current thread by calling the pthread_ cancel function with the ID of the current thread. 

#include <pthread.h> int pthread_cancel(pthread_t tid); 
Returns: 0 if OK, nonzero on error 

12.3.5 Reaping Terminated Threads 
Threads wait for other threads to terminate by calling the pthread_join function. 
#include  <pthread.h>  
int  pthread_join(pthread_t  tid,  void  **thread_return);  
Returns: 0 if OK, nonzero on error  

The pthread_join function blocks until thread tid terminates, assigns the generic (void *) pointer returned by the thread routine to the location pointed to by thread_return, and then reaps any memory resources held by the terminated thread. 
Notice that, unlike the Unix wait function, the pthread_join function can only wait for a speci.c thread to terminate. There is no way to instruct pthread_ wait to wait for an arbitrary thread to terminate. This can complicate our code by forcing us to use other, less intuitive mechanisms to detect process termination. Indeed, Stevens argues convincingly that this is a bug in the speci.cation [109]. 
12.3.6 Detaching Threads 
At any point in time, a thread is joinable or detached. A joinable thread can be reaped and killed by other threads. Its memory resources (such as the stack) are not freed until it is reaped by another thread. In contrast, a detached thread cannot be reaped or killed by other threads. Its memory resources are freed automatically by the system when it terminates. 
By default, threads are created joinable. In order to avoid memory leaks, each joinable thread should either be explicitly reaped by another thread, or detached by a call to the pthread_detach function. 
#include <pthread.h> int pthread_detach(pthread_t tid); 
Returns: 0 if OK, nonzero on error 

The pthread_detach function detaches the joinable thread tid. Threads can detach themselves by calling pthread_detach with an argument of pthread_ self(). 
Although some of our examples will use joinable threads, there are good rea-sons to use detached threads in real programs. For example, a high-performance Web server might create a new peer thread each time it receives a connection re-quest from a Web browser. Since each connection is handled independently by a separate thread, it is unnecessary¡ªand indeed undesirable¡ªfor the server to ex-plicitly wait for each peer thread to terminate. In this case, each peer thread should detach itself before it begins processing the request so that its memory resources can be reclaimed after it terminates. 
12.3.7 Initializing Threads 
The pthread_once function allows you to initialize the state associated with a thread routine. 
#include <pthread.h> pthread_once_t once_control = PTHREAD_ONCE_INIT; int pthread_once(pthread_once_t *once_control, 
void (*init_routine)(void)); 
Always returns 0 
The once_control variable is a global or static variable that is always initial-ized to PTHREAD_ONCE_INIT. The .rst time you call pthread_once with an argument of once_control, it invokes init_routine, which is a function with no input arguments that returns nothing. Subsequent calls to pthread_once with the same once_control variable do nothing. The pthread_once function is useful whenever you need to dynamically initialize global variables that are shared by multiple threads. We will look at an example in Section 12.5.5. 
12.3.8 A Concurrent Server Based on Threads 
Figure 12.14 shows the code for a concurrent echo server based on threads. The overall structure is similar to the process-based design. The main thread repeat-edly waits for a connection request and then creates a peer thread to handle the request. While the code looks simple, there are a couple of general and somewhat subtle issues we need to look at more closely. The .rst issue is how to pass the con-nected descriptor to the peer thread when we call pthread_create. The obvious approach is to pass a pointer to the descriptor, as in the following: 
connfd = Accept(listenfd, (SA *) &clientaddr, &clientlen); 
Pthread_create(&tid, NULL, thread, &connfd); 
code/conc/echoservert.c 

#include "csapp.h" 
void echo(int connfd); void *thread(void *vargp); 
int main(int argc, char **argv) 
{ int listenfd, *connfdp, port; socklen_t clientlen=sizeof(struct sockaddr_in); struct sockaddr_in clientaddr; pthread_t tid; 
if (argc != 2) { fprintf(stderr, "usage: %s <port>\n", argv[0]); exit(0); 
} port = atoi(argv[1]); 
listenfd = Open_listenfd(port); 
while (1) { connfdp = Malloc(sizeof(int)); *connfdp = Accept(listenfd, (SA *) &clientaddr, &clientlen); Pthread_create(&tid, NULL, thread, connfdp); 
} } 

/* Thread routine */ 
void *thread(void *vargp) 
{ int connfd = *((int *)vargp); Pthread_detach(pthread_self()); Free(vargp); echo(connfd); Close(connfd); return NULL; 
} 
code/conc/echoservert.c 

Figure 12.14 Concurrent echo server based on threads. 
Then we have the peer thread dereference the pointer and assign it to a local variable, as follows: 
void *thread(void *vargp) { int connfd = *((int *)vargp); . 
. 
. } 
This would be wrong, however, because it introduces a race between the as-signment statement in the peer thread and the accept statement in the main thread. If the assignment statement completes before the next accept, then the lo-cal connfd variable in the peer thread gets the correct descriptor value. However, if the assignment completes after the accept, then the local connfd variable in the peer thread gets the descriptor number of the next connection. The unhappy result is that two threads are now performing input and output on the same descriptor. In order to avoid the potentially deadly race, we must assign each connected de-scriptor returned by accept to its own dynamically allocated memory block, as shown in lines 21¨C22. We will return to the issue of races in Section 12.7.4. 
Another issue is avoiding memory leaks in the thread routine. Since we are not explicitly reaping threads, we must detach each thread so that its memory resources will be reclaimed when it terminates (line 31). Further, we must be careful to free the memory block that was allocated by the main thread (line 32). 
Practice Problem 12.5 
In the process-based server in Figure 12.5, we were careful to close the connected descriptor in two places: the parent and child processes. However, in the threads-based server in Figure 12.14, we only closed the connected descriptor in one place: the peer thread. Why? 
12.4 
Shared 
Variables 
in 
Threaded 
Programs 

From a programmer¡¯s perspective, one of the attractive aspects of threads is the ease with which multiple threads can share the same program variables. However, this sharing can be tricky. In order to write correctly threaded programs, we must have a clear understanding of what we mean by sharing and how it works. 
There are some basic questions to work through in order to understand whether a variable in a C program is shared or not: (1) What is the underlying memory model for threads? (2) Given this model, how are instances of the vari-able mapped to memory? (3) Finally, how many threads reference each of these instances? The variable is shared if and only if multiple threads reference some instance of the variable. 
To keep our discussion of sharing concrete, we will use the program in Fig-ure 12.15 as a running example. Although somewhat contrived, it is nonetheless useful to study because it illustrates a number of subtle points about sharing. The example program consists of a main thread that creates two peer threads. The 
code/conc/sharing.c 

1 #include "csapp.h" 2 #define N 2 3 void *thread(void *vargp); 4 5 char **ptr; /* Global variable */ 6 7 int main() 8 { 9 int i; 
10 pthread_t tid; 11 char *msgs[N] = { 12 "Hello from foo", 13 "Hello from bar" 14 }; 15 16 ptr = msgs; 17 for (i=0;i<N; i++) 18 Pthread_create(&tid, NULL, thread, (void *)i); 19 Pthread_exit(NULL); 
20 } 21 22 void *thread(void *vargp) 23 { 24 int myid = (int)vargp; 25 static int cnt = 0; 26 printf("[%d]: %s (cnt=%d)\n", myid, ptr[myid], ++cnt); 27 return NULL; 
28 } 
code/conc/sharing.c 

Figure 12.15 Example program that illustrates different aspects of sharing. 
main thread passes a unique ID to each peer thread, which uses the ID to print a personalized message, along with a count of the total number of times that the thread routine has been invoked. 
12.4.1 Threads Memory Model 
A pool of concurrent threads runs in the context of a process. Each thread has its own separate thread context, which includes a thread ID, stack, stack pointer, program counter, condition codes, and general-purpose register values. Each thread shares the rest of the process context with the other threads. This includes the entire user virtual address space, which consists of read-only text (code), read/write data, the heap, and any shared library code and data areas. The threads also share the same set of open .les. 
In an operational sense, it is impossible for one thread to read or write the register values of another thread. On the other hand, any thread can access any location in the shared virtual memory. If some thread modi.es a memory location, then every other thread will eventually see the change if it reads that location. Thus, registers are never shared, whereas virtual memory is always shared. 
The memory model for the separate thread stacks is not as clean. These stacks are contained in the stack area of the virtual address space, and are usually accessed independently by their respective threads. We say usually rather than always, because different thread stacks are not protected from other threads. So if a thread somehow manages to acquire a pointer to another thread¡¯s stack, then it can read and write any part of that stack. Our example program shows this in line 26, where the peer threads reference the contents of the main thread¡¯s stack indirectly through the global ptr variable. 
12.4.2 Mapping Variables to Memory 
Variables in threaded C programs are mapped to virtual memory according to their storage classes: 
. Global variables. A global variable is any variable declared outside of a func-tion. At run time, the read/write area of virtual memory contains exactly one instance of each global variable that can be referenced by any thread. For ex-ample, the global ptr variable declared in line 5 has one run-time instance in the read/write area of virtual memory. When there is only one instance of a variable, we will denote the instance by simply using the variable name¡ªin this case, ptr. 
. Local automatic variables. A local automatic variable is one that is declared inside a function without the static attribute. At run time, each thread¡¯s stack contains its own instances of any local automatic variables. This is true even if multiple threads execute the same thread routine. For example, there is one instance of the local variable tid, and it resides on the stack of the main thread. We will denote this instance as tid.m. As another example, there are two instances of the local variable myid, one instance on the stack of peer thread 0, and the other on the stack of peer thread 1. We will denote these instances as myid.p0 and myid.p1, respectively. 
. Local static variables. A local static variable is one that is declared inside a function with the static attribute. As with global variables, the read/write area of virtual memory contains exactly one instance of each local static variable declared in a program. For example, even though each peer thread in our example program declares cnt in line 25, at run time there is only one instance of cnt residing in the read/write area of virtual memory. Each peer thread reads and writes this instance. 
12.4.3 Shared Variables 
We say that a variable v is shared if and only if one of its instances is referenced by more than one thread. For example, variable cnt in our example program is 

shared because it has only one run-time instance and this instance is referenced by both peer threads. On the other hand, myid is not shared because each of its two instances is referenced by exactly one thread. However, it is important to realize that local automatic variables such as msgs can also be shared. 
Practice Problem 12.6 
A. Using the analysis from Section 12.4, .ll each entry in the following table with ¡°Yes¡± or ¡°No¡± for the example program in Figure 12.15. In the .rst column, the notation v.t denotes an instance of variable v residing on the local stack for thread t, where t is either m (main thread), p0 (peer thread 0), or p1 (peer thread 1). 
Variable Referenced by Referenced by Referenced by instance main thread? peer thread 0? peer thread 1? 
ptr 
cnt i.m msgs.m myid.p0 myid.p1 


B. Given the analysis in Part A, which of the variables ptr, cnt, i, msgs, and myid are shared? 
12.5 
Synchronizing 
Threads 
with 
Semaphores 

Shared variables can be convenient, but they introduce the possibility of nasty synchronization errors. Consider the badcnt.c program in Figure 12.16, which creates two threads, each of which increments a global shared counter variable called cnt. Since each thread increments the counter niters times, we expect its .nal value to be 2 ¡Á niters. This seems quite simple and straightforward. However, when we run badcnt.c on our Linux system, we not only get wrong answers, we get different answers each time! 
linux> ./badcnt 1000000 BOOM! cnt=1445085 
linux> ./badcnt 1000000 BOOM! cnt=1915220 
linux> ./badcnt 1000000 BOOM! cnt=1404746 
1  #include  "csapp.h"  
2  
3  void  *thread(void  *vargp);  /*  Thread  routine  prototype  */  
4  
/*  Global  shared  variable  */  
6  volatile  int  cnt  =  0;  /*  Counter  */  
7  
8  int  main(int  argc,  char  **argv)  
9  {  
int  niters;  
11  pthread_t  tid1,  tid2;  
12  
13  /*  Check  input  argument  */  
14  if  (argc  !=  2)  {  
printf("usage:  %s  <niters>\n",  argv[0]);  
16  exit(0);  
17  }  
18  niters  =  atoi(argv[1]);  
19  
/*  Create  threads  and  wait  for  them  to  finish  */  
21  Pthread_create(&tid1,  NULL,  thread,  &niters);  
22  Pthread_create(&tid2,  NULL,  thread,  &niters);  
23  Pthread_join(tid1,  NULL);  
24  Pthread_join(tid2,  NULL);  
26  /* Check result */  
27  if  (cnt  !=  (2  * niters))  
28  printf("BOOM!  cnt=%d\n",  cnt);  
29  else  
printf("OK  cnt=%d\n",  cnt);  
31  exit(0);  
32  }  
33  
34  /*  Thread  routine  */  
void  *thread(void  *vargp)  
36  {  
37  int  i,  niters  =  *((int  *)vargp);  
38  
39  for  (i  =  0;  i  <  niters;  i++)  
cnt++;  
41  
42  return  NULL;  
43  }  
code/conc/badcnt.c  
Figure 12.16 badcnt.c: An improperly synchronized counter program. 


Asm code for thread i 


Hi : Head 

C code for thread i 
Li : Load cnt 

for (i 0; i < niters; i) 
Ui : Update cnt
     cnt; 
Si : Store cnt Ti : Tail 

movl (%rdi),%ecx movl $0,%edx cmpl %ecx,%edx jge .L13  
.L11: movl cnt(%rip),%eax incl %eax movl %eax,cnt(%rip) 
 incl %edx cmpl %ecx,%edx jl .L11 .L13:  
Figure 12.17 Assembly code for the counter loop (lines 39¨C40) in badcnt.c. 


So what went wrong? To understand the problem clearly, we need to study the assembly code for the counter loop (lines 39¨C40), as shown in Figure 12.17. We will .nd it helpful to partition the loop code for thread i into .ve parts: 
.  Hi: The block of instructions at the head of the loop  
.  Li: The instruction that loads the shared variable cnt  into register %eaxi,  
where %eaxi  denotes the value of register %eax in thread i  
.  Ui: The instruction that updates (increments) %eaxi  
.  Si: The instruction that stores the updated value of %eaxi  back to the shared  
variable cnt  
.  Ti: The block of instructions at the tail of the loop  

Notice that the head and tail manipulate only local stack variables, while Li, Ui, and Si manipulate the contents of the shared counter variable. 
When the two peer threads in badcnt.c run concurrently on a uniprocessor, the machine instructions are completed one after the other in some order. Thus, each concurrent execution de.nes some total ordering (or interleaving) of the in-structions in the two threads. Unfortunately, some of these orderings will produce correct results, but others will not. 
Here is the crucial point: In general, there is no way for you to predict whether the operating system will choose a correct ordering for your threads. For example, Figure 12.18(a) shows the step-by-step operation of a correct instruction ordering. After each thread has updated the shared variable cnt, its value in memory is 2, which is the expected result. On the other hand, the ordering in Figure 12.18(b) produces an incorrect value for cnt. The problem occurs because thread 2 loads cnt in step 5, after thread 1 loads cnt in step 2, but before thread 1 stores its up-dated value in step 6. Thus, each thread ends up storing an updated counter value of 1. We can clarify these notions of correct and incorrect instruction orderings with the help of a device known as a progress graph, which we introduce in the next section. 
Step  Thread  Instr  %eax1  %eax2  cnt  Step  Thread  Instr  %eax1  %eax2  cnt  
1  1  H1  ¡ª  ¡ª  0  1  1  H1  ¡ª  ¡ª  0  
2  1  L1  0  ¡ª  0  2  1  L1  0  ¡ª  0  
3  1  U1  1  ¡ª  0  3  1  U1  1  ¡ª  0  
4  1  S1  1  ¡ª  1  4  2  H2  ¡ª  ¡ª  0  
5  2  H2  ¡ª  ¡ª  1  5  2  L2  ¡ª  0  0  
6  2  L2  ¡ª  1  1  6  1  S1  1  ¡ª  1  
7  2  U2  ¡ª  2  1  7  1  T1  1  ¡ª  1  
8  2  S2  ¡ª  2  2  8  2  U2  ¡ª  1  1  
9  2  T2  ¡ª  2  2  9  2  S2  ¡ª  1  1  
10  1  T1  1  ¡ª  2  10  2  T2  ¡ª  1  1  
Figure 12.18 Instruction orderings for the .rst loop iteration in badcnt.c. 


(a) Correct ordering (b) Incorrect ordering 
Practice Problem 12.7 
Complete the table for the following instruction ordering of badcnt.c: 
Step Thread Instr %eax1 %eax2 cnt 
1  1  H1  ¡ª  ¡ª  0  
2  1  L1  
3  2  H2  
4  2  L2  
5  2  U2  
6  2  S2  
7  1  U1  
8  1  S1  
9  1  T1  
10  2  T2  

Does this ordering result in a correct value for cnt? 
12.5.1 Progress Graphs 
A progress graph models the execution of n concurrent threads as a trajectory through an n-dimensional Cartesian space. Each axis kcorresponds to the progress of thread k. Each point (I1,I2,...,I )represents the state where thread k (k= 
n 
1,...,n) has completed instruction Ik. The origin of the graph corresponds to the initial state where none of the threads has yet completed an instruction. 
Figure 12.19 shows the two-dimensional progress graph for the .rst loop iteration of the badcnt.c program. The horizontal axis corresponds to thread 1, the vertical axis to thread 2. Point (L1,S2)corresponds to the state where thread 1 has completed L1 and thread 2 has completed S2. 
Section 12.5 Synchronizing Threads with Semaphores 961 

Figure 12.19 Thread 2 
Progress graph for the .rst loop iteration of badcnt.c. T2 
S2 
U2 
L2 
H2 
Thread 1 

Figure 12.20 Thread 2 
An example trajectory. 
T2 
S2 
U2 
L2 
H2 

A progress graph models instruction execution as a transition from one state to another. A transition is represented as a directed edge from one point to an adjacent point. Legal transitions move to the right (an instruction in thread 1 completes) or up (an instruction in thread 2 completes). Two instructions cannot complete at the same time¡ªdiagonal transitions are not allowed. Programs never run backwards, so transitions that move down or to the left are not legal either. 
The execution history of a program is modeled as a trajectory through the state space. Figure 12.20 shows the trajectory that corresponds to the following instruction ordering: 
H1,L1,U1,H2,L2,S1,T1,U2,S2,T2 

For thread i, the instructions (Li,Ui,Si)that manipulate the contents of the shared variable cnt constitute a critical section (with respect to shared variable 



Figure 12.21 

Safe and unsafe trajec-tories. The intersection of the critical regions forms an unsafe region. Trajec-tories that skirt the unsafe region correctly update the counter variable. 
Thread 2 
trajectory Critical section wrt cnt 
Thread 1 

cnt) that should not be interleaved with the critical section of the other thread. In other words, we want to ensure that each thread has mutually exclusive access to the shared variable while it is executing the instructions in its critical section. The phenomenon in general is known as mutual exclusion. 
On the progress graph, the intersection of the two critical sections de.nes a region of the state space known as an unsafe region. Figure 12.21 shows the unsafe region for the variable cnt. Notice that the unsafe region abuts, but does not include, the states along its perimeter. For example, states (H1,H2)and (S1,U2) abut the unsafe region, but are not part of it. A trajectory that skirts the unsafe region is known as a safe trajectory. Conversely, a trajectory that touches any part of the unsafe region is an unsafe trajectory. Figure 12.21 shows examples of safe and unsafe trajectories through the state space of our example badcnt.c program. The upper trajectory skirts the unsafe region along its left and top sides, and thus is safe. The lower trajectory crosses the unsafe region, and thus is unsafe. 
Any safe trajectory will correctly update the shared counter. In order to guarantee correct execution of our example threaded program¡ªand indeed any concurrent program that shares global data structures¡ªwe must somehow syn-chronize the threads so that they always have a safe trajectory. A classic approach is based on the idea of a semaphore, which we introduce next. 
Practice Problem 12.8 
Using the progress graph in Figure 12.21, classify the following trajectories as either safe or unsafe. 
A. H1,L1,U1,S1,H2,L2,U2,S2,T2,T1 
B. H2,L2,H1,L1,U1,S1,T1,U2,S2,T2 
C. H1,H2,L2,U2,S2,L1,U1,S1,T1,T2 

12.5.2 Semaphores 
Edsger Dijkstra, a pioneer of concurrent programming, proposed a classic solution to the problem of synchronizing different execution threads based on a special type of variable called a semaphore. A semaphore, s, is a global variable with a nonnegative integer value that can only be manipulated by two special operations, called P and V: 
. P(s):If sis nonzero, then P decrements sand returns immediately. If s is zero, then suspend the thread until s becomes nonzero and the process is restarted by a V operation. After restarting, the P operation decrements s and returns control to the caller. 
. V(s):The V operation increments s by 1. If there are any threads blocked at a P operation waiting for s to become nonzero, then the V operation restarts exactly one of these threads, which then completes its P operation by decrementing s. 
The test and decrement operations in P occur indivisibly, in the sense that once the semaphore s becomes nonzero, the decrement of s occurs without in-terruption. The increment operation in V also occurs indivisibly, in that it loads, increments, and stores the semaphore without interruption. Notice that the de.-nition of V does not de.ne the order in which waiting threads are restarted. The only requirement is that the V must restart exactly one waiting thread. Thus, when several threads are waiting at a semaphore, you cannot predict which one will be restarted as a result of the V. 
The de.nitions of P and V ensure that a running program can never enter a state where a properly initialized semaphore has a negative value. This property, known as the semaphore invariant, provides a powerful tool for controlling the trajectories of concurrent programs, as we shall see in the next section. 
The Posix standard de.nes a variety of functions for manipulating sema-phores. 
#include <semaphore.h> 
int sem_init(sem_t *sem, 0, unsigned int value); int sem_wait(sem_t *s); /* P(s) */ int sem_post(sem_t *s); /* V(s) */ 
Returns: 0 if OK, .1 on error 

The sem_init function initializes semaphore sem to value. Each semaphore must be initialized before it can be used. For our purposes, the middle argument is always 0. Programs perform P and V operations by calling the sem_wait and sem_post functions, respectively. For conciseness, we prefer to use the following equivalent P and V wrapper functions instead: 
#include  "csapp.h"  
void void  P(sem_t V(sem_t  *s); *s);  /* /*  Wrapper Wrapper  function function  for for  sem_wait sem_post  */ */ Returns: nothing  


Aside Origin of the names P and V 
Edsger Dijkstra (1930¨C2002) was originally from the Netherlands. The names P and V come from the Dutch words Proberen (to test) and Verhogen (to increment). 
12.5.3 Using Semaphores for Mutual Exclusion 
Semaphores provide a convenient way to ensure mutually exclusive access to shared variables. The basic idea is to associate a semaphore s, initially 1, with each shared variable (or related set of shared variables) and then surround the corresponding critical section with P(s)and V(s)operations. 
A semaphore that is used in this way to protect shared variables is called a binary semaphore because its value is always 0 or 1. Binary semaphores whose purpose is to provide mutual exclusion are often called mutexes. Performing a P operation on a mutex is called locking the mutex. Similarly, performing the V operation is called unlocking the mutex. A thread that has locked but not yet unlocked a mutex is said to be holding the mutex. A semaphore that is used as a counter for a set of available resources is called a counting semaphore. 
The progress graph in Figure 12.22 shows how we would use binary sema-phores to properly synchronize our example counter program. Each state is la-beled with the value of semaphore s in that state. The crucial idea is that this combination of P and V operations creates a collection of states, called a forbid-den region, where s<0. Because of the semaphore invariant, no feasible trajectory can include one of the states in the forbidden region. And since the forbidden re-gion completely encloses the unsafe region, no feasible trajectory can touch any part of the unsafe region. Thus, every feasible trajectory is safe, and regardless of the ordering of the instructions at run time, the program correctly increments the counter. 
In an operational sense, the forbidden region created by the P and V op-erations makes it impossible for multiple threads to be executing instructions in the enclosed critical region at any point in time. In other words, the semaphore operations ensure mutually exclusive access to the critical region. 
Putting it all together, to properly synchronize the example counter program in Figure 12.16 using semaphores, we .rst declare a semaphore called mutex: 
volatile int cnt = 0; /* Counter */ sem_t mutex; /* Semaphore that protects counter */ 
Thread 2 


H1 P(s) L1 U1 S1 V(s) T1 
Figure 12.22 Using semaphores for mutual exclusion. The infeasible states where s<0 de.ne a forbidden region that surrounds the unsafe region and prevents any feasible trajectory from touching the unsafe region. 
and then initialize it to unity in the main routine: 
Sem_init(&mutex, 0, 1); /* mutex=1*/ 
Finally, we protect the update of the shared cnt variable in the thread routine by surrounding it with P and V operations: 
for (i = 0; i < niters; i++) { 
P(&mutex); 
cnt++; 

V(&mutex); 
} 

When we run the properly synchronized program, it now produces the correct answer each time. 
linux> ./goodcnt 1000000 OK cnt=2000000 
linux> ./goodcnt 1000000 OK cnt=2000000 
Aside Limitations of progress graphs 
Progress graphs give us a nice way to visualize concurrent program execution on uniprocessors and to understand why we need synchronization. However, they do have limitations, particularly with respect to concurrent execution on multiprocessors, where a set of CPU/cache pairs share the same main memory. Multiprocessors behave in ways that cannot be explained by progress graphs. In particular, a multiprocessor memory system can be in a state that does not correspond to any trajectory in a progress graph. Regardless, the message remains the same: always synchronize accesses to your shared variables, regardless if you¡¯re running on a uniprocessor or a multiprocessor. 
12.5.4 Using Semaphores to Schedule Shared Resources 
Another important use of semaphores, besides providing mutual exclusion, is to schedule accesses to shared resources. In this scenario, a thread uses a semaphore operation to notify another thread that some condition in the program state has become true. Two classical and useful examples are the producer-consumer and readers-writers problems. 
Producer-Consumer Problem 
The producer-consumer problem is shown in Figure 12.23. A producer and con-sumer thread share a bounded buffer with n slots. The producer thread repeatedly produces new items and inserts them in the buffer. The consumer thread repeat-edly removes items from the buffer and then consumes (uses) them. Variants with multiple producers and consumers are also possible. 
Since inserting and removing items involves updating shared variables, we must guarantee mutually exclusive access to the buffer. But guaranteeing mutual exclusion is not suf.cient. We also need to schedule accesses to the buffer. If the buffer is full (there are no empty slots), then the producer must wait until a slot becomes available. Similarly, if the buffer is empty (there are no available items), then the consumer must wait until an item becomes available. 
Producer-consumer interactions occur frequently in real systems. For exam-ple, in a multimedia system, the producer might encode video frames while the consumer decodes and renders them on the screen. The purpose of the buffer is to reduce jitter in the video stream caused by data-dependent differences in the en-coding and decoding times for individual frames. The buffer provides a reservoir of slots to the producer and a reservoir of encoded frames to the consumer. Another common example is the design of graphical user interfaces. The producer detects 

Figure 12.23 Producer-consumer problem. The producer generates items and inserts them into a bounded buffer. The consumer removes items from the buffer and then consumes them. 
code/conc/sbuf.h 

1 typedef struct { 
2 int *buf; /* Buffer array */ 
3 int n; /* Maximum number of slots */ 
4 int front; /* buf[(front+1)%n] is first item */ 
5 int rear; /* buf[rear%n] is last item */ 
6 sem_t mutex; /* Protects accesses to buf */ 
7 sem_t slots; /* Counts available slots */ 
8 sem_t items; /* Counts available items */ 
9 } sbuf_t; 
code/conc/sbuf.h 

Figure 12.24 sbuf_t: Bounded buffer used by the Sbuf package. 
mouse and keyboard events and inserts them in the buffer. The consumer removes the events from the buffer in some priority-based manner and paints the screen. 
In this section, we will develop a simple package, called Sbuf, for building producer-consumer programs. In the next section, we look at how to use it to build an interesting concurrent server based on prethreading. Sbuf manipulates bounded buffers of type sbuf_t (Figure 12.24). Items are stored in a dynamically allocated integer array (buf) with n items. The front and rear indices keep track of the .rst and last items in the array. Three semaphores synchronize access to the buffer. The mutex semaphore provides mutually exclusive buffer access. Semaphores slots and items are counting semaphores that count the number of empty slots and available items, respectively. 
Figure 12.25 shows the implementation of Sbuf function. The sbuf_init function allocates heap memory for the buffer, sets front and rear to indicate an empty buffer, and assigns initial values to the three semaphores. This function is called once, before calls to any of the other three functions. The sbuf_deinit function frees the buffer storage when the application is through using it. The sbuf_insert function waits for an available slot, locks the mutex, adds the item, unlocks the mutex, and then announces the availability of a new item. The sbuf_ remove function is symmetric. After waiting for an available buffer item, it locks the mutex, removes the item from the front of the buffer, unlocks the mutex, and then signals the availability of a new slot. 
Practice Problem 12.9 
Let p denote the number of producers, c the number of consumers, and n the buffer size in units of items. For each of the following scenarios, indicate whether the mutex semaphore in sbuf_insert and sbuf_remove is necessary or not. 
A. p= 1, c= 1, n>1 
B. p= 1, c= 1, n= 1 
C. p>1, c>1, n= 1 
code/conc/sbuf.c  
1  #include  "csapp.h"  
2  #include  "sbuf.h"  
3  
4  /*  Create  an  empty,  bounded,  shared  FIFO  buffer  with  n  slots  */  
void  sbuf_init(sbuf_t  *sp,  int  n)  
6  {  
7  sp->buf  =  Calloc(n,  sizeof(int));  
8  sp->n  =  n;  /*  Buffer  holds  max  of  n  items  */  
9  sp->front  =  sp->rear  =  0;  /*  Empty  buffer  iff  front  ==  rear  */  
Sem_init(&sp->mutex,  0,  1);  /*  Binary  semaphore  for  locking  */  
11  Sem_init(&sp->slots,  0,  n);  /*  Initially,  buf  has  n  empty  slots  */  
12  Sem_init(&sp->items,  0,  0);  /*  Initially,  buf  has  zero  data  items  */  
13  }  
14  
/*  Clean  up  buffer  sp  */  
16  void  sbuf_deinit(sbuf_t  *sp)  
17  {  
18  Free(sp->buf);  
19  }  
21  /*  Insert  item  onto  the  rear  of  shared  buffer  sp  */  
22  void  sbuf_insert(sbuf_t  *sp,  int  item)  
23  {  
24  P(&sp->slots);  /*  Wait  for  available  slot  */  
P(&sp->mutex);  /*  Lock  the  buffer  */  
26  sp->buf[(++sp->rear)%(sp->n)]  =  item;  /*  Insert  the  item  */  
27  V(&sp->mutex);  /*  Unlock  the  buffer  */  
28  V(&sp->items);  /*  Announce  available  item  */  
29  }  
31  /*  Remove  and  return  the  first  item  from  buffer  sp  */  
32  int  sbuf_remove(sbuf_t  *sp)  
33  {  
34  int item;  
P(&sp->items);  /* Wait for available item */  
36  P(&sp->mutex);  /* Lock the buffer */  
37  item  =  sp->buf[(++sp->front)%(sp->n)];  /*  Remove  the  item  */  
38  V(&sp->mutex);  /*  Unlock  the  buffer  */  
39  V(&sp->slots);  /*  Announce  available  slot  */  
return item;  
41  }  
code/conc/sbuf.c  
Figure 12.25  Sbuf: A package for synchronizing concurrent access to bounded  
buffers.  

Readers-Writers Problem 
The readers-writers problem is a generalization of the mutual exclusion problem. A collection of concurrent threads are accessing a shared object such as a data struc-ture in main memory or a database on disk. Some threads only read the object, while others modify it. Threads that modify the object are called writers. Threads that only read it are called readers. Writers must have exclusive access to the ob-ject, but readers may share the object with an unlimited number of other readers. In general, there are an unbounded number of concurrent readers and writers. 
Readers-writers interactions occur frequently in real systems. For example, in an online airline reservation system, an unlimited number of customers are al-lowed to concurrently inspect the seat assignments, but a customer who is booking a seat must have exclusive access to the database. As another example, in a mul-tithreaded caching Web proxy, an unlimited number of threads can fetch existing pages from the shared page cache, but any thread that writes a new page to the cache must have exclusive access. 
The readers-writers problem has several variations, each based on the priori-ties of readers and writers. The .rst readers-writers problem, which favors readers, requires that no reader be kept waiting unless a writer has already been granted permission to use the object. In other words, no reader should wait simply because a writer is waiting. The second readers-writers problem, which favors writers, re-quires that once a writer is ready to write, it performs its write as soon as possible. Unlike the .rst problem, a reader that arrives after a writer must wait, even if the writer is also waiting. 
Figure 12.26 shows a solution to the .rst readers-writers problem. Like the solutions to many synchronization problems, it is subtle and deceptively simple. The w semaphore controls access to the critical sections that access the shared object. The mutex semaphore protects access to the shared readcnt variable, which counts the number of readers currently in the critical section. A writer locks the w mutex each time it enters the critical section, and unlocks it each time it leaves. This guarantees that there is at most one writer in the critical section at any point in time. On the other hand, only the .rst reader to enter the critical section locks w, and only the last reader to leave the critical section unlocks it. The w mutex is ignored by readers who enter and leave while other readers are present. This means that as long as a single reader holds the w mutex, an unbounded number of readers can enter the critical section unimpeded. 
A correct solution to either of the readers-writers problems can result in starvation, where a thread blocks inde.nitely and fails to make progress. For example, in the solution in Figure 12.26, a writer could wait inde.nitely while a stream of readers arrived. 
Practice Problem 12.10 
The solution to the .rst readers-writers problem in Figure 12.26 gives priority to readers, but this priority is weak in the sense that a writer leaving its critical section might restart a waiting writer instead of a waiting reader. Describe a scenario where this weak priority would allow a collection of writers to starve a reader. 
/* Global variables */ int readcnt; /* Initially=0*/ sem_t mutex, w; /* Both initially=1*/ 
void reader(void) void writer(void) {{ 
while (1) { while (1) { P(&mutex); P(&w); readcnt++; if (readcnt == 1) /* First in */ /* Critical section */ 
P(&w); /* Writing happens */ V(&mutex); 
V(&w); /* Critical section */ } /* Reading happens */ } 
P(&mutex); readcnt--; if (readcnt == 0) /* Last out */ 

V(&w); V(&mutex); } } 
Figure 12.26 Solution to the .rst readers-writers problem. Favors readers over writers. 

Aside Other synchronization mechanisms 
We have shown you how to synchronize threads using semaphores, mainly because they are simple, clas-sical, and have a clean semantic model. But you should know that other synchronization techniques exist as well. For example, Java threads are synchronized with a mechanism called a Java monitor [51], which provides a higher level abstraction of the mutual exclusion and scheduling capabilities of semaphores; in fact monitors can be implemented with semaphores. As another example, the Pthreads interface de-.nes a set of synchronization operations on mutex and condition variables. Pthreads mutexes are used for mutual exclusion. Condition variables are used for scheduling accesses to shared resources, such as the bounded buffer in a producer-consumer program. 
12.5.5 Putting It Together: A Concurrent Server Based on Prethreading 
We have seen how semaphores can be used to access shared variables and to schedule accesses to shared resources. To help you understand these ideas more clearly, let us apply them to a concurrent server based on a technique called prethreading. 


Figure 12.27 Organization of a prethreaded concurrent server. A set of existing threads repeatedly remove and process connected descriptors from a bounded buffer. 
In the concurrent server in Figure 12.14, we created a new thread for each new client. A disadvantage of this approach is that we incur the nontrivial cost of creating a new thread for each new client. A server based on prethreading tries to reduce this overhead by using the producer-consumer model shown in Figure 12.27. The server consists of a main thread and a set of worker threads. The main thread repeatedly accepts connection requests from clients and places the resulting connected descriptors in a bounded buffer. Each worker thread repeatedly removes a descriptor from the buffer, services the client, and then waits for the next descriptor. 
Figure 12.28 shows how we would use the Sbuf package to implement a prethreaded concurrent echo server. After initializing buffer sbuf (line 23), the main thread creates the set of worker threads (lines 26¨C27). Then it enters the in.nite server loop, accepting connection requests and inserting the resulting connected descriptors in sbuf. Each worker thread has a very simple behavior. It waits until it is able to remove a connected descriptor from the buffer (line 39), and then calls the echo_cnt function to echo client input. 
The echo_cnt function in Figure 12.29 is a version of the echo function from Figure 11.21 that records the cumulative number of bytes received from all clients in a global variable called byte_cnt. This is interesting code to study because it shows you a general technique for initializing packages that are called from thread routines. In our case, we need to initialize the byte_cnt counter and the mutex semaphore. One approach, which we used for the Sbuf and Rio packages, is to require the main thread to explicitly call an initialization function. Another approach, shown here, uses the pthread_once function (line 19) to call the initialization function the .rst time some thread calls the echo_cnt function. The advantage of this approach is that it makes the package easier to use. The disadvantage is that every call to echo_cnt makes a call to pthread_once, which most times does nothing useful. 
Once the package is initialized, the echo_cnt function initializes the Rio buffered I/O package (line 20) and then echoes each text line that is received from the client. Notice that the accesses to the shared byte_cnt variable in lines 23¨C25 are protected by P and V operations. 
code/conc/echoservert_pre.c  
1  #include "csapp.h"  
2  #include "sbuf.h"  
3  #define NTHREADS 4  
4  #define SBUFSIZE 16  
6  void echo_cnt(int connfd);  
7  void *thread(void *vargp);  
8  
9  sbuf_t sbuf; /* Shared buffer of connected descriptors */  
11  int main(int argc, char **argv)  
12  {  
13  int i, listenfd, connfd, port;  
14  socklen_t clientlen=sizeof(struct sockaddr_in);  
struct sockaddr_in clientaddr;  
16  pthread_t tid;  
17  
18  if (argc != 2) {  
19  fprintf(stderr, "usage: %s <port>\n", argv[0]);  
exit(0);  
21  }  
22  port = atoi(argv[1]);  
23  sbuf_init(&sbuf, SBUFSIZE);  
24  listenfd = Open_listenfd(port);  
26  for (i = 0; i < NTHREADS; i++) /* Create worker threads */  
27  Pthread_create(&tid, NULL, thread, NULL);  
28  
29  while (1) {  
connfd = Accept(listenfd, (SA *) &clientaddr, &clientlen);  
31  sbuf_insert(&sbuf, connfd); /* Insert connfd in buffer */  
32  }  
33  }  
34  
void *thread(void *vargp)  
36  {  
37  Pthread_detach(pthread_self());  
38  while (1) {  
39  int connfd = sbuf_remove(&sbuf); /* Remove connfd from buffer */  
echo_cnt(connfd); /* Service client */  
41  Close(connfd);  
42  }  
43  }  
code/conc/echoservert_pre.c  
Figure 12.28 A prethreaded concurrent echo server. The server uses a producer- 
consumer model with one producer and multiple consumers.  

code/conc/echo_cnt.c 

Section 12.5 Synchronizing Threads with Semaphores 973 

1  #include  "csapp.h"  
2  
3  static  int  byte_cnt;  /*  Byte  counter  */  
4  static  sem_t  mutex;  /*  and  the  mutex  that  protects  it  */  
5  
6  static  void  init_echo_cnt(void)  
7  {  
8  Sem_init(&mutex,  0,  1);  
9  byte_cnt = 0;  
10  }  
11  
12  void  echo_cnt(int  connfd)  
13  {  
14  int  n;  
15  char  buf[MAXLINE];  
16  rio_t  rio;  
17  static  pthread_once_t  once  =  PTHREAD_ONCE_INIT;  
18  
19  Pthread_once(&once,  init_echo_cnt);  
20  Rio_readinitb(&rio,  connfd);  
21  while((n  =  Rio_readlineb(&rio,  buf,  MAXLINE))  !=  0)  {  
22  P(&mutex);  
23  byte_cnt += n;  
24  printf("thread %d received %d (%d total) bytes on fd %d\n",  
25  (int) pthread_self(), n, byte_cnt, connfd);  
26  V(&mutex);  
27  Rio_writen(connfd, buf, n);  
28  }  
29  }  
code/conc/echo_cnt.c  

Figure 12.29 echo_cnt: A version of echo that counts all bytes received from clients. 
Aside Event-driven programs based on threads 
I/O multiplexing is not the only way to write an event-driven program. For example, you might have noticed that the concurrent prethreaded server that we just developed is really an event-driven server with simple state machines for the main and worker threads. The main thread has two states (¡°waiting for connection request¡± and ¡°waiting for available buffer slot¡±), two I/O events (¡°connection request arrives¡± and ¡°buffer slot becomes available¡±), and two transitions (¡°accept connection request¡± and ¡°insert buffer item¡±). Similarly, each worker thread has one state (¡°waiting for available buffer item¡±), one I/O event (¡°buffer item becomes available¡±), and one transition (¡°remove buffer item¡±). 
12.6 
Using 
Threads 
for 
Parallelism 

Thus far in our study of concurrency, we have assumed concurrent threads execut-ing on uniprocessor systems. However, many modern machines have multi-core processors. Concurrent programs often run faster on such machines because the operating system kernel schedules the concurrent threads in parallel on multi-ple cores, rather than sequentially on a single core. Exploiting such parallelism is critically important in applications such as busy Web servers, database servers, and large scienti.c codes, and it is becoming increasingly useful in mainstream applications such as Web browsers, spreadsheets, and document processors. 
Figure 12.30 shows the set relationships between sequential, concurrent, and parallel programs. The set of all programs can be partitioned into the disjoint sets of sequential and concurrent programs. A sequential program is written as a single logical .ow. A concurrent program is written as multiple concurrent .ows. A parallel program is a concurrent program running on multiple processors. Thus, the set of parallel programs is a proper subset of the set of concurrent programs. 
A detailed treatment of parallel programs is beyond our scope, but studying a very simple example program will help you understand some important aspects of parallel programming. For example, consider how we might sum the sequence of integers 0,...,n. 1 in parallel. Of course, there is a closed-form solution for this particular problem, but nonetheless it is a concise and easy-to-understand exem-plar that will allow us to make some interesting points about parallel programs. 
The most straightforward approach is to partition the sequence into tdisjoint regions, and then assign each of t different threads to work on its own region. For simplicity, assume that nis a multiple of t, such that each region has n/telements. The main thread creates t peer threads, where each peer thread kruns in parallel on its own processor core and computes sk, which is the sum of the elements in region k. Once the peer threads have completed, the main thread computes the .nal result by summing each sk. 
Figure 12.31 shows how we might implement this simple parallel sum algo-rithm. In lines 27¨C32, the main thread creates the peer threads and then waits for them to terminate. Notice that the main thread passes a small integer to each peer thread that serves as a unique thread ID. Each peer thread will use its thread ID to determine which portion of the sequence it should work on. This idea of passing a small unique thread ID to the peer threads is a general technique that is used in many parallel applications. After the peer threads have terminated, the psum vec-tor contains the partial sums computed by each peer thread. The main thread then 
Figure 12.30 

Relationships between the sets of sequential, concurrent, and parallel programs. 

code/conc/psum.c 

1 #include "csapp.h" 2 #define MAXTHREADS 32 3 4 void *sum(void *vargp); 
6 /* Global shared variables */ 7 long psum[MAXTHREADS]; /* Partial sum computed by each thread */ 8 long nelems_per_thread; /* Number of elements summed by each thread */ 9 
int main(int argc, char **argv) 
11 { 

12 long i, nelems, log_nelems, nthreads, result = 0; 
13 pthread_t tid[MAXTHREADS]; 
14 int myid[MAXTHREADS]; 
16 /* Get input arguments */ 
17 if (argc != 3) { 
18 printf("Usage: %s <nthreads> <log_nelems>\n", argv[0]); 
19 exit(0); } 
21 nthreads = atoi(argv[1]); 
22 log_nelems = atoi(argv[2]); 
23 nelems = (1L << log_nelems); 
24 nelems_per_thread = nelems / nthreads; 
26 /* Create peer threads and wait for them to finish */ 
27 for (i = 0; i < nthreads; i++) { 
28 myid[i] = i; 
29 Pthread_create(&tid[i], NULL, sum, &myid[i]); } 
31 for (i = 0; i < nthreads; i++) 
32 Pthread_join(tid[i], NULL); 
33 

34 /* Add up the partial sums computed by each thread */ for (i = 0; i < nthreads; i++) 
36 result += psum[i]; 
37 

38 /* Check final answer */ 
39 if (result != (nelems * (nelems-1))/2) printf("Error: result=%ld\n", result); 
41 
42 exit(0); 
43 } 
code/conc/psum.c 

Figure 12.31 Simple parallel program that uses multiple threads to sum the elements of a sequence. 
976  Chapter 12  Concurrent Programming  
code/conc/psum.c  
1  void  *sum(void  *vargp)  
2  {  
3  int  myid  =  *((int  *)vargp);  /*  Extract  the  thread  ID  */  
4  long  start  =  myid  * nelems_per_thread;  /*  Start  element  index  */  
5  long  end  =  start  + nelems_per_thread;  /*  End  element  index  */  
6  long  i,  sum  =  0;  
7  
8  for  (i  =  start;  i  <  end;  i++)  {  
9  sum  +=  i;  
10  }  
11  psum[myid]  =  sum;  
12  
13  return  NULL;  
14  }  
code/conc/psum.c  
Figure 12.32  Thread routine for the program in Figure 12.31.  
sums up the elements of the psum vector (lines 35¨C36), and uses the closed-form  
solution to verify the result (lines 39¨C40).  
Figure 12.32 shows the function that each peer thread executes. In line 3,  
the thread extracts the thread ID from the thread argument, and then uses this  
ID to determine the region of the sequence it should work on (lines 4¨C5). In  
lines 8¨C10, the thread operates on its portion of the sequence, and then updates  
its entry in the partial sum vector (line 11). Notice that we are careful to give each  
peer thread a unique memory location to update, and thus it is not necessary to  
synchronize access to the psum array with semaphore mutexes. The only necessary  
synchronization in this particular case is that the main thread must wait for each  
of the children to .nish so that it knows that each entry in psum is valid.  
Figure 12.33 shows the total elapsed running time of the program in Fig- 
ure 12.31 as a function of the number of threads. In each case, the program runs  
on a system with four processor cores and sums a sequence of n= 231 elements.  
We see that running time decreases as we increase the number of threads, up to  
four threads, at which point it levels off and even starts to increase a little. In the  
ideal case, we would expect the running time to decrease linearly with the num- 
ber of cores. That is, we would expect running time to drop by half each time we  
double the number of threads. This is indeed the case until we reach the point  
(t>4) where each of the four cores is busy running at least one thread. Running  
time actually increases a bit as we increase the number of threads because of the  
overhead of context switching multiple threads on the same core. For this reason,  
parallel programs are often written so that each core runs exactly one thread.  
Although absolute running time is the ultimate measure of any program¡¯s  
performance, there are some useful relative measures, known as speedup and  
ef.ciency, that can provide insight into how well a parallel program is exploiting  

Figure 12.33 1.8 
Performance of the program in Figure 12.31 1.6 on a multi-core machine 
1.4 

with four cores. Summing a sequence of 231 elements. 1.2 
1.0 
0.8 
0.6 
0.4 
0.2 
0 1 24816 
Threads 

potential parallelism. The speedup of a parallel program is typically de.ned as 
T1
S = 
p 
T
p 

where p is the number of processor cores and Tk is the running time on k cores. This formulation is sometimes referred to as strong scaling. When T1 is the execution time of a sequential version of the program, then S is called the absolute speedup.
p 

When T1 is the execution time of the parallel version of the program running on one core, then S is called the relative speedup. Absolute speedup is a truer mea-
p 

sure of the bene.ts of parallelism than relative speedup. Parallel programs often suffer from synchronization overheads, even when they run on one processor, and these overheads can arti.cially in.ate the relative speedup numbers because they increase the size of the numerator. On the other hand, absolute speedup is more dif.cult to measure than relative speedup because measuring absolute speedup requires two different versions of the program. For complex parallel codes, creat-ing a separate sequential version might not be feasible, either because the code is too complex or the source code is not available. 
A related measure, known as ef.ciency, is de.ned as 
Elapsed time (s) 

S
pT1
E == 
p 
p pTp 

and is typically reported as a percentage in the range (0, 100]. Ef.ciency is a mea-sure of the overhead due to parallelization. Programs with high ef.ciency are spending more time doing useful work and less time synchronizing and commu-nicating than programs with low ef.ciency. 
Threads (t)  1  2  4  8  16  
Cores (p)  1  2  4  4  4  
Running time (Tp)  1.56  0.81  0.40  0.40  0.45  
Speedup (Sp)  1  1.9  3.9  3.9  3.5  
Ef.ciency (Ep)  100%  95%  98%  98%  88%  
Figure 12.34 Speedup and parallel ef.ciency for the execution times in Figure 12.33. 


Figure 12.34 shows the different speedup and ef.ciency measures for our example parallel sum program. Ef.ciencies over 90% such as these are very good, but do not be fooled. We were able to achieve high ef.ciency because our problem was trivially easy to parallelize. In practice, this is not usually the case. Parallel programming has been an active area of research for decades. With the advent of commodity multi-core machines whose core count is doubling every few years, parallel programming continues to be a deep, dif.cult, and active area of research. 
There is another view of speedup, known as weak scaling, which increases the problem size along with the number of processors, such that the amount of work performed on each processor is held constant as the number of processors increases. With this formulation, speedup and ef.ciency are expressed in terms of the total amount of work accomplished per unit time. For example, if we can double the number of processors and do twice the amount of work per hour, then we are enjoying linear speedup and 100% ef.ciency. 
Weak scaling is often a truer measure than strong scaling because it more accurately re.ects our desire to use bigger machines to do more work. This is par-ticularly true for scienti.c codes, where the problem size can be easily increased, and where bigger problem sizes translate directly to better predictions of nature. However, there exist applications whose sizes are not so easily increased, and for these applications strong scaling is more appropriate. For example, the amount of work performed by real-time signal processing applications is often determined by the properties of the physical sensors that are generating the signals. Changing the total amount of work requires using different physical sensors, which might not be feasible or necessary. For these applications, we typically want to use parallelism to accomplish a .xed amount of work as quickly as possible. 
Practice Problem 12.11 
Fill in the blanks for the parallel program in the following table. Assume strong 
scaling.  
Threads (t) Cores (p)  1 1  2 2  4 4  
Running time (Tp) Speedup (Sp) Ef.ciency (Ep)  12 100%  8 1.5  6 50%  


12.7 
Other 
Concurrency 
Issues 

You probably noticed that life got much more complicated once we were asked to synchronize accesses to shared data. So far, we have looked at techniques for mutual exclusion and producer-consumer synchronization, but this is only the tip of the iceberg. Synchronization is a fundamentally dif.cult problem that raises issues that simply do not arise in ordinary sequential programs. This section is a survey (by no means complete) of some of the issues you need to be aware of when you write concurrent programs. To keep things concrete, we will couch our discussion in terms of threads. Keep in mind, however, that these are typical of the issues that arise when concurrent .ows of any kind manipulate shared resources. 
12.7.1 Thread Safety 
When we program with threads, we must be careful to write functions that have a property called thread safety. A function is said to be thread-safe if and only if it will always produce correct results when called repeatedly from multiple concurrent threads. If a function is not thread-safe, then we say it is thread-unsafe. 
We can identify four (nondisjoint) classes of thread-unsafe functions: 
. Class 1: Functions that do not protect shared variables. We have already en-countered this problem with the thread function in Figure 12.16, which in-crements an unprotected global counter variable. This class of thread-unsafe function is relatively easy to make thread-safe: protect the shared variables with synchronization operations such as P and V . An advantage is that it does not require any changes in the calling program. A disadvantage is that the synchronization operations will slow down the function. 
. Class 2: Functions that keep state across multiple invocations. A pseudo-random number generator is a simple example of this class of thread-unsafe function. Consider the pseudo-random number generator package in Fig-ure 12.35. The rand function is thread-unsafe because the result of the current invocation depends on an intermediate result from the previous iteration. When we call rand repeatedly from a single thread after seeding it with a call to srand, we can expect a repeatable sequence of numbers. However, this assumption no longer holds if multiple threads are calling rand. 
The only way to make a function such as rand thread-safe is to rewrite it so that it does not use any static data, relying instead on the caller to pass the state information in arguments. The disadvantage is that the programmer is now forced to change the code in the calling routine as well. In a large program where there are potentially hundreds of different call sites, making such modi.cations could be nontrivial and prone to error. 
. Class 3: Functions that return a pointer to a static variable.Some functions, such as ctime and gethostbyname, compute a result in a static variable and then return a pointer to that variable. If we call such functions from concurrent threads, then disaster is likely, as results being used by one thread are silently overwritten by another thread. 
code/conc/rand.c 
1 unsigned int next = 1; 2 3 /* rand -return pseudo-random integer on 0..32767 */ 4 int rand(void) 5 { 6 next = next*1103515245 + 12345; 7 return (unsigned int)(next/65536) % 32768; 
8 } 
9 10 /* srand -set seed for rand() */ 11 void srand(unsigned int seed) 12 { 13 next = seed; 
14 } 
code/conc/rand.c 
Figure 12.35 A thread-unsafe pseudo-random number generator [58]. 
There are two ways to deal with this class of thread-unsafe functions. One option is to rewrite the function so that the caller passes the address of the variable in which to store the results. This eliminates all shared data, but it requires the programmer to have access to the function source code. 
If the thread-unsafe function is dif.cult or impossible to modify (e.g., the code is very complex or there is no source code available), then another option is to use the lock-and-copy technique. The basic idea is to associate a mutex with the thread-unsafe function. At each call site, lock the mutex, call the thread-unsafe function, copy the result returned by the function to a private memory location, and then unlock the mutex. To minimize changes to the caller, you should de.ne a thread-safe wrapper function that performs the lock-and-copy, and then replace all calls to the thread-unsafe function with calls to the wrapper. For example, Figure 12.36 shows a thread-safe wrapper for ctime that uses the lock-and-copy technique. 
. Class 4: Functions that call thread-unsafe functions. If a function f calls a thread-unsafe function g,is f thread-unsafe? It depends. If g is a class 2 function that relies on state across multiple invocations, then f is also thread-unsafe and there is no recourse short of rewriting g. However, if g is a class 1 or class 3 function, then f can still be thread-safe if you protect the call site and any resulting shared data with a mutex. We see a good example of this in Figure 12.36, where we use lock-and-copy to write a thread-safe function that calls a thread-unsafe function. 
12.7.2 Reentrancy 
There is an important class of thread-safe functions, known as reentrant functions, that are characterized by the property that they do not reference any shared data 
code/conc/ctime_ts.c 

1 char *ctime_ts(const time_t *timep, char *privatep) 2 { 3 char *sharedp; 4 5 P(&mutex); 6 sharedp = ctime(timep); 7 strcpy(privatep, sharedp); /* Copy string from shared to private */ 8 V(&mutex); 9 return privatep; 
10 } 
code/conc/ctime_ts.c 

Figure 12.36 Thread-safe wrapper function for the C standard library ctime function. Uses the lock-and-copy technique to call a class 3 thread-unsafe function. 
Relationships between the sets of reentrant, thread-safe, and non-thread-safe functions. 

when they are called by multiple threads. Although the terms thread-safe and reentrant are sometimes used (incorrectly) as synonyms, there is a clear technical distinction that is worth preserving. Figure 12.37 shows the set relationships be-tween reentrant, thread-safe, and thread-unsafe functions. The set of all functions is partitioned into the disjoint sets of thread-safe and thread-unsafe functions. The set of reentrant functions is a proper subset of the thread-safe functions. 
Reentrant functions are typically more ef.cient than nonreentrant thread-safe functions because they require no synchronization operations. Furthermore, the only way to convert a class 2 thread-unsafe function into a thread-safe one is to rewrite it so that it is reentrant. For example, Figure 12.38 shows a reentrant version of the rand function from Figure 12.35. The key idea is that we have replaced the static next variable with a pointer that is passed in by the caller. 
Is it possible to inspect the code of some function and declare a priori that it is reentrant? Unfortunately, it depends. If all function arguments are passed by value (i.e., no pointers) and all data references are to local automatic stack variables (i.e., no references to static or global variables), then the function is explicitly reentrant, in the sense that we can assert its reentrancy regardless of how it is called. 
However, if we loosen our assumptions a bit and allow some parameters in our otherwise explicitly reentrant function to be passed by reference (that is, we allow them to pass pointers) then we have an implicitly reentrant function, in the sense that it is only reentrant if the calling threads are careful to pass pointers 
code/conc/rand_r.c 
1 /* rand_r -a reentrant pseudo-random integer on 0..32767 */ 2 int rand_r(unsigned int *nextp) 3 { 4 *nextp = *nextp * 1103515245 + 12345; 5 return (unsigned int)(*nextp / 65536) % 32768; 
6 } 
code/conc/rand_r.c 
Figure 12.38 rand_r: A reentrant version of the rand function from Figure 12.35. 
to nonshared data. For example, the rand_r function in Figure 12.38 is implicitly reentrant. 
We always use the term reentrant to include both explicit and implicit reen-trant functions. However, it is important to realize that reentrancy is sometimes a property of both the caller and the callee, and not just the callee alone. 
Practice Problem 12.12 
The ctime_ts function in Figure 12.36 is thread-safe, but not reentrant. Explain. 
12.7.3 Using Existing Library Functions in Threaded Programs 
Most Unix functions, including the functions de.ned in the standard C library (such as malloc, free, realloc, printf, and scanf), are thread-safe, with only a few exceptions. Figure 12.39 lists the common exceptions. (See [109] for a com-plete list.) The asctime, ctime, and localtime functions are popular functions for converting back and forth between different time and date formats. The gethost-byname, gethostbyaddr, and inet_ntoa functions are frequently used network programming functions that we encountered in Chapter 11. The strtok function is a deprecated function (one whose use is discouraged) for parsing strings. 
With the exceptions of rand and strtok, all of these thread-unsafe functions are of the class 3 variety that return a pointer to a static variable. If we need to call one of these functions in a threaded program, the least disruptive approach to the caller is to lock-and-copy. However, the lock-and-copy approach has a number of disadvantages. First, the additional synchronization slows down the program. Second, functions such as gethostbyname that return pointers to complex struc-tures of structures require a deep copy of the structures in order to copy the entire structure hierarchy. Third, the lock-and-copy approach will not work for a class 2 thread-unsafe function such as rand that relies on static state across calls. 
Therefore, Unix systems provide reentrant versions of most thread-unsafe functions. The names of the reentrant versions always end with the ¡°_r¡± suf.x. For example, the reentrant version of gethostbyname is called gethostbyname_r. We recommend using these functions whenever possible. 

Section 12.7 Other Concurrency Issues 983 

Thread-unsafe function  Thread-unsafe class  Unix thread-safe version  
rand  2  rand_r  
strtok  2  strtok_r  
asctime  3  asctime_r  
ctime  3  ctime_r  
gethostbyaddr  3  gethostbyaddr_r  
gethostbyname  3  gethostbyname_r  
inet_ntoa  3  (none)  
localtime  3  localtime_r  

Figure 12.39 Common thread-unsafe library functions. 
12.7.4 Races 

A race occurs when the correctness of a program depends on one thread reaching point x in its control .ow before another thread reaches point y. Races usually occur because programmers assume that threads will take some particular trajec-tory through the execution state space, forgetting the golden rule that threaded programs must work correctly for any feasible trajectory. 
An example is the easiest way to understand the nature of races. Consider the simple program in Figure 12.40. The main thread creates four peer threads and passes a pointer to a unique integer ID to each one. Each peer thread copies the ID passed in its argument to a local variable (line 21), and then prints a message containing the ID. It looks simple enough, but when we run this program on our system, we get the following incorrect result: 
unix> ./race Hello from thread 1 Hello from thread 3 Hello from thread 2 Hello from thread 3 
The problem is caused by a race between each peer thread and the main thread. Can you spot the race? Here is what happens. When the main thread cre-ates a peer thread in line 12, it passes a pointer to the local stack variable i. At this point, the race is on between the next call to pthread_create in line 12 and the dereferencing and assignment of the argument in line 21. If the peer thread exe-cutes line 21 before the main thread executes line 12, then the myid variable gets the correct ID. Otherwise, it will contain the ID of some other thread. The scary thing is that whether we get the correct answer depends on how the kernel sched-ules the execution of the threads. On our system it fails, but on other systems it might work correctly, leaving the programmer blissfully unaware of a serious bug. 
To eliminate the race, we can dynamically allocate a separate block for each integer ID, and pass the thread routine a pointer to this block, as shown in When we run this program on our system, we now get the correct result: 
code/conc/race.c  
1  #include  "csapp.h"  
2  #define  N  4  
3  
4  void  *thread(void  *vargp);  
5  
6  int  main()  
7  {  
8  pthread_t tid[N];  
9  int i;  
10  
11  for  (i=0;i<N;  i++)  
12  Pthread_create(&tid[i],  NULL,  thread,  &i);  
13  for  (i=0;i<N;  i++)  
14  Pthread_join(tid[i],  NULL);  
15  exit(0);  
16  }  
17  
18  /*  Thread  routine  */  
19  void  *thread(void  *vargp)  
20  {  
21  int  myid  =  *((int  *)vargp);  
22  printf("Hello  from  thread  %d\n",  myid);  
23  return NULL;  
24  }  
code/conc/race.c  
Figure 12.40  A program with a race.  
Figure 12.41 (lines 12¨C14). Notice that the thread routine must free the block in order to avoid a memory leak. 


unix> ./norace Hello from thread 0 Hello from thread 1 Hello from thread 2 Hello from thread 3 
Practice Problem 12.13 
In Figure 12.41, we might be tempted to free the allocated memory block immedi-ately after line 15 in the main thread, instead of freeing it in the peer thread. But this would be a bad idea. Why? 
code/conc/norace.c 

1 #include "csapp.h" 2 #define N 4 3 4 void *thread(void *vargp); 5 6 int main() 7 { 8 pthread_t tid[N]; 9 int i, *ptr; 
10 

11 for (i=0;i<N; i++) { 
12 ptr = Malloc(sizeof(int)); 
13 *ptr = i; 
14 Pthread_create(&tid[i], NULL, thread, ptr); 
15 
} 16 for (i=0;i<N; i++) 17 Pthread_join(tid[i], NULL); 18 exit(0); 

19 
} 20 21 /* Thread routine */ 22 void *thread(void *vargp) 23 { 24 int myid = *((int *)vargp); 25 Free(vargp); 26 printf("Hello from thread %d\n", myid); 27 return NULL; 


28 } 
code/conc/norace.c 

Figure 12.41 A correct version of the program in Figure 12.40 without a race. 
Practice Problem 12.14 
A. In Figure 12.41, we eliminated the race by allocating a separate block for each integer ID. Outline a different approach that does not call the malloc or free functions. 
B. What are the advantages and disadvantages of this approach? 
12.7.5 Deadlocks 
Semaphores introduce the potential for a nasty kind of run-time error, called deadlock, where a collection of threads are blocked, waiting for a condition that will never be true. The progress graph is an invaluable tool for understanding deadlock. For example, Figure 12.42 shows the progress graph for a pair of threads that use two semaphores for mutual exclusion. From this graph, we can glean some important insights about deadlock: 

.  The programmer has incorrectly ordered the P and V operations such that  
the forbidden regions for the two semaphores overlap. If some execution  
trajectory happens to reach the deadlock state d, then no further progress is  
possible because the overlapping forbidden regions block progress in every  
legal direction. In other words, the program is deadlocked because each  
thread is waiting for the other to do a V operation that will never occur.  
.  The overlapping forbidden regions induce a set of states called the deadlock  
region. If a trajectory happens to touch a state in the deadlock region, then  
deadlock is inevitable. Trajectories can enter deadlock regions, but they can  
never leave.  
.  Deadlock is an especially dif.cult issue because it is not always predictable.  
Some lucky execution trajectories will skirt the deadlock region, while others  
will be trapped by it. Figure 12.42 shows an example of each. The implications  
for a programmer are scary. You might run the same program 1000 times  

Thread 2 


Figure 12.43 Progress graph for a deadlock-free program. 
without any problem, but then the next time it deadlocks. Or the program might work .ne on one machine but deadlock on another. Worst of all, the error is often not repeatable because different executions have different trajectories. 
Programs deadlock for many reasons and avoiding them is a dif.cult problem in general. However, when binary semaphores are used for mutual exclusion, as in Figure 12.42, then you can apply the following simple and effective rule to avoid deadlocks: 
Mutex lock ordering rule: A program is deadlock-free if, for each pair of mutexes (s, t) in the program, each thread that holds both s and t simultaneously locks them in the same order. 
For example, we can .x the deadlock in Figure 12.42 by locking s .rst, then t in each thread. Figure 12.43 shows the resulting progress graph. 
Practice Problem 12.15 
Consider the following program, which attempts to use a pair of semaphores for mutual exclusion. 
Initially:s=1,t=0. 
Thread 1: Thread 2: P(s); P(s); V(s); V(s); P(t); P(t); V(t); V(t); 
A. Draw the progress graph for this program. 
B. Does it always deadlock? 
C. If so, what simple change to the initial semaphore values will eliminate the potential for deadlock? 
D. Draw the progress graph for the resulting deadlock-free program. 
12.8 
Summary 

A concurrent program consists of a collection of logical .ows that overlap in time. In this chapter, we have studied three different mechanisms for building concur-rent programs: processes, I/O multiplexing, and threads. We used a concurrent network server as the motivating application throughout. 
Processes are scheduled automatically by the kernel, and because of their separate virtual address spaces, they require explicit IPC mechanisms in order to share data. Event-driven programs create their own concurrent logical .ows, which are modeled as state machines, and use I/O multiplexing to explicitly sched-ule the .ows. Because the program runs in a single process, sharing data between .ows is fast and easy. Threads are a hybrid of these approaches. Like .ows based on processes, threads are scheduled automatically by the kernel. Like .ows based on I/O multiplexing, threads run in the context of a single process, and thus can share data quickly and easily. 
Regardless of the concurrency mechanism, synchronizing concurrent accesses to shared data is a dif.cult problem. The P and V operations on semaphores have been developed to help deal with this problem. Semaphore operations can be used to provide mutually exclusive access to shared data, as well as to schedule access to resources such as the bounded buffers in producer-consumer systems and shared objects in readers-writers systems. A concurrent prethreaded echo server provides a compelling example of these usage scenarios for semaphores. 
Concurrency introduces other dif.cult issues as well. Functions that are called by threads must have a property known as thread safety. We have identi.ed four classes of thread-unsafe functions, along with suggestions for making them thread-safe. Reentrant functions are the proper subset of thread-safe functions that do not access any shared data. Reentrant functions are often more ef.cient than nonreentrant functions because they do not require any synchronization primitives. Some other dif.cult issues that arise in concurrent programs are races and deadlocks. Races occur when programmers make incorrect assumptions about how logical .ows are scheduled. Deadlocks occur when a .ow is waiting for an event that will never happen. 

Bibliographic 
Notes 

Semaphore operations were introduced by Dijkstra [37]. The progress graph concept was introduced by Coffman [24] and later formalized by Carson and Reynolds [17]. The readers-writers problem was introduced by Courtois et al. [31]. Operating systems texts describe classical synchronization problems such as the dining philosophers, sleeping barber, and cigarette smokers problems in more de-tail [98, 104, 112]. The book by Butenhof [16] is a comprehensive description of the Posix threads interface. The paper by Birrell [7] is an excellent introduction to threads programming and its pitfalls. The book by Reinders [86] describes a C/C++ library that simpli.es the design and implementation of threaded programs. Sev-eral texts cover the fundamentals of parallel programming on multi-core sys-tems [50, 67]. Pugh identi.es weaknesses with the way that Java threads interact through memory and proposes replacement memory models [84]. Gustafson pro-posed the weak scaling speedup model [46] as an alternative to strong scaling. 
Homework 
Problems 

12.16 ¡ô 

Write a version of hello.c (Figure 12.13) that creates and reaps njoinable peer threads, where nis a command line argument. 
12.17 ¡ô 

A. The program in Figure 12.44 has a bug. The thread is supposed to sleep for 1 second and then print a string. However, when we run it on our system, nothing prints. Why? 
B. You can .x this bug by replacing the exit function in line 9 with one of two different Pthreads function calls. Which ones? 
12.18 ¡ô 

Using the progress graph in Figure 12.21, classify the following trajectories as either safe or unsafe. 
A. H2,L2,U2,H1,L1,S2,U1,S1,T1,T2 
B. H2,H1,L1,U1,S1,L2,T1,U2,S2,T2 
C. H1,L1,H2,L2,U2,S2,U1,S1,T1,T2 
12.19 ¡ô¡ô 

The solution to the .rst readers-writers problem in Figure 12.26 gives a somewhat weak priority to readers because a writer leaving its critical section might restart a waiting writer instead of a waiting reader. Derive a solution that gives stronger priority to readers, where a writer leaving its critical section will always restart a waiting reader if one exists. 
code/conc/hellobug.c  
1  #include "csapp.h"  
2  void *thread(void *vargp);  
3  
4  int main()  
5  {  
6  pthread_t tid;  
7  
8  Pthread_create(&tid, NULL,  thread, NULL);  
9  exit(0);  
10  }  
11  
12  /* Thread routine */  
13  void *thread(void *vargp)  
14  {  
15  Sleep(1);  
16  printf("Hello, world!\n");  
17  return NULL;  
18  }  
code/conc/hellobug.c  
Figure 12.44 Buggy program for Problem 12.17.  

12.20 ¡ô¡ô¡ô 
Consider a simpler variant of the readers-writers problem where there are at most N readers. Derive a solution that gives equal priority to readers and writers, in the sense that pending readers and writers have an equal chance of being granted access to the resource. Hint: You can solve this problem using a single counting semaphore and a single mutex. 
12.21 ¡ô¡ô¡ô¡ô 
Derive a solution to the second readers-writers problem, which favors writers instead of readers. 
12.22 ¡ô¡ô 
Test your understanding of the select function by modifying the server in Fig-ure 12.6 so that it echoes at most one text line per iteration of the main server loop. 
12.23 ¡ô¡ô 
The event-driven concurrent echo server in Figure 12.8 is .awed because a mali-cious client can deny service to other clients by sending a partial text line. Write an improved version of the server that can handle these partial text lines without blocking. 
12.24 ¡ô 

The functions in the Rio I/O package (Section 10.4) are thread-safe. Are they reentrant as well? 
12.25 ¡ô 

In the prethreaded concurrent echo server in Figure 12.28, each thread calls the echo_cnt function (Figure 12.29). Is echo_cnt thread-safe? Is it reentrant? Why or why not? 
12.26 ¡ô¡ô¡ô 

Use the lock-and-copy technique to implement a thread-safe nonreentrant version of gethostbyname called gethostbyname_ts. A correct solution will use a deep copy of the hostent structure protected by a mutex. 
12.27 ¡ô¡ô 

Some network programming texts suggest the following approach for reading and writing sockets: Before interacting with the client, open two standard I/O streams on the same open connected socket descriptor, one for reading and one for writing: 
FILE *fpin, *fpout; 
fpin = fdopen(sockfd, "r"); 
fpout = fdopen(sockfd, "w"); 
When the server has .nished interacting with the client, close both streams as follows: 
fclose(fpin); 

fclose(fpout); 
However, if you try this approach in a concurrent server based on threads, you will create a deadly race condition. Explain. 
12.28 ¡ô 

In Figure 12.43, does swapping the order of the two V operations have any effect on whether or not the program deadlocks? Justify your answer by drawing the progress graphs for the four possible cases: 
Case 1 Case 2 Case 3 Case 4 
Thread 1  Thread 2  Thread 1  Thread 2  Thread 1  Thread 2  Thread 1  Thread 2  
P(s)  P(s)  P(s)  P(s)  P(s)  P(s)  P(s)  P(s)  
P(t)  P(t)  P(t)  P(t)  P(t)  P(t)  P(t)  P(t)  
V(s)  V(s)  V(s)  V(t)  V(t)  V(s)  V(t)  V(t)  
V(t)  V(t)  V(t)  V(s)  V(s)  V(t)  V(s)  V(s)  

12.29 ¡ô 
Can the following program deadlock? Why or why not? 
Initially:a=1,b=1,c=1. 
Thread 1: Thread 2: P(a); P(c); P(b); P(b); V(b); V(b); P(c); V(c); V(c); V(a); 
12.30 ¡ô 
Consider the following program that deadlocks. 
Initially:a=1,b=1,c=1. 
Thread 1: Thread 2: Thread 3: P(a); P(c); P(c); P(b); P(b); V(c); V(b); V(b); P(b); P(c); V(c); P(a); V(c); P(a); V(a); V(a); V(a); V(b); 
A. For each thread, list the pairs of mutexes that it holds simultaneously. 
B. If a<b<c, which threads violate the mutex lock ordering rule? 
C. For these threads, show a new lock ordering that guarantees freedom from deadlock. 
12.31 ¡ô¡ô¡ô 
Implement a version of the standard I/O fgets function, called tfgets, that times out and returns NULL if it does not receive an input line on standard input within 5 seconds. Your function should be implemented in a package called tfgets-proc.c using process, signals, and nonlocal jumps. It should not use the Unix alarm function. Test your solution using the driver program in Figure 12.45. 
12.32 ¡ô¡ô¡ô 
Implement a version of the tfgets function from Problem 12.31 that uses the select function. Your function should be implemented in a package called tfgets-select.c. Test your solution using the driver program from Problem 
12.31. You may assume that standard input is assigned to descriptor 0. 
12.33 ¡ô¡ô¡ô 
Implement a threaded version of the tfgets function from Problem 12.31. Your function should be implemented in a package called tfgets-thread.c. Test your solution using the driver program from Problem 12.31. 

code/conc/tfgets-main.c  
1  #include "csapp.h"  
2  
3  char *tfgets(char *s, int size,  FILE  *stream);  
4  
5  int main()  
6  {  
7  char buf[MAXLINE];  
8  
9  if (tfgets(buf, MAXLINE,  stdin) ==  NULL)  
10  printf("BOOM!\n");  
11  else  
12  printf("%s", buf);  
13  
14  exit(0);  
15  }  
code/conc/tfgets-main.c  
Figure 12.45 Driver program for Problems 12.31¨C12.33. 


12.34 ¡ô¡ô¡ô 

Write a parallel threaded version of an N ¡Á M matrix multiplication kernel. Com-pare the performance to the sequential case. 
12.35 ¡ô¡ô¡ô 

Implement a concurrent version of the Tiny Web server based on processes. Your solution should create a new child process for each new connection request. Test your solution using a real Web browser. 
12.36 ¡ô¡ô¡ô 

Implement a concurrent version of the Tiny Web server based on I/O multiplexing. Test your solution using a real Web browser. 
12.37 ¡ô¡ô¡ô 

Implement a concurrent version of the Tiny Web server based on threads. Your solution should create a new thread for each new connection request. Test your solution using a real Web browser. 
12.38 ¡ô¡ô¡ô¡ô 

Implement a concurrent prethreaded version of the Tiny Web server. Your solu-tion should dynamically increase or decrease the number of threads in response to the current load. One strategy is to double the number of threads when the buffer becomes full, and halve the number of threads when the buffer becomes empty. Test your solution using a real Web browser. 
12.39 ¡ô¡ô¡ô¡ô 
A Web proxy is a program that acts as a middleman between a Web server and browser. Instead of contacting the server directly to get a Web page, the browser contacts the proxy, which forwards the request on to the server. When the server replies to the proxy, the proxy sends the reply on to the browser. For this lab, you will write a simple Web proxy that .lters and logs requests: 
A. In the .rst part of the lab, you will set up the proxy to accept requests, parse the HTTP, forward the requests to the server, and return the results back to the browser. Your proxy should log the URLs of all requests in a log .le on disk, and it should also block requests to any URL contained in a .lter .le on disk. 
B. In the second part of the lab, you will upgrade your proxy to deal with multiple open connections at once by spawning a separate thread to deal with each request. While your proxy is waiting for a remote server to respond to a request so that it can serve one browser, it should be working on a pending request from another browser. 
Check your proxy solution using a real Web browser. 
Solutions 
to 
Practice 
Problems 

Solution to Problem 12.1 (page 939) 
When the parent forks the child, it gets a copy of the connected descriptor and the reference count for the associated .le table is incremented from 1 to 2. When the parent closes its copy of the descriptor, the reference count is decremented from 2 to 1. Since the kernel will not close a .le until the reference counter in its .le table goes to 0, the child¡¯s end of the connection stays open. 
Solution to Problem 12.2 (page 939) 
When a process terminates for any reason, the kernel closes all open descriptors. Thus, the child¡¯s copy of the connected .le descriptor will be closed automatically when the child exits. 
Solution to Problem 12.3 (page 942) 
Recall that a descriptor is ready for reading if a request to read 1 byte from that descriptor would not block. If EOF becomes true on a descriptor, then the descriptor is ready for reading because the read operation will return immediately with a zero return code indicating EOF. Thus, typing ctrl-d causes the select function to return with descriptor 0 in the ready set. 
Solution to Problem 12.4 (page 947) 
We reinitialize the pool.ready_set variable before every call to select because it serves as both an input and output argument. On input, it contains the read set. On output, it contains the ready set. 

Solution to Problem 12.5 (page 954) 
Since threads run in the same process, they all share the same descriptor table. No matter how many threads use the connected descriptor, the reference count for the connected descriptor¡¯s .le table is equal to 1. Thus, a single close operation is suf.cient to free the memory resources associated with the connected descriptor when we are through with it. 
Solution to Problem 12.6 (page 957) 
The main idea here is that stack variables are private, while global and static variables are shared. Static variables such as cnt are a little tricky because the sharing is limited to the functions within their scope¡ªin this case, the thread routine. 
A. Here is the table: 
Variable Referenced by Referenced by Referenced by 
instance main thread? peer thread 0 ? peer thread 1? 
ptr  yes  yes  yes  
cnt  no  yes  yes  
i.m  yes  no  no  
msgs.m  yes  yes  yes  
myid.p0  no  yes  no  
myid.p1  no  no  yes  

Notes: 

ptr: A global variable that is written by the main thread and read by the peer threads. cnt: A static variable with only one instance in memory that is read and written by the two peer threads. 
i.m: A local automatic variable stored on the stack of the main thread. Even though its value is passed to the peer threads, the peer threads never reference it on the stack, and thus it is not shared. 
msgs.m: A local automatic variable stored on the main thread¡¯s stack and referenced indirectly through ptr by both peer threads. myid.0 and myid.1: Instances of a local automatic variable residing on the stacks of peer threads 0 and 1, respectively. 
B. Variables ptr, cnt, and msgs are referenced by more than one thread, and thus are shared. 
Solution to Problem 12.7 (page 960) 
The important idea here is that you cannot make any assumptions about the ordering that the kernel chooses when it schedules your threads. 
Step Thread Instr %eax1 %eax2 cnt 
11 H1¡ª ¡ª0 
21 L10 ¡ª0 
32 H2¡ª ¡ª0 
42 L2¡ª 00 
52 U2¡ª 10 
62 S2¡ª 11 
71 U11 ¡ª1 
81 S11 ¡ª1 
91 T11 ¡ª1 
10 2 T21 ¡ª1 
Variable cnt has a .nal incorrect value of 1. 
Solution to Problem 12.8 (page 962) 
This problem is a simple test of your understanding of safe and unsafe trajectories in progress graphs. Trajectories such as A and C that skirt the critical region are safe and will produce correct results. 
A. H1,L1,U1,S1,H2,L2,U2,S2,T2,T1: safe 
B. H2,L2,H1,L1,U1,S1,T1,U2,S2,T2: unsafe 
C. H1,H2,L2,U2,S2,L1,U1,S1,T1,T2: safe 
Solution to Problem 12.9 (page 967) 
A. p= 1, c= 1, n>1: Yes, the mutex semaphore is necessary because the producer and consumer can concurrently access the buffer. 
B. p= 1, c= 1, n= 1: No, the mutex semaphore is not necessary in this case, because a nonempty buffer is equivalent to a full buffer. When the buffer contains an item, the producer is blocked. When the buffer is empty, the consumer is blocked. So at any point in time, only a single thread can access the buffer, and thus mutual exclusion is guaranteed without using the mutex. 
C. p>1, c>1, n= 1: No, the mutex semaphore is not necessary in this case either, by the same argument as the previous case. 
Solution to Problem 12.10 (page 969) 
Suppose that a particular semaphore implementation uses a LIFO stack of threads for each semaphore. When a thread blocks on a semaphore in a Poperation, its ID is pushed onto the stack. Similarly, the V operation pops the top thread ID from the stack and restarts that thread. Given this stack implementation, an adversarial writer in its critical section could simply wait until another writer blocks on the semaphore before releasing the semaphore. In this scenario, a waiting reader might wait forever as two writers passed control back and forth. 
Notice that although it might seem more intuitive to use a FIFO queue rather than a LIFO stack, using such a stack is not incorrect and does not violate the semantics of the P and V operations. 

Solution to Problem 12.11 (page 978) 
This problem is a simple sanity check of your understanding of speedup and parallel ef.ciency: 
Threads (t) 124 Cores (p) 124 
Running time (T )12 8 6 Speedup (S ) 1 1.52 
p 

p Ef.ciency (Ep) 100% 75% 50% 
Solution to Problem 12.12 (page 982) 
The ctime_ts function is not reentrant because each invocation shares the same static variable returned by the gethostbyname function. However, it is thread-safe because the accesses to the shared variable are protected by P and V opera-tions, and thus are mutually exclusive. 
Solution to Problem 12.13 (page 984) 
If we free the block immediately after the call to pthread_create in line 15, then we will introduce a new race, this time between the call to free in the main thread, and the assignment statement in line 25 of the thread routine. 
Solution to Problem 12.14 (page 985) 
A. Another approach is to pass the integer i directly, rather than passing a pointer to i: 
for (i=0;i<N; i++) Pthread_create(&tid[i], NULL, thread, (void *)i); 
In the thread routine, we cast the argument back to an int and assign it to myid: 
int myid = (int) vargp; 
B. The advantage is that it reduces overhead by eliminating the calls to malloc and free. A signi.cant disadvantage is that it assumes that pointers are at least as large as ints. While this assumption is true for all modern systems, it might not be true for legacy or future systems. 
Solution to Problem 12.15 (page 987) 
A. The progress graph for the original program is shown in Figure 12.46. 
B. The program always deadlocks, since any feasible trajectory is eventually trapped in a deadlock state. 
C. To eliminate the deadlock potential, initialize the binary semaphore t to 1 instead of 0. 
D. The progress graph for the corrected program is shown in Figure 12.47. 
Thread 2 
V(t) 
. . . 
. . . 
. . . 
P(t) 
. . . 
V(s) 
. . . 
P(s) 
Initially 
s 1 t 0 
. . . 

Forbidden  
region  
for t  
Forbidden region for s  Forbidden region for t  
Figure 12.46 Progress graph for a program that deadlocks. 


Thread 1 
. . . P(s) . . . V(s) . . . P(t) . . . V(t) 
Thread 2 

. . . P(s) . . . V(s) . . . P(t) . . . V(t) 

APPENDIX 
A 
Error 
Handling 
Programmers should always check the error codes returned by system-level func-tions. There are many subtle ways that things can go wrong, and it only makes sense to use the status information that the kernel is able to provide us. Unfortunately, programmers are often reluctant to do error checking because it clutters their code, turning a single line of code into a multi-line conditional statement. Error checking is also confusing because different functions indicate errors in different ways. 
We were faced with a similar problem when writing this text. On the one hand, we would like our code examples to be concise and simple to read. On the other hand, we do not want to give students the wrong impression that it is OK to skip error checking. To resolve these issues, we have adopted an approach based on error-handling wrappers that was pioneered by W. Richard Stevens in his network programming text [109]. 
The idea is that given some base system-level function foo, we de.ne a wrapper function Foo with identical arguments, but with the .rst letter capitalized. The wrapper calls the base function and checks for errors. If it detects an error, the wrapper prints an informative message and terminates the process. Otherwise, it returns to the caller. Notice that if there are no errors, the wrapper behaves exactly like the base function. Put another way, if a program runs correctly with wrappers, it will run correctly if we render the .rst letter of each wrapper in lowercase and recompile. 
The wrappers are packaged in a single source .le (csapp.c) that is compiled and linked into each program. A separate header .le (csapp.h) contains the function prototypes for the wrappers. 
This appendix gives a tutorial on the different kinds of error handling in Unix systems, and gives examples of the different styles of error-handling wrappers. Copies of the csapp.h and csapp.c .les are available on the CS:APP Web page. 
A.1 
Error 
Handling 
in 
Unix 
Systems 

The systems-level function calls that we will encounter in this book use three different styles for returning errors: Unix-style, Posix-style, and DNS-style. 
Unix-Style Error Handling 
Functions such as fork and wait that were developed in the early days of Unix (as well as some older Posix functions) overload the function return value with both error codes and useful results. For example, when the Unix-style wait function encounters an error (e.g., there is no child process to reap) it returns .1 and sets the global variable errno to an error code that indicates the cause of the error. If wait completes successfully, then it returns the useful result, which is the PID of the reaped child. Unix-style error-handling code is typically of the following form: 
1 if ((pid = wait(NULL)) < 0) { 2 fprintf(stderr, "wait error: %s\n", strerror(errno)); 3 exit(0); 
4 } 
The strerror function returns a text description for a particular value of errno. 
Posix-Style Error Handling 
Many of the newer Posix functions such as Pthreads use the return value only to indicate success (0) or failure (nonzero). Any useful results are returned in function arguments that are passed by reference. We refer to this approach as Posix-style error handling. For example, the Posix-style pthread_create function indicates success or failure with its return value and returns the ID of the newly created thread (the useful result) by reference in its .rst argument. Posix-style error-handling code is typically of the following form: 
1 if ((retcode = pthread_create(&tid, NULL, thread, NULL)) != 0) { 2 fprintf(stderr, "pthread_create error: %s\n", strerror(retcode)); 3 exit(0); 
4 } 
DNS-Style Error Handling 
The gethostbyname and gethostbyaddr functions that retrieve DNS (Domain Name System) host entries have yet another approach for returning errors. These functions return a NULL pointer on failure and set the global h_errno variable. DNS-style error handling is typically of the following form: 
1 if ((p = gethostbyname(name)) == NULL) { 2 fprintf(stderr, "gethostbyname error: %s\n:", hstrerror(h_errno)); 3 exit(0); 
4 } 

Summary of Error-Reporting Functions 
Thoughout this book, we use the following error-reporting functions to accommo-date different error-handling styles. 
#include "csapp.h" 
void unix_error(char *msg); void posix_error(int code, char *msg); void dns_error(char *msg); void app_error(char *msg); 
Returns: nothing 

As their names suggest, the unix_error, posix_error, and dns_error func-tions report Unix-style, Posix-style, and DNS-style errors and then terminate. The app_error function is included as a convenience for application errors. It simply prints its input and then terminates. Figure A.1 shows the code for the error-reporting functions. 
A.2 
Error-Handling 
Wrappers 

Here are some examples of the different error-handling wrappers: 
. Unix-style error-handling wrappers. Figure A.2 shows the wrapper for the Unix-style wait function. If the wait returns with an error, the wrapper prints an informative message and then exits. Otherwise, it returns a PID to the caller. Figure A.3 shows the wrapper for the Unix-style kill function. Notice that this function, unlike Wait, returns void on success. 
. Posix-style error-handling wrappers. Figure A.4 shows the wrapper for the Posix-style pthread_detach function. Like most Posix-style functions, it does not overload useful results with error-return codes, so the wrapper returns void on success. 
. DNS-style error-handling wrappers. Figure A.5 shows the error-handling wrapper for the DNS-style gethostbyname function. 
code/src/csapp.c 
1 void unix_error(char *msg) /* Unix-style error */ 2 { 3 fprintf(stderr, "%s: %s\n", msg, strerror(errno)); 4 exit(0); 
5 } 6 7 void posix_error(int code, char *msg) /* Posix-style error */ 8 { 9 fprintf(stderr, "%s: %s\n", msg, strerror(code)); 
10 exit(0); 
11 
} 12 13 void dns_error(char *msg) /* DNS-style error */ 14 { 15 fprintf(stderr, "%s: DNS error %d\n", msg, h_errno); 16 exit(0); 

17 
} 18 19 void app_error(char *msg) /* Application error */ 20 { 21 fprintf(stderr, "%s\n", msg); 22 exit(0); 


23 } 
code/src/csapp.c 
Figure A.1 Error-reporting functions. 
code/src/csapp.c 
1 pid_t Wait(int *status) 2 { 3 pid_t pid; 4 5 if ((pid = wait(status)) < 0) 6 unix_error("Wait error"); 7 return pid; 
8 } 
code/src/csapp.c 
Figure A.2 Wrapper for Unix-style wait function. 
code/src/csapp.c 

1 void Kill(pid_t pid, int signum) 2 { 3 int rc; 4 5 if ((rc = kill(pid, signum)) < 0) 6 unix_error("Kill error"); 
7 } 
code/src/csapp.c 

Figure A.3 Wrapper for Unix-style kill function. 
code/src/csapp.c 

1 void Pthread_detach(pthread_t tid) { 2 int rc; 3 4 if ((rc = pthread_detach(tid)) != 0) 5 posix_error(rc, "Pthread_detach error"); 
6 } 
code/src/csapp.c 

Figure A.4 Wrapper for Posix-style pthread_detach function. 
code/src/csapp.c 

1 struct hostent *Gethostbyname(const char *name) 2 { 3 struct hostent *p; 4 5 if ((p = gethostbyname(name)) == NULL) 6 dns_error("Gethostbyname error"); 7 return p; 
8 } 
code/src/csapp.c 

Figure A.5 Wrapper for DNS-style gethostbyname function. 
This page intentionally left blank 

References 

[1] Advanced Micro Devices, Inc. Software Opti-mization Guide for AMD64 Processors, 2005. Publication Number 25112. 
[2] Advanced Micro Devices, Inc. AMD64 Arch-itecture Programmer¡¯s Manual, Volume 1: Application Programming, 2007. Publication Number 24592. 
[3] Advanced Micro Devices, Inc. AMD64 Ar-chitecture Programmer¡¯s Manual, Volume 3: General-Purpose and System Instructions, 2007. Publication Number 24594. 
[4] K. Arnold, J. Gosling, and D. Holmes. The Java Programming Language, Fourth Edition. Prentice Hall, 2005. 
[5] V. Bala, E. Duesterwald, and S. Banerjiia. Dynamo: A transparent dynamic optimization system. In Proceedings of the 1995 ACM Conference on Programming Language Design and Implementation (PLDI), pages 1¨C12, June 2000. 
[6] T. Berners-Lee, R. Fielding, and H. Frystyk. Hypertext transfer protocol -HTTP/1.0. RFC 1945, 1996. 
[7] A. Birrell. An introduction to programming with threads. Technical Report 35, Digital Systems Research Center, 1989. 
[8] A. Birrell, M. Isard, C. Thacker, and T. Wobber. A design for high-performance .ash disks. SIGOPS Operating Systems Review, 41(2), 2007. 
[9] R. Blum. Professional Assembly Language. Wiley, 2005. 
[10] S. Borkar. Thousand core chips¡ªa technology perspective. In Design Automation Conference, pages 746¨C749. ACM, 2007. 
[11] D. Bovet and M. Cesati. Understanding the Linux Kernel, Third Edition. O¡¯Reilly Media, Inc, 2005. 
[12] A. Demke Brown and T. Mowry. Taming the memory hogs: Using compiler-inserted releases to manage physical memory intelligently. In 
Proceedings of the Fourth Symposium on Operating Systems Design and Implementation (OSDI), pages 31¨C44, October 2000. 
[13] R. E. Bryant. Term-level veri.cation of a pipelined CISC microprocessor. Technical Report CMU-CS-05-195, Carnegie Mellon University, School of Computer Science, 2005. 
[14] R. E. Bryant and D. R. O¡¯Hallaron. Introduc-ing computer systems from a programmer¡¯s perspective. In Proceedings of the Technical Symposium on Computer Science Education (SIGCSE). ACM, February 2001. 
[15] B. R. Buck and J. K. Hollingsworth. An API for runtime code patching. Journal of High Performance Computing Applications, 14(4):317¨C324, June 2000. 
[16] D. Butenhof. Programming with Posix Threads. Addison-Wesley, 1997. 
[17] S. Carson and P. Reynolds. The geometry of semaphore programs. ACM Transactions on Programming Languages and Systems, 9(1):25¨C 53, 1987. 
[18] J. B. Carter, W. C. Hsieh, L. B. Stoller, M. R. Swanson, L. Zhang, E. L. Brunvand, A. Davis, C.-C. Kuo, R. Kuramkote, M. A. Parker, 
L. Schaelicke, and T. Tateyama. Impulse: Building a smarter memory controller. In Pro-ceedings of the Fifth International Symposium on High Performance Computer Architecture (HPCA), pages 70¨C79, January 1999. 
¡§ 
How to write fast numerical code: A small in-
troduction. In Generative and Transformational 
Techniques in Software Engineering II, volume 
5235, pages 196¨C259. Springer-Verlag Lecture 
Notes in Computer Science, 2008. 
[19] S. Chellappa, F. Franchetti, and M. Puschel.
[20] P. Chen, E. Lee, G. Gibson, R. Katz, and D. Pat-terson. RAID: High-performance, reliable secondary storage. ACM Computing Surveys, 26(2), June 1994. 
1005 

[21] S. Chen, P. Gibbons, and T. Mowry. Improving index performance through prefetching. In Proceedings of the 2001 ACM SIGMOD Conference. ACM, May 2001. 
[22] T. Chilimbi, M. Hill, and J. Larus. Cache-conscious structure layout. In Proceedings of the 1999 ACM Conference on Programming Language Design and Implementation (PLDI), pages 1¨C12. ACM, May 1999. 
[23] B. Cmelik and D. Keppel. Shade: A fast instruction-set simulator for execution pro-.ling. In Proceedings of the 1994 ACM SIG-METRICS Conference on Measurement and Modeling of Computer Systems, pages 128¨C137, May 1994. 
[24] E. Coffman, M. Elphick, and A. Shoshani. System deadlocks. ACM Computing Surveys, 3(2):67¨C78, June 1971. 
[25] D. Cohen. On holy wars and a plea for peace. IEEE Computer, 14(10):48¨C54, October 1981. 
[26] Intel Corporation. Intel 64 and IA-32 Archi-tectures Optimization Reference Manual, 2009. Order Number 248966. 
[27] Intel Corporation. Intel 64 and IA-32 Archi-tectures Software Developer¡¯s Manual, Vol-ume 1: Basic Architecture, 2009. Order Number 253665. 
[28] Intel Corporation. Intel 64 and IA-32 Architec-tures Software Developer¡¯s Manual, Volume 2: Instruction Set Reference A¨CM, 2009. Order Number 253667. 
[29] Intel Corporation. Intel 64 and IA-32 Architec-tures Software Developer¡¯s Manual, Volume 2: Instruction Set Reference N¨CZ, 2009. Order Number 253668. 
[30] Intel Corporation. Intel 64 and IA-32 Architec-tures Software Developer¡¯s Manual, Volume 3a: System Programming Guide, Part 1, 2009. Order Number 253669. 
[31] P. J. Courtois, F. Heymans, and D. L. Parnas. Concurrent control with ¡°readers¡± and ¡°writ-ers.¡± Commun. ACM, 14(10):667¨C668, 1971. 
[32] 
C. Cowan, P. Wagle, C. Pu, S. Beattie, and 

J. 
Walpole. Buffer over.ows: Attacks and defenses for the vulnerability of the decade. In 


DARPA Information Survivability Conference and Expo (DISCEX), March 2000. 
[33] J. H. Crawford. The i486 CPU: Executing instructions in one clock cycle. IEEE Micro, 10(1):27¨C36, February 1990. 
[34] V. Cuppu, B. Jacob, B. Davis, and T. Mudge. A performance comparison of contemporary DRAM architectures. In Proceedings of the Twenty-Sixth International Symposium on Computer Architecture (ISCA), Atlanta, GA, May 1999. IEEE. 
[35] B. Davis, B. Jacob, and T. Mudge. The new DRAM interfaces: SDRAM, RDRAM, and variants. In Proceedings of the Third Inter-national Symposium on High Performance Computing (ISHPC), Tokyo, Japan, October 2000. 
[36] E. Demaine. Cache-oblivious algorithms and data structures. In Lecture Notes in Computer Science. Springer-Verlag, 2002. 
[37] E. W. Dijkstra. Cooperating sequential pro-cesses. Technical Report EWD-123, Technolog-ical University, Eindhoven, The Netherlands, 1965. 
[38] C. Ding and K. Kennedy. Improving cache performance of dynamic applications through data and computation reorganizations at run time. In Proceedings of the 1999 ACM Conference on Programming Language Design and Implementation (PLDI), pages 229¨C241. ACM, May 1999. 
[39] M. Dowson. The Ariane 5 software failure. SIG-SOFT Software Engineering Notes, 22(2):84, 1997. 
[40] M. W. Eichen and J. A. Rochlis. With micro-scope and tweezers: An analysis of the Internet virus of November, 1988. In IEEE Symposium on Research in Security and Privacy, 1989. 
[41] R. Fielding, J. Gettys, J. Mogul, H. Frystyk, 
L. Masinter, P. Leach, and T. Berners-Lee. Hypertext transfer protocol -HTTP/1.1. RFC 2616, 1999. 
[42] M. Frigo, C. E. Leiserson, H. Prokop, and 
S. Ramachandran. Cache-oblivious algorithms. In Proceedings of the 40th IEEE Symposium on Foundations of Computer Science (FOCS ¡¯99), pages 285¨C297. IEEE, August 1999. 
[43] M. Frigo and V. Strumpen. The cache complex-ity of multithreaded cache oblivious algorithms. 

In SPAA ¡¯06: Proceedings of the Eighteenth Annual ACM Symposium on Parallelism in Algorithms and Architectures, pages 271¨C280, New York, NY, USA, 2006. ACM. 
[44] 
G. Gibson, D. Nagle, K. Amiri, J. Butler, 

F. 
Chang, H. Gobioff, C. Hardin, E. Riedel, 


D. Rochberg, and J. Zelenka. A cost-effective, high-bandwidth storage architecture. In Pro-ceedings of the International Conference on Architectural Support for Programming Lan-guages and Operating Systems (ASPLOS). ACM, October 1998. 
[45] G. Gibson and R. Van Meter. Network attached storage architecture. Communications of the ACM, 43(11), November 2000. 
[46] J. Gustafson. Reevaluating Amdahl¡¯s law. Communications of the ACM, 31(5), August 1988. 
[47] L. Gwennap. New algorithm improves branch prediction. Microprocessor Report, 9(4), March 1995. 
[48] S. P. Harbison and G. L. Steele, Jr. C, A Reference Manual, Fifth Edition. Prentice Hall, 2002. 
[49] J. L. Hennessy and D. A. Patterson. Computer Architecture: A Quantitative Approach, Fourth Edition. Morgan Kaufmann, 2007. 
[50] M. Herlihy and N. Shavit. The Art of Multi-processor Programming. Morgan Kaufmann, 2008. 
[51] C. A. R. Hoare. Monitors: An operating system structuring concept. Communications of the ACM, 17(10):549¨C557, October 1974. 
[52] Intel Corporation. Tool Interface Standards Portable Formats Speci.cation, Version 1.1, 1993. Order Number 241597. 
[53] 
F. Jones, B. Prince, R. Norwood, J. Hartigan, 

W. 
Vogley, C. Hart, and D. Bondurant. A new era of fast dynamic RAMs. IEEE Spectrum, pages 43¨C39, October 1992. 


[54] R. Jones and R. Lins. Garbage Collection: Algorithms for Automatic Dynamic Memory Management. Wiley, 1996. 
[55] 
M. Kaashoek, D. Engler, G. Ganger, H. Briceo, 

R. 
Hunt, D. Maziers, T. Pinckney, R. Grimm, 

J. 
Jannotti, and K. MacKenzie. Application per-formance and .exibility on Exokernel systems. 


In Proceedings of the Sixteenth Symposium on Operating System Principles (SOSP), October 1997. 
[56] R. Katz and G. Borriello. Contemporary Logic Design, Second Edition. Prentice Hall, 2005. 
[57] B. Kernighan and D. Ritchie. The C Program-ming Language, First Edition. Prentice Hall, 1978. 
[58] B. Kernighan and D. Ritchie. The C Program-ming Language, Second Edition. Prentice Hall, 1988. 
[59] B. W. Kernighan and R. Pike. The Practice of Programming. Addison-Wesley, 1999. 
[60] T. Kilburn, B. Edwards, M. Lanigan, and 
F. Sumner. One-level storage system. IRE Transactions on Electronic Computers, EC-11:223¨C235, April 1962. 
[61] D. Knuth. The Art of Computer Programming, Volume 1: Fundamental Algorithms, Second Edition. Addison-Wesley, 1973. 
[62] J. Kurose and K. Ross. Computer Networking: A Top-Down Approach, Fifth Edition. Addison-Wesley, 2009. 
[63] M. Lam, E. Rothberg, and M. Wolf. The cache performance and optimizations of blocked al-gorithms. In Proceedings of the International Conference on Architectural Support for Pro-gramming Languages and Operating Systems (ASPLOS). ACM, April 1991. 
[64] J. R. Larus and E. Schnarr. EEL: Machine-independent executable editing. In Proceedings of the 1995 ACM Conference on Programming Language Design and Implementation (PLDI), June 1995. 
[65] C. E. Leiserson and J. B. Saxe. Retiming synchronous circuitry. Algorithmica, 6(1¨C6), June 1991. 
[66] J. R. Levine. Linkers and Loaders. Morgan Kaufmann, San Francisco, 1999. 
[67] C. Lin and L. Snyder. Principles of Parallel Programming. Addison-Wesley, 2008. 
[68] Y. Lin and D. Padua. Compiler analysis of irregular memory accesses. In Proceedings of the 2000 ACM Conference on Programming Language Design and Implementation (PLDI), pages 157¨C168. ACM, June 2000. 

[69] J. L. Lions. Ariane 5 Flight 501 failure. Technical report, European Space Agency, July 1996. 
[70] S. Macguire. Writing Solid Code. Microsoft Press, 1993. 
[71] 
S. A. Mahlke, W. Y. Chen, J. C. Gyllenhal, and 

W. 
W. Hwu. Compiler code transformations for superscalar-based high-performance systems. In Supercomputing. ACM, 1992. 


[72] E. Marshall. Fatal error: How Patriot over-looked a Scud. Science, page 1347, March 13, 1992. 
[73] M. Matz, J. Hubicka, A. Jaeger, and M. Mitchell.¡¦ System V application binary interface AMD64 architecture processor supplement. Technical report, AMD64.org, 2009. 
[74] 
J. Morris, M. Satyanarayanan, M. Conner, 

J. 
Howard, D. Rosenthal, and F. Smith. Andrew: A distributed personal computing environment. Communications of the ACM, March 1986. 


[75] T. Mowry, M. Lam, and A. Gupta. Design and evaluation of a compiler algorithm for prefetching. In Proceedings of the International Conference on Architectural Support for Pro-gramming Languages and Operating Systems (ASPLOS). ACM, October 1992. 
[76] S. S. Muchnick. Advanced Compiler Design and Implementation. Morgan Kaufmann, 1997. 
[77] S. Nath and P. Gibbons. Online maintenance of very large random samples on .ash storage. In Proceedings of VLDB¡¯08. ACM, August 2008. 
[78] M. Overton. Numerical Computing with IEEE Floating Point Arithmetic. SIAM, 2001. 
[79] D. Patterson, G. Gibson, and R. Katz. A case for redundant arrays of inexpensive disks (RAID). In Proceedings of the 1998 ACM SIGMOD Conference. ACM, June 1988. 
[80] L. Peterson and B. Davie. Computer Networks: A Systems Approach, Fourth Edition. Morgan Kaufmann, 2007. 
[81] J. Pincus and B. Baker. Beyond stack smashing: Recent advances in exploiting buffer overruns. IEEE Security and Privacy, 2(4):20¨C27, 2004. 
[82] S. Przybylski. Cache and Memory Hierarchy Design: A Performance-Directed Approach. Morgan Kaufmann, 1990. 
[83] W. Pugh. The Omega test: A fast and practical integer programming algorithm for depen-dence analysis. Communications of the ACM, 35(8):102¨C114, August 1992. 
[84] W. Pugh. Fixing the Java memory model. In Proceedings of the Java Grande Conference, June 1999. 
[85] J. Rabaey, A. Chandrakasan, and B. Nikolic. Digital Integrated Circuits: A Design Perspec-tive, Second Edition. Prentice Hall, 2003. 
[86] J. Reinders. Intel Threading Building Blocks. O¡¯Reilly, 2007. 
[87] D. Ritchie. The evolution of the Unix time-sharing system. AT&T Bell Laboratories Technical Journal, 63(6 Part 2):1577¨C1593, October 1984. 
[88] D. Ritchie. The development of the C language. In Proceedings of the Second History of Pro-gramming Languages Conference, Cambridge, MA, April 1993. 
[89] D. Ritchie and K. Thompson. The Unix time-sharing system. Communications of the ACM, 17(7):365¨C367, July 1974. 
[90] T. Romer, G. Voelker, D. Lee, A. Wolman, 
W. Wong, H. Levy, B. Bershad, and B. Chen. In-strumentation and optimization of Win32/Intel executables using Etch. In Proceedings of the USENIX Windows NT Workshop, Seattle, Washington, August 1997. 
[91] M. Satyanarayanan, J. Kistler, P. Kumar, 
M. Okasaki, E. Siegel, and D. Steere. Coda: A highly available .le system for a distributed workstation environment. IEEE Transactions on Computers, 39(4):447¨C459, April 1990. 
[92] J. Schindler and G. Ganger. Automated disk drive characterization. Technical Report CMU-CS-99-176, School of Computer Science, Carnegie Mellon University, 1999. 
[93] F. B. Schneider and K. P. Birman. The monocul-ture risk put into context. IEEE Security and Privacy, 7(1), January 2009. 
[94] R. C. Seacord. Secure Coding in C and C++. Addison-Wesley, 2006. 
[95] H. Shacham, M. Page, B. Pfaff, E.-J. Goh, 
N. Modadugu, and D. Boneh. On the effec-tiveness of address-space randomization. In 
Proceedings of the 11th ACM Conference on 

Computer and Communications Security (CCS ¡¯04), pages 298¨C307. ACM, 2004. 
[96] J. P. Shen and M. Lipasti. Modern Processor De-sign: Fundamentals of Superscalar Processors. McGraw Hill, 2005. 
[97] B. Shriver and B. Smith. The Anatomy of a High-Performance Microprocessor: A Systems Perspective. IEEE Computer Society, 1998. 
[98] A. Silberschatz, P. Galvin, and G. Gagne. Operating Systems Concepts, Eighth Edition. Wiley, 2008. 
[99] R. Singhal. Intel next generation Nehalem microarchitecture. In Intel Developer¡¯s Forum, 2008. 
[100] R. Skeel. Roundoff error and the Patriot missile. SIAM News, 25(4):11, July 1992. 
[101] A. Smith. Cache memories. ACM Computing Surveys, 14(3), September 1982. 
[102] E. H. Spafford. The Internet worm program: An analysis. Technical Report CSD-TR-823, Department of Computer Science, Purdue University, 1988. 
[103] A. Srivastava and A. Eustace. ATOM: A sys-tem for building customized program analysis tools. In Proceedings of the 1994 ACM Confer-ence on Programming Language Design and Implementation (PLDI), June 1994. 
[104] W. Stallings. Operating Systems: Internals and Design Principles, Sixth Edition. Prentice Hall, 2008. 
[105] W. R. Stevens. TCP/IP Illustrated, Volume 1: The Protocols. Addison-Wesley, 1994. 
[106] W. R. Stevens. TCP/IP Illustrated, Volume 2: The Implementation. Addison-Wesley, 1995. 
[107] W. R. Stevens. TCP/IP Illustrated, Volume 3: TCP for Transactions, HTTP, NNTP and the Unix domain protocols. Addison-Wesley, 1996. 
[108] W. R. Stevens. Unix Network Programming: Interprocess Communications, Second Edition, volume 2. Prentice Hall, 1998. 
[109] W. R. Stevens, B. Fenner, and A. M. Rudoff. Unix Network Programming: The Sockets Networking API, Third Edition, volume 1. Prentice Hall, 2003. 
References 1009 
[110] W. R. Stevens and S. A. Rago. Advanced Programming in the Unix Environment, Second Edition. Addison-Wesley, 2008. 
[111] T. Stricker and T. Gross. Global address space, non-uniform bandwidth: A memory system performance characterization of parallel sys-tems. In Proceedings of the Third International Symposium on High Performance Computer Architecture (HPCA), pages 168¨C179, San An-tonio, TX, February 1997. IEEE. 
[112] A. Tanenbaum. Modern Operating Systems, Third Edition. Prentice Hall, 2007. 
[113] A. Tanenbaum. Computer Networks, Fourth Edition. Prentice Hall, 2002. 
[114] K. P. Wadleigh and I. L. Crawford. Software Optimization for High-Performance Comput-ing: Creating Faster Applications. Prentice Hall, 2000. 
[115] J. F. Wakerly. Digital Design Principles and Practices, Fourth Edition. Prentice Hall, 2005. 
[116] M. V. Wilkes. Slave memories and dynamic storage allocation. IEEE Transactions on Electronic Computers, EC-14(2), April 1965. 
[117] P. Wilson, M. Johnstone, M. Neely, and D. Boles. Dynamic storage allocation: A survey and critical review. In International Workshop on Memory Management, Kinross, Scotland, 1995. 
[118] M. Wolf and M. Lam. A data locality algorithm. In Conference on Programming Language Design and Implementation (SIGPLAN), pages 30¨C44, June 1991. 
[119] J. Wylie, M. Bigrigg, J. Strunk, G. Ganger, 
H. Kiliccote, and P. Khosla. Survivable informa-tion storage systems. IEEE Computer, August 2000. 
[120] T.-Y. Yeh and Y. N. Patt. Alternative implemen-tation of two-level adaptive branch prediction. In International Symposium on Computer Ar-chitecture, pages 451¨C461, 1998. 
[121] X. Zhang, Z. Wang, N. Gloy, J. B. Chen, and 
M. D. Smith. System support for automatic pro.ling and optimization. In Proceedings of the Sixteenth ACM Symposium on Operating Systems Principles (SOSP), pages 15¨C26, October 1997. 
This page intentionally left blank 
Index 


Page numbers of de.ning references are italicized. Entries that belong to a hard-ware or software system are followed by a tag in brackets that identi.es the system, along with a brief description to jog your memory. Here is the list of tags and their 
meanings.  
[C]  C language construct  
[C Stdlib]  C standard library function  
[CS:APP]  Program or function developed in this text  
[HCL]  HCL language construct  
[IA32]  IA32 machine language instruction  
[Unix]  Unix program, function, variable, or constant  
[x86-64]  x86-64 machine language instruction  
[Y86]  Y86 machine language instruction  

& [C] address of operation logic gates, 353 
pointers, 
44, 
175, 
234, 
252 

* [C] dereference pointer operation, 
175 
$ for 
immediate 
operands, 
169 
! [HCL] Not operation, 353 
|| [HCL] Or operation, 353 
< left hoinky, 878 
<< [C] 
left 
shift 
operator, 
54¨C56 
<< ¡°put 
to¡± 
operator 
(C++), 
862 
-> [C] dereference and select .eld 
operator, 242 
> right hoinky, 878 
>> ¡°get 
from¡± 
operator 
(C++), 
862 
>> [C] 
right 
shift 
operator, 
54¨C56 
. (periods) in dotted-decimal 
notation, 
893 
+t two¡¯s-complement addition, 83 

w 

-t two¡¯s-complement negation, 87 

w 

*t two¡¯s-complement multiplication, 
w 

+u unsigned addition, 82 

w 

-u unsigned negation, 82 

w 

*u unsigned multiplication, 88 

w 
.a archive .les, 668 

a.out .les, 658 

Abel, 
Niels 
Henrik, 
82 

abelian group, 82 

ABI (Application Binary Interface), 294 

abort exception class, 706 

aborts, 
708¨C709 

absolute addressing relocation type, 673, 
675¨C676 

absolute speedup of parallel programs, 977 

abstract model of processor operation, 
502¨C508 

abstractions, 
24¨C25 

accept [Unix] wait for client connection 
request, 
902, 
907, 
907¨C908 

access disks, 
578¨C580 
IA32 
registers, 
168¨C169 
data 
movement, 
171¨C177 
operand speci.ers, 169¨C170 
main 
memory, 
567¨C570 
x86-64 
registers, 
273¨C277 

access permission 
bits, 
864 

access time for disks, 573, 
573¨C575 

accumulators, 
multiple, 
514¨C518 

Acorn RISC Machines (ARM) ISAs, 
334 
processor 
architecture, 
344 

actions, signal, 742 
active sockets, 905 
actuator arms, 573 
acyclic 
networks, 
354 
adapters, 
8, 
577 
add [IA32/x86-64] 
add, 
178, 
277 
add-client [CS:APP] add client to 
list, 
943, 
945 
add every signal to signal set function, 
753 

add operation 
in 
execute 
stage, 
387 
add signal to signal set function, 
753 

addb [IA32/x86-64] 
instruction, 
177, 

277 
adder [CS:APP] CGI adder, 918 
addition 
.oating-point, 
113¨C114 
IA32, 
177 
two¡¯s-complement, 83, 
83¨C87 
unsigned, 
79¨C83, 
82 
x86-64, 
277¨C278 
Y86, 338 

additive inverse, 49 

1011 

addl [IA32/x86-64] 
instruction, 
177, 
272, 
277 
addl [Y86] add, 338, 
383 
addq [x86-64] 
instruction, 
272, 
277 
address exceptions, status code for, 
384 


address-of operator (&) [C] pointers, 44, 
175, 
234, 
252 
address order of free lists, 835 
address partitioning in caches, 598 
address-space layout randomization (ASLR), 262 
address spaces, 778 
child 
processes, 
721 
private, 714 
virtual, 
778¨C779 
address translation, 777, 
787 
caches 
and 
VM 
integration, 
791 
Core 
i7, 
800¨C803 
end-to-end, 
794¨C799 
multi-level 
page 
tables, 
792¨C 
794 
optimizing, 
802 
overview, 
787¨C790 
TLBs 
for, 
791¨C793 
addresses and addressing byte 
ordering, 
39¨C42 
effective, 170, 
673 
.at, 159 
Internet, 890 
invalid 
address 
status 
code, 
344 
I/O 
devices, 
579 
IP, 892, 
893¨C895 
machine-level 
programs, 
160¨C161 
operands, 
170 
out-of-bounds. See buffer over.ow physical 
vs. 
virtual, 
777¨C778 
pointers, 
234, 
252 
procedure 
return, 
220 
segmented, 
264 
sockets, 899, 
901¨C902 
structures, 
241¨C243 
symbol 
relocation, 
672¨C677 
virtual, 777 
virtual 
memory, 
33 
Y86, 
337, 
340 
addressing 
modes, 
170 
addw [IA32/x86-64] 
instruction, 
177, 
277 
adjacency matrices, 642 
ADR [Y86] status code indicating invalid address, 344 

Advanced Micro Devices (AMD), 156, 
159, 
267 
AMD64 microprocessors, 267, 
269 
Intel 
compatibility, 
159 
x86-64. See x86-64 microprocessors Advanced Research Projects Agency (ARPA), 
900 
AFS 
(Andrew 
File 
System), 
591 
aggregate 
data 
types, 
161 
aggregate payloads, 819 
%ah [IA32] bits 8¨C15 of register %eax, 168 
%ah [x86-64] bits 8¨C15 of register %rax, 274 
%al [IA32] bits 0¨C7 bits of register %eax, 168, 
170 
%al [x86-64] bits 0¨C7 of register %rax, 274 
alarm [Unix] schedule alarm to self, 742, 
743 
alarm.c [CS:APP] program, 743 
algebra, 
Boolean, 
48¨C51, 
49 
aliasing, memory, 477, 
478, 
494 
.align directive, 
346 
alignment data, 248, 
248¨C251 
memory 
blocks, 
818 
stack 
space, 
226 
x86-64, 
291 
alloca [Unix] stack storage allocation 
function, 
261 
allocate and initialize bounded buffer function, 968 
allocate 
heap 
block 
function, 
832, 

834 
allocate heap storage function, 814 
allocated bit, 821 
allocated blocks vs. free, 813 
placement, 
822¨C823 
allocation blocks, 
832 
dynamic memory. See dynamic memory allocation pages, 
783¨C784 
allocators block 
allocation, 
832 
block 
freeing 
and 
coalescing, 
832 
free 
list 
creation, 
830¨C832 
free 
list 
manipulation, 
829¨C830 
general 
design, 
827¨C829 
practice 
problems, 
832¨C835 

requirements 
and 
goals, 
817¨C819 
styles, 813¨C814 
Alpha processors introduction, 
268 
RISC, 
343 
alternate representations of signed integers, 
63 
ALUADD [Y86] function code for addition operation, 384 
ALUs (Arithmetic/Logic Units), 9 
combinational circuits, 359¨C360 
in 
execute 
stage, 
364 
sequential Y86 implementation, 387¨C389 
always taken branch prediction strategy, 
407 
AMD (Advanced Micro Devices), 156, 
159, 
267 
Intel 
compatibility, 
159 
x86-64. See x86-64 microprocessors AMD64 microprocessors, 267, 
269 
Amdahl, 
Gene, 
545 
Amdahl¡¯s law, 
475, 
540, 
545, 
545¨C547 
American National Standards Institute (ANSI), 4 
C standards, 4,32 
static 
libraries, 
667 
ampersand (&) logic gates, 353 
pointers, 
44, 
175, 
234, 
252 
monoand 
[IA32/x86-64] 
and, 
178, 
277 
and operations Boolean, 
48¨C49 
execute 
stage, 
387 
HCL 
expressions, 
354¨C355 
logic gates, 353 
logical, 
54 
andl [Y86] and, 338 
Andreesen, 
Marc, 
912 
Andrew 
File 
System 
(AFS), 
591 
anonymous .les, 807 
ANSI (American National Standards Institute), 4 
C standards, 4,32 
static 
libraries, 
667 
AOK [Y86] status code for normal operation, 344 
app_error [CS:APP] reports application errors, 1001 
Application Binary Interface (ABI), 294 


applications, loading and linking shared 
libraries 
from, 
683¨C686 
ar Unix archiver, 669, 
690 
Archimedes, 
131 
architecture .oating-point, 292 
Y86. See Y86 instruction set architecture archives, 668 
areal density of disks, 572 
areas shared, 808 
swap, 807 
virtual memory, 804 
arguments execve function, 
730 
IA32, 
226¨C228 
Web 
servers, 
917¨C918 
x86-64, 
283¨C284 
arithmetic, 
31, 
177 
integer. See integer arithmetic latency 
and 
issue 
time, 
501¨C502 
load 
effective 
address, 
177¨C178 
pointer, 
233¨C234, 
846 
saturating, 
125 
shift operations, 55, 
96¨C97, 
178¨C180 
special, 
182¨C185, 
278¨C279 
unary 
and 
binary, 
178¨C179 
x86-64 
instructions, 
277¨C279 
arithmetic/logic units (ALUs), 9 
combinational circuits, 359¨C360 
in 
execute 
stage, 
364 
sequential Y86 implementation, 387¨C389 
ARM (Acorn RISC Machines) ISAs, 
334 
processor 
architecture, 
344 
arms, actuator, 573 
ARPA (Advanced Research Projects Agency), 
900 
ARPANET, 
900 
arrays, 
232 
basic 
principles, 
232¨C233 
declarations, 
232¨C233, 
238 
DRAM, 562 
.xed-size, 
237¨C238 
machine-code 
representation, 
161 
nested, 
235¨C236 
pointer 
arithmetic, 
233¨C234 
pointer 
relationships, 
43, 
252 
stride, 
588 
variable-sized, 
238¨C241 

ASCII 
standard, 
3 
character codes, 46 
limitations, 
47 
asctime function, 
982¨C983 
ASLR (address-space layout randomization), 262 
asm directive, 
267 
assembler 
directives, 
346 
assemblers, 5, 
154, 
160 
assembly 
code, 
5, 
154 
with 
C 
programs, 
266¨C267 
formatting, 
165¨C167 
Y86, 
340 
assembly 
phase, 
5 
associate socket address with descriptor function, 904, 
904¨C 
905 
associative 
caches, 
606¨C609 
associative memory, 607 
associativity caches, 
614¨C615 
.oating-point 
addition, 
113¨C114 
.oating-point 
multiplication, 
114 
integer 
multiplication, 
30 
unsigned 
addition, 
82 
asterisk (*) dereference pointer operation, 
175, 
234, 
252 
asymmetric ranges in two¡¯s-complement representation, 61¨C62, 
71 
asynchronous interrupts, 706 
atexit function, 
680 
Atom 
system, 
692 
ATT assembly-code format, 166 
arithmetic 
instructions, 
279 
cltd instruction, 
184 
gcc, 294 
vs. Intel, 
166¨C167 
operands, 
169, 
178, 
186 
Y86 
instructions, 
337¨C338 
automatic variables, 956 
%ax [IA32] low-order 16 bits of register %eax, 
168, 
170 
%ax [x86-64] low-order 16 bits of register %rax, 
274 

B2T (binary to two¡¯s-complement conversion), 60, 67, 
89 
B2U (binary to unsigned conversion), 59, 
67, 
76, 
89 
background processes, 733¨C734 
backlogs for listening sockets, 905 

backups 
for 
disks, 
592 
backward taken, forward not taken (BTFNT) branch prediction strategy, 
407 
bad 
pointers 
and 
virtual 
memory, 
843 
badcnt.c [CS:APP] improperly synchronized 
program, 
957¨C 
960, 
958 
bandwidth, read, 621 
base 
registers, 
170 
bash [Unix] 
Unix 
shell 
program, 
733 
basic 
blocks, 
548 
Bell 
Laboratories, 
32 
Berkeley sockets, 901 
Berners-Lee, Tim, 
912 
best-.t block placement policy, 822, 
823 
%bh [IA32] bits 8¨C15 of register %ebx, 168 
%bh [x86-64] bits 8¨C15 of register %rbx, 274 
bi-endian ordering convention, 40 
biased number encoding, 103, 
103¨C 
106 
biasing 
in 
division, 
96¨C97 
big endian byte ordering, 40 
bigram 
statistics, 
542 
bijections, 59,61 
billions of .oating-point operations per 
second 
(giga.ops), 
525 
/bin/kill program, 
739¨C740 
binary .les, 3 
binary 
notation, 
30 
binary points, 100, 
100¨C101 
binary representations conversions with 
hexadecimal, 
34¨C35 
signed 
and 
unsigned, 
65¨C69 
to two¡¯s-complement, 60, 
67, 
89 
to 
unsigned, 
59 
fractional, 
100¨C103 
machine 
language, 
178¨C179 
binary semaphores, 964 
binary translation, 691¨C692 
binary 
tree 
structure, 
245¨C246 
bind [Unix] associate socket addr with descriptor, 
902, 
904, 904¨C905 
binding, lazy, 688, 689 
binutils 
package, 
690 
bistable 
memory 
cells, 
561 
bit-level 
operations, 
51¨C53 


bit 
representation, 
expansion, 
71¨C75 
bit 
vectors, 
48, 
49¨C50 
bits, 
3 
overview, 
30 
union access to, 
246 
%bl [IA32] bits 0¨C7 of register %ebx, 168 
%bl [x86-64] bits 0¨C7 of register %rbx, 274 
block and unblock signals function, 
753 
block 
offset 
bits, 
598 
block 
pointers, 
829 
block size caches, 
614 
minimum, 822 
blocked bit 
vectors, 
739 
blocked signals, 738, 
739, 
745 
blocking signals, 
753¨C754 
for temporal locality, 629 
blocks aligning, 
818 
allocated, 813, 
822¨C823 
vs. cache 
lines, 
615 
caches, 593, 
596, 
614 
coalescing, 
824, 
832 
epilogue, 
829 
free 
lists, 
820¨C822 
freeing, 
832 
heap, 813 
logical disk, 575, 
575¨C576, 
582 
prologue, 828 
referencing 
data 
in, 
847 
splitting, 
823 
in 
SSDs, 
582 
bodies, response, 915 
bool [HCL] 
bit-level 
signal, 
354 
Boole, George, 
48 
Boolean algebra and functions, 48 
HCL, 
354¨C355 
logic 
gates, 
353 
properties, 
49 
working 
with, 
48¨C51 
Boolean rings, 49 
bottlenecks, 
540 
Amdahl¡¯s 
law, 
545¨C547 
program 
pro.ling, 
540¨C545 
bottom 
of 
stack, 
173 
boundary 
tags, 
824¨C826, 
825, 
833 
bounded 
buffers, 
966, 
966¨C967 
bounds latency, 
496, 
502 

throughput, 
497, 
502 
BoundsChecker 
product, 
692 
%bp [x86-64] low-order 16 bits of register %rbp, 274 
%bpl [x86-64] bits 0¨C7 of register %rbp, 
274 
branch 
prediction, 
208¨C209, 
498, 
499 
misprediction handling, 434 
performance, 
526¨C531 
Y86 pipelining, 407 
branches, conditional, 
161, 
193, 
193¨C197 
break command in gdb, 255 
break statements with switch, 
215 
breakpoints, 
254¨C255 
bridged Ethernet, 888, 
889 
bridges Ethernet, 888 
I/O, 568 
browsers, 
911, 
912 
BSD Unix, 658 
.bss section, 659 
BTFNT (backward taken, forward not taken) branch prediction strategy, 
407 
bubbles, pipeline, 414, 
414¨C415, 
437¨C438 
buddies, 838 
buddy systems, 837, 
837¨C838 
buffer over.ow execution code regions limits for, 266¨C267 
memory-related 
bugs, 
844 
overview, 
256¨C261 
stack corruption detection for, 263¨C265 
stack 
randomization 
for, 
261¨C262 
vulnerabilities, 
7 
buffered 
I/O 
functions, 
868¨C872 
buffers bounded, 966, 
966¨C967 
read, 
868, 
870¨C871 
store, 
534¨C535 
streams, 
879¨C880 
bus 
transactions, 
567 
buses, 8, 
567 
designs, 
568 
I/O, 576 
memory, 568 
%bx [IA32] low-order 16 bits of register %ebx, 
168 
%bx [x86-64] low-order 16 bits of register %rbx, 
274 

bypassing 
for 
data 
hazards, 
416¨C418 
byte 
order, 
39¨C46 
disassembled 
code, 
193 
network, 
893 
unions, 
247 
bytes, 3, 33 
copying, 
125 
range, 
34 
register 
operations, 
169 
Y86 
encoding, 
340¨C341 

C language assembly 
code 
with, 
266¨C267 
bit-level 
operations, 
51¨C53 
.oating-point representation, 114¨C117 
history, 
4, 
32 
logical 
operations, 
54 
shift 
operations, 
54¨C56 
static 
libraries, 
667¨C670 
C++ 
language, 
661 
linker 
symbols, 
663¨C664 
objects, 
241¨C242 
reference 
parameters, 
226 
software 
exceptions, 
703¨C704, 
760 
.c source .les, 
4¨C5, 
655 
C 
standard 
library, 
4¨C5, 
5 
C90 standard, 32 
C99 standard, 32 
integral 
data 
types, 
58 
long 
long 
integers, 
39 
cache 
block 
offset 
(CO), 
797 
cache blocks, 596 
cache-friendly 
code, 
616, 
616¨C620 
cache lines cache 
sets, 
596 
vs. sets and blocks, 615 
cache 
oblivious 
algorithms, 
630 
cache pollution, 717 
cache 
set 
index 
(CI), 
797 
cache tags (CT), 797 
cached 
pages, 
780 
caches and cache memory, 592, 
596 
address 
translation, 
797 
anatomy, 
612¨C613 
associativity, 
614¨C615 
cache-friendly 
code, 
616, 
616¨C620 
data, 
499, 
612, 
613 
direct-mapped. See direct-mapped caches DRAM, 
780 
fully 
associative, 
608¨C609 
hits, 593 


importance, 
12¨C13 
instruction, 
498, 
612, 
613 
locality 
in, 
587, 
625¨C629, 
784 
managing, 595 
memory 
mountains, 
621¨C625 
misses, 
448, 
594, 
594¨C595 
overview, 
592¨C593 
page 
allocation, 
783¨C784 
page faults, 782, 
782¨C783 
page 
hits, 
782 
page tables, 780, 780¨C781 
performance, 
531, 
614¨C615, 
620¨C 

629 
practice 
problems, 
609¨C611 
proxy, 915 
purpose, 
560 
set associative, 606, 606¨C608 
size, 
614 
SRAM, 780 
symbols, 
598 
virtual 
memory 
with, 
779¨C784, 
791 
write 
issues, 
611¨C612 
write 
strategies, 
615 
Y86 
pipelining, 
447¨C448 

call [IA32/1486] procedure call, 221¨C222, 
339 

call [Y86] instruction de.nition, 
339 
instruction code for, 384 
pipelined 
implementations, 
407 
processing 
steps, 
372 

callee procedures, 220, 
223¨C224, 
285 
callee 
saved 
registers, 
223, 
287, 
289 
caller procedures, 220, 223¨C224, 
285 
caller 
saved 
registers, 
223, 
287 
calling environments, 759 
calloc function 
dynamic memory allocation, 814¨C815 

security 
vulnerability, 
92 
callq [x86-64] 
procedure 
call, 
282 
calls, 17, 707, 
707¨C708 

error handling, 
717¨C718 
Linux/IA32 
systems, 
710¨C711 
performance, 
490¨C491 
slow, 745 

canary 
values, 
263¨C264 
canceling mispredicted branch handling, 434 

capacity caches, 597 
disks, 571, 
571¨C573 

capacity misses, 595 
cards, graphics, 577 
carry .ag condition code (CF), 185 
CAS (Column Access Strobe) 
requests, 563 
case expressions in HCL, 357, 357¨C359 

casting, 
42 
.oating-point 
values, 
115¨C116 
pointers, 
252¨C253, 
827 
signed 
values, 
65¨C66 

catching signals, 738, 
740, 744 

cells DRAM, 562, 
563 
SRAM, 561 

central processing units (CPUs), 9, 
9¨C10, 
497 
Core i7. See Core i7 microproces-
sors early 
instruction 
sets, 
342 
effective cycle time, 585 
embedded, 
344 
Intel. See Intel microprocessors logic design. See logic design many-core, 
449 
multi-core, 
16, 
22, 
158, 
586, 
934 
overview, 
334¨C336 
pipelining. See pipelining RAM, 
363 
sequential Y86 implementation. 
See sequential Y86 implemen-
tation superscalar, 
24, 
448¨C449, 
497 
trends, 
584¨C585 
Y86. See Y86 instruction set 
architecture Cerf, 
Vinton, 
900 
CERT (Computer Emergency 
Response 
Team), 
92 
CF [IA32/x86-64] carry .ag condition code, 185 
CGI (Common Gateway Interface) program, 916¨C917 
%ch [IA32] bits 8¨C15 of register %ecx, 168 
%ch [x86-64] bits 8¨C15 of register 
%rcx, 
274 
chains, proxy, 915 
char data 
type, 
57, 
270 
character 
codes, 
46 
check-clients function, 
943, 
946 
child processes, 720 

creating, 
721¨C723 
default 
behavior, 
724 

error 
conditions, 
725¨C726 
exit 
status, 
725 
reaping, 723, 
723¨C729 
waitpid function, 
726¨C729 

CI (cache set index), 797 

circuits combinational, 354, 
354¨C360 
retiming, 401 
sequential, 361 

CISC (complex instruction set computers), 342, 
342¨C344 
%cl [IA32] bits 0¨C7 of register %ecx, 168 
%cl [x86-64] bits 0¨C7 of register %rcx, 
274 
Clarke, Dave, 
900 
classes 
data 
hazards, 
412¨C413 
exceptions, 
706¨C708 
instructions, 
171 
size, 836 
storage, 
956 

clear signal set function, 753 
client-server model, 886, 
886¨C887 
clienterror [CS:APP] Tiny helper 
function, 922¨C923 

clients client-server model, 886 
telnet, 20¨C21 

clock 
signals, 
361 
clocked 
registers, 
380¨C381 
clocking 
in 
logic 
design, 
361¨C363 
close [Unix] close .le, 865 
close operations for .les, 863, 
865 
close shared library function, 685 
cltd [IA32] convert double word to 
quad 
word, 
182, 
184 
cltq [x86-64] convert double word to 
quad 
word, 
279 
cmova [IA32/x86-64] move if unsigned 
greater, 
210 
cmovae [IA32/x86-64] move if unsigned greater or equal, 
210 
cmovb [IA32/x86-64] move if unsigned 
less, 
210 
cmovbe [IA32/x86-64] move if unsigned less or equal, 
210 
cmove [IA32/x86-64] move when equal, 
210, 
339 
cmovg [IA32/x86-64] move if greater, 210, 
339 
cmovge [IA32/x86-64] move if greater or 
equal, 
210, 
339 


cmovl [IA32/x86-64] move if less, 210, 
339 

cmovle [IA32/x86-64] move if less or equal, 
210, 
339 

cmovna [IA32/x86-64] move if not unsigned 
greater, 
210 

cmovnae [IA32/x86-64] move if unsigned greater or equal, 
210 

cmovnb [IA32/x86-64] move if not unsigned 
less, 
210 

cmovnbe [IA32/x86-64] move if not unsigned less or equal, 
210 

cmovne [IA32/x86-64] move if not equal, 
210, 
339 

cmovng [IA32/x86-64] move if not greater, 
210 

cmovnge [IA32/x86-64] move if not greater 
or 
equal, 
210 

cmovnl [IA32/x86-64] move if not less, 
210 

cmovnle [IA32/x86-64] move if not less or equal, 
210 

cmovns [IA32/x86-64] move if nonnegative, 210 
cmovnz [IA32/x86-64] move if not zero, 
210 

cmovs [IA32/x86-64] move if negative, 
210 

cmovz [IA32/x86-64] move if zero, 210 

cmp [IA32/x86-64] 
compare, 
186, 
280 

cmpb [IA32/x86-64] compare byte, 186 

cmpl [IA32/x86-64] compare double word, 
186 

cmpq [x86-64] compare quad word, 280 

cmpw [IA32/x86-64] compare word, 186 

cmtest 
script, 
443 


CO (cache block offset), 797 

coalescing 
blocks, 
832 
with 
boundary 
tags, 
824¨C826 
free, 824 
memory, 820 

Cocke, John, 
342 


code performance 
strategies, 
539 
pro.lers, 
540¨C545 
representing, 
47 
self-modifying, 413 
Y86 instructions, 339, 
341 

code motion, 487 
code segments, 678, 
679¨C680 
COFF (Common Object File format), 
658 
Cohen, 
Danny, 
41 
cold caches, 594 
cold misses, 594 
Cold 
War, 
900 
collectors, garbage, 813, 
838 

basics, 
839¨C840 
conservative, 839, 
842 
Mark&Sweep, 
840¨C842 

Column Access Strobe (CAS) 
requests, 563 
column-major sum function, 617 
combinational circuits, 354, 
354¨C360 
Common Gateway Interface (CGI) 
program, 916¨C917 
Common Object File format (COFF), 658 
Compaq Computer Corp. RISC 
processors, 
343 
compare byte instruction (cmpb), 
186 
compare double word instruction 
(cmpl), 
186 
compare 
instructions, 
186, 
280 
compare quad word instruction 
(cmpq), 
280 
compare word instruction (cmpw), 
186 
compilation 
phase, 
5 
compilation systems, 5, 
6¨C7 
compile time, 654 
compiler drivers, 4, 
655¨C657 
compilers, 5, 
154 

optimizing capabilities and 
limitations, 
476¨C480 
process, 
159¨C160 
purpose, 
162 

complement instruction (Not), 
178 
complex instruction set computers 
(CISC), 342, 
342¨C344 
compulsory misses, 594 
computation stages in pipelining, 
400¨C401 
computational 
pipelines, 
392¨C393 
computed goto, 216 
Computer Emergency Response 
Team (CERT), 
92 
computer systems, 2 
concurrency, 
934 

ECF 
for, 
703 
.ow 
synchronizing, 
755¨C759 
and 
parallelism, 
21¨C22 

run, 713 

thread-level, 
22¨C23 
concurrent execution, 713 
concurrent .ow, 713, 
713¨C714 
concurrent processes, 16 
concurrent 
programming, 
934¨C935 

deadlocks, 
985¨C988 
with 
I/O 
multiplexing, 
939¨C947 
library 
functions 
in, 
982¨C983 
with 
processes, 
935¨C939 
races, 
983¨C985 
reentrancy 
issues, 
980¨C982 
shared 
variables, 
954¨C957 
summary, 
988¨C989 
threads, 
947¨C954 

for 
parallelism, 
974¨C978 

safety issues, 979¨C980 
concurrent programs, 934 
concurrent servers, 934 

based 
on 
I/O 
multiplexing, 
939¨C947 
based 
on 
prethreading, 
970¨C973 
based 
on 
processes, 
936¨C937 
based 
on 
threads, 
952¨C954 

condition code registers de.nition, 
185 
hazards, 
413 
SEQ 
timing, 
380¨C381 

condition codes, 185, 
185¨C187 
accessing, 
187¨C189 
Y86, 
337¨C338 

condition 
variables, 
970 
conditional 
branches, 
161, 
193, 
193¨C197 
conditional move instructions, 
206¨C 

213, 
373, 
388-389, 
527, 
529¨C530 
conditional 
x86-64 
operations, 
270 
con.ict misses, 594, 
603¨C606 
connect [Unix] establish connection 
with server, 903 
connected descriptors, 907, 
908 
connections 
EOF 
on, 
909 
Internet, 892, 
899¨C900 
I/O 
devices, 
576¨C578 
persistent, 915 

conservative garbage collectors, 839, 

842 
constant words, 340 
constants 
free 
lists, 
829¨C830 
maximum 
and 
minimum 
values, 
63 
multiplication, 
92¨C95 
for 
ranges, 
62 
Unix, 
725 

content dynamic, 
916¨C919 
serving, 912 
Web, 911, 
912¨C914 


context switches, 16, 
716¨C717 

contexts, 716 
processes, 16, 712 
thread, 947, 955 


continue command 
in 
ADB, 
255 
Control Data Corporation 6600 processor, 
500 
control dependencies in pipelining, 399, 
408 
control .ow exceptional. See exceptional control .ow (ECF) 
logical, 712, 
712¨C713 
control 
hazards, 
408 
control instructions for x86-64 
processors, 
279¨C282 
control logic blocks, 377, 
379, 
383, 
405 
control 
logic 
in 
pipelining, 
431 
control mechanism combinations, 
438¨C440 
control 
mechanisms, 
437¨C438 
design testing and verifying, 
442¨C444 
implementation, 
440¨C442 
special 
control 
cases, 
432¨C436 
special 
control 
conditions, 
436¨C437 

control 
structures, 
185 
condition 
codes, 
185¨C189 
conditional 
branches, 
193¨C197 
conditional move instructions, 
206¨C213 
jumps, 
189¨C193 
loops. See loops optimization 
levels, 
254 
switch statements, 
213¨C219 

control 
transfer, 
221¨C223, 
702 

controllers disk, 575, 
575¨C576 
I/O 
devices, 
8 
memory, 563, 
564 


conventional 
DRAMs, 
562¨C564 
conversions 
binary with 
hexadecimal, 
34¨C35 
signed 
and 
unsigned, 
65¨C69 
to two¡¯s-complement, 60, 
67, 
89 
to 
unsigned, 
59 

.oating-point 
values, 
115¨C116 
lowercase, 
487¨C489 
convert active socket to listening socket function, 905 
convert application-to-network function, 894 
convert double word to quad word instruction, 
182, 
279 
convert host-to-network long function, 893 
convert host-to-network short function, 893 
convert network-to-application function, 894 
convert network-to-host long function, 893 
convert network-to-host short function, 893 
convert quad word to oct word 
instruction (cqto), 
279 

coprocessors, 
292 

copy_elements function, 
91¨C92 

copy .le descriptor function, 878 

copy_from_kernel function, 
78¨C79 

copy-on-write technique, 808¨C809 

copying bytes 
in 
memory, 
125 
descriptor 
tables, 
878 
text 
.les, 
870 

Core 
2 
microprocessors, 
158, 
568 

Core 
i7 
microprocessors, 
22¨C23, 
158 
address 
translation, 
800¨C803 
branch misprediction penalty, 208¨C209 
caches, 
613 
CPE 
performance, 
485¨C486 
functional unit performance, 500¨C502 
load 
performance, 
531 
memory 
mountain, 
623 
operation, 
497¨C500 
out-of-order 
processing, 
500 
page 
table 
entries, 
800¨C802 
performance, 
273 
QuickPath 
interconnect, 
568 
virtual 
memory, 
799¨C803 

core memory, 737 

cores in 
multi-core 
processors, 
158, 
586, 
934 

counting semaphores, 964 

CPE (cycles per element) metric, 480, 
482, 
485¨C486 

cpfile [CS:APP] text .le copy, 870 

CPI (cycles per instruction) .ve-stage 
pipelines, 
448¨C449 
in 
performance 
analysis, 
444¨C446 

CPUs. See central processing units (CPUs) 
cqto [x86-64] convert quad word to oct 
word, 
279 

CR3 register, 800 

create/change environment variable function, 732 

create child process function, 720, 721¨C723 

create thread function, 950 

critical 
paths, 
476, 
502, 
506¨C507, 
513, 
517, 
521¨C522 

critical sections in progress graphs, 
961 

CS:APP header 
.les, 
725 
wrapper 
functions, 
718, 
999 

csapp.c [CS:APP] CS:APP wrapper functions, 
718, 
999 

csapp.h [CS:APP] CS:APP header .le, 
718, 
725, 
999 

csh [Unix] 
Unix 
shell 
program, 
733 

CT (cache tags), 797 

ctest 
script, 
443 

ctime function, 
982¨C983 

ctime_ts [CS:APP] thread-safe non-reentrant wrapper for ctime, 
981 

ctrl-c keys nonlocal 
jumps, 
760, 
762 
signals, 
738, 
740, 
771 

ctrl-z keys, 
741, 
771 

%cx [IA32] low-order 16 bits of register %ecx, 274 

%cx [x86-64] low-order 16 bits of register %rcx, 274 

cycles per element (CPE) metric, 480, 
482, 
485¨C486 

cycles per instruction (CPI) .ve-stage 
pipelines, 
448¨C449 
in 
performance 
analysis, 
444¨C446 

cylinders disk, 571 
spare, 576, 
581 

d-caches 
(data 
caches), 
499, 
612, 
613 

data conditional 
transfers, 
206¨C213 
forwarding, 
415¨C418, 
416 
sizes, 
38¨C39 
data alignment, 248, 248¨C251 
data 
caches 
(d-caches), 
499, 
612, 
613 
data 
dependencies 
in 
pipelining, 
398, 


408¨C410 
data-.ow 
graphs, 
502¨C507 
data formats in machine-level 
programming, 
167¨C168 

data hazards classes, 
412¨C413 
forwarding 
for, 
415¨C418 
load/use, 
418¨C421 
stalling, 
413¨C415 
Y86 
pipelining, 
408¨C412 

data 
memory 
in 
SEQ 
timing, 
380 
data movement instructions, 
171¨C 
177, 
275¨C277 

data references locality, 
587¨C588 
PIC, 
687¨C688 

.data section, 659 
data segments, 679 
data structures 

heterogeneous. See heterogeneous data structures 
x86-64 
processors, 
290¨C291 
data types. See types database 
transactions, 
887 
datagrams, 892 
ddd debugger, 254 
DDR SDRAM (Double Data-Rate 
Synchronous DRAM), 566 
deadlocks, 985, 
985¨C988 
deallocate heap storage function, 815 
.debug section, 659 
debugging, 
254¨C256 
dec [IA32/x86-64] 
decrement, 
178 
decimal 
notation, 
30 
decimal 
system 
conversions, 
35¨C37 
declarations 
arrays, 
232¨C233, 
238 
pointers, 
39 
public 
and 
private, 
661 
structures, 
241¨C244 
unions, 
244¨C245 

decode stage instruction 
processing, 
364, 
366, 

368¨C377 
PIPE 
processor, 
426¨C429 
SEQ, 
385¨C387 

decoding 
instructions, 
498 
decrement instruction (dec), 
178¨C179 
deep 
copies, 
982 
deep 
pipelining, 
397¨C398 
default actions with signal, 742 
default behavior for child processes, 
724 
deferred coalescing, 824 
#define preprocessor directive 
constants, 
237 

macro expansion, 
160 
delete command 
in 
GDB, 
255 
delete environment variable 
function, 732 
DELETE 
method 
in 
HTTP, 
915 
delete signal from signal set function, 
753 
delivering signals, 738 
delivery mechanisms for protocols, 
890 
demand paging, 783 
demand-zero pages, 807 
demangling process, 663, 
663¨C664 
DeMorgan¡¯s 
laws, 
461 
denormalized .oating-point value, 
105, 
105¨C110 
dependencies control 
in 
pipelining 
systems, 
399, 
408 
data 
in 
pipelining 
systems, 
398, 

408¨C410 
reassociation 
transformations, 
521 
write/read, 
534¨C536 

dereferencing 
pointers, 
44, 
175¨C176, 

234, 
252, 
843 
descriptor 
sets, 
939, 
940 
descriptor 
tables, 
875¨C876, 
878 
descriptors, 863 

connected and listening, 907, 
908 

socket, 902 
destination hosts, 889 
detach thread function, 951 
detached threads, 951 
detaching 
threads, 
951¨C952 
%dh [IA32] bits 8¨C15 of register %edx, 
168 
%dh [x86-64] bits 8¨C15 of register %rdx, 274 
%di [x86-64] low-order 16 bits of register %rdi, 
274 

diagrams hardware, 
377 
pipeline, 392 

Digital Equipment Corporation Alpha 
processor, 
268 
VAX computer Boolean 
operations, 
53 
Dijkstra, 
Edsger, 
963¨C964 

%dil [x86-64] bits 0¨C7 of register %rdi, 
274 

DIMM (Dual Inline Memory Module), 564 

direct 
jumps, 
190 

direct-mapped caches, 599 
con.ict 
misses, 
603¨C606 
example, 
601¨C603 
line 
matching, 
599¨C600 
line 
replacement, 
600¨C601 
set 
selection, 
599 
word 
selection, 
600 

direct memory access (DMA), 
10, 

579 

directives, assembler, 
166, 
346 

directory .les, 874 

dirty bits in cache, 612 
Core i7, 801 

dirty pages, 801 

disassemble command in GDB, 255 

disassemblers, 
41, 
64, 
163, 
164¨C165 

disks, 
570 
accessing, 
578¨C580 
anatomy, 
580¨C581 
backups, 
592 
capacity, 571, 
571¨C573 
connecting, 
576¨C578 
controllers, 575, 
575¨C576 
geometry, 570¨C571 
logical 
blocks, 
575¨C576 
operation, 
573¨C575 
trends, 
584¨C585 

distributing 
software, 
684 

division instructions, 
182¨C184, 
279 
Linux/IA32 
system 
errors, 
709 
by 
powers 
of 
two, 
95¨C98 

divl [IA32/x86-64] unsigned divide, 182, 
184 

divq [x86-64] 
unsigned 
divide, 
279 

DIXtrac tool, 580, 
580¨C581 

%dl [IA32] bits 0¨C7 of register %edx, 168 

%dl [x86-64] bits 0¨C7 of register %rdx, 274 

dlclose [Unix] close shared library, 
685 

dlerror [Unix] report shared library error, 685 

DLLs (Dynamic Link Libraries), 682 
dlopen [Unix] open shared libary, 

684 


dlsym [Unix] get address of shared library symbol, 684 
DMA 
(direct 
memory 
access), 
10, 

579 
DMA transfer, 579 
DNS (Domain Name System), 896 
dns_error [CS:APP] reports DNS-
style errors, 1001 
DNS-style 
error 
handling, 
1000, 
1001 
do [C] variant of while loop, 
197¨C200 
doit [CS:APP] Tiny helper function, 
920, 
921 
dollar signs ($) for immediate 
operands, 
169 
domain names, 892, 
895¨C899 
Domain Name System (DNS), 896 
dotprod [CS:APP] vector dot 
product, 603 
dots (.) in dotted-decimal notation, 
893 
dotted-decimal notation, 893, 
894 
double [C] double-precision .oating 
point, 
114, 
115 
Double Data-Rate Synchronous 
DRAM (DDR SDRAM), 566 
double data 
type, 
270¨C271 
double-precision representation 
C, 
39, 
114¨C117 
IEEE, 103, 
104 
machine-level 
data, 
168 

double 
words, 
167 
DRAM. See Dynamic RAM 
(DRAM) DRAM arrays, 562 
DRAM cells, 562, 
563 
drivers, compiler, 4, 
655¨C657 
Dual Inline Memory Module 
(DIMM), 564 
dup2 [Unix] copy .le descriptor, 878 
%dx [IA32] low-order 16 bits of 
register %edx, 
168 
%dx [x86-64] low-order 16 bits of 
register %rdx, 
274 
dynamically 
generated 
code, 
266 
dynamic 
content, 
684, 
916¨C919 
Dynamic Link Libraries (DLLs), 682 
dynamic linkers, 682 
dynamic 
linking, 
681¨C683, 
682 
dynamic memory allocation 
allocated 
block 
placement, 
822¨C 
823 

allocator 
design, 
827¨C832 
allocator requirements and goals, 817¨C819 
coalescing with boundary tags, 
824¨C826 
coalescing 
free 
blocks, 
824 
explicit 
free 
lists, 
835 
fragmentation, 
819¨C820 
heap 
memory 
requests, 
823 
implementation 
issues, 
820 
implicit 
free 
lists, 
820¨C822 
malloc and free functions, 
814¨C816 
overview, 
812¨C814 
purpose, 
816¨C817 
segregated 
free 
lists, 
836¨C838 
splitting 
free 
blocks, 
823 

dynamic memory allocators, 813¨C 
814 

Dynamic 
RAM 
(DRAM), 
9, 
562 
caches, 780, 782, 
782¨C783 
conventional, 
562¨C564 
enhanced, 
565¨C566 
historical 
popularity, 
566 
modules, 564, 
565 
vs. SRAM, 
562 
trends, 
584¨C585 

dynamic Web content, 912 

E-way 
set 
associative 
caches, 
606 
%eax [x86-64] low-order 32 bits of 
register %rax, 
274 
%eax [IA32/Y86] 
register, 
168, 
337 
%ebp [x86-64] low-order 32 bits of 
register %rbp, 
274 
%ebp [IA32/Y86] frame pointer register, 
168, 
337 
%ebx [x86-64] low-order 32 bits of 
register %rbx, 
274 
%ebx [IA32/Y86] 
register, 
168, 
337 
ECF. See exceptional control .ow 
(ECF) ECHILD 
return 
code, 
725, 
727 
echo function, 
257¨C258, 
263 
echo [CS:APP] read and echo input 
lines, 911 
echo_cnt [CS:APP] counting version of echo, 971, 
973 
echoclient.c [CS:APP] echo client, 908¨C909, 909 
echoserveri.c [CS:APP] iterative echo server, 
908, 
910 
echoservers.c [CS:APP] 
Index 1019 
concurrent echo server based on 
I/O 
multiplexing, 
944 

echoservert.c [CS:APP] concurrent echo server based on threads, 953 

echoservert_pre.c [CS:APP] prethreaded concurrent echo server, 972 

%ecx [x86-64] low-order 32 bits of 
register %rcx, 
274 
%ecx [IA32/x86-64] 
register, 
168, 
274 
%edi [x86-64] low-order 32 bits of 
register %rdi, 
274 
%edi [IA32/x86-64] 
register, 
168, 
274 
EDO DRAM (Extended Data Out 
DRAM), 566 
%edx [x86-64] low-order 32 bits of 
register %rdx, 
274 
%edx [IA32/Y86] 
register, 
168, 
337 
EEPROMs (Electrically Erasable 
Programmable ROMs), 567 
effective 
addresses, 
170, 
673 
effective cycle time, 585 
ef.ciency of parallel programs, 977, 
978 
EINTR 
return 
code, 
725 
%eip [IA32] 
program 
counter, 
161 
Electrically Erasable Programmable 
ROMs (EEPROMs), 567 
ELF. See Executable and Linkable 
Format (ELF) EM64T 
processor, 
158 
embedded 
processors, 
344 
encapsulation, 890 
encodings in machine-level 
programs, 
159¨C160 
code 
examples, 
162¨C165 
code 
overview, 
160¨C161 
Y86 
instructions, 
339¨C342 

end-of-.le (EOF) condition, 863, 

909 
entry points, 678, 679 
environment 
variables 
lists, 
731¨C732 
EOF (end-of-.le) condition, 863, 
909 
ephemeral ports, 899 
epilogue 
blocks, 
829 
EPIPE error return 
code, 
927 
Erasable Programmable ROMs 
(EPROMs), 567 
errno [Unix] Unix error variable, 
1000 

error-correcting codes for memory, 562 


error handling system 
calls, 
717¨C718 
Unix 
systems, 
1000¨C1001 
wrappers, 718, 999, 
1001¨C1003 
error-reporting functions, 718 
errors child 
processes, 
725¨C726 
link-time, 
7 
off-by-one, 
845 
race, 755, 
755¨C759 
reporting, 
1001 
synchronization, 957 
%esi [x86-64] low-order 32 bits of register %rsi, 274 
%esi [IA32/Y86] 
register, 
168, 
337 
%esp [x86-64] low-order 32 bits of stack 
pointer 
register 
%rsp, 
274 
%esp [IA32/Y86] stack pointer register, 
168, 
337 
establish connection with server functions, 903¨C904 
establish listening socket function, 905, 
905¨C906 
etest 
script, 
443 
Ethernet segments, 888, 
889 
Ethernet technology, 888 
EUs 
(execution 
units), 
497, 
499 
eval [CS:APP] shell helper routine, 734, 735 
event-driven programs, 942 
based 
on 
I/O 
multiplexing, 
942¨C947 
based on threads, 
973 
events, 703 
scheduling, 
743 
state machines, 942 
evicting blocks, 594 
exabytes, 
270 
exact-size 
integer 
types, 
62¨C63 
excepting 
instructions, 
421 
exception handlers, 704, 
705 
exception handling in 
instruction 
processing, 
364¨C365 
Y86, 
344¨C345, 
420¨C423, 
435¨C436 
exception numbers, 705 
exception table base registers, 705 
exception 
tables, 
704, 
705 
exceptional control .ow (ECF), 702 
exceptions, 
703¨C711 
importance, 
702¨C703 
nonlocal 
jumps, 
759¨C762 
process control. See processes signals. See signals summary, 
763 

system 
call 
error 
handling, 
717¨C718 
exceptions, 703 
anatomy, 
703¨C704 
classes, 
706¨C708 
data 
alignment, 
249 
handling, 
704¨C706 
Linux/IA32 
systems, 
708¨C711 
status code for, 384 
synchronous, 707 
Y86, 
337 
exclamation points (!) for Not operation, 
54, 
353 
Exclusive-Or Boolean operation, 48 
exclusive-or instruction (xor) IA32, 
178 
Y86, 338 
Executable and Linkable Format (ELF), 658 
executable 
object 
.les, 
678¨C679 
headers, 
658¨C659 
relocation, 
673 
segment header tables, 678 
symbol 
tables, 
660¨C662 
executable 
code, 
160 
executable object .les, 4 
creating, 656 
description, 657 
loading, 
679¨C681 
running, 
7 
segment 
header 
tables, 
678¨C679 
executable object programs, 4 
execute 
access, 
266 
execute 
disable 
bit, 
801 
execute stage instruction 
processing, 
364, 
366, 
368¨C377 
PIPE 
processor, 
429¨C430 
SEQ, 
387¨C389 
execution concurrent, 713 
parallel, 714 
speculative, 498, 
499, 
527 
tracing, 
367, 
369¨C370, 
373¨C375, 
382 
executable 
code 
regions, 
266¨C267 
execution 
units 
(EUs), 
497, 
499 
execve [Unix] load program, 730 
arguments and environment variables, 
730¨C732 
child 
processes, 
681, 
684 
loading 
programs, 
679 
running 
programs, 
733¨C736 
virtual 
memory, 
810 

exit [C Stdlib] terminate process, 680, 
719 
exit status, 719, 
725 
expanding 
bit 
representation, 
71¨C75 
expansion slots, 577 
explicit allocator requirements and goals, 
817¨C819 
explicit dynamic memory allocators, 
813 

explicit 
free 
lists, 
835 
explicit thread termination, 950 
explicitly reentrant functions, 981 
exploit 
code, 
260¨C261 
exponents in .oating-point representation, 103 
extend_heap [CS:APP] allocator: extend 
heap, 
830, 
831 
Extended Data Out DRAM (EDO DRAM), 566 
extended precision .oating-point representation, 128 
IA32, 
116 
machine-level 
data, 
168 
x86-64 
processors, 
271 
external 
exceptions 
in 
pipelining, 
420 
external fragmentation, 819, 
819¨C820 

fall through in switch statements, 215 
false fragmentation, 824 
Fast Page Mode DRAM (FPM DRAM), 566 
fault exception class, 706 
faulting instructions, 707 
faults, 
708 
Linux/IA32 
systems, 
709, 
806¨C807 
Y86 
pipelining 
caches, 
448 
FD_CLR [Unix] clear bit in descriptor set, 939, 
940 
FD_ISSET [Unix] bit turned on in descriptor set?, 939, 
940, 
942 
FD_SET [Unix] set bit in descriptor set, 939, 
940 
FD_ZERO [Unix] clear descriptor set, 939, 
940 
feedback 
in 
pipelining, 
398¨C400, 
403 
feedback 
paths, 
375, 
399 
fetch .le metadata function, 873¨C874 
fetch stage instruction 
processing, 
364, 
366, 
368¨C377 
PIPE 
processor, 
424¨C425 
SEQ, 
383¨C385 


fetches, locality, 
588¨C589 
fgets function, 
258 
Fibonacci 
(Pisano), 
30 
.eld-programmable gate arrays 
(FPGAs), 
444 
FIFOs, 
937 
.le descriptors, 863 
.le position, 863 
.le tables, 716, 
875 
.le type, 
879 
.les, 19 


as abstraction, 
25 
anonymous, 807 
binary, 3 
metadata, 
873¨C875 
object. See object .les register, 9, 161, 
339¨C340, 
362¨C363, 

380, 
499 
regular, 807, 874 
sharing, 
875¨C877 
system-level I/O. See system-level 
I/O 

Unix, 862, 
862¨C863 
fingerd daemon, 
260 
finish command 
in 
GDB, 
255 
.rmware, 567 
.rst .t block placement policy, 822, 

823 
.rst-level domain names, 896 
.rst readers-writers problem, 969 
.ts, segregated, 836, 
837 
.ve-stage 
pipelines, 
448¨C449 
.xed-size 
arrays, 
237¨C238 
.ash memory, 567 
.ash 
translation 
layers, 
582¨C583 
.at addressing, 159 
float [C] single-precision .oating 
point, 114, 270 
.oating-point representation and 
programs, 
99¨C100 
architecture, 292 
arithmetic, 
31 
C, 
114¨C117 
denormalized values, 105, 
105¨C110 
encodings, 
30 
extended 
precision, 
116, 
128 
fractional 
binary 
numbers, 
100¨C 

103 
IEEE, 
103¨C105 
machine-level representation, 
292¨C293 
normalized value, 103, 
103¨C104 
operations, 
113¨C114 
over.ow, 
116¨C117 
pi, 
131 
rounding, 
110¨C113 
special 
values, 
105 
SSE 
architecture, 
292 
x86-64 
processors, 
270, 
492 
x87 
architecture, 
156¨C157, 
292 

.ows concurrent, 713, 713¨C714 
control, 702 
logical, 712, 
712¨C713 
parallel, 713¨C714 
synchronizing, 
755¨C759 

.ushed 
instructions, 
499 
FNONE [Y86] default function code, 
384 
footers of blocks, 825 
for [C] general loop statement, 
203¨C206 
forbidden regions, 964 
foreground processes, 734 
fork [Unix] create child process, 720 

child 
processes, 
684 
example, 
721¨C723 
running 
programs, 
733¨C736 
virtual 
memory, 
809¨C810 

fork.c [CS:APP] fork example, 721 
formal 
veri.cation, 
443¨C444 
format 
strings, 
43 
formats for machine-level data, 
167¨C168 
formatted disk capacity, 576 
formatted 
printing, 
43 
formatting 
disks, 
576 
machine-level 
code, 
165¨C167 

forwarding for 
data 
hazards, 
415¨C418 
load, 456 

forwarding 
priority, 
427¨C428 
FPGAs (.eld-programmable gate arrays), 
444 
FPM DRAM (Fast Page Mode 
DRAM), 566 
fprintf [C 
Stdlib] 
function, 
43 
fractional 
binary 
numbers, 
100¨C103 
fractional .oating-point representa-
tion, 
103¨C110, 
128 
fragmentation, 819 
dynamic memory allocation, 819¨C820 
false, 824 
frame 
pointer, 
219 

frames Ethernet, 888 
stack, 219, 
219¨C221, 
249, 
284¨C287 

free [C Stdlib] deallocate heap storage, 815, 
815¨C816 

free blocks, 813 
coalescing, 
824 
splitting, 
823 

free bounded buffer function, 968 
free heap block function, 833 
free heap blocks, referencing data in, 
847 

free lists creating, 
830¨C832 
dynamic memory allocation, 
820¨C822 
explicit, 
835 
implicit, 822 
manipulating, 
829¨C830 
segregated, 
836¨C838 

free 
software, 
6 
FreeBSD open source operating 
system, 
78¨C79 
freeing 
blocks, 
832 
Freescale 
processor 
family, 
334 

RISC 
design, 
342 
front 
side 
bus 
(FSB), 
568 
fstat [Unix] fetch .le metadata, 
873¨C874 
full duplex connections, 899 
full 
duplex 
streams, 
880 
fully associative caches, 608, 
608¨C609 
fully linked executable object .les, 
678 
fully pipelined functional units, 501 
function calls 
performance 
strategies, 
539 
PIC, 
688¨C690 
function codes in Y86 instructions, 
339¨C340 
functional 
units, 
499¨C502 
functions 
parameter 
passing 
to, 
226 
pointers 
to, 
253 
reentrant, 980 
static 
libraries, 
667¨C670 
system-level, 710 
thread-safe and thread-unsafe, 
979, 
979¨C981 
-funroll-loops option, 
512 

gaps, disk sectors, 571, 
576 
garbage, 
838 
garbage collection, 814, 
838 


garbage collectors, 813, 
838 
basics, 
839¨C840 
conservative, 839, 842 
Mark&Sweep, 
840¨C842 

overview, 
838¨C839 
gates, 
logic, 
353 
gcc (GNU Compiler Collection) 
compiler ATT 
format 
for, 
294 
code 
formatting, 
165¨C166 
inline 
substitution, 
479 
loop 
unrolling, 
512 
optimizations, 
254¨C256 
options, 
32¨C33, 
476 
support for SIMD instructions, 
524¨C525 
working 
with, 
159¨C160 
gdb GNU 
debugger, 
163, 
254, 
254¨C256 
general 
protection 
faults, 
709 
general-purpose registers 
IA32, 
168¨C169 
x86-64, 
273¨C275 
Y86, 
336¨C337 


geometry of disks, 570¨C571 
get address of shared library symbol 
function, 685 
get DNS host entry functions, 896 
¡°get 
from¡± 
operator 
(C++), 
862 
GET method in HTTP, 915 
get 
parent 
process 
ID 
function, 
719 
get process group ID function, 739 
get process ID function, 719 
get thread ID function, 950 
getenv [C Stdlib] read environment 
variable, 732 
gethostbyaddr [Unix] get DNS host entry, 896, 
982¨C983 
gethostbyname [Unix] get DNS host 
entry, 896, 
982¨C983 
getpeername function, 
78¨C79 
getpgrp [Unix] get process group 
ID, 739 
getpid [Unix] get process ID, 719 
getppid [Unix] get parent process 
ID, 719 
getrusage [Unix] 
function, 
784 
gets function, 
256¨C259 
GHz (gigahertz), 480 
giga-instructions per second (GIPS), 
392 

gigabytes, 
572 
giga.ops, 
525 
gigahertz 
(GHz), 
480 
GIPS (giga-instructions per second), 
392 
global IP Internet. See Internet Global Offset Table (GOT), 687, 

688¨C690 
global symbols, 660, 664¨C667 
global 
variable 
mapping, 
956 
GNU Compiler Collection. See gcc 
(GNU Compiler Collection) 
compiler GNU 
project, 
6 
GOT (Global Offset Table), 687, 

688¨C690 
goto [C] control transfer statement, 
193, 
216 
goto code, 
193¨C194 
gprof Unix pro.ler, 540, 
541¨C542 
gradual 
under.ow, 
105 
granularity of concurrency, 947 
graphic user interfaces for debuggers, 
254 
graphics adapters, 577 
graphs 
data-.ow, 
502¨C507 
process, 721, 
722 
progress. See progress graphs reachability, 839 

greater than signs (>) ¡°get 
from¡± 
operator, 
862 
right 
hoinkies, 
878 

groups abelian, 
82 
process, 739 

guard 
values, 
263 

h_errno [Unix] DNS error variable, 
1000 
.h header .les, 669 
halt [Y86] halt instruction 
execution, 339 
exceptions, 
344, 
420¨C422 
instruction code for, 384 
in 
pipelining, 
439 
status code for, 384 

handlers exception, 704, 
705 
interrupt, 
706 
signal, 
738, 
742, 
744 

handling 
signals, 
744 
issues, 
745¨C751 

portable, 
752¨C753 
hardware caches. See caches and cache memory Hardware Control Language (HCL), 
352 

Boolean 
expressions, 
354¨C355 
integer 
expressions, 
355¨C360 
logic 
gates, 
353 

hardware description languages 
(HDLs), 353, 
444 
hardware 
exceptions, 
704 
hardware 
interrupts, 
706 
hardware 
management, 
14¨C15 
hardware 
organization, 
7¨C8 

buses, 
8 
I/O 
devices, 
8¨C9 
main 
memory, 
9 
processors, 
9¨C10 

hardware 
registers, 
361¨C362 
hardware 
structure 
for 
Y86, 
375¨C379 
hardware 
units, 
375¨C377, 
380 
hash 
tables, 
544¨C545 
hazards 
in 
pipelining, 
336, 
408 

forwarding 
for, 
415¨C418 
load/use, 
418¨C420 
overview, 
408¨C412 
stalling 
for, 
413¨C415 

HCL (Hardware Control Language), 
352 

Boolean 
expressions, 
354¨C355 
integer 
expressions, 
355¨C360 
logic 
gates, 
353 

HDLs (hardware description 
languages), 353, 
444 
head crashes, 573 
HEAD 
method 
in 
HTTP, 
915 
header .les 
static libraries, 669 
system, 
725 
header tables in ELF, 658, 
678, 
678¨C679 

headers blocks, 821 
ELF, 658 
Ethernet, 888 
request, 914 
response, 915 

heap, 18, 
813 
dynamic memory allocation, 
813¨C814 
Linux 
systems, 
679 
referencing 
data 
in, 
847 
requests, 
823 


hello [CS:APP] C hello program, 2, 

10¨C12 
help command, 
255 
Hennessy, 
John, 
342, 
448 
heterogeneous 
data 
structures, 
241 

data 
alignment, 
248¨C251 
structures, 
241¨C244 
unions, 
244¨C248 
x86-64, 
290¨C291 

hexadecimal (hex) notation, 34, 
34¨C37 

hierarchies domain 
name, 
895 
storage devices, 13, 
13¨C14, 
591, 

591¨C595 
high-level design performance 
strategies, 
539 
hit rates, 614 
hit times, 614 
hits 

cache, 593, 
614 
write, 612 
hlt [IA32/x86-64] halt instruction, 
339 


HLT [Y86] status code indicating halt 
instruction, 344 
hoinkies, 878 
holding mutexes, 964 
Horner, William, 
508 
Horner¡¯s method, 508 
host bus adapters, 577 
host bus interfaces, 577 
host entry structures, 896 
host information program command, 
894 


hostent [Unix] DNS host entry structure, 896 
hostinfo [CS:APP] get DNS host 
entry, 897 
hostname command, 894 
hosts 
client-server model, 887 
network, 889 
number 
of, 
898 

htest 
script, 
443 
HTML (Hypertext Markup Language), 911, 
911¨C912 

htonl [Unix] convert host-to-network long, 893 
htons [Unix] convert host-to-network short, 893 

HTTP. See Hypertext Transfer Protocol (HTTP) hubs, 888 
hyperlinks, 911 
Hypertext Markup Language 
(HTML), 911, 
911¨C912 
Hypertext Transfer Protocol 
(HTTP), 911 
dynamic 
content, 
916¨C919 
requests, 914, 
914¨C915 
responses, 915, 
915¨C916 
transactions, 
914 

hyperthreading, 
22, 
158 
HyperTransport 
interconnect, 
568 

i-caches 
(instruction 
caches), 
498, 

612, 
613 
.i .les, 
5, 
655 
i386 
Intel 
microprocessors, 
157, 

269 

i486 
Intel 
microprocessors, 
157 

IA32 (Intel Architecture 32-bit) array 
access, 
233 
condition codes, 185 
conditional move instructions, 
207¨C209 
data 
alignment, 
249 
exceptions, 
708¨C711 
extended-precision .oating point, 
116 
machine 
language, 
155¨C156 
microprocessors, 
44, 
158 
registers, 168, 
168¨C169 

data 
movement, 
171¨C177 
operand speci.ers, 169¨C170 

vs. Y86, 
342, 
345¨C346 
IA32-EM64T 
microprocessors, 
269 
IA64 Itanium instruction set, 269 
iaddl [Y86] immediate add, 452 
IBM 
out-of-order 
processing, 
500 
processor 
family, 
334 
RISC 
design, 
342¨C343 

ICALL [Y86] instruction code for call instruction, 384 

ICANN (Internet Corporation for Assigned Names and Numbers), 896 

icode 
(Y86 
instruction 
code), 
364, 
383 
ICUs (instruction control units), 497¨C498 
idivl [IA32/x86-64] signed divide, 182, 
183 
idivq [x86-64] 
signed 
divide, 
279 

IDs (identi.ers) processes, 
719¨C720 
register, 
339¨C340 

IEEE. See Institute for Electrical and 
Electronic Engineers (IEEE) description, 
100 
Posix 
standards, 
15 

IEEE .oating-point representation denormalized, 
105 
normalized, 
103¨C104 
special 
values, 
105 
Standard 
754, 
99 
standards, 
99¨C100 

if [C] 
conditional 
statement, 
194¨C 
196 
ifun 
(Y86 
instruction 
function), 
364, 
383 
IHALT [Y86] instruction code for halt instruction, 384 

IIRMOVL [Y86] instruction code for irmovl instruction, 384 
ijk matrix multiplication, 626, 
626¨C 

628 
IJXX [Y86] instruction code for jump instructions, 384 
ikj matrix multiplication, 626, 
626¨C 

628 
illegal instruction exception, 384 
imem_error signal, 
384 
immediate add instruction (iaddl), 
452 
immediate coalescing, 824 
immediate 
offset, 
170 
immediate 
operands, 
169 
immediate to register move 
instruction (irmovl), 337 
implicit dynamic memory allocators, 
813¨C814 
implicit 
free 
lists, 
820¨C822, 
822 
implicit thread termination, 950 
implicitly reentrant functions, 981 
implied leading 1 representation, 104 
IMRMOVL [Y86] instruction code for 
mrmovl instruction, 384 
imul [IA32/x86-64] 
multiply, 
178 
imull [IA32/x86-64] signed multiply, 
182 

imulq [x86-64] 
signed 
multiply, 
279 
in [HCL] set membership test, 
360¨C361 

in_addr [Unix] IP address structure, 
893 

inc [IA32/x86-64] 
increment, 
178 
incl [IA32/x86-64] 
increment, 
179 
include .les, 669 
#include preprocessor directive, 

160 
increment instruction (inc), 
178¨C179 
inde.nite 
integer 
values, 
116 
index.html .le, 
912¨C913 
index 
registers, 
170 
indexes for direct-mapped caches, 
605¨C606 
indirect 
jumps, 
190, 
216 
inef.ciencies 
in 
loops, 
486¨C490 
inet_aton [Unix] convert 
application-to-network, 894 
inet_ntoa [Unix] convert network-
to-application, 894, 
982¨C983 
in.nite 
precision, 
80 
in.nity 
constants, 
115 


representation, 
104¨C105 
info frame command, 
255 
info registers command, 
255 
information, 
2¨C3 
information access 
IA32 
registers, 
168¨C169 
data 
movement, 
171¨C177 
operand speci.ers, 169¨C170 

x86-64 
registers, 
273¨C277 
information 
storage, 
33 
addressing and byte ordering, 
39¨C46 
bit-level 
operations, 
51¨C53 
Boolean 
algebra, 
48¨C51 
code, 
47 
data 
sizes, 
38¨C39 
disks. See disks .oating-point representation. See 
.oating-point representation 
and programs hexadecimal, 
34¨C37 
integers. See integers locality. See locality memory. See memory segregated, 836 
shift 
operations, 
54¨C56 
strings, 
46¨C47 
summary, 
629¨C630 
words, 
38 

init function, 723 
init_pool [CS:APP] initialize client pool, 
943, 
945 
initialize nonlocal handler jump function, 759 

initialize nonlocal jump functions, 
759 
initialize read buffer function, 868, 

870 
initialize semaphore function, 963 
initialize thread function, 952 
initializing 
threads, 
952 
inline 
assembly, 
267 
inline 
substitution, 
254, 
479 
inlining, 
254, 
479 
INOP [Y86] instruction code for nop 
instruction, 384 
input events, 942 
input/output. See I/O (input/output) insert item in bounded buffer 
function, 968 
install portable handler function, 752 
installing signal handlers, 744 
Institute for Electrical and Electronic 
Engineers (IEEE) description, 
100 
.oating-point representation 
denormalized, 
105 
normalized, 
103¨C104 
special 
values, 
105 
standards, 99¨C100 

Posix 
standards, 
15 
instr_regids signal, 
383 
instr_valC signal, 
383 
instr_valid signal, 
383¨C384 
instruction 
caches 
(i-caches), 
498, 

612, 
613 
instruction 
code 
(icode), 
364, 
383 
instruction control units (ICUs), 
497¨C498 
instruction 
function 
(ifun), 
364, 
383 
instruction-level 
parallelism, 
23¨C24, 

475, 
496¨C497, 
539 
instruction memory in SEQ timing, 380 
instruction set architectures (ISAs), 
9, 
24, 
160, 
334 
instruction 
set 
simulators, 
348 
instructions 
classes, 
171 
decoding, 
498 
excepting, 
421 
fetch 
locality, 
588¨C589 
issuing, 
406¨C407 
jump, 
10, 
189¨C193 
load, 
10 
low-level. See machine-level 
programming move, 
206¨C213, 
527, 
529¨C530 
pipelining, 
446¨C447, 
527 
privileged, 715 
sequential Y86 implementation. 
See sequential Y86 implemen-
tation store, 
10 
update, 
10 
Y86. See Y86 instruction set 
architecture instructions per cycle 
(IPC), 
449 
int data types 
integral, 
58 

x86-64 
processors, 
270 
int [HCL] integer signal, 356 
INT_MAX constant, 
62 
INT_MIN constant, 
62 
integer 
arithmetic, 
79, 
178 

division 
by 
powers 
of 
two, 
95¨C98 
multiplication 
by 
constants, 
92¨C95 
overview, 
98¨C99 
two¡¯s-complement 
addition, 
83¨C87 
two¡¯s-complement multiplication, 
89¨C92 
two¡¯s-complement 
negation, 
87¨C88 
unsigned 
addition, 
79¨C83 

integer bits in .oating-point 
representation, 
128 
integer 
expressions 
in 
HCL, 
355¨C360 
integer inde.nite values, 116 
integer operation instructions, 384 
integer registers 
IA32, 
168¨C169 
x86-64, 
273¨C275 
Y86, 
336¨C337 

integers, 
30, 
56¨C57 
arithmetic operations. See integer 
arithmetic bit-level 
operations, 
51¨C53 
bit representation expansion, 
71¨C75 
byte 
order, 
41 
data 
types, 
57¨C58 
shift 
operations, 
54¨C56 
signed and unsigned conversions, 
65¨C71 
signed vs. unsigned guidelines, 
76¨C79 
truncating, 
75¨C76 
two¡¯s-complement representation, 
60¨C65 
unsigned 
encoding, 
58¨C60 
integral data types, 57, 
57¨C58 


integration 
of 
caches 
and 
VM, 
791 

Intel assembly-code format vs. ATT, 
166¨C167 
gcc, 
294 

Intel microprocessors 8086, 
24, 
157, 
267 
conditional move instructions, 
207¨C209 
coprocessors, 
292 
Core i7. See Core i7 microproces-
sors data 
alignment, 
249 
evolution, 
157¨C158 
.oating-point 
representation, 
128 
i386, 
157, 
269 
IA32. See IA32 (Intel Architecture 
32-bit) northbridge and southbridge 
chipsets, 
568 
out-of-order 
processing, 
500 
x86-64. See x86-64 microprocessors 
interconnected networks (internets), 888, 
889¨C890 

interfaces bus, 
568 
host bus, 577 


interlocks, load, 
420 
internal 
exceptions 
in 
pipelining, 
420 
internal fragmentation, 819 
internal 
read 
function, 
871 
International Standards Organiza-
tion 
(ISO), 
4, 
32 


Internet, 889 
connections, 
899¨C900 
domain 
names, 
895¨C899 
IP 
addresses, 
893¨C895 
organization, 
891¨C893 
origins, 
900 

Internet addresses, 890 
Internet Corporation for Assigned Names and Numbers (ICANN), 
896 
Internet domain names, 892 
Internet Domain Survey, 898 
Internet 
hosts, 
number 
of, 
898 
Internet Protocol (IP), 892 
Internet Software Consortium, 898 
Internet 
worm, 
260 
internets (interconnected networks), 
888, 
889¨C890 
interpretation 
of 
bit 
patterns, 
30 
interprocess communication (IPC), 
937 

interrupt handlers, 706 
interruptions, 
745 
interrupts, 706, 
706¨C707 
interval 
counting 
schemes, 
541¨C542 
INTN_MAX [C] maximum value of 
N-bit 
signed 
data 
type, 
63 
INTN_MIN [C] minimum value of N-bit 
signed 
data 
type, 
63 
intN_t [C] N-bit signed integer data 
type, 
63 
invalid 
address 
status 
code, 
344 
invalid memory reference exceptions, 
435 
invariants, semaphore, 963 
I/O (input/output), 8, 862 

memory-mapped, 578 
ports, 579 
redirection, 877, 877¨C879 
system-level. See system-level I/O Unix, 19, 862, 
862¨C863 

I/O bridges, 568 
I/O buses, 576 
I/O devices, 8¨C9 

addressing, 
579 
connecting, 
576¨C578 
I/O multiplexing, 935 
concurrent programming with, 939¨C947 
event-driven servers based on, 942¨C947 
pros 
and 
cons, 
947¨C948 
IOPL [Y86] instruction code for integer operation instructions, 
384 
IP (Internet Protocol), 892 
IP address structure, 893, 
894 
IP addresses, 892, 
893¨C895 
IPC (instructions per cycle), 
449 
IPC (interprocess communication), 
937 

IPOPL [Y86] instruction code for popl instruction, 384 
IPUSHL [Y86] instruction code for pushl instruction, 384 
IRET [Y86] instruction code for ret instruction, 384 

IRMMOVL [Y86] instruction code for rmmovl instruction, 384 
irmovl [Y86] immediate to register 
move, 337 
constant 
words 
for, 
340 
instruction code for, 384 
processing 
steps, 
367¨C368 

Index 1025 
IRRMOVL [Y86] instruction code for rrmovl instruction, 384 
ISA 
(instruction 
set 
architecture), 
9, 
24, 
160, 
334 
ISO (International Standards 
Organization), 
4, 
32 
ISO C90 C standard, 32 
ISO C99 C standard, 32, 
39, 
58 
isPtr function, 
842 
issue time for arithmetic operations, 
501, 
502 
issuing 
instructions, 
406¨C407 
Itanium instruction set, 269 
iteration, 
256 
iterative servers, 908 
iterative 
sorting 
routines, 
544 

ja [IA32/x86-64] jump if unsigned greater, 
190 
jae [IA32/x86-64] jump if unsigned greater or equal, 
190 

Java language, 
661 
byte 
code, 
293 
linker 
symbols, 
663¨C664 
numeric 
ranges, 
63 
objects 
in, 
241¨C242 
software 
exceptions, 
703¨C704, 
760 

Java monitors, 
970 
Java Native Interface (JNI), 685 
jb [IA32/x86-64] jump if unsigned 
less, 
190 
jbe [IA32/x86-64] jump if unsigned less or equal, 
190 
je [IA32/x86-64/Y86] jump when equal, 
190, 
338¨C339, 
373 
jg [IA32/x86-64/Y86] jump if greater, 190, 
338¨C339 
jge [IA32/x86-64/Y86] jump if greater or equal, 
190, 
338¨C339 
jik matrix multiplication, 626, 
626¨C 
628 
jki matrix multiplication, 626, 
626¨C 
628 
jl [IA32/x86-64/Y86] jump if less, 190, 
338¨C339 
jle [IA32/x86-64/Y86] jump if less or 
equal, 
190, 
338¨C339 
jmp [IA32/x86-64/Y86] jump unconditionally, 190, 338¨C339 
jna [IA32/x86-64] jump if not unsigned 
greater, 
190 
jnae [IA32/x86-64] jump if not unsigned greater or equal, 
190 


jnb [IA32/x86-64] jump if not unsigned 
less, 
190 

jnbe [IA32/x86-64] jump if not unsigned less or equal, 
190 

jne [IA32/x86-64/Y86] jump if not equal, 
190, 
338¨C339 

jng [IA32/x86-64] jump if not greater, 
190 

jnge [IA32/x86-64] jump if not greater or equal, 
190 
JNI (Java Native Interface), 685 

jnl [IA32/x86-64] jump if not less, 190 

jnle [IA32/x86-64] jump if not less or 
equal, 
190 

jns [IA32/x86-64] jump if nonnegative, 190 
jnz [IA32/x86-64] jump if not zero, 190 

jobs, 740 

joinable threads, 951 


js [IA32/x86-64] jump if negative, 190 

jtest 
script, 
443 


jump if greater instruction (jg), 
190, 
338¨C339 

jump if greater or equal instruction (jge), 
190, 
338¨C339 

jump if less instruction (jl), 
190, 
338¨C339 

jump if less or equal instruction (jle), 
190, 
338¨C339 

jump if negative instruction (js), 
190 

jump if nonnegative instruction (jns), 
190 

jump if not equal instruction (jne), 190, 
338¨C339 

jump if not greater instruction (jng), 190 

jump if not greater or equal instruction (jnge), 
190 

jump if not less instruction (jnl), 
190 

jump if not less or equal instruction (jnle), 
190 

jump if not unsigned greater instruction (jna), 
190 

jump if not unsigned less instruction (jnb), 
190 

jump if not unsigned less or equal instruction (jnbe), 
190 

jump if not zero instruction (jnz), 190 

jump if unsigned greater instruction (ja), 
190 
jump if unsigned greater or equal instruction (jae), 
190 
jump if unsigned less instruction (jb), 190 
jump if unsigned less or equal instruction (jbe), 
190 
jump if zero instruction (jz), 
190 
jump 
instructions, 
10, 
189¨C193 
direct, 
190 
indirect, 
190, 
216 
instruction code for, 384 
nonlocal, 
703, 
759, 
759¨C762 
targets, 
190 
jump tables, 213, 
216, 
705 
jump unconditionally instruction (jmp), 190, 
190, 
338¨C339 
jump when equal instruction (je), 
338 

just-in-time 
compilation, 
266, 
294 
jz [IA32/x86-64] 
jump 
if 
zero, 
190 

K&R (C book), 4 
Kahan, 
William, 
99¨C100 
Kahn, 
Robert, 
900 
kernel mode exception handlers, 706 
processes, 
714¨C716, 
715 
system calls, 708 
kernels, 18, 
680 
exception numbers, 705 
virtual 
memory, 
803¨C804 
Kernighan, 
Brian, 
2, 
4, 
15, 
32, 
253, 
849, 
882 
keyboard, 
signals 
from, 
740¨C741 
kij matrix multiplication, 626, 
626¨C 
628 
kill.c [CS:APP] kill example, 
741 
kill command in gdb debugger, 
255 
kill [Unix] send signal, 741 
kji matrix multiplication, 626, 
626¨C 
628 
Knuth, 
Donald, 
823, 
825 
ksh [Unix] 
Unix 
shell 
program, 
733 

l 
suf.x, 
168 
L1 cache, 13, 596 
L2 cache, 13, 596 
L3 cache, 596 
LANs (local area networks), 888, 
889¨C891 

last-in .rst-out (LIFO) free list order, 835 
stack 
discipline, 
172 
latency arithmetic operations, 501, 
502 
disks, 574 
instruction, 
392 
load 
operations, 
531¨C532 
pipelining, 391 
latency 
bounds, 
496, 
502 
lazy binding, 688, 
689 
ld Unix static linker, 657 
ld-linux.so linker, 683 
ldd tool, 
690 
LEA [IA32/x86-64] 
instruction, 
93 
leaf 
procedures, 
284 
leaks, memory, 
847, 
954 
leal [IA32] load effective address, 177, 
177¨C178, 
252, 
278 
leaq [x86-64] load effective address, 277 
least-frequently-used (LFU) replacement 
policies, 
608 
least-recently-used (LRU) replacement policies, 594, 
608 
least 
squares 
.t, 
480, 
482 
leave [IA32/x86-64/Y86] prepare stack 
for 
return, 
221¨C222, 
228, 

453 
left hoinkies (<), 878 
length 
of 
strings, 
77 
less than signs (<) left hoinkies, 878 
¡°put 
to¡± 
operator, 
862 
levels optimization, 
254, 
256, 
476 
storage, 
591 
LFU (least-frequently-used) replacement 
policies, 
608 
libc library, 
879 
libraries in concurrent programming, 982¨C983 
header 
.les, 
77 
shared, 18, 
681¨C686, 
682 
standard 
I/O, 
879¨C880 
static, 667, 
667¨C672 
LIFO (last-in .rst-out) free list order, 835 
stack 
discipline, 
172 
limits.h .le, 
62, 
71 


line matching direct-mapped 
caches, 
599¨C600 
fully 
associative 
caches, 
608 
set 
associative 
caches, 
607¨C608 

line replacement direct-mapped 
caches, 
600¨C601 
set 
associative 
caches, 
608 

.line section, 659 
linear address spaces, 778 
link-time 
errors, 
7 
linkers and linking, 5, 
154, 
160 

compiler 
drivers, 
655¨C657 
dynamic, 
681¨C683, 
682 
object .les, 657, 
657¨C658 

executable, 678¨C681 
loading, 679¨C681 
relocatable, 658¨C659 
tools for, 690 

overview, 
654¨C655 
position-independent 
code, 
687¨C 

690 
relocation, 
672¨C678 
shared libraries from applications, 
683¨C686 
static, 657 
summary, 
691 
symbol 
resolution, 
663¨C672 
symbol 
tables, 
660¨C662 
virtual 
memory 
for, 
785 

linking 
phase, 
5 


Linux 
operating 
system, 
19¨C20, 
44 
code 
segments, 
679¨C680 
data 
alignment, 
249 
dynamic 
linker 
interfaces, 
685 
and ELF, 658 
exceptions, 
708¨C711 
signals, 737 
virtual 
memory, 
803¨C807 

Lisp 
language, 
80 
listen [Unix] convert active socket 
to listening socket, 905 
listening 
descriptors, 
907¨C908 
listening sockets, 905 
little endian byte ordering, 40 
load effective address instruction 
(leal, leaq), 
177¨C178, 
252 
load forwarding, 456 
load 
instructions, 
10 
load 
interlocks, 
420 
load 
operations, 
498¨C499 
load 
penalty 
in 
CPI, 
445 
load performance of memory, 
531¨C532 
load program function, 730 
load/store architecture in CISC vs. 
RISC, 
343 
load time for code, 654 
load/use data hazards, 418, 418¨C421 
loaders, 657, 679 
loading 
concepts, 
681 
executable 
object 
.les, 
679¨C681 
programs, 
730¨C732 
shared libraries from applications, 
683¨C686 
virtual 
memory 
for, 
785¨C786 
local area networks (LANs), 888, 
889¨C891 
local automatic variables, 956 
local registers in loop segments, 
504¨C505 
local static variables, 956 
local symbols, 660 
locality, 
13, 
560, 
586, 
586¨C587 

blocking for, 629 
caches, 
625¨C629, 
784 
exploiting, 
629 
forms, 587, 
595 
instruction 
fetches, 
588¨C589 
program 
data 
references, 
587¨C588 
summary, 
589¨C591 

localtime function, 
982¨C983 
lock-and-copy technique, 980, 
981 
locking mutexes 
lock ordering rule, 987 
for semaphores, 964 
logic 
design, 
352 
combinational 
circuits, 
354¨C360, 

392 
logic 
gates, 
353 
memory 
and 
clocking, 
361¨C363 
set 
membership, 
360¨C361 

logic gates, 353 
logic 
synthesis, 
336, 
353, 
444 
logical blocks 
disks, 575, 
575¨C576 

SSDs, 
582 
logical 
control 
.ow, 
712¨C713 
logical 
operations, 
54, 
177 

discussion, 
180¨C182 
shift, 55, 
95, 
178¨C180 
unary 
and 
binary, 
178¨C179 

long [C] 
integer 
data 
type, 
39, 
57¨C58, 
270 
long double [C] extended-precision .oating point, 115, 
168 
270 

Index 1027 
long integers with x86-64 processors, 270 
long long [C] 
integer 
data 
type, 
39, 
57¨C58, 
270¨C271 
long words in machine-level data, 168 
longjmp [C Stdlib] nonlocal jump, 
703, 
759, 760 
loop registers, 505 
loop 
unrolling, 
480, 
482, 
509 

Core 
i7, 
551 
overview, 
509¨C513 
with reassociation transforma-
tions, 
519¨C521 
loopback addresses, 897 
loops, 
197 

do-while, 197¨C200 
for, 
203¨C206 
inef.ciencies, 
486¨C490 
reverse engineering, 
199 
segments, 
504¨C505 
for 
spatial 
locality, 
625¨C629 
while, 200¨C203 

low-level instructions. See machine-
level programs low-level 
optimizations, 
539 
lowercase 
conversions, 
487¨C489 
LRU (least-recently-used) 
replacement policies, 594, 

608 
lseek [Unix] 
function, 
866¨C867 
lvalues 
(C) 
for 
pointers, 
252 

machine 
checks, 
709 
machine 
code, 
154 
machine-level programs 
arithmetic. See arithmetic arrays. See arrays buffer over.ow. See buffer 
over.ow control. See control structures data-.ow 
graphs 
from, 
503¨C507 
data 
formats, 
167¨C168 
data movement instructions, 
171¨C177, 
275¨C277 
encodings, 
159¨C167 
.oating-point 
programs, 
292¨C293 
gdb debugger, 
254¨C256 
heterogeneous data structures. See 
heterogeneous data structures historical 
perspective, 
156¨C159 
information 
access, 
168¨C169 
instructions, 4 


machine-level programs (continued) operand 
speci.ers, 
169¨C170 
overview, 
154¨C156 
pointer 
principles, 
252¨C253 
procedures. See procedures x86-64. See x86-64 microprocessors 
macros for 
free 
lists, 
829¨C830 

main memory, 9 
accessing, 
567¨C570 
memory modules, 564 

main threads, 948 
malloc [C Stdlib] allocate heap 
storage, 
32, 
679, 
813, 
814 
alignment 
with, 
250 
dynamic memory allocation, 
814¨C816 
man ascii command, 
46 
mandatory 
alignment, 
249 
mangling process, 663, 
663¨C664 
many-core 
processors, 
449 
map disk object into memory 
function, 810 


mapping memory. See memory mapping variables, 
956 

maps, zone, 
580¨C581 
mark phase in Mark&Sweep, 840 
Mark&Sweep algorithm, 839 
Mark&Sweep garbage collectors, 
840, 840¨C842 
masking 
operations, 
52 
matrices 

adjacency, 642 
multiplying, 
625¨C629 
maximum two¡¯s-complement 
number, 
61 
maximum 
unsigned 
number, 
59 
maximum values, constants 
for, 
63 
McCarthy, 
John, 
839 
McIlroy, Doug, 
15 
mem_init [CS:APP] heap model, 828 
mem_sbrk [CS:APP] sbrk emulator, 
828 


membership, set, 
360¨C361 

memcpy [Unix] copy bytes from one region of memory to another, 125 

memory, 
560 
accessing, 
567¨C570 
aliasing, 477, 478, 
494 
associative, 607 
caches. See caches and cache 
memory copying 
bytes 
in, 
125 
data 
alignment 
in, 
248¨C251 
data 
hazards, 
413 
design, 
363 
dynamic. See dynamic memory 
allocation hierarchy, 13, 
13¨C14, 
591, 
591¨C595 
interfacing 
with 
processor, 
447¨C 

448 
leaks, 
847, 
954 
load 
performance, 
531¨C532 
in 
logic 
design, 
361¨C363 
machine-level 
programming, 
160 
main, 9, 
564, 
567¨C570 
mapping. See memory mapping nonvolatile, 567 
performance, 
531¨C539 
protecting, 
266, 
786¨C787 
RAM. See random-access 
memories (RAM) ROM, 567 
threads, 
955¨C956 
trends, 
583¨C586 
virtual. See virtual memory (VM) Y86, 
337 

memory buses, 568 
memory controllers, 563, 
564 
memory management units (MMUs), 
778, 
780 
memory-mapped I/O, 578 
memory mapping, 786 

areas, 807, 807 
execve function, 
810 
fork function, 
809¨C810 
in 
loading, 
681 
objects, 
807¨C809 
user-level, 
810¨C812 

memory mountains, 621, 
621¨C625 

memory references operands, 
170 
out-of-bounds. See buffer over.ow in 
performance, 
491¨C496 
pipelining 
exceptions, 
435 

memory stage instruction 
processing, 
364, 
366, 

368¨C377 
PIPE 
processor, 
430¨C431 
SEQ, 
389¨C390 
Y86 
pipelining, 
403 

memory system, 560 
memory utilization, 818, 
818¨C819 
metadata, 873, 873¨C875 
metastable 
states, 
561 

methods HTTP, 915 
objects, 
242 

micro-operations, 
498 
microarchitecture, 
10, 
496 
microprocessors. See central 
processing units (CPUs) Microsoft Windows operating system, 
44, 
249 
MIME (Multipurpose Internet Mail 
Extensions) types, 912 
minimum block size, 822 
minimum two¡¯s-complement 
number, 
61 

minimum values constants, 
63 
two¡¯s-complement representation, 
61 

mispredicted branches canceling, 434 
performance 
penalties, 
445, 
499, 

526¨C531 

misses, caches, 
448, 
594 
kinds, 
594¨C595 
penalties, 614, 
780 
rates, 614 

mm_coalesce [CS:APP] allocator: boundary tag coalescing, 
833 

mm_free [CS:APP] allocator: free heap 
block, 
832, 
833 
mm_ijk [CS:APP] matrix multiply ijk, 626 
mm_ikj [CS:APP] matrix multiply ikj, 626 
mm_init [CS:APP] allocator: initialize 
heap, 
830, 
831 
mm_jik [CS:APP] matrix multiply jik, 626 
mm_jki [CS:APP] matrix multiply jki, 626 
mm_kij [CS:APP] matrix multiply kij, 626 
mm_kji [CS:APP] matrix multiply kji, 626 
mm_malloc [CS:APP] allocator: allocate 
heap 
block, 
832, 
834 
mmap [Unix] map disk object into memory, 810, 
810¨C812 
MMUs (memory management units), 
778, 780 
Mockapetris, Paul, 
900 
mode bits, 715 


modern 
processor 
operation, 
496¨C 
509 

modes kernel, 706, 
708 
processes, 
714¨C716, 
715 
user, 706 

modular 
arithmetic, 
80¨C81 

modules DRAM, 564, 
565 
object, 657¨C658 


monitors, 
Java, 
970 
monotonicity 
assumption, 
819 
monotonicity 
property, 
114 
Moore, Gordon, 
158¨C159 
Moore¡¯s Law, 158, 
158¨C159 
mosaic browser, 
912 
motherboards, 
8 
Motorola 
68020 
processor, 
268 
RISC 
processors, 
343 
mov [IA32/x86-64] move data, 171, 
276 
movabsq [x86-64] move absolute quad 
word, 
276 
movb [IA32/x86-64] move byte, 171¨C172 
Move absolute quad word instruction 
(movabsq), 
276 
move byte instruction (movb), 
171 
Move data instructions (mov), 171, 

171¨C177, 
276 
move double word instruction (movl), 
171 
move if greater instruction (cmovg), 210, 
339 
move if greater or equal instruction (cmovge), 
210, 
339 
move if less instruction (cmovl), 
210, 

339 


move if less or equal instruction (cmovle), 
210, 
339 
move if negative instruction (cmovs), 210 
move if nonnegative instruction (cmovns), 
210 
move if not equal instruction (cmovne), 
210, 
339 
move if not greater instruction (cmovng), 
210 
move if not greater or equal instruction (cmovnge), 
210 
move if not less instruction (cmovnl), 210 

move if not less or equal instruction (cmovnle), 
210 
move if not unsigned greater instruction (cmovna), 
210 
move if not unsigned less instruction (cmovnb), 
210 
move if not unsigned less or equal instruction (cmovnbe), 
210 
move if not zero instruction (cmovnz), 
210 
move if unsigned greater instruction (cmova), 
210 
move if unsigned greater or equal instruction (cmovae), 
210 
move if unsigned less instruction (cmovb), 
210 
move if unsigned less or equal 
instruction (cmovbe), 
210 
move if zero instruction (cmovz), 
210 
move instructions, conditional, 
206¨C213 
move quad word instruction (movq), 276 
move sign-extended byte to double word instruction (movsbl), 
171 
move sign-extended byte to quad word instruction (movsbq), 
276 
move sign-extended byte to word instruction (movsbw), 
171 

move sign-extended double word to quad word instruction (movslq), 
276 

move sign-extended word to double word instruction (movswl), 
171 
move sign-extended word to quad word instruction (movswq), 
276 
move when equal instruction (move), 
339 

move with sign extension instructions (movs), 
171, 
276 
move with zero extension instructions 
(movz), 
171, 
276 
move word instruction (movw), 
171 
move zero-extended byte to double 
word instruction (movzbl), 
171 
move zero-extended byte to quad word instruction (movzbq), 
276 
move zero-extended byte to word instruction (movzbw), 
171 
move zero-extended word to double word instruction (movzwl), 
171 
move zero-extended word to quad word instruction (movzwq), 
276 

moves, 
conditional, 
527, 
529¨C530 

movl [IA32/x86-64] move double word, 
171 

movq [IA32/x86-64] move quad word, 272, 
276 

movs [IA32/x86-64] move with sign extension, 
171¨C172, 
172, 
276 

movsbl [IA32/x86-64] move sign-extended byte to double word, 171¨C172 

movsbq [x86-64] move sign-extended byte 
to 
quad 
word, 
276 

movsbw [IA32/x86-64] move sign-extended 
byte 
to 
word, 
171 

movslq [x86-64] move sign-extended double 
word 
to 
quad 
word, 
276, 
278 

movss .oating-point move instruction, 
492 

movswl [IA32/x86-64] move sign-extended word to double word, 171 

movswq [x86-64] move sign-extended word 
to 
quad 
word, 
276 

movw [IA32/x86-64] move word, 
171 
movz [IA32/x86-64] move with zero extension, 
171, 
172, 
276 

movzbl [IA32/x86-64] move zero-extended byte to double word, 171¨C172 

movzbq [x86-64] move zero-extended byte 
to 
quad 
word, 
276 

movzbw [IA32/x86-64] move zero-extended 
byte 
to 
word, 
171 

movzwl [IA32/x86-64] move zero-extended word to double word, 171 

movzwq [x86-64] move zero-extended word 
to 
quad 
word, 
276 

mrmovl [Y86] memory to register move 
instruction, 
368 

mull [IA32/x86-64] unsigned multiply, 182 

mulq [x86-64] 
unsigned 
multiply, 
279 

mulss .oating-point multiply instruction, 
492 

multi-core 
processors, 
16, 
22, 
158, 
586, 
934 

multi-level 
page 
tables, 
792¨C794 

multi-threading, 
17, 
22 

Multics, 
15 

multicycle 
instructions, 
446¨C447 

multidimensional 
arrays, 
235¨C236 
multimedia 
applications, 
156¨C157 
multiple accumulators in parallelism, 

514¨C518 
multiple zone recording, 572 
multiplexing, I/O, 935 

concurrent programming with, 939¨C947 
event-driven servers based on, 942¨C947 
pros 
and 
cons, 
947¨C948 

multiplexors, 354, 
354¨C355 
HCL with case expression, 357 
word-level, 
357¨C358 

multiplication constants, 
92¨C95 
.oating-point, 
113¨C114 
instructions, 
182 
matrices, 
625¨C629 
two¡¯s-complement, 89, 
89¨C92 
unsigned, 88, 
182, 
182, 
279 

multiply de.ned global symbols, 664¨C667 
multiply 
instruction, 
178, 
182, 
279, 
492 
multiported random-access memory, 
362 
multiprocessor 
systems, 
22 
Multipurpose Internet Mail 
Extensions (MIME) types, 
912 
multitasking, 713 
multiway 
branch 
statements, 
213¨C219 
munmap [Unix] unmap disk object, 
812 


mutexes lock ordering rule, 987 
Pthreads, 
970 
for semaphores, 964 

mutual exclusion progress graphs, 962 
semaphores 
for, 
964¨C965 

mutually exclusive access, 962 

\n (newline character), 3 
n-gram 
statistics, 
542¨C543 
names 
data 
types, 
43 
domain, 892, 
895¨C899 
mangling and demangling 
processes, 663, 
663¨C664 
protocols, 
890 
Y86 
pipelines, 
406 

naming conventions for Y86 signals, 405¨C406 

NaN (not-a-number) constants, 
115 
representation, 
104, 
105 

nanoseconds (ns), 480 
National Science Foundation (NSF), 
900 
neg [IA32/x86-64] 
negate, 
178 
negate 
instruction, 
178 
negation, two¡¯s-complement, 87, 

87¨C88 
negative over.ow, 83,84 
Nehalem 
microarchitecture, 
497, 
799 
nested 
arrays, 
235¨C236 
nested 
structures, 
244 
NetBurst 
microarchitecture, 
157 
network adapters, 577 
network byte order, 893 
network clients, 20, 
886 
Network 
File 
System 
(NFS), 
591 
network 
programming, 
886 

client-server 
model, 
886¨C887 
Internet. See Internet networks, 
887¨C891 
sockets interface. See sockets 
interface summary, 
927¨C928 
tiny Web 
server, 
919¨C927 
Web 
servers, 
911¨C919 

network servers, 21, 
886 

networks, 
20¨C21 
acyclic, 
354 
LANs, 888, 
889¨C891 
WANs, 889, 
889¨C890 

never taken (NT) branch prediction 
strategy, 
407 
newline character (\n), 3 
next .t block placement policy, 822, 

823 
nexti command 
in 
GCB, 
255 
NFS 
(Network 
File 
System), 
591 
nm tool, 
690 
no-execute (NX) memory protection, 
266 

no operation nop instruction instruction code for, 384 
pipelining, 
409¨C411 
rep as, 
281 
in 
stack 
randomization, 
262 

no-write-allocate approach, 612 
nodes, root, 839 
nondeterminism, 728 
nondeterministic behavior, 728 
nonexistent variables, referencing, 
846 
nonlocal 
jumps, 
703, 
759, 
759¨C762 
nonuniform 
partitioning, 
395¨C397 
nonvolatile memory, 567 
nop instruction 
instruction code for, 384 
pipelining, 
409¨C411 
rep as, 
281 

nop 
sleds, 
262 
norace.c [CS:APP] Pthreads program without a race, 985 
normal 
operation 
status 
code, 
344, 

384 

normalized values, .oating-point, 
103, 
103¨C104 
northbridge 
chipsets, 
568 
not-a-number NaN 
constants, 
115 

representation, 
104, 
105 
Not [IA32/x86-64] 
complement, 
178 
Not operation 
Boolean, 
48¨C49 
C 
operators, 
54 
logic gates, 353 

ns (nanoseconds), 480 
NSF (National Science Foundation), 
900 
NSFNET, 
900 
ntohl [Unix] convert network-to-
host long, 893 
ntohs [Unix] convert network-to-host short, 893 
number systems conversions. See 
conversions numeric 
limit 
declarations, 
71 
numeric ranges 
integral 
types, 
57¨C58 
Java standard, 
63 
NX (no-execute) memory protection, 266 

.o .les, 
5, 
163, 
655 
objdump tool, 
163, 
254, 
674, 
690 
object 
.les, 
160, 
163 

executable. See executable object 
.les forms, 
162, 
657 
relocatable, 
5, 
655, 
657, 658¨C659 
tools, 
690 
object modules, 657¨C658 


objects memory-mapped, 
807¨C809 
private, 808, 
809 
program, 
33 
shared, 682, 
807¨C809, 808 
as struct, 241¨C242 

oct 
words, 
279 
OF [IA32/x86-64/486] over.ow .ag 
condition code, 185, 
337 
off-by-one 
errors, 
845 
offsets 
GOTs, 687, 
688¨C690 
memory 
references, 
170 
PPOs, 789 
structures, 
241¨C242 
unions, 
245 
VPOs, 788 

one-operand multiply instructions, 182, 
278¨C279 
ones¡¯-complement representation, 
63 
open [Unix] open .le, 863, 
863¨C865 
open_clientfd [CS:APP] establish 
connection with server, 903, 
903¨C904 
open_listenfd [CS:APP] establish a listening socket, 905, 
905¨C906 
open operations for .les, 862¨C863, 

863¨C865 
open shared library function, 684 
open 
source 
operating 
systems, 
78¨C79 
operand 
speci.ers, 
169¨C170 
operating systems (OS), 14 

.les, 
19 
hardware 
management, 
14¨C15 
kernels, 18 
Linux, 
19¨C20, 
44 
processes, 
16¨C17 
threads, 
17 
Unix, 
32 
virtual 
memory, 
17¨C19 
Windows, 
44, 
249 

operations bit-level, 
51¨C53 
logical, 
54 
shift, 
54¨C56 

optest 
script, 
443 


optimization address 
translation, 
802 
compiler, 
160 
levels, 
254, 
256, 
476 

program performance. See 
performance optimization 
blockers, 
475, 
478 
OPTIONS 
method, 
915 
or [IA32/x86-64] 
or, 
178 
Or operation 
Boolean, 
48¨C49 
C 
operators, 
54 
HCL 
expressions, 
354¨C355 
logic gates, 353 

order, bytes, 
39¨C46 
disassembled 
code, 
193 
network, 893 
unions, 
247 

origin servers, 915 
OS. See operating systems (OS) Ossanna, 
Joe, 
15 
Ousterhout, 
John 
K., 
474 
out-of-bounds memory references. 
See buffer over.ow out-of-core 
algorithms, 
268 
out-of-order execution, 497 

.ve-stage 
pipelines, 
449 
history, 
500 

over.ow arithmetic, 81, 
125 
buffer. See buffer over.ow .oating-point values, 116¨C 

117 
identifying, 
86 
in.nity 
representation, 
105 
multiplication, 
93 
negative, 83,84 
operations, 
30 
positive, 84 

over.ow .ag condition code (OF), 185, 
337 
overloaded 
functions, 
663 

P semaphore operation, 963, 
964 
P [CS:APP] wrapper function for 
Posix sem_wait, 963, 
964 
P6 
microarchitecture, 
157 
PA (physical addresses), 777 

vs. virtual, 
777¨C778 
packages, processor, 799 
packet headers, 890 
packets, 890 
padding 
alignment, 
250¨C251 
blocks, 821 
Y86, 
341 

page faults Linux/IA32 
systems, 
709, 
806¨C807 
memory 
caches, 
448 
pipelining caches, 782, 
782¨C783 

page frames, 779 
page 
hits 
in 
caches, 
782 
page table base registers (PTBRs), 
788 

page table entries (PTEs), 781, 
782 
Core 
i7, 
800¨C802 
TLBs 
for, 
791¨C794, 
797 

page table entry addresses (PTEAs), 791 

page 
tables, 
716, 
797 
caches, 780, 
780¨C781 
multi-level, 
792¨C794 

paged in pages, 783 
paged out pages, 783 
pages 
allocation, 
783¨C784 
demand zero, 807 
dirty, 801 
physical, 779, 
779¨C780 
SSDs, 
582 
virtual, 
266, 
779, 
779¨C780 

paging, 783 
parallel execution, 714 
parallel .ows, 713¨C714 
parallel programs, 974 
parallelism, 
21¨C22, 
513¨C514 

instruction-level, 
23¨C24, 
475, 

496¨C497, 
539 
multiple 
accumulators, 
514¨C518 
reassociation transformations, 
518¨C523 
SIMD, 
24¨C25, 
523¨C524 
threads 
for, 
974¨C978 

parent processes, 719¨C720 
parse_uri [CS:APP] Tiny helper function, 
923, 
924 
parseline [CS:APP] shell helper routine, 736 

partitioning addresses, 598 
nonuniform 
in 
pipelining, 
395¨C397 

Pascal 
reference 
parameters, 
226 
passing arguments for x86-64 processors, 
283¨C284 
parameters 
to 
functions, 
226 
pointers 
to 
structures, 
242 

Patterson, 
David, 
342, 
448 


pause [Unix] suspend until signal arrives, 730 
payloads aggregate, 819 
Ethernet, 888 
protocol, 
890 
PC. See program counter (PC) PC-relative addressing jumps, 
190¨C193, 
191 
operands, 
275 
symbol references, 673, 
674¨C675 
Y86, 
340 
PC selection stage in PIPE processor, 424¨C425 
PC update stage instruction 
processing, 
364, 
366, 
368¨C377 
SEQ, 
390 
PCI (Peripheral Component Interconnect) 
bus, 
576 
PE (Portable Executable) format, 658 
peak 
utilization 
metric, 
818¨C819, 
819 
peer threads, 948 
pending bit vectors, 739 
pending signals, 738 
Pentium 
II 
microprocessors, 
157 
Pentium 
III 
microprocessors, 
157 
Pentium 
4 
microprocessors, 
157, 
269 
Pentium 
4E 
microprocessors, 
158, 
273 
PentiumPro 
microprocessors, 
157 
conditional move instructions, 
207 
out-of-order 
processing, 
500 
performance, 
6 
Amdahl¡¯s 
law, 
545¨C547 
basic 
strategies, 
539 
bottlenecks, 
540¨C547 
branch prediction and mispredic-tion 
penalties, 
526¨C531 
caches, 
531, 
614¨C615, 
620¨C629 
compiler capabilities and limitations, 
476¨C480 
expressing, 
480¨C482 
limiting 
factors, 
525¨C531 
loop 
inef.ciencies, 
486¨C490 
loop unrolling, 509, 
509¨C513 
memory, 
531¨C539 
memory 
references, 
491¨C496 
modern 
processors, 
496¨C509 
overview, 
474¨C476 
parallelism. See parallelism procedure 
calls, 
490¨C491 

program 
example, 
482¨C486 
program 
pro.ling, 
540¨C545 
register 
spilling, 
525¨C526 
relative, 
493¨C494 
results 
summary, 
524¨C525 
SEQ, 
391 
summary, 
547¨C548 
Y86 
pipelining, 
444¨C446 
periods (.) in dotted-decimal notation, 
893 
Peripheral Component Interconnect (PCI) 
bus, 
576 
persistent connections in HTTP, 915 
physical address spaces, 778 
physical addresses (PA), 777 
vs. 
virtual, 
777¨C778 
Y86, 
337 
physical page numbers (PPNs), 788 
physical page offset (PPO), 789 
physical pages (PPs), 779, 
779¨C780 
pi in .oating-point representation, 131 
PIC (position-independent code), 
687 

data 
references, 
687¨C688 
function 
calls, 
688¨C690 
picoseconds (ps), 392, 
480 
PIDs (process IDs), 719 
pins, DRAM, 
562¨C563 
PIPE¨C processor, 401, 
403, 
405¨C409 
PIPE 
processor 
stages, 
418¨C419, 
423¨C424 
decode 
and 
write-back, 
426¨C429 
execute, 
429¨C430 
memory, 
430¨C431 
PC 
selection 
and 
fetch, 
424¨C425 
pipelining, 
208, 
391 
computational, 
392¨C393 
deep, 
397¨C398 
diagram, 392 
.ve-stage, 
448¨C449 
functional 
units, 
501¨C502 
instruction, 
527 
limitations, 
394¨C395 
nonuniform 
partitioning, 
395¨C397 
operation, 
393¨C394 
registers, 
393, 
406 
store 
operation, 
532¨C533 
systems 
with 
feedback, 
398¨C400 
Y86. See Y86 pipelined implementations pipes, 
937 
Pisano, Leonardo 
(Fibonacci), 
30 

placement memory blocks, 820, 
822¨C823 
policies, 594, 822 
platters, disk, 570, 
571 
PLT (procedure linkage table), 688, 
689¨C690 
pmap tool, 
762 
point-to-point connections, 899 
pointers, 
33 
arithmetic, 
233¨C234, 
846 
arrays, relationship 
to, 
43, 
252 
block, 829 
creating, 
44, 
175 
declaring, 
39 
dereferencing, 
44, 
175¨C176, 
234, 
252, 
843 
examples, 
174¨C176 
frame, 
219 
to 
functions, 
253 
machine-level 
data, 
167 
principles, 
252¨C253 
role, 
34 
stack, 
219 
to 
structures, 
242¨C243 
virtual 
memory, 
843¨C846 
void*,44 
pollution, cache, 717 
polynomial evaluation, 507, 508, 551¨C552 
pools of peer threads, 948 
pop double word instruction (popl), 171, 
173, 
339 
pop 
instructions 
in 
x86 
models, 
352 
pop operations on stack, 172, 
172¨C174 
pop quad word instruction (popq), 276 
popl instruction behavior 
of, 
350¨C351 
instruction code for, 384 
processing 
steps, 
369, 
371 
Y86, 339, 
340 
popl [IA32/Y86] pop double word, 171, 
173, 
339 
popq [x86-64] 
pop 
quad 
word, 
276 
Portable Executable (PE) format, 658 
portable 
signal 
handling, 
752¨C753 
ports Ethernet, 888 
Internet, 899 
I/O, 579 
register 
.les, 
362 
.pos directive, 
346 


position-independent code (PIC), 
687 


data 
references, 
687¨C688 

function 
calls, 
688¨C690 
positive over.ow, 84 
posix_error [CS:APP] reports 
Posix-style errors, 1001 
Posix 
standards, 
15 
Posix-style error handling, 1000, 
1001 
Posix threads, 948, 
948¨C949 
POST 
method, 
915¨C916, 
918 
PowerPC 
processor 
family, 
334 

RISC 
design, 
342¨C343 
powers 
of 
two, 
division 
by, 
95¨C98 
PPNs (physical page numbers), 788 
PPO (physical page offset), 789 
PPs (physical pages), 779, 
779¨C780 
precedence 
of 
shift 
operations, 
56 
precision 
.oating-point, 103, 
104, 
116, 
128 
in.nite, 
80 

prediction branch, 
208¨C209 
misprediction 
penalties, 
526¨C531 
Y86 
pipelining, 
403, 
406¨C408 

preempted processes, 713 
prefetching 
mechanism, 
623 
pre.x sum, 480, 
481, 
538, 
552 
prepare stack for return instruction 
function (leave), 
221¨C222453 
preprocessors, 5, 
160 
prethreading, 970, 970¨C973 
principle of locality, 586, 
587 
print command 
in 
GDB, 
255 
printf [C Stdlib] formatted printing 
function formatted 
printing, 
43 
numeric 
values 
with, 
70 

priorities PIPE processor forwarding sources, 
427¨C428 

write 
ports, 
387 
private address space, 714 
private areas, 808 
private copy-on-write structures, 809 
private 
declarations, 
661 
private objects, 808, 
809 
privileged instructions, 715 
/proc .lesystem, 715, 
762¨C763 
procedure call instruction, 339 
procedure linkage table (PLT), 688, 

689¨C690 
procedure return 
instruction, 
281, 

339 

procedures, 
219 
call 
performance, 
490¨C491 
control 
transfer, 
221¨C223 
example, 
224¨C229 
recursive, 
229¨C232 
register 
usage 
conventions, 
223¨C 

224 
stack 
frame 
structure, 
219¨C221 
x86-64 
processors, 
282 

process contexts, 16, 
716 
process graphs, 721, 
722 
process groups, 739 
process IDs, 719 
process 
tables, 
716 
processes, 16, 
712, 
718 

background, 733 
concurrent 
.ow, 
712¨C714, 
713 
concurrent programming with, 
935¨C939 
concurrent servers based on, 
936¨C937 
context 
switches, 
716¨C717 
creating 
and 
terminating, 
719¨C723 
default 
behavior, 
724 
error conditions, 
725¨C726 
exit 
status, 
725 
foreground, 734 
IDs, 
719¨C720 
loading programs, 681, 730¨C 

732 
overview, 
16¨C17 
private 
address 
space, 
714 
vs. programs, 
732¨C733 
pros 
and 
cons, 
937 
reaping, 723, 
723¨C729 
running 
programs, 
730¨C736 
sleeping, 
729¨C730 
tools, 
762¨C763 
user and 
kernel 
modes, 
714¨C715 
waitpid function, 
726¨C729 

processor-memory gap, 12, 
586 
processor packages, 799 
processor states, 703 
processors. See central processing 
units (CPUs) procmask1.c [CS:APP] shell program 
with 
race, 
756 
procmask2.c [CS:APP] shell program 
without 
race, 
757 
producer-consumer problem, 966, 
966¨C968 

pro.lers 
code, 
475 
pro.ling, program, 
540¨C545 
program 
counter 
(PC), 
9 

data 
hazards, 
412 
%eip, 161 
in 
fetch 
stage, 
364 
%rip, 275 
SEQ 
timing, 
380 
Y86 instruction set architecture, 
337 
Y86 
pipelining, 
403, 
406¨C408 
program data references locality, 587¨C588 

program registers data 
hazards, 
412 
Y86, 
336¨C337 

programmable ROMs (PROMs), 
567 
programmer-visible state, 336, 
336¨C337 

programs code 
and 
data, 
18 
concurrent. See concurrent 
programming forms, 
4¨C5 
loading 
and 
running, 
730¨C732 
machine-level. See machine-level 
programming objects, 
33 
vs. processes, 
732¨C733 
pro.ling, 
540¨C545 
running, 
10¨C12, 
733¨C736 
Y86, 
345¨C350 

progress graphs, 959, 
960¨C963 
deadlock regions, 986, 
987 
forbidden regions, 964 
limitations, 
966 

prologue blocks, 828 
PROMs (programmable ROMs), 
567 

protection, 
memory, 
786¨C787 
protocol software, 889¨C890 
protocols, 890 
proxy caches, 915 
proxy chains, 915 
ps (picoseconds), 392, 
480 
ps tool, 
762 
pseudo-random number generator 
functions, 980 
psum.c [CS:APP] simple parallel sum program, 975 
PTBRs (page table base registers), 
788 


PTEAs (page table entry addresses), 791 

PTEs (page table entries), 781, 
782 
Core 
i7, 
800¨C802 
TLBs 
for, 
791¨C794, 
797 

pthread_cancel [Unix] terminate another thread, 951 
pthread_create [Unix] create a thread, 
949, 
950 
pthread_detach [Unix] detach thread, 951, 
952 
pthread_exit [Unix] terminate current thread, 950 
pthread_join [Unix] reap a thread, 
951 


pthread_once [Unix] initialize a thread, 952, 
971 
pthread_self [Unix] get thread ID, 
950 
Pthreads, 948, 
948¨C949, 
970 
public 
declarations, 
661 
Purify 
product, 
692 
push double word instruction 
(pushl), 
171, 
173, 
339 
push 
instructions 
in 
x86 
models, 
352 
push operations on stack, 172, 

172¨C174 
push quad word instruction (pushq), 276 

pushl [Y86] 
push, 
338¨C339 
instruction code for, 384 
processing 
steps, 
369¨C370 

pushl [IA32] 
push 
double 
word, 
171, 

173 


pushq [x86-64] 
push 
quad 
word, 
276 
PUT 
method 
in 
HTTP, 
915 
¡°put 
to¡± 
operator 
(C++), 
862 

qsort 
function, 
544 


quad words machine-level 
data, 
167 
x86-64 
processors, 
270, 
277 

queued 
signals, 
745 
QuickPath 
interconnect, 
568, 
800 
quit command 
in 
GDB, 
255 

R_386_32 relocation type, 673 
R_386_PC32 relocation type, 673 
%r8 [x86-64] 
program 
register, 
274 
%r8d [x86-64] low-order 32 bits of 
register %r8, 
274 
%r8w [x86-64] low-order 16 bits of register %r8, 
274 

%r9 [x86-64] 
program 
register, 
274 
%r9d [x86-64] low-order 32 bits of register %r9, 
274 
%r9w [x86-64] low-order 16 bits of 
register %r9, 
274 
%r10 [x86-64] 
program 
register, 
274 
%r10d [x86-64] low-order 32 bits of 
register %r10, 
274 
%r10w [x86-64] low-order 16 bits of 
register %r10, 
274 
%r11 [x86-64] 
program 
register, 
274 
%r11d [x86-64] low-order 32 bits of 
register %r11, 
274 
%r11w [x86-64] low-order 16 bits of 
register %r11, 
274 
%r12 [x86-64] 
program 
register, 
274 
%r12d [x86-64] low-order 32 bits of 
register %r12, 
274 
%r12w [x86-64] low-order 16 bits of 
register %r12, 
274 
%r13 [x86-64] 
program 
register, 
274 
%r13d [x86-64] low-order 32 bits of 
register %r13, 
274 
%r13w [x86-64] low-order 16 bits of 
register %r13, 
274 
%r14 [x86-64] 
program 
register, 
274 
%r14d [x86-64] low-order 32 bits of 
register %r14, 
274 
%r14w [x86-64] low-order 16 bits of 
register %r14, 
274 
%r15 [x86-64] 
program 
register, 
274 
%r15d [x86-64] low-order 32 bits of 
register %r15, 
274 
%r15w [x86-64] low-order 16 bits of register %r15, 
274 
race.c [CS:APP] program with a 
race, 984 
race conditions, 
954 
races, 755 

concurrent 
programming, 
983¨C985 
exposing, 
759 
signals, 
755¨C759 

RAM. See random-access memories 
(RAM) Rambus DRAM (RDRAM), 566 
rand [CS:APP] pseudo-random 
number generator, 980, 
982¨C 

983 
rand_r function, 
982 
random-access memories (RAM), 
361, 
561 
dynamic. See Dynamic RAM (DRAM) 
multiported, 
362 
processors, 
363 
SEQ 
timing, 
380 
static. See Static RAM (SRAM) 
random 
operations 
in 
SSDs, 
582¨C583 
random replacement policies, 594 
ranges 
asymmetric, 
61¨C62, 
71 
bytes, 
34 
constants 
for, 
62 
integral 
types, 
57¨C58 
Java standard, 
63 

RAS (Row Access Strobe) requests, 
563 

%rax [x86-64] 
program 
register, 
274 
%rbp [x86-64] 
program 
register, 
274 
%rbx [x86-64] 
program 
register, 
274 
%rcx [x86-64] 
program 
register, 
274 
%rdi [x86-64] 
program 
register, 
274 
RDRAM (Rambus DRAM), 566 
%rdx [x86-64] 
program 
register, 
274 
reachability graphs, 839 
reachable nodes, 839 
read 
access, 
266 
read and echo input lines function, 
911 
read bandwidth, 621 
read environment variable function, 
732 
read/evaluate steps, 733 
read [Unix] read .le, 865, 
865¨C866 
Read-Only Memory (ROM), 567 
read operations 
buffered, 
868, 
870¨C871 
disk 
sectors, 
578¨C579 
.le 
metadata, 
873¨C875 
.les, 863, 
865¨C866 
SSDs, 
582 
unbuffered, 
867¨C868 
uninitialized 
memory, 
843¨C844 

read 
ports, 
362 
read_requesthdrs [CS:APP] Tiny 
helper function, 923 
read sets, 940 
read throughput, 621 
read transactions, 567, 
568¨C569 
read/write heads, 573 
readelf tool, 662, 
690 
readers-writers problem, 969, 
969¨C 

970 
readline function, 
873 
readn function, 
873 
ready read descriptors, 940 
ready sets, 940 
realloc function, 
814¨C815 
reap thread function, 951 
reaping 

child 
processes, 
723, 
723¨C729 
threads, 951 
rearranging signals in pipelines, 405¨C406 
reassociation 
transformations, 
511, 

518, 
518¨C523, 
548 
receiving signals, 738, 
742, 742¨C745 
recording density, 571 
recording zones, 572 
recursive 
procedures, 
229¨C232 
red 
zones 
in 
stack, 
289 
redirection, I/O, 877, 
877¨C879 
reduced instruction set computers 
(RISC), 
291, 
342 
vs. CISC, 
342¨C344 
IA32 
extensions, 
267 
SPARC 
processors, 
448 

reentrancy 
issues, 
980¨C982 
reentrant functions, 980 
reference, function parameters 
passed 
by, 
226 
reference bits, 801 
reference 
counts, 
875 
reference machines, 485 
referencing 
data 
in 
free 
heap 
blocks, 
847 

nonexistent 
variables, 
846 
refresh, 
DRAM, 
562 
regions, deadlock, 
986, 987 
register .les, 9, 
161 

contents, 
362¨C363, 
499 
purpose, 
339¨C340 
SEQ 
timing, 
380 

register 
identi.ers, 
339¨C340, 
384 
register 
operands, 
170 
register speci.er bytes, 340 
register to memory move instruction 
(rmmovl), 337 
register to register move instruction (rrmovl), 337 

registers, 9 
clocked, 361 
data 
hazards, 
412¨C413 
hardware, 
361¨C362 
IA32, 
116, 
168, 168¨C169 
loop segments, 504¨C505 
pipeline, 
393, 
406 
procedures, 
223¨C224 
program, 
336¨C337, 
361¨C363, 
412 
renaming, 500 
saving, 
287¨C290 
spilling, 240, 
240¨C241, 
525¨C526 
x86-64, 
270, 
273¨C275, 
287¨C290 
Y86, 340, 
401¨C405 

regular .les, 807, 874 
.rel.data section, 
659 
.rel.text section, 
659 
relabeling 
signals, 
405¨C406 
relative 
performance, 
493¨C494 
relative speedup in parallel programs, 
977 
reliable connections, 899 
relocatable 
object 
.les, 
5, 
655, 
657, 
658¨C659 

relocation, 657, 672 
algorithm, 
673¨C674, 
674 
entries, 
672¨C673, 
673 
PC-relative 
references, 
674¨C675 
practice 
problems, 
676¨C677 

remove item from bounded buffer 
function, 968 
renaming registers, 500 
rep [IA32/x86-64] string repeat 
instruction, used as 
no-op, 
281 
repeating 
string 
instruction, 
281 
replacement policies, 594 
replacing blocks, 594 
report shared library error function, 
685 

reporting 
errors, 
1001 
request headers in HTTP, 914 
request 
lines 
in 
HTTP, 
914 
requests 
client-server 
model, 
886 
HTTP, 
914, 
914¨C915 
Requests for Comments (RFCs), 
928 

reset 
con.guration 
in 
pipelining, 
438 
resident sets, 784 
resources 
client-server model, 886 

shared, 
966¨C970 
RESP [Y86] register ID for %esp, 384 
response bodies in HTTP, 915 
response headers in HTTP, 915 
response lines in HTTP, 915 
responses 
client-server 
model, 
886 
HTTP, 
915, 
915¨C916 
restart.c [CS:APP] nonlocal jump example, 762 
restrictions, alignment, 
248¨C251 

ret instruction instruction code for, 384 
processing 
steps, 
372, 
374¨C375 
Y86 
pipelining, 
407¨C408, 
432¨C436, 

438¨C439 
ret [IA32/x86-64/Y86] procedure 
return, 
221¨C222, 
281, 
339 
retiming circuits, 401 
retirement 
units, 
499 
return addresses 
predicting, 
408 

procedures, 
220 
return 
penalty 
in 
CPI, 
445 
reverse engineering 
loops, 
199 

machine 
code, 
155 
Revolutions per minute (RPM), 571 
RFCs (Requests for Comments), 928 
rfork.c [CS:APP] wrapper that 
exposes 
races, 
758 
ridges 
in 
memory 
mountains, 
621¨C624 
right hoinkies (>), 878 
right 
shift 
operations, 
55, 
178 
rings, Boolean, 49 
rio [CS:APP] robust I/O package, 
867 

buffered 
functions, 
868¨C872 
origins, 
873 
unbuffered 
functions, 
867¨C868 

rio_read [CS:APP] internal read function, 
871 
rio_readinitb [CS:APP] initialize read buffer, 868, 
870 
rio_readlineb [CS:APP] robust buffered 
read, 
868, 872 
rio_readn [CS:APP] robust unbuffered read, 867, 
867¨C869 
rio_readnb [CS:APP] robust 
buffered read, 868, 
872 
rio_t [CS:APP] 
read 
buffer, 
870 
rio_writen [CS:APP] robust 
unbuffered write, 867, 
867¨C869 
%rip [x86-64] 
program 
counter, 
275 
RISC (reduced instruction set 
computers), 
291, 
342 
vs. CISC, 
342¨C344 
IA32 
extensions, 
267 
SPARC 
processors, 
448 

Ritchie, 
Dennis, 
4, 
15, 
32, 
882 
rmmovl [Y86] register to memory 
move, 337 
instruction code for, 384 
processing 
steps, 
368¨C369 


RNONE [Y86] ID for indicating no 
register, 384 
Roberts, 
Lawrence, 
900 
robust buffered read functions, 868, 

872 


Robust I/O (rio) package, 867 
buffered 
functions, 
868¨C872 
origins, 
873 
unbuffered 
functions, 
867¨C868 

robust unbuffered read function, 867, 
867¨C869 
robust unbuffered write function, 
867, 
867¨C869 
.rodata section, 658 
ROM (Read-Only Memory), 567 
root nodes, 839 
rotating disks term, 571 
rotational latency of disks, 574 
rotational rate of disks, 570 
round-down mode, 111 
round-to-even mode, 110, 
115 
round-to-nearest mode, 110 
round-toward-zero mode, 111 
round-up mode, 111 
rounding 
in 
division, 
96¨C97 
.oating-point representation, 
110¨C113 
rounding modes, 110, 
110¨C111 
routers, Ethernet, 888 
routines, thread, 949¨C950 
Row Access Strobe (RAS) requests, 
563 
row-major array order, 235, 
588 
row-major sum function, 617, 617¨C 

618 
RPM (revolutions per minute), 571 
rrmovl [Y86] register to register 
move, 337, 
384 
%rsi [x86-64] 
program 
register, 
274 
%rsp [x86-64] stack pointer register, 
274, 
285 
run command 
in 
GDB, 
255 
run concurrency, 713 
run time 
linking, 654 
shared libraries, 682 
stack, 
161 


running in parallel, 714 
processes, 719 
programs, 
10¨C12, 
730¨C736 

.s assembly-language 
.les, 
5, 
162¨C 
163, 
655 
SA [CS:APP] shorthand for struct sockaddr, 902 
SADR [Y86] status code for address 
exception, 384 
safe optimization, 477 
safe trajectories in progress graphs, 
962 

sal [IA32/x86-64] 
shift 
left, 
178, 
180 
salq [IA32/x86-64] 
instruction, 
277 
SAOK [Y86] status code for normal 
operation, 384 
sar [IA32/x86-64] shift arithmetic 
right, 
178, 
180 
SATA interfaces, 577 
saturating 
arithmetic, 
125 
sbrk [C Stdlib] extend the heap, 814, 

815 
emulator, 828 
heap 
memory, 
823 

Sbuf [CS:APP] shared bounded buffer package, 967, 
968 
sbuf_deinit [CS:APP] free bounded buffer, 968 
sbuf_init [CS:APP] allocate and initialize bounded buffer, 968 
sbuf_insert [CS:APP] insert item in a bounded buffer, 968 
sbuf_remove [CS:APP] remove item from bounded buffer, 968 
sbuf_t [CS:APP] bounded buffer used by Sbuf package, 967 
scalar code performance summary, 524¨C525 
scale factor in memory references, 
170 
scaling parallel programs, 977¨C978 
scanf function, 
843 
schedule alarm to self function, 742 
schedulers, 716 
scheduling, 716 

events, 
743 

shared 
resources, 
966¨C970 
scripts, CGI, 
917 
SCSI interfaces, 577 
SDRAM (synchronous DRAM), 566 
second-level domain names, 896 
second readers-writers problem, 969 
sectors, disks, 571, 
575 

reading, 
578¨C579 
spare, 
581 

security 
holes, 
7 
security 
monoculture, 
261 
security vulnerabilities 
getpeername function, 
78¨C79 
XDR 
library, 
91¨C92 
seeds for pseudo-random number 
generators, 980 
seek operations, 573, 
863 
seek time for disks, 573, 
574 
segment header tables, 678, 
678¨C 

679 
segmentation 
faults, 
709 
segmented 
addressing, 
264 
segments 
code, 678, 
679¨C680 
data, 679 
Ethernet, 888, 
889 
virtual memory, 804 

segregated .ts, 836, 
837 
segregated 
free 
lists, 
836¨C838 
segregated storage, 836 
select [Unix] wait for I/O events, 
939 
self-loops, 942 
self-modifying code, 413 
sem_init [Unix] initialize 
semaphore, 963 
sem_post [Unix] V operation, 963 
sem_wait [Unix] P operation, 963 
semaphores, 963, 
963¨C964 

concurrent server example, 
970¨C 

973 
for 
mutual 
exclusion, 
964¨C965 
for scheduling shared resources, 
966¨C970 
sending signals, 738, 
739¨C742 
separate compilation, 654 
SEQ+ Y86 processor design, 400, 

400¨C401 
SEQ Y86 processor design. See 
sequential Y86 implementation sequential circuits, 361 
sequential 
execution, 
185 
sequential operations in SSDs, 
582¨C583 
sequential reference patterns, 588 
sequential Y86 implementation, 364 

decode and write-back stage, 
385¨C387 
execute 
stage, 
387¨C389 
fetch 
stage, 
383¨C385 
hardware 
structure, 
375¨C379 


instruction processing stages, 
364¨C375 
memory 
stage, 
389¨C390 
PC 
update 
stage, 
390 
performance, 
391 
timing, 
379¨C383 

serve_dynamic [CS:APP] Tiny helper function, 926, 
926¨C927 

serve_static [CS:APP] Tiny helper function, 
924¨C926, 
925 

servers, 21 
client-server model, 886 
concurrent. See concurrent servers network, 21 
Web. See Web servers 
services in client-server model, 886 

serving dynamic 
content, 
916¨C919 
Web content, 912 

set associative caches, 606 
line matching and word selection, 607¨C608 
line 
replacement, 
608 
set selection, 607 

set index bits, 598 


set on equal instruction (sete), 
187 

set on greater instruction (setg), 
187 

set on greater or equal instruction (setge), 
187 

set on less instruction (setl), 
187 

set on less or equal instruction (setle), 
187 

set on negative instruction (sets), 187 

set on nonnegative instruction (setns), 
187 

set on not equal instruction (setne), 187 

set on not greater instruction (setng), 
187 

set on not greater or equal instruction (setnge), 
187 

set on not less instruction (setnl), 187 

set on not less or equal instruction (setnle), 
187 

set on not zero instruction (setnz), 187 

set on unsigned greater instruction (seta), 
187 

set on unsigned greater or equal instruction (setae), 
187 
set on unsigned less instruction (setb), 
187 

set on unsigned less or equal instruction (setge), 
187 

set on unsigned not greater instruction (setna), 
187 

set on unsigned not less instruction (setnb), 
187 

set on unsigned not less or equal instruction (setnbe), 
187 

set on zero instruction (setz), 
187 

set process group ID function, 739 

set selection direct-mapped 
caches, 
599 
fully 
associative 
caches, 
608 
set associative caches, 607 

seta [IA32/x86-64] set on unsigned greater, 
187 

setae [IA32/x86-64] set on unsigned greater or equal, 
187 

setb [IA32/x86-64] set on unsigned less, 
187 

setbe [IA32/x86-64] set on unsigned less or equal, 
187 

sete [IA32/x86-64] set on equal, 
187 

setenv [Unix] create/change environment variable, 732 

setg [IA32/x86-64] set on greater, 187 

setge [IA32/x86-64] set on greater or equal, 
187 

setjmp [C Stdlib] initialzie nonlocal jump, 
703, 
759, 
760 

setjmp.c [CS:APP] nonlocal jump example, 761 

setl [IA32/x86-64] set on less, 
187 

setle [IA32/x86-64] set on less or equal, 
187 

setna [IA32/x86-64] set on unsigned not 
greater, 
187 

setnae [IA32/x86-64] set on unsigned not less or equal, 187 

setnb [IA32/x86-64] set on unsigned not 
less, 
187 

setnbe [IA32/x86-64] set on unsigned not less or equal, 187 

setne [IA32/x86-64] set on not equal, 187 

setng [IA32/x86-64] set on not greater, 
187 

Index 1037 
setnge [IA32/x86-64] set on not greater or equal, 
187 
setnl [IA32/x86-64] set on not less, 187 
setnle [IA32/x86-64] set on not less or 
equal, 
187 
setns [IA32/x86-64] set on nonnegative, 187 setnz [IA32/x86-64] set on not zero, 187 
setpgid [Unix] set process group ID, 739 

sets vs. cache 
lines, 
615 
membership, 
360¨C361 

sets [IA32/x86-64] set on negative, 187 
setz [IA32/x86-64] set on zero, 187 
SF [IA32/x86-64/Y86] sign .ag 
condition code, 185, 
337 
sh [Unix] 
Unix 
shell 
program, 
733 
Shannon, 
Claude, 
48 
shared areas, 808 
shared libraries, 18, 
682 

dynamic 
linking 
with, 
681¨C683 
loading and linking from 
applications, 
683¨C686 
shared object .les, 657 
shared objects, 682, 
807¨C809, 
808 
shared resources, scheduling, 
966¨C 

970 
shared variables, 954, 
954¨C957 
sharing 
.les, 
875¨C877 
virtual 
memory 
for, 
786 

sharing.c [CS:APP] sharing in Pthreads programs, 955 
shellex.c [CS:APP] shell main 
routine, 734 
shells, 7, 
733 
shift 
operations, 
54¨C56 

for 
division, 
95¨C98 
machine 
language, 
179¨C180 
for 
multiplication, 
92¨C95 
shift arithmetic right instruction, 
178 
shift 
left 
instruction, 
178 
shift 
logical 
right 
instruction, 
178 

shl [IA32/x86-64] 
shift 
left, 
178, 
180 
SHLT [Y86] status code for halt, 384 
short counts, 866 


short [C] 
integer 
data 
types, 
39 
ranges, 
57 
with 
x86-64 
processors, 
270 

shr [IA32/x86-64] shift logical right, 178, 
180 

%si [x86-64] low-order 16 bits of register %rsi, 274 

side 
effects, 
479 


sigaction [Unix] install portable handler, 752 

sigaddset [Unix] add signal to signal set, 753 

sigdelset [Unix] delete signal from signal set, 753 

sigemptyset [Unix] clear a signal set, 753 

sigfillset [Unix] add every signal to signal set, 753 

SIGINT signal, 745 


sigint1.c [CS:APP] catches SIGINT signal, 745 

sigismember [Unix] test signal set membership, 753 

siglongjmp [Unix] initialize nonlocal jump, 759, 
760 

sign bits .oating-point 
representation, 
128 
two¡¯s-complement representation, 60 

sign extension, 72, 72¨C73 

sign .ag condition code (SF), 185, 
337 

sign-magnitude 
representation, 
63 

signal function, 743 


Signal [CS:APP] portable version 
of signal, 752 
signal handlers, 744 
installing, 
742 
signal1.c [CS:APP] .awed signal handler, 
747¨C748 
signal2.c [CS:APP] .awed signal handler, 
749¨C750 
signal3.c [CS:APP] .awed signal handler, 
751 
signal4.c [CS:APP] portable signal handling example, 754 

signals, 702, 
736¨C737, 
736¨C738 
blocking 
and 
unblocking, 
753¨C754 
enabling 
and 
disabling, 
50 
.ow 
synchronizing, 
755¨C759 
handling 
issues, 
745¨C751 
portable 
handling, 
752¨C753 
processes, 
719 
receiving, 742, 
742¨C745 
sending, 738, 739¨C742 
terminology, 
738¨C739 
Y86 pipelined implementations, 
405¨C406 
signed 
divide 
instruction, 
182, 
183, 279 

signed 
integers, 
30, 
58 
alternate 
representations, 
63 
shift 
operations, 
55 
two¡¯s-complement encoding, 
60¨C65 
unsigned 
conversions, 
65¨C71 
signed multiply instruction, 182, 182, 279 
signed representations programming 
advice, 
76¨C79 
signed size type, 866 
signi.cands in .oating-point 
representation, 103 
signs for .oating-point representa-
tion, 
103 
SIGPIPE 
signal, 
927 
sigprocmask [Unix] block and 
unblock signals, 753, 757 
sigsetjmp [Unix] initialize nonlocal handler jump, 759, 
760 
%sil [x86-64] bits 0¨C7 of register 
%rsi, 274 
SimAquarium 
game, 
619 
SIMD (single-instruction, multiple-
data) 
parallelism, 
24¨C25, 

523¨C524 

SIMM (Single Inline Memory Module), 564 
simple segregated storage, 836, 836¨C837 
simplicity in instruction processing, 
365 
simultaneous 
multi-threading, 
22 
single-bit 
data 
connections, 
377 
Single Inline Memory Module 
(SIMM), 564 
single-instruction, multiple-data (SIMD) 
parallelism, 
24¨C25, 

523¨C524 

single-precision .oating-point 
representation IEEE, 103, 
104 
machine-level 
data, 
168 
support 
for, 
39 

SINS [Y86] status code for illegal instruction exception, 384 

size blocks, 822 
caches, 
614 
data, 
38¨C39 
word, 8, 38 

size classes, 836 
size_t [Unix] unsigned size type, 
77¨C78, 
92, 
866 
size tool, 
690 
sizeof [C] compute size of object, 
44, 120¨C122, 
125 
sleep [Unix] suspend process, 729 
slow system calls, 745 
.so .les, 682 
sockaddr [Unix] generic socket 
address structure, 902 
sockaddr_in [Unix] Internet-style socket address structure, 
901¨C902 
socket addresses, 899 
socket 
descriptors, 
880, 
902 
socket function, 
902¨C903 
socket pairs, 899 
sockets, 
874, 
899 
sockets interface, 900, 900¨C901 

accept function, 
907¨C908 
address 
structures, 
901¨C902 
bind function, 
904¨C905 
connect function, 
903 
example, 
908¨C911 
listen function, 
905 
open_clientfd function, 
903¨C904 
open_listenfd function, 
905¨C 

906 

socket function, 
902¨C903 
Software 
Engineering 
Institute, 
92 
software exceptions 
C++ 
and 
Java, 
760 
ECF 
for, 
703¨C704 
vs. hardware, 
704 

Solaris, 15 
and ELF, 658 
Sun Microsystems operating 
system, 
44 

solid-state 
disks 
(SSDs), 
571, 
581 
bene.ts, 
567 
operation, 
581¨C583 

sorting 
performance, 
544 
source .les, 3 
source hosts, 889 
source programs, 3 
southbridge 
chipsets, 
568 
Soviet 
Union, 
900 


%sp [x86-64] low-order 16 bits of stack pointer register %rsp, 
274 

SPARC 64-bit 
version, 
268 
.ve-stage 
pipelines, 
448¨C449 
RISC 
processors, 
343 
Sun 
Microsystems 
processor, 
44 

spare cylinders, 576, 
581 
spare 
sectors, 
581 
spatial locality, 587 

caches, 
625¨C629 
exploiting, 
595 
special 
arithmetic 
operations, 
182¨C 
185, 
278¨C279 
special control conditions in Y86 
pipelining detecting, 
436¨C437 
handling, 
432¨C436 


speci.ers, 
operand, 
169¨C170 
speculative execution, 498, 
499, 
527 
speedup of parallel programs, 977, 
978 
spilling, register, 240, 240¨C241, 
525¨C526 
spindles, disks, 570 
%spl [x86-64] bits 0¨C7 of stack pointer 
register %rsp, 
274 

splitting free 
blocks, 
823 
memory blocks, 820 


sprintf [C 
Stdlib] 
function, 
43, 
259 
Sputnik, 
900 
squashing mispredicted branch 
handling, 
434 
SRAM (Static RAM), 13, 
561, 
561¨C562 
cache. See caches and cache 
memory vs. DRAM, 
562 
trends, 
584¨C585 


SRAM cells, 561 
srand [CS:APP] pseudo-random number generator seed, 980 

SSDs 
(solid-state 
disks), 
571, 
581 
bene.ts, 
567 
operation, 
581¨C583 

SSE (Streaming SIMD Extensions) 
instructions, 
156¨C157 
data 
alignment 
exceptions, 
249 
parallelism, 
523¨C524 

SSE2 (Streaming SIMD Extensions, version 
2), 
292¨C293 
ssize_t [Unix] signed size type, 866 

stack 
corruption 
detection, 
263¨C265 

stack frames, 219, 
219¨C221 
alignment 
on, 
249 
x86-64 
processors, 
284¨C287 

stack 
pointers, 
219, 
289 
stack 
protectors, 
263 
stack 
randomization, 
261¨C262 
stacks, 18, 
172, 
172¨C174 

buffer 
over.ow, 
844 
byte 
alignment, 
226 
with execve function, 
731¨C732 
machine-level 
programs, 
161 
over.ow. See buffer over.ow recursive 
procedures, 
229¨C232 
Y86 
pipelining, 
408 

stages, SEQ, 
364¨C375 
decode 
and 
write-back, 
385¨C387 
execute, 
387¨C389 
fetch, 
383¨C385 
memory 
stage, 
389¨C390 
PC 
update, 
390 

stalling, pipeline, 
413¨C415, 
437¨C438 
Stallman, 
Richard, 
6, 
15 
standard C library, 4, 
4¨C5 
standard error .les, 863 
standard I/O library, 879, 
879¨C880 
standard input .les, 863 
standard output .les, 863 
startup code, 680 
starvation in readers-writers 
problem, 969 
stat [Unix] fetch .le metadata, 873 
state machines, 942 
states 
bistable 
memory, 
561 
deadlock, 986 
processor, 703 
programmer-visible, 336, 336¨C337 
in progress graphs, 961 
state machines, 942 

static libraries, 667, 667¨C672 
static linkers, 657 
static linking, 657 
Static RAM (SRAM), 13, 
561, 

561¨C562 
cache. See caches and cache 
memory vs. DRAM, 
562 
trends, 
584¨C585 

static [C] variable and function 
attribute, 660, 661, 
956 
static Web content, 912 
status 
code 
registers, 
413 

status codes HTTP, 916 
Y86, 
344¨C345, 
345 

status messages in HTTP, 916 
STDERR_FILENO [Unix] constant for 
standard error descriptor, 863 
stderr stream, 
879 
STDIN_FILENO [Unix] constant for 
standard input descriptor, 863 
stdin stream, 
879 
stdint.h .le, 
63 
stdio.h [Unix] standard I/O library 
header 
.le, 
77¨C78 
stdlib, 4, 
4¨C5 
STDOUT_FILENO [Unix] constant for 
standard output descriptor, 863 
stdout stream, 
879 
stepi command 
in 
GDB, 
255 
Stevens, W. Richard, 
873, 
882, 
928, 

999 
stopped processes, 719 
storage. See information storage storage 
classes 
for 
variables, 
956 
storage 
device 
hierarchy, 
13¨C14 
store 
buffers, 
534¨C535 
store 
instructions, 
10 
store 
operations, 
499 
store performance of memory, 
532¨C537 
strace tool, 
762 
straight-line 
code, 
185 
strcat function, 
259 
strcpy function, 
259 
Streaming SIMD Extensions (SSE) 
instructions, 
156¨C157 
data 
alignment 
exceptions, 
249 
parallelism, 
523¨C524 

Streaming SIMD Extensions, version 2 
(SSE2), 
292¨C293 

streams, 879 
buffers, 
879¨C880 
full 
duplex, 
880 

strerror function, 
718 
stride-1 reference patterns, 588 
stride-k reference patterns, 588 
string repeat instruction (rep), 
281 
strings 
in 
buffer 
over.ow, 
256¨C259 
length, 
77 
lowercase 
conversions, 
487¨C489 
representing, 
46¨C47 

strings tool, 
690 
strip tool, 
690 
strlen function, 
77, 
487¨C489 
strong scaling, 977 
strong symbols, 664 
.strtab section, 659 
strtok function, 
982¨C983 
struct [C] 
structure 
data 
type, 
241 
structures 

address, 
901¨C902 
heterogeneous. See heterogeneous 
data structures machine-level 
programs, 
161 
x86-64 
processors, 
290¨C291 

sub [IA32/x86-64] 
subtract, 
178 
subdomains, 896 
subl [Y86] subtract, 338, 
367 
substitution, 
inline, 
479 
subtract instruction (sub), 
178, 
338 
subtract operation in execute stage, 
387 
sumarraycols [CS:APP] column-major sum, 617 
sumarrayrows [CS:APP] row-major sum, 617, 
617¨C618 
sumvec [CS:APP] vector sum, 616, 
616¨C617 

Sun 
Microsystems, 
44 
.ve-stage 
pipelines, 
448¨C449 
RISC 
processors, 
343 
security 
vulnerability, 
91¨C92 
SPARC 
architecture, 
268 
workstations, 
268 

supercells, 562, 
563¨C564 
superscalar 
processors, 
24, 
448¨C449, 

497 
supervisor mode, 715 
surfaces, disks, 570, 
575 
suspend process function, 729 
suspend until signal arrives function, 
730 
suspended processes, 719 
swap areas, 807 
swap .les, 807 
swap space, 807 
swapped in pages, 783 
swapped out pages, 783 
swapping pages, 783 
sweep phase in Mark&Sweep 
garbage collectors, 840 
Swift, 
Jonathan, 
40¨C41 
switch [C] multiway branch 
statement, 
213¨C219 
switches, 
context, 
716¨C717 

symbol resolution, 657, 
663¨C664 
multiply de.ned global symbols, 664¨C667 

static 
libraries, 
667¨C672 
symbol tables, 659, 
660¨C662 
symbolic 
methods, 
443 
symbols 
address 
translation, 
788 
caches, 
598 
relocation, 
672¨C678 
strong and weak, 664 

.symtab section, 659 

synchronization .ow, 
755¨C759 
Java threads, 
970 
progress graphs, 962 
threads, 
957¨C960 

progress graphs, 960¨C963 
with semaphores. See 
semaphores synchronization errors, 957 
synchronous DRAM (SDRAM), 566 
/sys .lesystem, 716 
syscall function, 
710 
system bus, 568 
system calls, 17, 
707, 707¨C708 

error-handling, 
717¨C718 
Linux/IA32 
systems, 
710¨C711 
slow, 745 

system-level functions, 710 

system-level I/O closing 
.les, 
865 
.le 
metadata, 
873¨C875 
I/O 
redirection, 
877¨C879 
opening 
.les, 
863¨C865 
packages 
summary, 
880¨C881 
reading 
.les, 
865¨C866 
rio package, 
867¨C873 
sharing 
.les, 
875¨C877 
standard, 
879¨C880 
summary, 
881¨C882 
Unix 
I/O, 
862¨C863 
writing 
.les, 
866¨C867 

System 
V 
Unix, 
15 
and ELF, 658 
semaphores, 
937 
shared 
memory, 
937 

T2B (two¡¯s complement to binary conversion), 66 
T2U (two¡¯s complement to unsigned conversion), 66, 
66¨C69 

tables descriptor, 
875¨C876, 
878 
exception, 
704, 
705 
GOTs, 687, 
688¨C690 
hash, 
544¨C545 
header, 658, 
678, 
678¨C679 
jump, 213, 
216, 
705 
page, 
716, 
780, 
780¨C781, 
792¨C794, 

797 
segment header, 678, 
678¨C679 
symbol, 659, 
660¨C662 

tag bits, 596¨C597, 
598 
tags, 
boundary, 
824¨C826, 
825, 
833 
targets, jump, 190, 
190¨C193 
TCP (Transmission Control 
Protocol), 892 
TCP/IP (Transmission Control Protocol/Internet Protocol), 
892 

tcsh [Unix] 
Unix 
shell 
program, 
733 
telnet remote login program, 914 
temporal locality, 587 

blocking for, 629 

exploiting, 
595 
terabytes, 
271 
terminate another thread function, 
951 

terminate current thread function, 
950 
terminate process function, 719 
terminated processes, 719 
terminating 
processes, 
719¨C723 

threads, 
950¨C951 
test [IA32/x86-64] 
test, 
186, 
280 
test byte instruction (testb), 
186 
test double word instruction (testl), 
186 
test 
instructions, 
186, 
280 
test quad word instruction (testq), 
280 
test signal set membership function, 
753 

test word instruction (testw), 
186 
testb [IA32/x86-64] 
test 
byte, 
186 
testing 
Y86 
pipeline 
design, 
442¨C443 
testl [IA32/x86-64] test double 
word, 
186 
testq [IA32/x86-64] test quad word, 
280 
testw [IA32/x86-64] 
test 
word, 
186 
text .les, 3, 
870 
text lines, 868 

text representation ASCII, 
46 
Unicode, 
47 

.text section, 658 
Thompson, 
Ken, 
15 
thrashing 

direct-mapped caches, 604 

pages, 784 
thread contexts, 947, 
955 
thread IDs (TIDs), 947 
thread-level concurrency, 
22¨C23 
thread-level 
parallelism, 
23 
thread routines, 949¨C950 
thread-safe functions, 979, 
979¨C981 
thread-unsafe functions, 979, 
979¨C 

980 
threads, 17, 
935, 
947, 
947¨C948 
concurrent server based on, 
952¨C954 
creating, 
950 
detaching, 
951¨C952 
execution 
model, 
948 
initializing, 
952 
library 
functions 
for, 
982¨C983 
mapping 
variables 
in, 
956 
memory 
models, 
955¨C956 
for 
parallelism, 
974¨C978 
Posix, 
948¨C949 
races, 
983¨C985 
reaping, 
951 
safety 
issues, 
979¨C980 
shared variables with, 954, 
954¨C 

957 


synchronizing, 
957¨C960 
progress graphs, 960¨C963 
with semaphores. See 
semaphores terminating, 
950¨C951 

throughput, 501 
dynamic memory allocators, 818 
pipelining for. See pipelining read, 621 

throughput 
bounds, 
497, 
502 
TIDs (thread IDs), 947 
time slicing, 713 
timing, SEQ, 
379¨C383 
tiny [CS:APP] Web server, 919, 

919¨C927 
TLB index (TLBI), 791 
TLB tags (TLBT), 791, 
797 
TLBI (TLB index), 791 

TLBs (translation lookaside buffers), 448, 
791, 
791¨C797 

TLBT (TLB tags), 791, 
797 

TMax (maximum two¡¯s-complement number), 61,62 

TMin (minimum two¡¯s-complement number), 61, 
62, 
71 

top of stack, 172, 
173 

top tool, 
762 

Torvalds, 
Linus, 
19 

touching pages, 807 

TRACE 
method, 
915 

tracing 
execution, 
367, 
369¨C370, 
373¨C375, 
382 

track density of disks, 571 

tracks, disks, 571, 
575 

trajectories in progress graphs, 961, 
962 

transactions bus, 567, 
568¨C570 
client-server model, 886 
client-server vs. database, 
887 
HTTP, 
914¨C916 

transfer time for disks, 574 

transfer units, 593 

transferring 
control, 
221¨C223 

transformations, reassociation, 
511, 
518, 
518¨C523, 
548 

transistors 
in 
Moore¡¯s 
Law, 
158¨C159 

transitions progress graphs, 961 
state machines, 942 

translating 
programs, 
4¨C5 

translation address. See address translation binary, 691¨C692 
switch statements, 
213 

translation lookaside buffers (TLBs), 448, 
791, 
791¨C797 

Transmission Control Protocol (TCP), 892 

Transmission Control Proto-col/Internet Protocol (TCP/IP), 892 

trap exception class, 706 

traps, 707, 
707¨C708 

tree 
height 
reduction, 
548 

tree 
structure, 
245¨C246 

truncating 
numbers, 
75¨C76 

two-operand multiply instructions, 182 

two-way 
parallelism, 
514¨C515 

Index 1041 
two¡¯s-complement representation addition, 83, 
83¨C87 
asymmetric 
range, 
61¨C62, 
71 
bit-level 
representation, 
88 
encodings, 
30 
maximum 
value, 
61 
minimum 
value, 
61 
multiplication, 89, 
89¨C92 
negation, 87, 
87¨C88 
signed and unsigned conversions, 
65¨C69 

signed numbers, 60, 
60¨C65 
typedef [C] 
type 
de.nition, 
42, 
43 
types 
conversions. See conversions .oating 
point, 
114¨C117 
IA32, 
167¨C168 
integral, 57, 
57¨C58 
machine-level, 
161, 
167¨C168 
MIME, 912 
naming, 
43 
pointers, 
33¨C34, 
252 
x86-64 
processors, 
270¨C271 

U2B (unsigned to binary conversion), 66,68 
U2T (unsigned to two¡¯s-complement conversion), 66, 
69, 
76 
UDP (Unreliable Datagram Protocol), 892 
UINTN_MAX [C] maximum value of N-bit 
unsigned 
data 
type, 
62 
uintN_t [C] N-bit unsigned integer 
data 
type, 
63 
umask function, 
864¨C865 
UMax (maximum unsigned number), 
59, 
61¨C62 
unallocated 
pages, 
779 
unary 
operations, 
178¨C179 
unblocking 
signals, 
753¨C754 
unbuffered 
input 
and 
output, 
867¨C868 
uncached 
pages, 
780 
under.ow, gradual, 
105 
Unicode 
characters, 
47 
uni.ed caches, 612 
Uniform Resource Identi.ers 
(URIs), 915 
uninitialized memory, reading, 
843¨C844 
unions, 
244¨C248 
uniprocessor 
systems, 
16, 
22 
United 
States, 
ARPA 
creation 
in, 
900 
Universal Resource Locators 

(URLs), 913 
Universal Serial Bus (USB), 577 
Unix 
4.xBSD, 
15, 
901 
unix_error [CS:APP] reports 
Unix-style errors, 718, 
1001 
Unix IPC, 937 
Unix operating systems, 15,32 

constants, 
725 
error-handling, 1000, 
1001 
I/O, 19, 
862, 
862¨C863 
static 
libraries, 
668 

Unix signals, 736 
unlocking mutexes, 964 
unmap disk object function, 812 
Unreliable Datagram Protocol 
(UDP), 892 
unrolling 
loops, 
480, 
482, 
509, 
509¨C513, 
551 
unsafe regions in progress graphs, 
962 


unsafe trajectories in progress graphs, 
962 


unsetenv [Unix] delete environment 
variable, 732 
unsigned data 
types, 
57 
unsigned 
representations, 
76¨C79 

addition, 
79¨C83, 
82 
conversions, 
65¨C71 
divide 
instruction, 
182, 
184, 
279 
encodings, 
30, 
58¨C60, 
59 
multiplication, 88, 
182, 
182, 
279 

unsigned size type, 866 
update 
instructions, 
10 
URIs (Uniform Resource 
Identi.ers), 915 
URLs (Universal Resource 
Locators), 913 
USB (Universal Serial Bus), 577 
user-level 
memory 
mapping, 
810¨C 

812 


user mode, 706 
processes, 
714¨C716, 
715 
regular functions in, 708 

user stack, 
18 
UTF-8 
characters, 
47 


v-node tables, 875 
V semaphore operation, 963, 
964 
V [CS:APP] wrapper function for 
Posix sem_post, 963, 
964 
VA. See virtual addresses (VA) valgrind program, 
548 

valid bit cache lines, 596, 
597 
page tables, 781 

values function parameters passed by, 226 

pointers, 
34, 
252 
variable-sized 
arrays, 
238¨C241 
variables 
mapping, 
956 
nonexistent, 
846 
shared, 954, 
954¨C957 
on stack, 
226¨C228 
storage 
classes, 
956 

VAX 
computer, 
53 
vector 
data 
types, 
24, 
482¨C485 
vector dot product function, 603 
vector sum function, 616, 
616¨C617 
vectors, bit, 48, 
49¨C50 
veri.cation 
in 
pipelining, 
443¨C444 
Verilog hardware description 
language for 
logic 
design, 
353 
Y86 pipelining implementation, 
444 
vertical bars || for or operation, 353 
Very Large Instruction Word 
(VLIW) format, 269 
VHDL hardware description 
language, 
353 
victim blocks, 594 
Video RAM (VRAM), 566 
virtual address spaces, 17, 
33, 
778 
virtual addresses (VA) 
machine-level programming, 
160¨C161 
vs. physical, 
777¨C778 
Y86, 
337 

virtual machines as abstraction, 
25 
Java byte 
code, 
293 

virtual memory (VM), 17, 
33, 
776 
as abstraction, 
25 
address 
spaces, 
778¨C779 
address translation. See address 
translation bugs, 
843¨C847 
for 
caching, 
779¨C784 
characteristics, 
776¨C777 
Core 
i7, 
799¨C803 
dynamic memory allocation. See 
dynamic memory allocation garbage 
collection, 
838¨C842 
Linux, 
803¨C807 
in 
loading, 
681 
mapping. See memory mapping for 
memory 
management, 
785¨C786 
for 
memory 
protection, 
786¨C787 
overview, 
17¨C19 
physical vs. virtual addresses, 
777¨C778 

summary, 
848 
virtual page numbers (VPNs), 788 
virtual page offset (VPO), 788 
virtual 
pages 
(VPs), 
266, 
779, 
779¨C780 
viruses, 
261¨C262 
VLIW (Very Large Instruction 
Word) format, 269 
VM. See virtual memory (VM) void* [C] 
untyped 
pointers, 
44 
VP 
(virtual 
pages), 
266, 
779, 
779¨C780 
VPNs (virtual page numbers), 788 
VPO (virtual page offset), 788 
VRAM (Video RAM), 566 
vtune program, 
548, 
692 
vulnerabilities, security, 
78¨C79 

wait [Unix] wait for child process, 
726 
wait for child process functions, 724, 
726, 
726¨C729 
wait for client connection request 
function, 907, 
907¨C908 
wait for I/O events function, 939 
wait.h .le, 
725 
wait sets, 724, 
724 
waitpid [Unix] wait for child 
process, 724, 
726¨C729 
waitpid1 [CS:APP] waitpid example, 727 
waitpid2 [CS:APP] waitpid example, 728 
WANs (wide area networks), 889, 

889¨C890 
warming up caches, 594 
weak scaling, 978 
weak symbols, 664 
wear leveling 
logic, 
583 
Web clients, 911, 
912 
Web 
servers, 
684, 
911 

basics, 
911¨C912 
dynamic 
content, 
916¨C919 
HTTP 
transactions, 
914¨C916 
tiny example, 
919¨C927 
Web 
content, 
912¨C914 

well-known ports, 899 
while [C] 
loop 
statement, 
200¨C203 
wide area networks (WANs), 889, 


889¨C890 
WIFEXITED 
constant, 
725 
WIFEXITSTATUS 
constant, 
725 
WIFSIGNALED 
constant, 
725 
WIFSTOPPED 
constant, 
725 
Windows 
operating 
system, 
44, 
249 
wire names in hardware diagrams, 
377 
WNOHANG 
constant, 
724¨C725 
word-level combinational circuits, 
355¨C360 


word selection direct-mapped 
caches, 
600 
fully 
associative 
caches, 
608 
set 
associative 
caches, 
607¨C608 

word size, 8, 
38 


words, 8 
machine-level 
data, 
167 
x86-64 
processors, 
270, 
277 

working sets, 595, 
784 
world-wide data connections in 
hardware 
diagrams, 
377 
World 
Wide 
Web, 
912 
worm programs, 
260¨C262 
wrappers, error-handling, 718, 
999, 

1001¨C1003 
write [Unix] write .le, 865, 
866¨C867 
write 
access, 
266 
write-allocate approach, 612 
write-back approach, 612 
write-back stage 
instruction 
processing, 
364, 
366, 

368¨C377 
PIPE 
processor, 
426¨C429 
SEQ, 
385¨C387 

write hits, 612 
write 
issues 
for 
caches, 
611¨C612 
write-only registers, 504 
write operations for .les, 863, 

866¨C867 

write ports priorities, 
387 
register 
.les, 
362 

write/read 
dependencies, 
534¨C536 
write 
strategies 
for 
caches, 
615 
write-through approach, 612 
write transactions, 567, 
569¨C570 
writen function, 
873 
writers in readers-writers problem, 
969¨C970 
writing operations, SSDs, 
582¨C583 
WSTOPSIG 
constant, 
725 
WTERMSIG 
constant, 
725 
WUNTRACED 
constant, 
724¨C725 

x86 
microprocessor 
line, 
156 
x86-64 
microprocessors, 
44, 
156, 
158, 

267 

argument 
passing, 
283¨C284 
arithmetic 
instructions, 
277¨C279 
assembly-code 
example, 
271¨C273 
control 
instructions, 
279¨C282 
data 
structures, 
290¨C291 
data 
types, 
270¨C271 
.oating-point 
code, 
492 
history 
and 
motivation, 
268¨C269 
information 
access, 
273¨C277 
machine 
language, 
155¨C156 
overview, 
267¨C268, 
270 
procedures, 
282 
register saving conventions, 
287¨C290 
registers, 
273¨C275 
stack 
frames, 
284¨C287 
summary, 
291 

x87 .oating-point architecture, 
156¨C157, 
292 
XDR 
library, 
91¨C92 
Xeon 
microprocessors, 
269 
XMM 
registers, 
492 
Xor [IA32/x86-64] 
exclusive-or, 
178 
xorl [Y86] exclusive-or, 338 
Y86 instruction set architecture, 
335¨C336 
CISC vs. RISC, 
342¨C344 
details, 
350¨C352 
exception 
handling, 
344¨C345 
vs. IA32, 
342 
instruction 
encoding, 
339¨C342 
instruction 
set, 
337¨C339 
programmer-visible 
state, 
336¨C337 
programs, 
345¨C350 
sequential implementation. See 
sequential Y86 implementation 
Y86 pipelined implementations, 400 
computation 
stages, 
400¨C401 
control logic. See control logic in 
pipelining exception 
handling, 
420¨C423 
hazards. See hazards in pipelining memory system interfacing, 
447¨C448 
multicycle 
instructions, 
446¨C447 
performance 
analysis, 
444¨C446 
predicted 
values, 
406¨C408 
signals, 
405¨C406 
stages. See PIPE processor stages testing, 
442¨C443 
veri.cation, 
443¨C444 
Verilog, 
444 

yas Y86 
assembler, 
348¨C349 
yis Y86 
instruction 
set 
simulator, 
348 

zero extension, 72 
zero .ag condition code (ZF), 185, 
337 
ZF [IA32/x86-64/Y86] zero .ag 
condition code, 185, 
337 
zombie processes, 723, 
723¨C724, 
746 
zones 
maps, 
580¨C581 
recording, 572 











