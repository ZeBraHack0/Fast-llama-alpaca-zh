{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step0: setup\n",
    "Firstly, we should install the required dependency and source code of the project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers==4.28.1 sentencepiece==0.1.97 google protobuf deepspeed==0.9.2 datasets -i https://pypi.tuna.tsinghua.edu.cn/simple  --trusted-host pypi.tuna.tsinghua.edu.cn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Chinese-LLaMA-Alpaca library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ymcui/Chinese-LLaMA-Alpaca.git ../../Chinese-LLaMA-Alpaca\n",
    "!git -C ../../Chinese-LLaMA-Alpaca checkout 7bc1f3d7c426e3685d14eb1e5614066650f94838"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install peft library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/huggingface/peft.git ../../peft\n",
    "!git -C ../../peft checkout 13e53fc\n",
    "!pip install ../../peft -i https://pypi.tuna.tsinghua.edu.cn/simple  --trusted-host pypi.tuna.tsinghua.edu.cn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we should setup the project by adding directories, downloading the model, and preprocessing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ../../cache\n",
    "!mkdir ../../output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"/workspace/llama-7b-hf\"\n",
    "!ln -s {model_dir} ../../"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a textbook of computer system for pretraining. Now let us clean the data simply. The original data has been placed on `../../data/book`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def clean_txt_file(input_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        data = file.readlines()\n",
    "\n",
    "    cleaned_lines = []\n",
    "    for line in data:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            cleaned_lines.append(line)\n",
    "\n",
    "    cleaned_data = '\\n'.join(cleaned_lines)\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8', errors='ignore') as file:\n",
    "        file.write(cleaned_data)\n",
    "\n",
    "def clean_txt_files_in_directory(in_directory, out_directory):\n",
    "    for filename in os.listdir(in_directory):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            input_file = os.path.join(in_directory, filename)\n",
    "            output_file = os.path.join(out_directory, \"cleaned_\" + filename)\n",
    "            clean_txt_file(input_file, output_file)\n",
    "\n",
    "in_directory = '../../data/book'  \n",
    "out_directory = '../../data/clean_book/'  \n",
    "\n",
    "clean_txt_files_in_directory(in_directory, out_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all preparation ready. Let us define some key directories for following usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_tokenizer_dir=\"../../llama-7b-hf\"\n",
    "chinese_sp_model_file=\"../../Chinese-LLaMA-Alpaca/scripts/chinese_sp.model\"\n",
    "output_dir=\"../../output\"\n",
    "script_dir=\"../../Chinese-LLaMA-Alpaca/scripts\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: merge tokens\n",
    "Secondly, let us merge the chinese vocabulary with the original vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32000 20000\n",
      "['<s>', '</s>', '<unk>']\n",
      "[1, 2, 0]\n",
      "{'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}\n",
      "32000\n",
      "Before:32000\n",
      "New model pieces: 49953\n",
      "Chinese-LLaMA tokenizer has been saved to merged_tokenizer_hf\n",
      "['<s>', '</s>', '<unk>']\n",
      "[1, 2, 0]\n",
      "{'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}\n",
      "Test text:\n",
      " 白日依山尽，黄河入海流。欲穷千里目，更上一层楼。\n",
      "The primary use of LLaMA is research on large language models, including\n",
      "Tokenized by LLaMA tokenizer:['▁', '白', '日', '<0xE4>', '<0xBE>', '<0x9D>', '山', '<0xE5>', '<0xB0>', '<0xBD>', '，', '黄', '河', '入', '海', '流', '。', '<0xE6>', '<0xAC>', '<0xB2>', '<0xE7>', '<0xA9>', '<0xB7>', '千', '里', '目', '，', '更', '上', '一', '<0xE5>', '<0xB1>', '<0x82>', '<0xE6>', '<0xA5>', '<0xBC>', '。', '<0x0A>', 'The', '▁primary', '▁use', '▁of', '▁L', 'La', 'MA', '▁is', '▁research', '▁on', '▁large', '▁language', '▁models', ',', '▁including']\n",
      "Tokenized by Chinese-LLaMA tokenizer:['▁白', '日', '依', '山', '尽', '，', '黄河', '入', '海', '流', '。', '欲', '穷', '千里', '目', '，', '更', '上', '一层', '楼', '。', '<0x0A>', 'The', '▁primary', '▁use', '▁of', '▁L', 'La', 'MA', '▁is', '▁research', '▁on', '▁large', '▁language', '▁models', ',', '▁including']\n"
     ]
    }
   ],
   "source": [
    "!python {script_dir}/merge_tokenizers.py --llama_tokenizer_dir {llama_tokenizer_dir} --chinese_sp_model_file {chinese_sp_model_file}\n",
    "!mv merged_tokenizer_hf {output_dir}\n",
    "!mv merged_tokenizer_sp {output_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step2: pretrain\n",
    "Now let us pretrain the model using the prepared data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we adjust the lora saving path in the original pretrain script\n",
    "!cp run_clm_pt_with_peft.py {script_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before start training, we should first learn to manage our training arguments using class inheritance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List, Dict, Any, Mapping\n",
    "from dataclasses import dataclass, field\n",
    "from transformers import TrainingArguments, MODEL_FOR_CAUSAL_LM_MAPPING\n",
    "from transformers.utils.versions import require_version\n",
    "\n",
    "MODEL_CONFIG_CLASSES = list(MODEL_FOR_CAUSAL_LM_MAPPING.keys())\n",
    "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name_or_path: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"The model checkpoint for weights initialization.Don't set if you want to train a model from scratch.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    tokenizer_name_or_path: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"The tokenizer for weights initialization.Don't set if you want to train a model from scratch.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    model_type: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"If training from scratch, pass a model type from the list: \" + \", \".join(MODEL_TYPES)},\n",
    "    )\n",
    "    config_overrides: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Override some existing default config settings when a model is trained from scratch. Example: \"\n",
    "                \"n_embd=10,resid_pdrop=0.2,scale_attn_weights=false,summary_type=cls_index\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
    "    )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
    "    )\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n",
    "    )\n",
    "    use_fast_tokenizer: bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n",
    "    )\n",
    "    model_revision: str = field(\n",
    "        default=\"main\",\n",
    "        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n",
    "    )\n",
    "    use_auth_token: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Will use the token generated when running `huggingface-cli login` (necessary to use this script \"\n",
    "                \"with private models).\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    torch_dtype: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Override the default `torch.dtype` and load the model under this dtype. If `auto` is passed, the \"\n",
    "                \"dtype will be automatically derived from the model's weights.\"\n",
    "            ),\n",
    "            \"choices\": [\"auto\", \"bfloat16\", \"float16\", \"float32\"],\n",
    "        },\n",
    "    )\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.config_overrides is not None and (self.config_name is not None or self.model_name_or_path is not None):\n",
    "            raise ValueError(\n",
    "                \"--config_overrides can't be used in combination with --config_name or --model_name_or_path\"\n",
    "            )\n",
    "\n",
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_dir: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    dataset_config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    train_file: Optional[str] = field(default=None, metadata={\"help\": \"The input training data file (a text file).\"})\n",
    "    validation_file: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"An optional input evaluation data file to evaluate the perplexity on (a text file).\"},\n",
    "    )\n",
    "    max_train_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n",
    "                \"value if set.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    max_eval_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n",
    "                \"value if set.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    streaming: bool = field(default=False, metadata={\"help\": \"Enable streaming mode\"})\n",
    "    block_size: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Optional input sequence length after tokenization. \"\n",
    "                \"The training dataset will be truncated in block of this size for training. \"\n",
    "                \"Default to the model max input length for single sentence inputs (take into account special tokens).\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    overwrite_cache: bool = field(\n",
    "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
    "    )\n",
    "    validation_split_percentage: Optional[float] = field(\n",
    "        default=0.05,\n",
    "        metadata={\n",
    "            \"help\": \"The percentage of the train set used as validation set in case there's no validation split\"\n",
    "        },\n",
    "    )\n",
    "    preprocessing_num_workers: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n",
    "    )\n",
    "    keep_linebreaks: bool = field(\n",
    "        default=True, metadata={\"help\": \"Whether to keep line breaks when using TXT files or not.\"}\n",
    "    )\n",
    "    data_cache_dir: Optional[str] = field(default=\"./\", metadata={\"help\": \"The datasets processed stored\"})\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.streaming:\n",
    "            require_version(\"datasets>=2.0.0\", \"The streaming feature requires `datasets>=2.0.0`\")\n",
    "\n",
    "@dataclass\n",
    "class MyTrainingArguments(TrainingArguments):\n",
    "    trainable : Optional[str] = field(default=\"q_proj,v_proj\")\n",
    "    lora_rank : Optional[int] = field(default=8)\n",
    "    lora_dropout : Optional[float] = field(default=0.1)\n",
    "    lora_alpha : Optional[float] = field(default=32.)\n",
    "    modules_to_save : Optional[str] = field(default=None)\n",
    "    debug_mode : Optional[bool] = field(default=False)\n",
    "    peft_path : Optional[str] = field(default=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We proivde a basic version of training arguments, you can create your own arguments by inheriting it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = ModelArguments(model_name_or_path=\"../../llama-7b-hf/\", \n",
    "                            tokenizer_name_or_path=\"../../output/merged_tokenizer_hf\",\n",
    "                            torch_dtype=\"float16\")\n",
    "\n",
    "data_args = DataTrainingArguments(dataset_dir=\"../../data/clean_book\",\n",
    "                                  data_cache_dir=\"../../cache\",\n",
    "                                  validation_split_percentage=0.001,\n",
    "                                  block_size=512,\n",
    "                                  preprocessing_num_workers=8)\n",
    "\n",
    "deepspeed_config_file = script_dir+\"/\"+\"ds_zero2_no_offload.json\" # we can not include deepspeed config path directly here otherwise we will init deepspeed environment in advance\n",
    "training_args =  MyTrainingArguments(per_device_train_batch_size=1,\n",
    "                                    per_device_eval_batch_size=1,\n",
    "                                    do_train=True,\n",
    "                                    seed=100,\n",
    "                                    fp16=True,\n",
    "                                    max_steps=100,\n",
    "                                    lr_scheduler_type=\"cosine\",\n",
    "                                    learning_rate=2e-4,\n",
    "                                    warmup_ratio=0.05,\n",
    "                                    weight_decay=0.01,\n",
    "                                    logging_strategy=\"steps\",\n",
    "                                    logging_steps=10,\n",
    "                                    logging_first_step=True,\n",
    "                                    save_strategy=\"steps\",\n",
    "                                    save_steps=500,\n",
    "                                    save_total_limit=3,\n",
    "                                    gradient_accumulation_steps=1,\n",
    "                                    gradient_checkpointing=True,\n",
    "                                    ddp_find_unused_parameters=False,\n",
    "                                    ddp_timeout=30000,\n",
    "                                    output_dir=\"../../output/llama-zh\",\n",
    "                                    overwrite_output_dir=True,\n",
    "                                    lora_rank=8,\n",
    "                                    lora_alpha=32,\n",
    "                                    trainable=\"q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj\",\n",
    "                                    modules_to_save=\"embed_tokens,lm_head\",\n",
    "                                    lora_dropout=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us start pretraining using the prepared arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/libbitsandbytes_cuda114.so\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /opt/conda did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/compat/lib'), PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vscode-local'), PosixPath('/c%3A/Users/zzz48/.vscode/extensions/ms-ceintl.vscode-language-pack-zh-hans-1.84.2023111509/translations/extensions/vscode.markdown-language-features.i18n.json')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('[\"/tmp/vscode-ssh-auth-99aaedd1-49f2-4aea-b7fd-66afd70c1944.sock\",\"/root/.gnupg/S.gpg-agent\"]')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('{\"*\"'), PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn/tcf.json\",\"_cacheRoot\"'), PosixPath('\"zh-cn\",\"availableLanguages\"'), PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn\",\"_resolvedLanguagePackCoreLocation\"'), PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn/corrupted.info\",\"_languagePackSupport\"'), PosixPath('true}'), PosixPath('\"zh-cn\"},\"_languagePackId\"'), PosixPath('\"b21449bfcc24b92b09a8c487b9b70068.zh-cn\",\"_translationsConfigFile\"'), PosixPath('\"zh-cn\",\"osLocale\"'), PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn/1a5daa3a0231a0fbba4f14db7ec463cf99d7768e\",\"_corruptedFile\"'), PosixPath('{\"locale\"')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/torchelastic_0wnbdoij/none_14j10fdh/attempt_0/1/error.json')}\n",
      "  warn(msg)\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 114\n",
      "CUDA SETUP: Loading binary /opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/libbitsandbytes_cuda114.so...\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/libbitsandbytes_cuda114.so\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /opt/conda did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/cuda/compat/lib'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vscode-local'), PosixPath('/c%3A/Users/zzz48/.vscode/extensions/ms-ceintl.vscode-language-pack-zh-hans-1.84.2023111509/translations/extensions/vscode.markdown-language-features.i18n.json')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('[\"/tmp/vscode-ssh-auth-99aaedd1-49f2-4aea-b7fd-66afd70c1944.sock\",\"/root/.gnupg/S.gpg-agent\"]')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('{\"*\"'), PosixPath('{\"locale\"'), PosixPath('true}'), PosixPath('\"zh-cn\"},\"_languagePackId\"'), PosixPath('\"zh-cn\",\"availableLanguages\"'), PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn/1a5daa3a0231a0fbba4f14db7ec463cf99d7768e\",\"_corruptedFile\"'), PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn/corrupted.info\",\"_languagePackSupport\"'), PosixPath('\"b21449bfcc24b92b09a8c487b9b70068.zh-cn\",\"_translationsConfigFile\"'), PosixPath('\"zh-cn\",\"osLocale\"'), PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn/tcf.json\",\"_cacheRoot\"'), PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn\",\"_resolvedLanguagePackCoreLocation\"')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/torchelastic_0wnbdoij/none_14j10fdh/attempt_0/3/error.json')}\n",
      "  warn(msg)\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 114\n",
      "CUDA SETUP: Loading binary /opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/libbitsandbytes_cuda114.so...\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/libbitsandbytes_cuda114.so\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /opt/conda did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/cuda/compat/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vscode-local'), PosixPath('/c%3A/Users/zzz48/.vscode/extensions/ms-ceintl.vscode-language-pack-zh-hans-1.84.2023111509/translations/extensions/vscode.markdown-language-features.i18n.json')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('[\"/tmp/vscode-ssh-auth-99aaedd1-49f2-4aea-b7fd-66afd70c1944.sock\",\"/root/.gnupg/S.gpg-agent\"]')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('\"zh-cn\"},\"_languagePackId\"'), PosixPath('\"b21449bfcc24b92b09a8c487b9b70068.zh-cn\",\"_translationsConfigFile\"'), PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn/tcf.json\",\"_cacheRoot\"'), PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn\",\"_resolvedLanguagePackCoreLocation\"'), PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn/corrupted.info\",\"_languagePackSupport\"'), PosixPath('{\"*\"'), PosixPath('true}'), PosixPath('\"zh-cn\",\"osLocale\"'), PosixPath('\"zh-cn\",\"availableLanguages\"'), PosixPath('{\"locale\"'), PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn/1a5daa3a0231a0fbba4f14db7ec463cf99d7768e\",\"_corruptedFile\"')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/torchelastic_0wnbdoij/none_14j10fdh/attempt_0/0/error.json')}\n",
      "  warn(msg)\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 114\n",
      "CUDA SETUP: Loading binary /opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/libbitsandbytes_cuda114.so...\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/libbitsandbytes_cuda114.so\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /opt/conda did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/compat/lib'), PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/c%3A/Users/zzz48/.vscode/extensions/ms-ceintl.vscode-language-pack-zh-hans-1.84.2023111509/translations/extensions/vscode.markdown-language-features.i18n.json'), PosixPath('vscode-local')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('[\"/tmp/vscode-ssh-auth-99aaedd1-49f2-4aea-b7fd-66afd70c1944.sock\",\"/root/.gnupg/S.gpg-agent\"]')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn/tcf.json\",\"_cacheRoot\"'), PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn/corrupted.info\",\"_languagePackSupport\"'), PosixPath('true}'), PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn\",\"_resolvedLanguagePackCoreLocation\"'), PosixPath('\"zh-cn\",\"osLocale\"'), PosixPath('{\"*\"'), PosixPath('\"b21449bfcc24b92b09a8c487b9b70068.zh-cn\",\"_translationsConfigFile\"'), PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn/1a5daa3a0231a0fbba4f14db7ec463cf99d7768e\",\"_corruptedFile\"'), PosixPath('\"zh-cn\",\"availableLanguages\"'), PosixPath('\"zh-cn\"},\"_languagePackId\"'), PosixPath('{\"locale\"')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/torchelastic_0wnbdoij/none_14j10fdh/attempt_0/5/error.json')}\n",
      "  warn(msg)\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 114\n",
      "CUDA SETUP: Loading binary /opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/libbitsandbytes_cuda114.so...\n",
      "bin /opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/libbitsandbytes_cuda114.so\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /opt/conda did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/cuda/compat/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/c%3A/Users/zzz48/.vscode/extensions/ms-ceintl.vscode-language-pack-zh-hans-1.84.2023111509/translations/extensions/vscode.markdown-language-features.i18n.json'), PosixPath('vscode-local')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('[\"/tmp/vscode-ssh-auth-99aaedd1-49f2-4aea-b7fd-66afd70c1944.sock\",\"/root/.gnupg/S.gpg-agent\"]')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('{\"locale\"'), PosixPath('\"b21449bfcc24b92b09a8c487b9b70068.zh-cn\",\"_translationsConfigFile\"'), PosixPath('\"zh-cn\",\"availableLanguages\"'), PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn\",\"_resolvedLanguagePackCoreLocation\"'), PosixPath('{\"*\"'), PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn/1a5daa3a0231a0fbba4f14db7ec463cf99d7768e\",\"_corruptedFile\"'), PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn/corrupted.info\",\"_languagePackSupport\"'), PosixPath('true}'), PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn/tcf.json\",\"_cacheRoot\"'), PosixPath('\"zh-cn\",\"osLocale\"'), PosixPath('\"zh-cn\"},\"_languagePackId\"')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/torchelastic_0wnbdoij/none_14j10fdh/attempt_0/6/error.json')}\n",
      "  warn(msg)\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 114\n",
      "CUDA SETUP: Loading binary /opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/libbitsandbytes_cuda114.so...\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/libbitsandbytes_cuda114.so\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /opt/conda did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/compat/lib'), PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vscode-local'), PosixPath('/c%3A/Users/zzz48/.vscode/extensions/ms-ceintl.vscode-language-pack-zh-hans-1.84.2023111509/translations/extensions/vscode.markdown-language-features.i18n.json')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('[\"/tmp/vscode-ssh-auth-99aaedd1-49f2-4aea-b7fd-66afd70c1944.sock\",\"/root/.gnupg/S.gpg-agent\"]')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn/tcf.json\",\"_cacheRoot\"'), PosixPath('\"b21449bfcc24b92b09a8c487b9b70068.zh-cn\",\"_translationsConfigFile\"'), PosixPath('{\"*\"'), PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn/corrupted.info\",\"_languagePackSupport\"'), PosixPath('\"zh-cn\"},\"_languagePackId\"'), PosixPath('true}'), PosixPath('\"zh-cn\",\"availableLanguages\"'), PosixPath('{\"locale\"'), PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn\",\"_resolvedLanguagePackCoreLocation\"'), PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn/1a5daa3a0231a0fbba4f14db7ec463cf99d7768e\",\"_corruptedFile\"'), PosixPath('\"zh-cn\",\"osLocale\"')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/torchelastic_0wnbdoij/none_14j10fdh/attempt_0/2/error.json')}\n",
      "  warn(msg)\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 114\n",
      "CUDA SETUP: Loading binary /opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/libbitsandbytes_cuda114.so...\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/libbitsandbytes_cuda114.so\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /opt/conda did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/compat/lib'), PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vscode-local'), PosixPath('/c%3A/Users/zzz48/.vscode/extensions/ms-ceintl.vscode-language-pack-zh-hans-1.84.2023111509/translations/extensions/vscode.markdown-language-features.i18n.json')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('[\"/tmp/vscode-ssh-auth-99aaedd1-49f2-4aea-b7fd-66afd70c1944.sock\",\"/root/.gnupg/S.gpg-agent\"]')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('true}'), PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn/corrupted.info\",\"_languagePackSupport\"'), PosixPath('{\"*\"'), PosixPath('\"zh-cn\"},\"_languagePackId\"'), PosixPath('\"zh-cn\",\"availableLanguages\"'), PosixPath('\"zh-cn\",\"osLocale\"'), PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn/1a5daa3a0231a0fbba4f14db7ec463cf99d7768e\",\"_corruptedFile\"'), PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn/tcf.json\",\"_cacheRoot\"'), PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn\",\"_resolvedLanguagePackCoreLocation\"'), PosixPath('\"b21449bfcc24b92b09a8c487b9b70068.zh-cn\",\"_translationsConfigFile\"'), PosixPath('{\"locale\"')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/torchelastic_0wnbdoij/none_14j10fdh/attempt_0/7/error.json')}\n",
      "  warn(msg)\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 114\n",
      "CUDA SETUP: Loading binary /opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/libbitsandbytes_cuda114.so...\n",
      "bin /opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/libbitsandbytes_cuda114.so\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /opt/conda did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/compat/lib'), PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vscode-local'), PosixPath('/c%3A/Users/zzz48/.vscode/extensions/ms-ceintl.vscode-language-pack-zh-hans-1.84.2023111509/translations/extensions/vscode.markdown-language-features.i18n.json')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('[\"/tmp/vscode-ssh-auth-99aaedd1-49f2-4aea-b7fd-66afd70c1944.sock\",\"/root/.gnupg/S.gpg-agent\"]')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn/tcf.json\",\"_cacheRoot\"'), PosixPath('true}'), PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn/corrupted.info\",\"_languagePackSupport\"'), PosixPath('\"zh-cn\"},\"_languagePackId\"'), PosixPath('\"zh-cn\",\"osLocale\"'), PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn\",\"_resolvedLanguagePackCoreLocation\"'), PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn/1a5daa3a0231a0fbba4f14db7ec463cf99d7768e\",\"_corruptedFile\"'), PosixPath('\"b21449bfcc24b92b09a8c487b9b70068.zh-cn\",\"_translationsConfigFile\"'), PosixPath('{\"locale\"'), PosixPath('{\"*\"'), PosixPath('\"zh-cn\",\"availableLanguages\"')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/torchelastic_0wnbdoij/none_14j10fdh/attempt_0/4/error.json')}\n",
      "  warn(msg)\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 114\n",
      "CUDA SETUP: Loading binary /opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/libbitsandbytes_cuda114.so...\n",
      "[2023-12-07 02:00:06,253] [INFO] [comm.py:622:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "12/07/2023 02:00:06 - WARNING - __main__ - Process rank: 7, device: cuda:7, n_gpu: 1distributed training: True, 16-bits training: True\n",
      "12/07/2023 02:00:11 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: True\n",
      "12/07/2023 02:00:11 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True\n",
      "[INFO|configuration_utils.py:666] 2023-12-07 02:00:11,666 >> loading configuration file ../../llama-7b-hf/config.json\n",
      "[INFO|configuration_utils.py:720] 2023-12-07 02:00:11,667 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"../../llama-7b-hf/\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1807] 2023-12-07 02:00:11,667 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:1807] 2023-12-07 02:00:11,667 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:1807] 2023-12-07 02:00:11,667 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1807] 2023-12-07 02:00:11,667 >> loading file tokenizer_config.json\n",
      "12/07/2023 02:00:11 - WARNING - __main__ - Process rank: 6, device: cuda:6, n_gpu: 1distributed training: True, 16-bits training: True\n",
      "12/07/2023 02:00:11 - INFO - __main__ - training datasets-cleaned_Computer_Systems has been loaded from disk\n",
      "12/07/2023 02:00:11 - INFO - datasets.arrow_dataset - Caching indices mapping at /workspace/flaz/cache/cleaned_Computer_Systems/train/cache-c9c1eccad010a608.arrow\n",
      "12/07/2023 02:00:11 - INFO - datasets.arrow_dataset - Caching indices mapping at /workspace/flaz/cache/cleaned_Computer_Systems/train/cache-f1d5bd11df8e7832.arrow\n",
      "12/07/2023 02:02:16 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: True\n",
      "12/07/2023 02:02:22 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: True\n",
      "12/07/2023 02:02:22 - WARNING - __main__ - Process rank: 5, device: cuda:5, n_gpu: 1distributed training: True, 16-bits training: True\n",
      "12/07/2023 02:02:22 - WARNING - __main__ - Process rank: 4, device: cuda:4, n_gpu: 1distributed training: True, 16-bits training: True\n",
      "12/07/2023 02:02:24 - INFO - __main__ - Num train_samples  1122\n",
      "12/07/2023 02:02:24 - INFO - __main__ - training example:\n",
      "12/07/2023 02:02:24 - WARNING - datasets.arrow_dataset - Loading cached split indices for dataset at /workspace/flaz/cache/cleaned_Computer_Systems/train/cache-c9c1eccad010a608.arrow and /workspace/flaz/cache/cleaned_Computer_Systems/train/cache-f1d5bd11df8e7832.arrow\n",
      "12/07/2023 02:02:24 - WARNING - datasets.arrow_dataset - Loading cached split indices for dataset at /workspace/flaz/cache/cleaned_Computer_Systems/train/cache-c9c1eccad010a608.arrow and /workspace/flaz/cache/cleaned_Computer_Systems/train/cache-f1d5bd11df8e7832.arrow\n",
      "12/07/2023 02:02:24 - WARNING - datasets.arrow_dataset - Loading cached split indices for dataset at /workspace/flaz/cache/cleaned_Computer_Systems/train/cache-c9c1eccad010a608.arrow and /workspace/flaz/cache/cleaned_Computer_Systems/train/cache-f1d5bd11df8e7832.arrow\n",
      "12/07/2023 02:02:24 - INFO - __main__ -  o p2 main2.c ./libvector.so<s> This creates an executable object .le p2 in a form that can be linked with libvector.so at run time. The basic idea is to do some of the linking statically when the executable .le is created, and then complete the linking process dynam-ically when the program is loaded.<s> It is important to realize that none of the code or data sections from libvector.so are actually copied into the executable p2 at this point. Instead, the linker copies some relocation and symbol table information that will allow references to code and data in libvector.so to be resolved at run time.<s> When the loader loads and runs the executable p2, it loads the partially linked executable p2, using the techniques discussed in Section 7.9. Next, it notices that p2<s> Figure 7.15 main2.c vector.h<s> Dynamic linking with shared libraries.<s> Relocatable object file<s> Partially linked executable object file<s> Fully linked executable in memory<s> contains a .interp section, which contains the path name of the dynamic linker, which is itself a shared object (e.g., ld-linux.so on Linux systems). Instead of passing control to the application, as it would normally do, the loader loads and runs the dynamic linker.<s> The dynamic linker then .nishes the linking task by performing the following relocations:<s> . Relocating the text and data of libc.so into some memory segment. . Relocating the text and data of libvector.so into another memory segment. . Relocating any references in p2 to symbols de.ned by libc.so and libvec-<s> tor.so.<s> Finally, the dynamic linker passes control to the application. From this point on, the locations of the shared libraries are .xed and do not change during execution of the program.<s> 7.11<s> Loading<s> and<s> Linking<s> Shared<s> Libraries<s> from<s> Applications<s> Up to this point, we have discussed the scenario in which the dynamic linker loads and links shared libraries when an application is loaded, just before it executes. However, it is also possible for an application to request the dynamic linker to load and link arbitrary shared libraries while the application is running, without having to link in the applications against those libraries at compile time.<s>Dynamic linking is a powerful and useful technique. Here are some examples in the real world:\n",
      "[INFO|modeling_utils.py:2531] 2023-12-07 02:02:24,490 >> loading weights file ../../llama-7b-hf/pytorch_model.bin.index.json\n",
      "[INFO|modeling_utils.py:1176] 2023-12-07 02:02:24,490 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.\n",
      "[INFO|configuration_utils.py:575] 2023-12-07 02:02:24,491 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n",
      "12/07/2023 02:02:24 - WARNING - datasets.arrow_dataset - Loading cached split indices for dataset at /workspace/flaz/cache/cleaned_Computer_Systems/train/cache-c9c1eccad010a608.arrow and /workspace/flaz/cache/cleaned_Computer_Systems/train/cache-f1d5bd11df8e7832.arrow\n",
      "12/07/2023 02:02:24 - WARNING - datasets.arrow_dataset - Loading cached split indices for dataset at /workspace/flaz/cache/cleaned_Computer_Systems/train/cache-c9c1eccad010a608.arrow and /workspace/flaz/cache/cleaned_Computer_Systems/train/cache-f1d5bd11df8e7832.arrow\n",
      "12/07/2023 02:02:24 - WARNING - datasets.arrow_dataset - Loading cached split indices for dataset at /workspace/flaz/cache/cleaned_Computer_Systems/train/cache-c9c1eccad010a608.arrow and /workspace/flaz/cache/cleaned_Computer_Systems/train/cache-f1d5bd11df8e7832.arrow\n",
      "12/07/2023 02:02:24 - WARNING - datasets.arrow_dataset - Loading cached split indices for dataset at /workspace/flaz/cache/cleaned_Computer_Systems/train/cache-c9c1eccad010a608.arrow and /workspace/flaz/cache/cleaned_Computer_Systems/train/cache-f1d5bd11df8e7832.arrow\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:04<00:00,  2.38s/it]\n",
      "[INFO|modeling_utils.py:3190] 2023-12-07 02:02:29,453 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:3198] 2023-12-07 02:02:29,453 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at ../../llama-7b-hf/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:04<00:00,  2.39s/it]\n",
      "[INFO|configuration_utils.py:535] 2023-12-07 02:02:29,456 >> loading configuration file ../../llama-7b-hf/generation_config.json\n",
      "[INFO|configuration_utils.py:575] 2023-12-07 02:02:29,456 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:04<00:00,  2.40s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:04<00:00,  2.44s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:05<00:00,  2.50s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:05<00:00,  2.51s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:05<00:00,  2.52s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:05<00:00,  2.59s/it]\n",
      "12/07/2023 02:02:53 - INFO - __main__ - Init new peft model\n",
      "12/07/2023 02:02:53 - INFO - __main__ - target_modules: ['q_proj', 'v_proj', 'k_proj', 'o_proj', 'gate_proj', 'down_proj', 'up_proj']\n",
      "12/07/2023 02:02:53 - INFO - __main__ - lora_rank: 8\n",
      "trainable params: 429203456 || all params: 6905475072 || trainable%: 6.2154080859739\n",
      "trainable params: 429203456 || all params: 6905475072 || trainable%: 6.2154080859739\n",
      "[INFO|trainer.py:564] 2023-12-07 02:03:58,013 >> max_steps is given, it will override any value given in num_train_epochs\n",
      "[INFO|trainer.py:621] 2023-12-07 02:03:58,013 >> Using cuda_amp half precision backend\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[2023-12-07 02:03:58,034] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.9.2, git-hash=unknown, git-branch=unknown\n",
      "trainable params: 429203456 || all params: 6905475072 || trainable%: 6.2154080859739\n",
      "trainable params: 429203456 || all params: 6905475072 || trainable%: 6.2154080859739\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "trainable params: 429203456 || all params: 6905475072 || trainable%: 6.2154080859739\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "trainable params: 429203456 || all params: 6905475072 || trainable%: 6.2154080859739\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "trainable params: 429203456 || all params: 6905475072 || trainable%: 6.2154080859739\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "trainable params: 429203456 || all params: 6905475072 || trainable%: 6.2154080859739\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "12/07/2023 02:04:02 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 0\n",
      "12/07/2023 02:04:02 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 5\n",
      "12/07/2023 02:04:02 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 1\n",
      "12/07/2023 02:04:02 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 3\n",
      "12/07/2023 02:04:02 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 7\n",
      "12/07/2023 02:04:03 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 6\n",
      "12/07/2023 02:04:03 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 4\n",
      "12/07/2023 02:04:10 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 2\n",
      "12/07/2023 02:04:10 - INFO - torch.distributed.distributed_c10d - Rank 2: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\n",
      "12/07/2023 02:04:10 - INFO - torch.distributed.distributed_c10d - Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\n",
      "12/07/2023 02:04:10 - INFO - torch.distributed.distributed_c10d - Rank 5: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\n",
      "12/07/2023 02:04:10 - INFO - torch.distributed.distributed_c10d - Rank 3: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\n",
      "12/07/2023 02:04:10 - INFO - torch.distributed.distributed_c10d - Rank 4: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\n",
      "12/07/2023 02:04:10 - INFO - torch.distributed.distributed_c10d - Rank 7: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\n",
      "12/07/2023 02:04:10 - INFO - torch.distributed.distributed_c10d - Rank 1: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\n",
      "12/07/2023 02:04:10 - INFO - torch.distributed.distributed_c10d - Rank 6: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\n",
      "[2023-12-07 02:04:10,746] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2023-12-07 02:04:10,746] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer\n",
      "[2023-12-07 02:04:10,747] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2023-12-07 02:04:10,778] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW\n",
      "[2023-12-07 02:04:10,778] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'transformers.optimization.AdamW'>\n",
      "[2023-12-07 02:04:10,778] [WARNING] [engine.py:1104:_do_optimizer_sanity_check] **** You are using ZeRO with an untested optimizer, proceed with caution *****\n",
      "[2023-12-07 02:04:10,778] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer\n",
      "[2023-12-07 02:04:10,778] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 100000000\n",
      "[2023-12-07 02:04:10,778] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 100000000\n",
      "[2023-12-07 02:04:10,778] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: False\n",
      "[2023-12-07 02:04:10,778] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False\n",
      "Using /root/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py38_cu117/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.4763612747192383 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.5125188827514648 seconds\n",
      "Time to load utils op: 0.5153262615203857 seconds\n",
      "Time to load utils op: 0.5101122856140137 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.5084495544433594 seconds\n",
      "Time to load utils op: 0.5042629241943359 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.5040209293365479 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.5043082237243652 seconds\n",
      "Rank: 7 partition count [8] and sizes[(53650432, False)] \n",
      "Rank: 1 partition count [8] and sizes[(53650432, False)] \n",
      "Rank: 0 partition count [8] and sizes[(53650432, False)] \n",
      "Rank: 6 partition count [8] and sizes[(53650432, False)] \n",
      "Rank: 4 partition count [8] and sizes[(53650432, False)] \n",
      "Rank: 2 partition count [8] and sizes[(53650432, False)] \n",
      "Rank: 3 partition count [8] and sizes[(53650432, False)] \n",
      "Rank: 5 partition count [8] and sizes[(53650432, False)] \n",
      "Using /root/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Using /root/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0007889270782470703 secondsNo modifications detected for re-loaded extension module utils, skipping build step...\n",
      "\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0007860660552978516 seconds\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0007889270782470703 seconds\n",
      "Time to load utils op: 0.0008323192596435547 seconds\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0008845329284667969 seconds\n",
      "Using /root/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0005524158477783203 seconds\n",
      "Using /root/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0008015632629394531 seconds\n",
      "[WARNING|logging.py:295] 2023-12-07 02:04:23,336 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "[WARNING|logging.py:295] 2023-12-07 02:04:23,336 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "[WARNING|logging.py:295] 2023-12-07 02:04:23,336 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "[WARNING|logging.py:295] 2023-12-07 02:04:23,337 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "[WARNING|logging.py:295] 2023-12-07 02:04:23,337 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "[WARNING|logging.py:295] 2023-12-07 02:04:23,337 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "[WARNING|logging.py:295] 2023-12-07 02:04:23,337 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "[2023-12-07 02:04:23,361] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states\n",
      "[2023-12-07 02:04:23,362] [INFO] [utils.py:786:see_memory_usage] MA 13.09 GB         Max_MA 13.19 GB         CA 13.24 GB         Max_CA 13 GB \n",
      "[2023-12-07 02:04:23,362] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 76.84 GB, percent = 15.3%\n",
      "[2023-12-07 02:04:23,482] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states\n",
      "[2023-12-07 02:04:23,483] [INFO] [utils.py:786:see_memory_usage] MA 13.49 GB         Max_MA 13.89 GB         CA 14.04 GB         Max_CA 14 GB \n",
      "[2023-12-07 02:04:23,483] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 77.07 GB, percent = 15.3%\n",
      "[2023-12-07 02:04:23,483] [INFO] [stage_1_and_2.py:489:__init__] optimizer state initialized\n",
      "[2023-12-07 02:04:23,590] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2023-12-07 02:04:23,591] [INFO] [utils.py:786:see_memory_usage] MA 13.49 GB         Max_MA 13.49 GB         CA 14.04 GB         Max_CA 14 GB \n",
      "[2023-12-07 02:04:23,591] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 77.2 GB, percent = 15.3%\n",
      "[2023-12-07 02:04:23,592] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW\n",
      "[2023-12-07 02:04:23,592] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2023-12-07 02:04:23,592] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.LambdaLR object at 0x7f3b94099670>\n",
      "[2023-12-07 02:04:23,593] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.999)]\n",
      "[2023-12-07 02:04:23,594] [INFO] [config.py:955:print] DeepSpeedEngine configuration:\n",
      "[2023-12-07 02:04:23,595] [INFO] [config.py:959:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2023-12-07 02:04:23,595] [INFO] [config.py:959:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2023-12-07 02:04:23,595] [INFO] [config.py:959:print]   amp_enabled .................. False\n",
      "[2023-12-07 02:04:23,595] [INFO] [config.py:959:print]   amp_params ................... False\n",
      "[2023-12-07 02:04:23,595] [INFO] [config.py:959:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2023-12-07 02:04:23,595] [INFO] [config.py:959:print]   bfloat16_enabled ............. False\n",
      "[2023-12-07 02:04:23,595] [INFO] [config.py:959:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2023-12-07 02:04:23,595] [INFO] [config.py:959:print]   checkpoint_tag_validation_enabled  True\n",
      "[2023-12-07 02:04:23,595] [INFO] [config.py:959:print]   checkpoint_tag_validation_fail  False\n",
      "[2023-12-07 02:04:23,595] [INFO] [config.py:959:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f3b940997f0>\n",
      "[2023-12-07 02:04:23,595] [INFO] [config.py:959:print]   communication_data_type ...... None\n",
      "[2023-12-07 02:04:23,595] [INFO] [config.py:959:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2023-12-07 02:04:23,595] [INFO] [config.py:959:print]   curriculum_enabled_legacy .... False\n",
      "[2023-12-07 02:04:23,595] [INFO] [config.py:959:print]   curriculum_params_legacy ..... False\n",
      "[2023-12-07 02:04:23,595] [INFO] [config.py:959:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2023-12-07 02:04:23,595] [INFO] [config.py:959:print]   data_efficiency_enabled ...... False\n",
      "[2023-12-07 02:04:23,595] [INFO] [config.py:959:print]   dataloader_drop_last ......... False\n",
      "[2023-12-07 02:04:23,595] [INFO] [config.py:959:print]   disable_allgather ............ False\n",
      "[2023-12-07 02:04:23,596] [INFO] [config.py:959:print]   dump_state ................... False\n",
      "[2023-12-07 02:04:23,596] [INFO] [config.py:959:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 100, 'delayed_shift': 2, 'min_scale': 1e-10}\n",
      "[2023-12-07 02:04:23,596] [INFO] [config.py:959:print]   eigenvalue_enabled ........... False\n",
      "[2023-12-07 02:04:23,596] [INFO] [config.py:959:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2023-12-07 02:04:23,596] [INFO] [config.py:959:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2023-12-07 02:04:23,596] [INFO] [config.py:959:print]   eigenvalue_layer_num ......... 0\n",
      "[2023-12-07 02:04:23,596] [INFO] [config.py:959:print]   eigenvalue_max_iter .......... 100\n",
      "[2023-12-07 02:04:23,596] [INFO] [config.py:959:print]   eigenvalue_stability ......... 1e-06\n",
      "[2023-12-07 02:04:23,596] [INFO] [config.py:959:print]   eigenvalue_tol ............... 0.01\n",
      "[2023-12-07 02:04:23,596] [INFO] [config.py:959:print]   eigenvalue_verbose ........... False\n",
      "[2023-12-07 02:04:23,596] [INFO] [config.py:959:print]   elasticity_enabled ........... False\n",
      "[2023-12-07 02:04:23,596] [INFO] [config.py:959:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2023-12-07 02:04:23,596] [INFO] [config.py:959:print]   fp16_auto_cast ............... False\n",
      "[2023-12-07 02:04:23,596] [INFO] [config.py:959:print]   fp16_enabled ................. True\n",
      "[2023-12-07 02:04:23,596] [INFO] [config.py:959:print]   fp16_master_weights_and_gradients  False\n",
      "[2023-12-07 02:04:23,596] [INFO] [config.py:959:print]   global_rank .................. 0\n",
      "[2023-12-07 02:04:23,596] [INFO] [config.py:959:print]   grad_accum_dtype ............. None\n",
      "[2023-12-07 02:04:23,596] [INFO] [config.py:959:print]   gradient_accumulation_steps .. 1\n",
      "[2023-12-07 02:04:23,596] [INFO] [config.py:959:print]   gradient_clipping ............ 1.0\n",
      "[2023-12-07 02:04:23,596] [INFO] [config.py:959:print]   gradient_predivide_factor .... 1.0\n",
      "[2023-12-07 02:04:23,596] [INFO] [config.py:959:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2023-12-07 02:04:23,596] [INFO] [config.py:959:print]   initial_dynamic_scale ........ 65536\n",
      "[2023-12-07 02:04:23,596] [INFO] [config.py:959:print]   load_universal_checkpoint .... False\n",
      "[2023-12-07 02:04:23,596] [INFO] [config.py:959:print]   loss_scale ................... 0\n",
      "[2023-12-07 02:04:23,596] [INFO] [config.py:959:print]   memory_breakdown ............. False\n",
      "[2023-12-07 02:04:23,596] [INFO] [config.py:959:print]   mics_hierarchial_params_gather  False\n",
      "[2023-12-07 02:04:23,596] [INFO] [config.py:959:print]   mics_shard_size .............. -1\n",
      "[2023-12-07 02:04:23,596] [INFO] [config.py:959:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2023-12-07 02:04:23,596] [INFO] [config.py:959:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2023-12-07 02:04:23,596] [INFO] [config.py:959:print]   optimizer_legacy_fusion ...... False\n",
      "[2023-12-07 02:04:23,596] [INFO] [config.py:959:print]   optimizer_name ............... None\n",
      "[2023-12-07 02:04:23,596] [INFO] [config.py:959:print]   optimizer_params ............. None\n",
      "[2023-12-07 02:04:23,596] [INFO] [config.py:959:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n",
      "[2023-12-07 02:04:23,596] [INFO] [config.py:959:print]   pld_enabled .................. False\n",
      "[2023-12-07 02:04:23,596] [INFO] [config.py:959:print]   pld_params ................... False\n",
      "[2023-12-07 02:04:23,596] [INFO] [config.py:959:print]   prescale_gradients ........... False\n",
      "[2023-12-07 02:04:23,596] [INFO] [config.py:959:print]   scheduler_name ............... None\n",
      "[2023-12-07 02:04:23,596] [INFO] [config.py:959:print]   scheduler_params ............. None\n",
      "[2023-12-07 02:04:23,596] [INFO] [config.py:959:print]   sparse_attention ............. None\n",
      "[2023-12-07 02:04:23,596] [INFO] [config.py:959:print]   sparse_gradients_enabled ..... False\n",
      "[2023-12-07 02:04:23,596] [INFO] [config.py:959:print]   steps_per_print .............. 2000\n",
      "[2023-12-07 02:04:23,596] [INFO] [config.py:959:print]   train_batch_size ............. 8\n",
      "[2023-12-07 02:04:23,596] [INFO] [config.py:959:print]   train_micro_batch_size_per_gpu  1\n",
      "[2023-12-07 02:04:23,597] [INFO] [config.py:959:print]   use_node_local_storage ....... False\n",
      "[2023-12-07 02:04:23,597] [INFO] [config.py:959:print]   wall_clock_breakdown ......... False\n",
      "[2023-12-07 02:04:23,597] [INFO] [config.py:959:print]   world_size ................... 8\n",
      "[2023-12-07 02:04:23,597] [INFO] [config.py:959:print]   zero_allow_untested_optimizer  True\n",
      "[2023-12-07 02:04:23,597] [INFO] [config.py:959:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=100000000 allgather_partitions=True allgather_bucket_size=100000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True\n",
      "[2023-12-07 02:04:23,597] [INFO] [config.py:959:print]   zero_enabled ................. True\n",
      "[2023-12-07 02:04:23,597] [INFO] [config.py:959:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2023-12-07 02:04:23,597] [INFO] [config.py:959:print]   zero_optimization_stage ...... 2\n",
      "[2023-12-07 02:04:23,597] [INFO] [config.py:945:print_user_config]   json = {\n",
      "    \"fp16\": {\n",
      "        \"enabled\": true, \n",
      "        \"loss_scale\": 0, \n",
      "        \"loss_scale_window\": 100, \n",
      "        \"initial_scale_power\": 16, \n",
      "        \"hysteresis\": 2, \n",
      "        \"min_loss_scale\": 1e-10\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 1.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 1.000000e+08, \n",
      "        \"contiguous_gradients\": true\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"steps_per_print\": 2.000000e+03, \n",
      "    \"train_batch_size\": 8, \n",
      "    \"train_micro_batch_size_per_gpu\": 1, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "Using /root/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0003657341003417969 seconds\n",
      "[INFO|trainer.py:1769] 2023-12-07 02:04:23,599 >> ***** Running training *****\n",
      "[INFO|trainer.py:1770] 2023-12-07 02:04:23,599 >>   Num examples = 1,122\n",
      "[INFO|trainer.py:1771] 2023-12-07 02:04:23,599 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1772] 2023-12-07 02:04:23,599 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:1773] 2023-12-07 02:04:23,599 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:1774] 2023-12-07 02:04:23,599 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1775] 2023-12-07 02:04:23,599 >>   Total optimization steps = 100\n",
      "[INFO|trainer.py:1776] 2023-12-07 02:04:23,602 >>   Number of trainable parameters = 429,203,456\n",
      "  0%|                                                   | 0/100 [00:00<?, ?it/s][WARNING|logging.py:295] 2023-12-07 02:04:23,643 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "[2023-12-07 02:04:25,442] [INFO] [loss_scaler.py:188:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 9.2695, 'learning_rate': 0.0, 'epoch': 0.01}                           \n",
      "  1%|▍                                          | 1/100 [00:01<02:59,  1.81s/it][2023-12-07 02:04:26,530] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "  2%|▊                                          | 2/100 [00:02<02:15,  1.39s/it][2023-12-07 02:04:27,618] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "  3%|█▎                                         | 3/100 [00:03<02:01,  1.25s/it][2023-12-07 02:04:28,703] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "  4%|█▋                                         | 4/100 [00:05<01:53,  1.18s/it][2023-12-07 02:04:29,787] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n",
      "  5%|██▏                                        | 5/100 [00:06<01:49,  1.15s/it][2023-12-07 02:04:30,878] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4096, reducing to 2048\n",
      "  7%|███                                        | 7/100 [00:09<02:04,  1.34s/it][2023-12-07 02:04:33,758] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2048, reducing to 1024\n",
      "{'loss': 6.4023, 'learning_rate': 0.00012, 'epoch': 0.07}                       \n",
      " 13%|█████▍                                    | 13/100 [00:18<02:25,  1.68s/it][2023-12-07 02:04:43,612] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1024, reducing to 512\n",
      " 16%|██████▋                                   | 16/100 [00:23<02:16,  1.63s/it][2023-12-07 02:04:48,185] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 512, reducing to 256\n",
      "{'loss': 4.8694, 'learning_rate': 0.00019803799658748094, 'epoch': 0.14}        \n",
      "{'loss': 2.997, 'learning_rate': 0.00018632564809575742, 'epoch': 0.21}         \n",
      "{'loss': 2.6136, 'learning_rate': 0.00016525857615241687, 'epoch': 0.28}        \n",
      "{'loss': 2.3076, 'learning_rate': 0.00013711972489182208, 'epoch': 0.35}        \n",
      "{'loss': 2.3333, 'learning_rate': 0.00010495837546732224, 'epoch': 0.43}        \n",
      "{'loss': 2.1986, 'learning_rate': 7.225970912381556e-05, 'epoch': 0.5}          \n",
      "{'loss': 2.1869, 'learning_rate': 4.256713373170564e-05, 'epoch': 0.57}         \n",
      "{'loss': 2.0829, 'learning_rate': 1.9098300562505266e-05, 'epoch': 0.64}        \n",
      "{'loss': 2.0436, 'learning_rate': 4.3964218465642355e-06, 'epoch': 0.71}        \n",
      "100%|█████████████████████████████████████████| 100/100 [02:49<00:00,  1.75s/it][INFO|trainer.py:2039] 2023-12-07 02:07:13,314 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 169.7124, 'train_samples_per_second': 4.714, 'train_steps_per_second': 0.589, 'train_loss': 3.032191162109375, 'epoch': 0.71}\n",
      "100%|█████████████████████████████████████████| 100/100 [02:49<00:00,  1.70s/it]\n",
      "[INFO|tokenization_utils_base.py:2171] 2023-12-07 02:07:13,317 >> tokenizer config file saved in ../../output/llama-zh/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2023-12-07 02:07:13,318 >> Special tokens file saved in ../../output/llama-zh/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =       0.71\n",
      "  train_loss               =     3.0322\n",
      "  train_runtime            = 0:02:49.71\n",
      "  train_samples            =       1122\n",
      "  train_samples_per_second =      4.714\n",
      "  train_steps_per_second   =      0.589\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nnodes 1 --nproc_per_node 8 {script_dir}/run_clm_pt_with_peft.py \\\n",
    "    --deepspeed {deepspeed_config_file} \\\n",
    "    --model_name_or_path {model_args.model_name_or_path} \\\n",
    "    --tokenizer_name_or_path {model_args.tokenizer_name_or_path} \\\n",
    "    --dataset_dir {data_args.dataset_dir} \\\n",
    "    --data_cache_dir {data_args.data_cache_dir} \\\n",
    "    --validation_split_percentage {data_args.validation_split_percentage} \\\n",
    "    --per_device_train_batch_size {training_args.per_device_train_batch_size} \\\n",
    "    --per_device_eval_batch_size {training_args.per_device_eval_batch_size} \\\n",
    "    --do_train {training_args.do_train}\\\n",
    "    --seed {training_args.seed} \\\n",
    "    --fp16 {training_args.fp16}\\\n",
    "    --max_steps {training_args.max_steps} \\\n",
    "    --lr_scheduler_type {training_args.lr_scheduler_type} \\\n",
    "    --learning_rate {training_args.learning_rate} \\\n",
    "    --warmup_ratio {training_args.warmup_ratio} \\\n",
    "    --weight_decay {training_args.weight_decay} \\\n",
    "    --logging_strategy {training_args.logging_strategy} \\\n",
    "    --logging_steps {training_args.logging_steps} \\\n",
    "    --save_strategy {training_args.save_strategy} \\\n",
    "    --save_total_limit {training_args.save_total_limit} \\\n",
    "    --save_steps {training_args.save_steps} \\\n",
    "    --gradient_accumulation_steps {training_args.gradient_accumulation_steps} \\\n",
    "    --preprocessing_num_workers {data_args.preprocessing_num_workers} \\\n",
    "    --block_size {data_args.block_size} \\\n",
    "    --output_dir {training_args.output_dir} \\\n",
    "    --overwrite_output_dir {training_args.overwrite_output_dir} \\\n",
    "    --ddp_timeout {training_args.ddp_timeout} \\\n",
    "    --logging_first_step {training_args.logging_first_step} \\\n",
    "    --lora_rank {training_args.lora_rank} \\\n",
    "    --lora_alpha {training_args.lora_alpha} \\\n",
    "    --trainable {training_args.trainable} \\\n",
    "    --modules_to_save {training_args.modules_to_save} \\\n",
    "    --lora_dropout {training_args.lora_dropout} \\\n",
    "    --torch_dtype {model_args.torch_dtype} \\\n",
    "    --gradient_checkpointing {training_args.gradient_checkpointing} \\\n",
    "    --ddp_find_unused_parameters {training_args.ddp_find_unused_parameters}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step3: merge pretrained lora\n",
    "Having trained the lora model, we need to merge it into the original llama model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we expose an interface to change the save path of tokenizers\n",
    "!cp merge_llama_with_chinese_lora.py {script_dir}\n",
    "!python {script_dir}/merge_llama_with_chinese_lora.py --base_model {llama_tokenizer_dir} --tokenizer_path {output_dir}/merged_tokenizer_hf --lora_model {output_dir}/llama-zh/lora --output_type huggingface --output_dir {output_dir}/merge-lora-hf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Superviser fine-tuning\n",
    "Now we can start fine-tuning the model using instructions. You should reprepare the arguments similar to that in the pretrain step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarly we adjust the lora saving path in the original pretrain script\n",
    "!cp run_clm_sft_with_peft.py {script_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SftDataTrainingArguments(DataTrainingArguments):\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    \"\"\"\n",
    "    max_seq_length: Optional[int] = field(default=512)\n",
    "\n",
    "sft_model_args, sft_data_args, sft_training_args = model_args, SftDataTrainingArguments(data_args), training_args\n",
    "# we only need to adjust some parameters\n",
    "sft_model_args.model_name_or_path = \"../../output/merge-lora-hf/\"\n",
    "sft_model_args.tokenizer_name_or_path = \"../../output/merged_tokenizer_hf/\"\n",
    "\n",
    "sft_data_args.dataset_dir = \"../../Chinese-LLaMA-Alpaca/data/\"\n",
    "sft_data_args.validation_file = \"../../data/alpaca_data.json\"\n",
    "sft_data_args.block_size = None\n",
    "sft_data_args.max_seq_length = 512\n",
    "\n",
    "sft_training_args.learning_rate = 1e-4\n",
    "sft_training_args.output_dir = \"../../output/llama-alpaca-zh\"\n",
    "sft_training_args.warmup_ratio = 0.03\n",
    "sft_training_args.weight_decay = 0\n",
    "sft_training_args.eval_steps = 250\n",
    "sft_training_args.evaluation_strategy = \"steps\"\n",
    "sft_training_args.do_eval = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/libbitsandbytes_cuda114.so\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /opt/conda did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/cuda/compat/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/c%3A/Users/zzz48/.vscode/extensions/ms-ceintl.vscode-language-pack-zh-hans-1.84.2023111509/translations/extensions/vscode.markdown-language-features.i18n.json'), PosixPath('vscode-local')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('[\"/tmp/vscode-ssh-auth-99aaedd1-49f2-4aea-b7fd-66afd70c1944.sock\",\"/root/.gnupg/S.gpg-agent\"]')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn/corrupted.info\",\"_languagePackSupport\"'), PosixPath('{\"*\"'), PosixPath('\"zh-cn\",\"availableLanguages\"'), PosixPath('{\"locale\"'), PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn/tcf.json\",\"_cacheRoot\"'), PosixPath('\"b21449bfcc24b92b09a8c487b9b70068.zh-cn\",\"_translationsConfigFile\"'), PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn\",\"_resolvedLanguagePackCoreLocation\"'), PosixPath('\"zh-cn\"},\"_languagePackId\"'), PosixPath('true}'), PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn/1a5daa3a0231a0fbba4f14db7ec463cf99d7768e\",\"_corruptedFile\"'), PosixPath('\"zh-cn\",\"osLocale\"')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/torchelastic_vze7f7nf/none_5xxllbfz/attempt_0/2/error.json')}\n",
      "  warn(msg)\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 114\n",
      "CUDA SETUP: Loading binary /opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/libbitsandbytes_cuda114.so...\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/libbitsandbytes_cuda114.so\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /opt/conda did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/cuda/compat/lib'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vscode-local'), PosixPath('/c%3A/Users/zzz48/.vscode/extensions/ms-ceintl.vscode-language-pack-zh-hans-1.84.2023111509/translations/extensions/vscode.markdown-language-features.i18n.json')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('[\"/tmp/vscode-ssh-auth-99aaedd1-49f2-4aea-b7fd-66afd70c1944.sock\",\"/root/.gnupg/S.gpg-agent\"]')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn\",\"_resolvedLanguagePackCoreLocation\"'), PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn/1a5daa3a0231a0fbba4f14db7ec463cf99d7768e\",\"_corruptedFile\"'), PosixPath('\"zh-cn\"},\"_languagePackId\"'), PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn/corrupted.info\",\"_languagePackSupport\"'), PosixPath('{\"*\"'), PosixPath('\"zh-cn\",\"availableLanguages\"'), PosixPath('\"zh-cn\",\"osLocale\"'), PosixPath('{\"locale\"'), PosixPath('true}'), PosixPath('\"b21449bfcc24b92b09a8c487b9b70068.zh-cn\",\"_translationsConfigFile\"'), PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn/tcf.json\",\"_cacheRoot\"')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/torchelastic_vze7f7nf/none_5xxllbfz/attempt_0/3/error.json')}\n",
      "  warn(msg)\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 114\n",
      "CUDA SETUP: Loading binary /opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/libbitsandbytes_cuda114.so...\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/libbitsandbytes_cuda114.so\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /opt/conda did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/cuda/compat/lib'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/c%3A/Users/zzz48/.vscode/extensions/ms-ceintl.vscode-language-pack-zh-hans-1.84.2023111509/translations/extensions/vscode.markdown-language-features.i18n.json'), PosixPath('vscode-local')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('[\"/tmp/vscode-ssh-auth-99aaedd1-49f2-4aea-b7fd-66afd70c1944.sock\",\"/root/.gnupg/S.gpg-agent\"]')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn/1a5daa3a0231a0fbba4f14db7ec463cf99d7768e\",\"_corruptedFile\"'), PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn\",\"_resolvedLanguagePackCoreLocation\"'), PosixPath('\"b21449bfcc24b92b09a8c487b9b70068.zh-cn\",\"_translationsConfigFile\"'), PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn/tcf.json\",\"_cacheRoot\"'), PosixPath('true}'), PosixPath('{\"*\"'), PosixPath('\"zh-cn\",\"osLocale\"'), PosixPath('\"zh-cn\"},\"_languagePackId\"'), PosixPath('\"zh-cn\",\"availableLanguages\"'), PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn/corrupted.info\",\"_languagePackSupport\"'), PosixPath('{\"locale\"')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/torchelastic_vze7f7nf/none_5xxllbfz/attempt_0/0/error.json')}\n",
      "  warn(msg)\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 114\n",
      "CUDA SETUP: Loading binary /opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/libbitsandbytes_cuda114.so...\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/libbitsandbytes_cuda114.so\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /opt/conda did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/cuda/compat/lib')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/c%3A/Users/zzz48/.vscode/extensions/ms-ceintl.vscode-language-pack-zh-hans-1.84.2023111509/translations/extensions/vscode.markdown-language-features.i18n.json'), PosixPath('vscode-local')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('[\"/tmp/vscode-ssh-auth-99aaedd1-49f2-4aea-b7fd-66afd70c1944.sock\",\"/root/.gnupg/S.gpg-agent\"]')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn\",\"_resolvedLanguagePackCoreLocation\"'), PosixPath('\"zh-cn\",\"availableLanguages\"'), PosixPath('true}'), PosixPath('\"zh-cn\"},\"_languagePackId\"'), PosixPath('\"b21449bfcc24b92b09a8c487b9b70068.zh-cn\",\"_translationsConfigFile\"'), PosixPath('{\"locale\"'), PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn/corrupted.info\",\"_languagePackSupport\"'), PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn/1a5daa3a0231a0fbba4f14db7ec463cf99d7768e\",\"_corruptedFile\"'), PosixPath('{\"*\"'), PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn/tcf.json\",\"_cacheRoot\"'), PosixPath('\"zh-cn\",\"osLocale\"')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/torchelastic_vze7f7nf/none_5xxllbfz/attempt_0/5/error.json')}\n",
      "  warn(msg)\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 114\n",
      "CUDA SETUP: Loading binary /opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/libbitsandbytes_cuda114.so...\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/libbitsandbytes_cuda114.so\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /opt/conda did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/cuda/compat/lib'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vscode-local'), PosixPath('/c%3A/Users/zzz48/.vscode/extensions/ms-ceintl.vscode-language-pack-zh-hans-1.84.2023111509/translations/extensions/vscode.markdown-language-features.i18n.json')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('[\"/tmp/vscode-ssh-auth-99aaedd1-49f2-4aea-b7fd-66afd70c1944.sock\",\"/root/.gnupg/S.gpg-agent\"]')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn/1a5daa3a0231a0fbba4f14db7ec463cf99d7768e\",\"_corruptedFile\"'), PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn\",\"_resolvedLanguagePackCoreLocation\"'), PosixPath('\"zh-cn\",\"osLocale\"'), PosixPath('true}'), PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn/tcf.json\",\"_cacheRoot\"'), PosixPath('{\"*\"'), PosixPath('\"zh-cn\"},\"_languagePackId\"'), PosixPath('\"zh-cn\",\"availableLanguages\"'), PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn/corrupted.info\",\"_languagePackSupport\"'), PosixPath('{\"locale\"'), PosixPath('\"b21449bfcc24b92b09a8c487b9b70068.zh-cn\",\"_translationsConfigFile\"')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/torchelastic_vze7f7nf/none_5xxllbfz/attempt_0/4/error.json')}\n",
      "  warn(msg)\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 114\n",
      "CUDA SETUP: Loading binary /opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/libbitsandbytes_cuda114.so...\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/libbitsandbytes_cuda114.so\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /opt/conda did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/compat/lib'), PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vscode-local'), PosixPath('/c%3A/Users/zzz48/.vscode/extensions/ms-ceintl.vscode-language-pack-zh-hans-1.84.2023111509/translations/extensions/vscode.markdown-language-features.i18n.json')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('[\"/tmp/vscode-ssh-auth-99aaedd1-49f2-4aea-b7fd-66afd70c1944.sock\",\"/root/.gnupg/S.gpg-agent\"]')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('\"zh-cn\",\"osLocale\"'), PosixPath('{\"*\"'), PosixPath('\"zh-cn\",\"availableLanguages\"'), PosixPath('\"b21449bfcc24b92b09a8c487b9b70068.zh-cn\",\"_translationsConfigFile\"'), PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn/1a5daa3a0231a0fbba4f14db7ec463cf99d7768e\",\"_corruptedFile\"'), PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn/corrupted.info\",\"_languagePackSupport\"'), PosixPath('{\"locale\"'), PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn/tcf.json\",\"_cacheRoot\"'), PosixPath('true}'), PosixPath('\"zh-cn\"},\"_languagePackId\"'), PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn\",\"_resolvedLanguagePackCoreLocation\"')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/torchelastic_vze7f7nf/none_5xxllbfz/attempt_0/7/error.json')}\n",
      "  warn(msg)\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 114\n",
      "CUDA SETUP: Loading binary /opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/libbitsandbytes_cuda114.so...\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/libbitsandbytes_cuda114.so\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /opt/conda did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/compat/lib'), PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/c%3A/Users/zzz48/.vscode/extensions/ms-ceintl.vscode-language-pack-zh-hans-1.84.2023111509/translations/extensions/vscode.markdown-language-features.i18n.json'), PosixPath('vscode-local')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('[\"/tmp/vscode-ssh-auth-99aaedd1-49f2-4aea-b7fd-66afd70c1944.sock\",\"/root/.gnupg/S.gpg-agent\"]')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn/1a5daa3a0231a0fbba4f14db7ec463cf99d7768e\",\"_corruptedFile\"'), PosixPath('{\"*\"'), PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn/tcf.json\",\"_cacheRoot\"'), PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn\",\"_resolvedLanguagePackCoreLocation\"'), PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn/corrupted.info\",\"_languagePackSupport\"'), PosixPath('\"zh-cn\",\"availableLanguages\"'), PosixPath('\"b21449bfcc24b92b09a8c487b9b70068.zh-cn\",\"_translationsConfigFile\"'), PosixPath('\"zh-cn\"},\"_languagePackId\"'), PosixPath('\"zh-cn\",\"osLocale\"'), PosixPath('{\"locale\"'), PosixPath('true}')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/torchelastic_vze7f7nf/none_5xxllbfz/attempt_0/1/error.json')}\n",
      "  warn(msg)\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 114\n",
      "CUDA SETUP: Loading binary /opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/libbitsandbytes_cuda114.so...\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/libbitsandbytes_cuda114.so\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /opt/conda did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/compat/lib'), PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/c%3A/Users/zzz48/.vscode/extensions/ms-ceintl.vscode-language-pack-zh-hans-1.84.2023111509/translations/extensions/vscode.markdown-language-features.i18n.json'), PosixPath('vscode-local')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('[\"/tmp/vscode-ssh-auth-99aaedd1-49f2-4aea-b7fd-66afd70c1944.sock\",\"/root/.gnupg/S.gpg-agent\"]')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn\",\"_resolvedLanguagePackCoreLocation\"'), PosixPath('\"zh-cn\",\"availableLanguages\"'), PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn/corrupted.info\",\"_languagePackSupport\"'), PosixPath('true}'), PosixPath('{\"*\"'), PosixPath('\"zh-cn\"},\"_languagePackId\"'), PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn/1a5daa3a0231a0fbba4f14db7ec463cf99d7768e\",\"_corruptedFile\"'), PosixPath('{\"locale\"'), PosixPath('\"b21449bfcc24b92b09a8c487b9b70068.zh-cn\",\"_translationsConfigFile\"'), PosixPath('\"zh-cn\",\"osLocale\"'), PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn/tcf.json\",\"_cacheRoot\"')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/torchelastic_vze7f7nf/none_5xxllbfz/attempt_0/6/error.json')}\n",
      "  warn(msg)\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 114\n",
      "CUDA SETUP: Loading binary /opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/libbitsandbytes_cuda114.so...\n",
      "[2023-12-07 04:06:52,088] [INFO] [comm.py:622:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "12/07/2023 04:09:02 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: True\n",
      "[ERROR|tokenization_utils_base.py:1042] 2023-12-07 04:09:02,598 >> Using pad_token, but it is not set yet.\n",
      "12/07/2023 04:09:06 - WARNING - __main__ - Process rank: 6, device: cuda:6, n_gpu: 1distributed training: True, 16-bits training: True\n",
      "12/07/2023 04:09:06 - WARNING - __main__ - Process rank: 5, device: cuda:5, n_gpu: 1distributed training: True, 16-bits training: True\n",
      "12/07/2023 04:09:06 - WARNING - __main__ - Process rank: 7, device: cuda:7, n_gpu: 1distributed training: True, 16-bits training: True\n",
      "12/07/2023 04:09:06 - WARNING - __main__ - Process rank: 4, device: cuda:4, n_gpu: 1distributed training: True, 16-bits training: True\n",
      "12/07/2023 04:09:06 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True\n",
      "12/07/2023 04:09:06 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: True\n",
      "12/07/2023 04:09:06 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: True\n",
      "[INFO|configuration_utils.py:666] 2023-12-07 04:09:06,666 >> loading configuration file ../../output/merge-lora-hf/config.json\n",
      "[INFO|configuration_utils.py:720] 2023-12-07 04:09:06,667 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"../../output/merge-lora-hf/\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 49953\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1807] 2023-12-07 04:09:06,667 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:1807] 2023-12-07 04:09:06,667 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:1807] 2023-12-07 04:09:06,667 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1807] 2023-12-07 04:09:06,667 >> loading file tokenizer_config.json\n",
      "[ERROR|tokenization_utils_base.py:1042] 2023-12-07 04:09:06,689 >> Using pad_token, but it is not set yet.\n",
      "[ERROR|tokenization_utils_base.py:1042] 2023-12-07 04:09:06,689 >> Using pad_token, but it is not set yet.\n",
      "[ERROR|tokenization_utils_base.py:1042] 2023-12-07 04:09:06,689 >> Using pad_token, but it is not set yet.\n",
      "[ERROR|tokenization_utils_base.py:1042] 2023-12-07 04:09:06,689 >> Using pad_token, but it is not set yet.\n",
      "[ERROR|tokenization_utils_base.py:1042] 2023-12-07 04:09:06,690 >> Using pad_token, but it is not set yet.\n",
      "[ERROR|tokenization_utils_base.py:1042] 2023-12-07 04:09:06,690 >> Using pad_token, but it is not set yet.\n",
      "[INFO|tokenization_utils_base.py:907] 2023-12-07 04:09:06,690 >> Assigning [PAD] to the pad_token key of the tokenizer\n",
      "[INFO|tokenization_utils.py:426] 2023-12-07 04:09:06,690 >> Adding [PAD] to the vocabulary\n",
      "[ERROR|tokenization_utils_base.py:1042] 2023-12-07 04:09:06,690 >> Using pad_token, but it is not set yet.\n",
      "12/07/2023 04:09:06 - INFO - __main__ - training files: ../../Chinese-LLaMA-Alpaca/data/alpaca_data_zh_51k.json\n",
      "12/07/2023 04:09:06 - WARNING - root - building dataset...\n",
      "12/07/2023 04:09:07 - INFO - datasets.builder - Using custom data configuration default-ba8f8677bad21b58\n",
      "12/07/2023 04:09:07 - INFO - datasets.info - Loading Dataset Infos from /opt/conda/lib/python3.8/site-packages/datasets/packaged_modules/json\n",
      "12/07/2023 04:09:07 - INFO - datasets.builder - Generating dataset json (/workspace/flaz/script/train/../../Chinese-LLaMA-Alpaca/data/alpaca_data_zh_51k/json/default-ba8f8677bad21b58/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n",
      "Downloading and preparing dataset json/default to /workspace/flaz/script/train/../../Chinese-LLaMA-Alpaca/data/alpaca_data_zh_51k/json/default-ba8f8677bad21b58/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...\n",
      "Downloading data files: 100%|██████████████████| 1/1 [00:00<00:00, 11184.81it/s]\n",
      "12/07/2023 04:09:07 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
      "12/07/2023 04:09:07 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
      "Extracting data files: 100%|████████████████████| 1/1 [00:00<00:00, 1697.41it/s]\n",
      "12/07/2023 04:09:07 - INFO - datasets.builder - Generating train split\n",
      "12/07/2023 04:09:08 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
      "Dataset json downloaded and prepared to /workspace/flaz/script/train/../../Chinese-LLaMA-Alpaca/data/alpaca_data_zh_51k/json/default-ba8f8677bad21b58/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4. Subsequent calls will reuse this data.\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 494.20it/s]\n",
      "12/07/2023 04:09:08 - INFO - datasets.arrow_dataset - Process #0 will write at /workspace/flaz/Chinese-LLaMA-Alpaca/data/alpaca_data_zh_51k/json/default-ba8f8677bad21b58/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-e51d88e4d0608d56_00000_of_00008.arrow\n",
      "12/07/2023 04:09:08 - INFO - datasets.arrow_dataset - Process #1 will write at /workspace/flaz/Chinese-LLaMA-Alpaca/data/alpaca_data_zh_51k/json/default-ba8f8677bad21b58/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-e51d88e4d0608d56_00001_of_00008.arrow\n",
      "12/07/2023 04:09:08 - INFO - datasets.arrow_dataset - Process #2 will write at /workspace/flaz/Chinese-LLaMA-Alpaca/data/alpaca_data_zh_51k/json/default-ba8f8677bad21b58/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-e51d88e4d0608d56_00002_of_00008.arrow\n",
      "12/07/2023 04:09:08 - INFO - datasets.arrow_dataset - Process #3 will write at /workspace/flaz/Chinese-LLaMA-Alpaca/data/alpaca_data_zh_51k/json/default-ba8f8677bad21b58/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-e51d88e4d0608d56_00003_of_00008.arrow\n",
      "12/07/2023 04:09:08 - INFO - datasets.arrow_dataset - Process #4 will write at /workspace/flaz/Chinese-LLaMA-Alpaca/data/alpaca_data_zh_51k/json/default-ba8f8677bad21b58/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-e51d88e4d0608d56_00004_of_00008.arrow\n",
      "12/07/2023 04:09:08 - INFO - datasets.arrow_dataset - Process #5 will write at /workspace/flaz/Chinese-LLaMA-Alpaca/data/alpaca_data_zh_51k/json/default-ba8f8677bad21b58/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-e51d88e4d0608d56_00005_of_00008.arrow\n",
      "12/07/2023 04:09:08 - INFO - datasets.arrow_dataset - Process #6 will write at /workspace/flaz/Chinese-LLaMA-Alpaca/data/alpaca_data_zh_51k/json/default-ba8f8677bad21b58/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-e51d88e4d0608d56_00006_of_00008.arrow\n",
      "12/07/2023 04:09:08 - INFO - datasets.arrow_dataset - Process #7 will write at /workspace/flaz/Chinese-LLaMA-Alpaca/data/alpaca_data_zh_51k/json/default-ba8f8677bad21b58/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-e51d88e4d0608d56_00007_of_00008.arrow\n",
      "12/07/2023 04:09:08 - INFO - datasets.arrow_dataset - Spawning 8 processes\n",
      "preprocessing on dataset (num_proc=8):   0%|   | 0/51179 [00:00<?, ? examples/s]12/07/2023 04:09:08 - INFO - datasets.arrow_dataset - Caching processed dataset at /workspace/flaz/Chinese-LLaMA-Alpaca/data/alpaca_data_zh_51k/json/default-ba8f8677bad21b58/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-e51d88e4d0608d56_00002_of_00008.arrow\n",
      "12/07/2023 04:09:08 - INFO - datasets.arrow_dataset - Caching processed dataset at /workspace/flaz/Chinese-LLaMA-Alpaca/data/alpaca_data_zh_51k/json/default-ba8f8677bad21b58/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-e51d88e4d0608d56_00001_of_00008.arrow\n",
      "12/07/2023 04:09:08 - INFO - datasets.arrow_dataset - Caching processed dataset at /workspace/flaz/Chinese-LLaMA-Alpaca/data/alpaca_data_zh_51k/json/default-ba8f8677bad21b58/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-e51d88e4d0608d56_00000_of_00008.arrow\n",
      "12/07/2023 04:09:08 - INFO - datasets.arrow_dataset - Caching processed dataset at /workspace/flaz/Chinese-LLaMA-Alpaca/data/alpaca_data_zh_51k/json/default-ba8f8677bad21b58/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-e51d88e4d0608d56_00004_of_00008.arrow\n",
      "12/07/2023 04:09:08 - INFO - datasets.arrow_dataset - Caching processed dataset at /workspace/flaz/Chinese-LLaMA-Alpaca/data/alpaca_data_zh_51k/json/default-ba8f8677bad21b58/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-e51d88e4d0608d56_00007_of_00008.arrow\n",
      "12/07/2023 04:09:08 - INFO - datasets.arrow_dataset - Caching processed dataset at /workspace/flaz/Chinese-LLaMA-Alpaca/data/alpaca_data_zh_51k/json/default-ba8f8677bad21b58/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-e51d88e4d0608d56_00006_of_00008.arrow\n",
      "12/07/2023 04:09:08 - INFO - datasets.arrow_dataset - Caching processed dataset at /workspace/flaz/Chinese-LLaMA-Alpaca/data/alpaca_data_zh_51k/json/default-ba8f8677bad21b58/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-e51d88e4d0608d56_00005_of_00008.arrow\n",
      "12/07/2023 04:09:08 - INFO - datasets.arrow_dataset - Caching processed dataset at /workspace/flaz/Chinese-LLaMA-Alpaca/data/alpaca_data_zh_51k/json/default-ba8f8677bad21b58/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-e51d88e4d0608d56_00003_of_00008.arrow\n",
      "12/07/2023 04:09:11 - INFO - datasets.arrow_dataset - Concatenating 8 shards    \n",
      "12/07/2023 04:09:13 - INFO - __main__ - Num train_samples  51179                \n",
      "12/07/2023 04:09:13 - INFO - __main__ - training example:\n",
      "12/07/2023 04:09:13 - WARNING - root - building dataset...\n",
      "12/07/2023 04:09:13 - WARNING - root - building dataset...\n",
      "12/07/2023 04:09:13 - WARNING - root - building dataset...\n",
      "12/07/2023 04:09:13 - WARNING - root - building dataset...\n",
      "12/07/2023 04:09:13 - WARNING - root - building dataset...\n",
      "12/07/2023 04:09:13 - WARNING - root - building dataset...\n",
      "12/07/2023 04:09:13 - WARNING - root - building dataset...\n",
      "12/07/2023 04:09:13 - INFO - __name__ - training datasets-../../Chinese-LLaMA-Alpaca/data/alpaca_data_zh_51k.json has been loaded from disk\n",
      "12/07/2023 04:09:13 - INFO - __name__ - training datasets-../../Chinese-LLaMA-Alpaca/data/alpaca_data_zh_51k.json has been loaded from disk\n",
      "12/07/2023 04:09:13 - INFO - __name__ - training datasets-../../Chinese-LLaMA-Alpaca/data/alpaca_data_zh_51k.json has been loaded from disk\n",
      "12/07/2023 04:09:13 - INFO - __name__ - training datasets-../../Chinese-LLaMA-Alpaca/data/alpaca_data_zh_51k.json has been loaded from disk\n",
      "12/07/2023 04:09:13 - INFO - __name__ - training datasets-../../Chinese-LLaMA-Alpaca/data/alpaca_data_zh_51k.json has been loaded from disk\n",
      "12/07/2023 04:09:13 - INFO - __name__ - training datasets-../../Chinese-LLaMA-Alpaca/data/alpaca_data_zh_51k.json has been loaded from disk\n",
      "12/07/2023 04:09:13 - INFO - __name__ - training datasets-../../Chinese-LLaMA-Alpaca/data/alpaca_data_zh_51k.json has been loaded from disk\n",
      "12/07/2023 04:09:13 - INFO - __main__ - <s> Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "我们如何在日常生活中减少用水？\n",
      "\n",
      "### Response:  1. 使用节水装置，如节水淋浴喷头和水龙头。 \n",
      "2. 使用水箱或水桶收集家庭废水，例如洗碗和洗浴。 \n",
      "3. 在社区中提高节水意识。 \n",
      "4. 检查水管和灌溉系统的漏水情况，并及时修复它们。 \n",
      "5. 洗澡时间缩短，使用低流量淋浴头节约用水。 \n",
      "6. 收集雨水，用于园艺或其他非饮用目的。 \n",
      "7. 刷牙或擦手时关掉水龙头。 \n",
      "8. 减少浇水草坪的时间。 \n",
      "9. 尽可能多地重复使用灰水（来自洗衣机、浴室水槽和淋浴的水）。 \n",
      "10. 只购买能源效率高的洗碗机和洗衣机。</s>\n",
      "12/07/2023 04:09:13 - INFO - __main__ - training files: ../../data/alpaca_data.json\n",
      "12/07/2023 04:09:13 - WARNING - root - building dataset...\n",
      "12/07/2023 04:09:13 - INFO - __name__ - training datasets-../../data/alpaca_data.json has been loaded from disk\n",
      "12/07/2023 04:09:13 - WARNING - root - building dataset...\n",
      "12/07/2023 04:09:13 - WARNING - root - building dataset...\n",
      "12/07/2023 04:09:13 - WARNING - root - building dataset...\n",
      "12/07/2023 04:09:13 - WARNING - root - building dataset...\n",
      "12/07/2023 04:09:13 - WARNING - root - building dataset...\n",
      "12/07/2023 04:09:13 - WARNING - root - building dataset...\n",
      "12/07/2023 04:09:13 - WARNING - root - building dataset...\n",
      "12/07/2023 04:09:13 - INFO - __main__ - Num eval_samples  3247\n",
      "12/07/2023 04:09:13 - INFO - __main__ - eval example:\n",
      "12/07/2023 04:09:13 - INFO - __name__ - training datasets-../../data/alpaca_data.json has been loaded from disk\n",
      "12/07/2023 04:09:13 - INFO - __name__ - training datasets-../../data/alpaca_data.json has been loaded from disk\n",
      "12/07/2023 04:09:13 - INFO - __name__ - training datasets-../../data/alpaca_data.json has been loaded from disk\n",
      "12/07/2023 04:09:13 - INFO - __name__ - training datasets-../../data/alpaca_data.json has been loaded from disk\n",
      "12/07/2023 04:09:13 - INFO - __name__ - training datasets-../../data/alpaca_data.json has been loaded from disk\n",
      "12/07/2023 04:09:13 - INFO - __name__ - training datasets-../../data/alpaca_data.json has been loaded from disk\n",
      "12/07/2023 04:09:13 - INFO - __name__ - training datasets-../../data/alpaca_data.json has been loaded from disk\n",
      "12/07/2023 04:09:13 - INFO - __main__ - <s> Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "给出三个保持健康的提示。\n",
      "\n",
      "### Response:  1.饮食要均衡，确保包括足够的水果和蔬菜。\n",
      "2.定期运动以保持身体活跃和强壮。\n",
      "3.保持充足的睡眠并保持一致的睡眠时间表。</s>\n",
      "[INFO|modeling_utils.py:2531] 2023-12-07 04:09:13,194 >> loading weights file ../../output/merge-lora-hf/pytorch_model.bin.index.json\n",
      "[INFO|modeling_utils.py:1176] 2023-12-07 04:09:13,194 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.\n",
      "[INFO|configuration_utils.py:575] 2023-12-07 04:09:13,196 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:04<00:00,  2.38s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:05<00:00,  2.84s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:05<00:00,  2.84s/it]\n",
      "[INFO|modeling_utils.py:3190] 2023-12-07 04:09:19,048 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:3198] 2023-12-07 04:09:19,048 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at ../../output/merge-lora-hf/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:05<00:00,  2.84s/it]\n",
      "[INFO|configuration_utils.py:535] 2023-12-07 04:09:19,051 >> loading configuration file ../../output/merge-lora-hf/generation_config.json\n",
      "[INFO|configuration_utils.py:575] 2023-12-07 04:09:19,051 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n",
      "12/07/2023 04:09:19 - INFO - __main__ - len(tokenizer):49954\n",
      "12/07/2023 04:09:19 - INFO - __main__ - resize the embedding size by the size of the tokenizer\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:05<00:00,  2.84s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:05<00:00,  2.84s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:05<00:00,  2.85s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:05<00:00,  2.86s/it]\n",
      "12/07/2023 04:09:43 - INFO - __main__ - Init new peft model\n",
      "12/07/2023 04:09:43 - INFO - __main__ - target_modules: ['q_proj', 'v_proj', 'k_proj', 'o_proj', 'gate_proj', 'down_proj', 'up_proj']\n",
      "12/07/2023 04:09:43 - INFO - __main__ - lora_rank: 8\n",
      "trainable params: 429211648 || all params: 6905483264 || trainable%: 6.215519342977586\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "trainable params: 429211648 || all params: 6905483264 || trainable%: 6.215519342977586\n",
      "12/07/2023 04:10:47 - INFO - __main__ - model.modules_to_save: ['embed_tokens', 'lm_head']\n",
      "trainable params: 429211648 || all params: 6905483264 || trainable%: 6.215519342977586\n",
      "[INFO|trainer.py:564] 2023-12-07 04:10:47,254 >> max_steps is given, it will override any value given in num_train_epochs\n",
      "[INFO|trainer.py:621] 2023-12-07 04:10:47,254 >> Using cuda_amp half precision backend\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[2023-12-07 04:10:47,275] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.9.2, git-hash=unknown, git-branch=unknown\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "trainable params: 429211648 || all params: 6905483264 || trainable%: 6.215519342977586\n",
      "trainable params: 429211648 || all params: 6905483264 || trainable%: 6.215519342977586\n",
      "trainable params: 429211648 || all params: 6905483264 || trainable%: 6.215519342977586\n",
      "trainable params: 429211648 || all params: 6905483264 || trainable%: 6.215519342977586\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "trainable params: 429211648 || all params: 6905483264 || trainable%: 6.215519342977586\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "12/07/2023 04:10:50 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 3\n",
      "12/07/2023 04:10:51 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 0\n",
      "12/07/2023 04:10:51 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 1\n",
      "12/07/2023 04:10:51 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 5\n",
      "12/07/2023 04:10:52 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 4\n",
      "12/07/2023 04:10:52 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 6\n",
      "12/07/2023 04:10:52 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 2\n",
      "12/07/2023 04:10:52 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 7\n",
      "12/07/2023 04:10:52 - INFO - torch.distributed.distributed_c10d - Rank 7: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\n",
      "12/07/2023 04:10:52 - INFO - torch.distributed.distributed_c10d - Rank 5: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\n",
      "12/07/2023 04:10:52 - INFO - torch.distributed.distributed_c10d - Rank 3: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\n",
      "12/07/2023 04:10:52 - INFO - torch.distributed.distributed_c10d - Rank 6: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\n",
      "12/07/2023 04:10:52 - INFO - torch.distributed.distributed_c10d - Rank 1: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\n",
      "12/07/2023 04:10:52 - INFO - torch.distributed.distributed_c10d - Rank 2: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\n",
      "12/07/2023 04:10:52 - INFO - torch.distributed.distributed_c10d - Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\n",
      "12/07/2023 04:10:52 - INFO - torch.distributed.distributed_c10d - Rank 4: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\n",
      "[2023-12-07 04:10:53,169] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2023-12-07 04:10:53,169] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer\n",
      "[2023-12-07 04:10:53,169] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2023-12-07 04:10:53,208] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW\n",
      "[2023-12-07 04:10:53,208] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'transformers.optimization.AdamW'>\n",
      "[2023-12-07 04:10:53,208] [WARNING] [engine.py:1104:_do_optimizer_sanity_check] **** You are using ZeRO with an untested optimizer, proceed with caution *****\n",
      "[2023-12-07 04:10:53,208] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer\n",
      "[2023-12-07 04:10:53,208] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 100000000\n",
      "[2023-12-07 04:10:53,208] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 100000000\n",
      "[2023-12-07 04:10:53,208] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: False\n",
      "[2023-12-07 04:10:53,208] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False\n",
      "Using /root/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py38_cu117/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.4146578311920166 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.40390658378601074 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.40367674827575684 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.403717041015625 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.4035501480102539 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.4037508964538574 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.40363430976867676 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.5044465065002441 seconds\n",
      "Rank: 0 partition count [8] and sizes[(53651456, False)] \n",
      "Rank: 2 partition count [8] and sizes[(53651456, False)] \n",
      "Rank: 3 partition count [8] and sizes[(53651456, False)] \n",
      "Rank: 1 partition count [8] and sizes[(53651456, False)] \n",
      "Rank: 4 partition count [8] and sizes[(53651456, False)] \n",
      "Rank: 6 partition count [8] and sizes[(53651456, False)] \n",
      "Rank: 5 partition count [8] and sizes[(53651456, False)] \n",
      "Rank: 7 partition count [8] and sizes[(53651456, False)] \n",
      "Using /root/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0005540847778320312 seconds\n",
      "Using /root/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0006203651428222656 seconds\n",
      "Using /root/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0008347034454345703 seconds\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0009069442749023438 seconds\n",
      "Using /root/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0010111331939697266 seconds\n",
      "Using /root/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0005633831024169922 seconds\n",
      "Using /root/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0005674362182617188 seconds\n",
      "[WARNING|logging.py:295] 2023-12-07 04:11:05,724 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "[WARNING|logging.py:295] 2023-12-07 04:11:05,725 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "[WARNING|logging.py:295] 2023-12-07 04:11:05,726 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "[WARNING|logging.py:295] 2023-12-07 04:11:05,726 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "[WARNING|logging.py:295] 2023-12-07 04:11:05,726 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "[WARNING|logging.py:295] 2023-12-07 04:11:05,730 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "[WARNING|logging.py:295] 2023-12-07 04:11:05,736 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "[2023-12-07 04:11:05,814] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states\n",
      "[2023-12-07 04:11:05,815] [INFO] [utils.py:786:see_memory_usage] MA 13.09 GB         Max_MA 13.19 GB         CA 13.24 GB         Max_CA 13 GB \n",
      "[2023-12-07 04:11:05,815] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 77.44 GB, percent = 15.4%\n",
      "[2023-12-07 04:11:05,923] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states\n",
      "[2023-12-07 04:11:05,924] [INFO] [utils.py:786:see_memory_usage] MA 13.49 GB         Max_MA 13.89 GB         CA 14.04 GB         Max_CA 14 GB \n",
      "[2023-12-07 04:11:05,924] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 77.65 GB, percent = 15.4%\n",
      "[2023-12-07 04:11:05,924] [INFO] [stage_1_and_2.py:489:__init__] optimizer state initialized\n",
      "[2023-12-07 04:11:06,028] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2023-12-07 04:11:06,029] [INFO] [utils.py:786:see_memory_usage] MA 13.49 GB         Max_MA 13.49 GB         CA 14.04 GB         Max_CA 14 GB \n",
      "[2023-12-07 04:11:06,029] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 78.04 GB, percent = 15.5%\n",
      "[2023-12-07 04:11:06,030] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW\n",
      "[2023-12-07 04:11:06,031] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2023-12-07 04:11:06,031] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.LambdaLR object at 0x7f1b4d0ebfa0>\n",
      "[2023-12-07 04:11:06,031] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.999)]\n",
      "[2023-12-07 04:11:06,032] [INFO] [config.py:955:print] DeepSpeedEngine configuration:\n",
      "[2023-12-07 04:11:06,033] [INFO] [config.py:959:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2023-12-07 04:11:06,033] [INFO] [config.py:959:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2023-12-07 04:11:06,033] [INFO] [config.py:959:print]   amp_enabled .................. False\n",
      "[2023-12-07 04:11:06,033] [INFO] [config.py:959:print]   amp_params ................... False\n",
      "[2023-12-07 04:11:06,033] [INFO] [config.py:959:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2023-12-07 04:11:06,033] [INFO] [config.py:959:print]   bfloat16_enabled ............. False\n",
      "[2023-12-07 04:11:06,033] [INFO] [config.py:959:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2023-12-07 04:11:06,033] [INFO] [config.py:959:print]   checkpoint_tag_validation_enabled  True\n",
      "[2023-12-07 04:11:06,033] [INFO] [config.py:959:print]   checkpoint_tag_validation_fail  False\n",
      "[2023-12-07 04:11:06,033] [INFO] [config.py:959:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f1b4d0fa940>\n",
      "[2023-12-07 04:11:06,033] [INFO] [config.py:959:print]   communication_data_type ...... None\n",
      "[2023-12-07 04:11:06,033] [INFO] [config.py:959:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2023-12-07 04:11:06,033] [INFO] [config.py:959:print]   curriculum_enabled_legacy .... False\n",
      "[2023-12-07 04:11:06,033] [INFO] [config.py:959:print]   curriculum_params_legacy ..... False\n",
      "[2023-12-07 04:11:06,034] [INFO] [config.py:959:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2023-12-07 04:11:06,034] [INFO] [config.py:959:print]   data_efficiency_enabled ...... False\n",
      "[2023-12-07 04:11:06,034] [INFO] [config.py:959:print]   dataloader_drop_last ......... False\n",
      "[2023-12-07 04:11:06,034] [INFO] [config.py:959:print]   disable_allgather ............ False\n",
      "[2023-12-07 04:11:06,034] [INFO] [config.py:959:print]   dump_state ................... False\n",
      "[2023-12-07 04:11:06,034] [INFO] [config.py:959:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 100, 'delayed_shift': 2, 'min_scale': 1e-10}\n",
      "[2023-12-07 04:11:06,034] [INFO] [config.py:959:print]   eigenvalue_enabled ........... False\n",
      "[2023-12-07 04:11:06,034] [INFO] [config.py:959:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2023-12-07 04:11:06,034] [INFO] [config.py:959:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2023-12-07 04:11:06,034] [INFO] [config.py:959:print]   eigenvalue_layer_num ......... 0\n",
      "[2023-12-07 04:11:06,034] [INFO] [config.py:959:print]   eigenvalue_max_iter .......... 100\n",
      "[2023-12-07 04:11:06,034] [INFO] [config.py:959:print]   eigenvalue_stability ......... 1e-06\n",
      "[2023-12-07 04:11:06,034] [INFO] [config.py:959:print]   eigenvalue_tol ............... 0.01\n",
      "[2023-12-07 04:11:06,034] [INFO] [config.py:959:print]   eigenvalue_verbose ........... False\n",
      "[2023-12-07 04:11:06,034] [INFO] [config.py:959:print]   elasticity_enabled ........... False\n",
      "[2023-12-07 04:11:06,034] [INFO] [config.py:959:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2023-12-07 04:11:06,034] [INFO] [config.py:959:print]   fp16_auto_cast ............... False\n",
      "[2023-12-07 04:11:06,034] [INFO] [config.py:959:print]   fp16_enabled ................. True\n",
      "[2023-12-07 04:11:06,034] [INFO] [config.py:959:print]   fp16_master_weights_and_gradients  False\n",
      "[2023-12-07 04:11:06,034] [INFO] [config.py:959:print]   global_rank .................. 0\n",
      "[2023-12-07 04:11:06,034] [INFO] [config.py:959:print]   grad_accum_dtype ............. None\n",
      "[2023-12-07 04:11:06,034] [INFO] [config.py:959:print]   gradient_accumulation_steps .. 1\n",
      "[2023-12-07 04:11:06,034] [INFO] [config.py:959:print]   gradient_clipping ............ 1.0\n",
      "[2023-12-07 04:11:06,034] [INFO] [config.py:959:print]   gradient_predivide_factor .... 1.0\n",
      "[2023-12-07 04:11:06,034] [INFO] [config.py:959:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2023-12-07 04:11:06,034] [INFO] [config.py:959:print]   initial_dynamic_scale ........ 65536\n",
      "[2023-12-07 04:11:06,034] [INFO] [config.py:959:print]   load_universal_checkpoint .... False\n",
      "[2023-12-07 04:11:06,034] [INFO] [config.py:959:print]   loss_scale ................... 0\n",
      "[2023-12-07 04:11:06,034] [INFO] [config.py:959:print]   memory_breakdown ............. False\n",
      "[2023-12-07 04:11:06,034] [INFO] [config.py:959:print]   mics_hierarchial_params_gather  False\n",
      "[2023-12-07 04:11:06,034] [INFO] [config.py:959:print]   mics_shard_size .............. -1\n",
      "[2023-12-07 04:11:06,034] [INFO] [config.py:959:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2023-12-07 04:11:06,034] [INFO] [config.py:959:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2023-12-07 04:11:06,034] [INFO] [config.py:959:print]   optimizer_legacy_fusion ...... False\n",
      "[2023-12-07 04:11:06,034] [INFO] [config.py:959:print]   optimizer_name ............... None\n",
      "[2023-12-07 04:11:06,034] [INFO] [config.py:959:print]   optimizer_params ............. None\n",
      "[2023-12-07 04:11:06,034] [INFO] [config.py:959:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n",
      "[2023-12-07 04:11:06,034] [INFO] [config.py:959:print]   pld_enabled .................. False\n",
      "[2023-12-07 04:11:06,034] [INFO] [config.py:959:print]   pld_params ................... False\n",
      "[2023-12-07 04:11:06,034] [INFO] [config.py:959:print]   prescale_gradients ........... False\n",
      "[2023-12-07 04:11:06,034] [INFO] [config.py:959:print]   scheduler_name ............... None\n",
      "[2023-12-07 04:11:06,034] [INFO] [config.py:959:print]   scheduler_params ............. None\n",
      "[2023-12-07 04:11:06,034] [INFO] [config.py:959:print]   sparse_attention ............. None\n",
      "[2023-12-07 04:11:06,034] [INFO] [config.py:959:print]   sparse_gradients_enabled ..... False\n",
      "[2023-12-07 04:11:06,035] [INFO] [config.py:959:print]   steps_per_print .............. 2000\n",
      "[2023-12-07 04:11:06,035] [INFO] [config.py:959:print]   train_batch_size ............. 8\n",
      "[2023-12-07 04:11:06,035] [INFO] [config.py:959:print]   train_micro_batch_size_per_gpu  1\n",
      "[2023-12-07 04:11:06,035] [INFO] [config.py:959:print]   use_node_local_storage ....... False\n",
      "[2023-12-07 04:11:06,035] [INFO] [config.py:959:print]   wall_clock_breakdown ......... False\n",
      "[2023-12-07 04:11:06,035] [INFO] [config.py:959:print]   world_size ................... 8\n",
      "[2023-12-07 04:11:06,035] [INFO] [config.py:959:print]   zero_allow_untested_optimizer  True\n",
      "[2023-12-07 04:11:06,035] [INFO] [config.py:959:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=100000000 allgather_partitions=True allgather_bucket_size=100000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True\n",
      "[2023-12-07 04:11:06,035] [INFO] [config.py:959:print]   zero_enabled ................. True\n",
      "[2023-12-07 04:11:06,035] [INFO] [config.py:959:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2023-12-07 04:11:06,035] [INFO] [config.py:959:print]   zero_optimization_stage ...... 2\n",
      "[2023-12-07 04:11:06,035] [INFO] [config.py:945:print_user_config]   json = {\n",
      "    \"fp16\": {\n",
      "        \"enabled\": true, \n",
      "        \"loss_scale\": 0, \n",
      "        \"loss_scale_window\": 100, \n",
      "        \"initial_scale_power\": 16, \n",
      "        \"hysteresis\": 2, \n",
      "        \"min_loss_scale\": 1e-10\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 1.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 1.000000e+08, \n",
      "        \"contiguous_gradients\": true\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"steps_per_print\": 2.000000e+03, \n",
      "    \"train_batch_size\": 8, \n",
      "    \"train_micro_batch_size_per_gpu\": 1, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "Using /root/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0003848075866699219 seconds\n",
      "[INFO|trainer.py:1769] 2023-12-07 04:11:06,037 >> ***** Running training *****\n",
      "[INFO|trainer.py:1770] 2023-12-07 04:11:06,037 >>   Num examples = 51,179\n",
      "[INFO|trainer.py:1771] 2023-12-07 04:11:06,037 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1772] 2023-12-07 04:11:06,037 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:1773] 2023-12-07 04:11:06,037 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:1774] 2023-12-07 04:11:06,037 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1775] 2023-12-07 04:11:06,037 >>   Total optimization steps = 100\n",
      "[INFO|trainer.py:1776] 2023-12-07 04:11:06,039 >>   Number of trainable parameters = 429,211,648\n",
      "  0%|                                                   | 0/100 [00:00<?, ?it/s][WARNING|logging.py:295] 2023-12-07 04:11:06,068 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "[2023-12-07 04:11:07,606] [INFO] [loss_scaler.py:188:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 9.9138, 'learning_rate': 0.0, 'epoch': 0.0}                            \n",
      "  1%|▍                                          | 1/100 [00:01<02:33,  1.55s/it][2023-12-07 04:11:08,417] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "  2%|▊                                          | 2/100 [00:02<01:49,  1.12s/it][2023-12-07 04:11:09,216] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "  3%|█▎                                         | 3/100 [00:03<01:34,  1.03it/s][2023-12-07 04:11:10,026] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "  4%|█▋                                         | 4/100 [00:03<01:27,  1.10it/s][2023-12-07 04:11:10,822] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n",
      "  9%|███▊                                       | 9/100 [00:10<02:02,  1.34s/it][2023-12-07 04:11:17,487] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4096, reducing to 2048\n",
      "{'loss': 9.0059, 'learning_rate': 9.997377845227576e-05, 'epoch': 0.0}          \n",
      " 13%|█████▍                                    | 13/100 [00:15<01:58,  1.36s/it][2023-12-07 04:11:22,651] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2048, reducing to 1024\n",
      "{'loss': 8.0483, 'learning_rate': 9.740045899781352e-05, 'epoch': 0.0}          \n",
      "{'loss': 6.8704, 'learning_rate': 8.987214052813604e-05, 'epoch': 0.0}          \n",
      "{'loss': 6.7768, 'learning_rate': 7.819785149254532e-05, 'epoch': 0.01}         \n",
      "{'loss': 6.4842, 'learning_rate': 6.359150361181715e-05, 'epoch': 0.01}         \n",
      "{'loss': 6.1092, 'learning_rate': 4.7571888894277604e-05, 'epoch': 0.01}        \n",
      "{'loss': 5.9102, 'learning_rate': 3.180475315182563e-05, 'epoch': 0.01}         \n",
      "{'loss': 5.9144, 'learning_rate': 1.7929589018443016e-05, 'epoch': 0.01}        \n",
      "{'loss': 6.01, 'learning_rate': 7.389158817201542e-06, 'epoch': 0.01}           \n",
      "{'loss': 6.0504, 'learning_rate': 1.2794737676536994e-06, 'epoch': 0.02}        \n",
      "100%|█████████████████████████████████████████| 100/100 [02:22<00:00,  1.47s/it][INFO|trainer.py:2039] 2023-12-07 04:13:28,758 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 142.7186, 'train_samples_per_second': 5.605, 'train_steps_per_second': 0.701, 'train_loss': 6.727057647705078, 'epoch': 0.02}\n",
      "100%|█████████████████████████████████████████| 100/100 [02:22<00:00,  1.43s/it]\n",
      "[INFO|tokenization_utils_base.py:2171] 2023-12-07 04:13:28,762 >> tokenizer config file saved in ../../output/llama-alpaca-zh/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2023-12-07 04:13:28,762 >> Special tokens file saved in ../../output/llama-alpaca-zh/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2228] 2023-12-07 04:13:28,762 >> added tokens file saved in ../../output/llama-alpaca-zh/added_tokens.json\n",
      "***** train metrics *****\n",
      "  epoch                    =       0.02\n",
      "  train_loss               =     6.7271\n",
      "  train_runtime            = 0:02:22.71\n",
      "  train_samples            =      51179\n",
      "  train_samples_per_second =      5.605\n",
      "  train_steps_per_second   =      0.701\n",
      "12/07/2023 04:13:34 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:3129] 2023-12-07 04:13:34,571 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3131] 2023-12-07 04:13:34,571 >>   Num examples = 3247\n",
      "[INFO|trainer.py:3134] 2023-12-07 04:13:34,571 >>   Batch size = 1\n",
      "100%|█████████████████████████████████████████| 406/406 [00:29<00:00, 13.86it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =       0.02\n",
      "  eval_loss               =      6.332\n",
      "  eval_runtime            = 0:00:29.35\n",
      "  eval_samples            =       3247\n",
      "  eval_samples_per_second =      110.6\n",
      "  eval_steps_per_second   =     13.829\n",
      "  perplexity              =   562.2976\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nnodes 1 --nproc_per_node 8 {script_dir}/run_clm_sft_with_peft.py \\\n",
    "    --deepspeed {deepspeed_config_file} \\\n",
    "    --model_name_or_path {sft_model_args.model_name_or_path} \\\n",
    "    --tokenizer_name_or_path {sft_model_args.tokenizer_name_or_path} \\\n",
    "    --dataset_dir {sft_data_args.dataset_dir} \\\n",
    "    --per_device_train_batch_size {sft_training_args.per_device_train_batch_size} \\\n",
    "    --per_device_eval_batch_size {sft_training_args.per_device_eval_batch_size} \\\n",
    "    --do_train {sft_training_args.do_train}\\\n",
    "    --do_eval {sft_training_args.do_eval} \\\n",
    "    --seed {sft_training_args.seed} \\\n",
    "    --fp16 {sft_training_args.fp16}\\\n",
    "    --max_steps {sft_training_args.max_steps} \\\n",
    "    --max_seq_length {sft_data_args.max_seq_length} \\\n",
    "    --lr_scheduler_type {sft_training_args.lr_scheduler_type} \\\n",
    "    --learning_rate {sft_training_args.learning_rate} \\\n",
    "    --warmup_ratio {sft_training_args.warmup_ratio} \\\n",
    "    --weight_decay {sft_training_args.weight_decay} \\\n",
    "    --logging_strategy {sft_training_args.logging_strategy} \\\n",
    "    --logging_steps {sft_training_args.logging_steps} \\\n",
    "    --save_strategy {sft_training_args.save_strategy} \\\n",
    "    --save_total_limit {sft_training_args.save_total_limit} \\\n",
    "    --save_steps {sft_training_args.save_steps} \\\n",
    "    --evaluation_strategy {sft_training_args.evaluation_strategy} \\\n",
    "    --eval_steps {sft_training_args.eval_steps} \\\n",
    "    --gradient_accumulation_steps {sft_training_args.gradient_accumulation_steps} \\\n",
    "    --preprocessing_num_workers {sft_data_args.preprocessing_num_workers} \\\n",
    "    --overwrite_output_dir {sft_training_args.overwrite_output_dir} \\\n",
    "    --ddp_timeout {sft_training_args.ddp_timeout} \\\n",
    "    --logging_first_step {sft_training_args.logging_first_step} \\\n",
    "    --lora_rank {sft_training_args.lora_rank} \\\n",
    "    --lora_alpha {sft_training_args.lora_alpha} \\\n",
    "    --trainable {sft_training_args.trainable} \\\n",
    "    --modules_to_save {sft_training_args.modules_to_save} \\\n",
    "    --lora_dropout {sft_training_args.lora_dropout} \\\n",
    "    --torch_dtype {sft_model_args.torch_dtype} \\\n",
    "    --gradient_checkpointing {sft_training_args.gradient_checkpointing} \\\n",
    "    --ddp_find_unused_parameters {sft_training_args.ddp_find_unused_parameters} \\\n",
    "    --validation_file {sft_data_args.validation_file} \\\n",
    "    --validation_split_percentage {sft_data_args.validation_split_percentage} \\\n",
    "    --output_dir {sft_training_args.output_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: merge fine-tuned lora\n",
    "Having fine-tuned the lora model, we need to merge it into the original llama-alpaca model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/libbitsandbytes_cuda114.so\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /opt/conda did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/cuda/compat/lib')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/c%3A/Users/zzz48/.vscode/extensions/ms-ceintl.vscode-language-pack-zh-hans-1.84.2023111509/translations/extensions/vscode.markdown-language-features.i18n.json'), PosixPath('vscode-local')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('[\"/tmp/vscode-ssh-auth-99aaedd1-49f2-4aea-b7fd-66afd70c1944.sock\",\"/root/.gnupg/S.gpg-agent\"]')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('{\"locale\"'), PosixPath('\"zh-cn\"},\"_languagePackId\"'), PosixPath('\"zh-cn\",\"availableLanguages\"'), PosixPath('\"b21449bfcc24b92b09a8c487b9b70068.zh-cn\",\"_translationsConfigFile\"'), PosixPath('true}'), PosixPath('\"zh-cn\",\"osLocale\"'), PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn\",\"_resolvedLanguagePackCoreLocation\"'), PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn/corrupted.info\",\"_languagePackSupport\"'), PosixPath('{\"*\"'), PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn/1a5daa3a0231a0fbba4f14db7ec463cf99d7768e\",\"_corruptedFile\"'), PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn/tcf.json\",\"_cacheRoot\"')}\n",
      "  warn(msg)\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 114\n",
      "CUDA SETUP: Loading binary /opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/libbitsandbytes_cuda114.so...\n",
      "Base model: ../../llama-7b-hf\n",
      "LoRA model(s) ['../../output/llama-zh/lora', '../../output/llama-alpaca-zh/lora']:\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:04<00:00,  2.26s/it]\n",
      "Peft version: 0.3.0.dev0\n",
      "Loading LoRA for 7B model\n",
      "Loading LoRA: ../../output/llama-zh/lora , tokenizer path: ../../output/llama-zh \n",
      "base_model vocab size: 32000\n",
      "tokenizer vocab size: 49953\n",
      "Extended vocabulary size to 49953\n",
      "Loading LoRA weights\n",
      "merging base_model.model.model.embed_tokens.weight\n",
      "merging base_model.model.lm_head.weight\n",
      "merging base_model.model.model.layers.0.self_attn.q_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.0.self_attn.k_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.0.self_attn.v_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.0.self_attn.o_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.0.mlp.gate_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.0.mlp.down_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.0.mlp.up_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.1.self_attn.q_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.1.self_attn.k_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.1.self_attn.v_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.1.self_attn.o_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.1.mlp.gate_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.1.mlp.down_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.1.mlp.up_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.2.self_attn.q_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.2.self_attn.k_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.2.self_attn.v_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.2.self_attn.o_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.2.mlp.gate_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.2.mlp.down_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.2.mlp.up_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.3.self_attn.q_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.3.self_attn.k_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.3.self_attn.v_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.3.self_attn.o_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.3.mlp.gate_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.3.mlp.down_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.3.mlp.up_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.4.self_attn.q_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.4.self_attn.k_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.4.self_attn.v_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.4.self_attn.o_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.4.mlp.gate_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.4.mlp.down_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.4.mlp.up_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.5.self_attn.q_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.5.self_attn.k_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.5.self_attn.v_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.5.self_attn.o_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.5.mlp.gate_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.5.mlp.down_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.5.mlp.up_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.6.self_attn.q_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.6.self_attn.k_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.6.self_attn.v_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.6.self_attn.o_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.6.mlp.gate_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.6.mlp.down_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.6.mlp.up_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.7.self_attn.q_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.7.self_attn.k_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.7.self_attn.v_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.7.self_attn.o_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.7.mlp.gate_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.7.mlp.down_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.7.mlp.up_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.8.self_attn.q_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.8.self_attn.k_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.8.self_attn.v_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.8.self_attn.o_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.8.mlp.gate_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.8.mlp.down_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.8.mlp.up_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.9.self_attn.q_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.9.self_attn.k_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.9.self_attn.v_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.9.self_attn.o_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.9.mlp.gate_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.9.mlp.down_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.9.mlp.up_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.10.self_attn.q_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.10.self_attn.k_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.10.self_attn.v_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.10.self_attn.o_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.10.mlp.gate_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.10.mlp.down_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.10.mlp.up_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.11.self_attn.q_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.11.self_attn.k_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.11.self_attn.v_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.11.self_attn.o_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.11.mlp.gate_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.11.mlp.down_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.11.mlp.up_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.12.self_attn.q_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.12.self_attn.k_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.12.self_attn.v_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.12.self_attn.o_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.12.mlp.gate_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.12.mlp.down_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.12.mlp.up_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.13.self_attn.q_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.13.self_attn.k_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.13.self_attn.v_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.13.self_attn.o_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.13.mlp.gate_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.13.mlp.down_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.13.mlp.up_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.14.self_attn.q_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.14.self_attn.k_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.14.self_attn.v_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.14.self_attn.o_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.14.mlp.gate_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.14.mlp.down_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.14.mlp.up_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.15.self_attn.q_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.15.self_attn.k_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.15.self_attn.v_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.15.self_attn.o_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.15.mlp.gate_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.15.mlp.down_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.15.mlp.up_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.16.self_attn.q_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.16.self_attn.k_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.16.self_attn.v_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.16.self_attn.o_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.16.mlp.gate_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.16.mlp.down_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.16.mlp.up_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.17.self_attn.q_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.17.self_attn.k_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.17.self_attn.v_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.17.self_attn.o_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.17.mlp.gate_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.17.mlp.down_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.17.mlp.up_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.18.self_attn.q_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.18.self_attn.k_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.18.self_attn.v_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.18.self_attn.o_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.18.mlp.gate_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.18.mlp.down_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.18.mlp.up_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.19.self_attn.q_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.19.self_attn.k_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.19.self_attn.v_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.19.self_attn.o_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.19.mlp.gate_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.19.mlp.down_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.19.mlp.up_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.20.self_attn.q_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.20.self_attn.k_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.20.self_attn.v_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.20.self_attn.o_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.20.mlp.gate_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.20.mlp.down_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.20.mlp.up_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.21.self_attn.q_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.21.self_attn.k_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.21.self_attn.v_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.21.self_attn.o_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.21.mlp.gate_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.21.mlp.down_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.21.mlp.up_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.22.self_attn.q_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.22.self_attn.k_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.22.self_attn.v_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.22.self_attn.o_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.22.mlp.gate_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.22.mlp.down_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.22.mlp.up_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.23.self_attn.q_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.23.self_attn.k_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.23.self_attn.v_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.23.self_attn.o_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.23.mlp.gate_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.23.mlp.down_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.23.mlp.up_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.24.self_attn.q_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.24.self_attn.k_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.24.self_attn.v_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.24.self_attn.o_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.24.mlp.gate_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.24.mlp.down_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.24.mlp.up_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.25.self_attn.q_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.25.self_attn.k_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.25.self_attn.v_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.25.self_attn.o_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.25.mlp.gate_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.25.mlp.down_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.25.mlp.up_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.26.self_attn.q_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.26.self_attn.k_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.26.self_attn.v_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.26.self_attn.o_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.26.mlp.gate_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.26.mlp.down_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.26.mlp.up_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.27.self_attn.q_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.27.self_attn.k_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.27.self_attn.v_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.27.self_attn.o_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.27.mlp.gate_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.27.mlp.down_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.27.mlp.up_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.28.self_attn.q_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.28.self_attn.k_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.28.self_attn.v_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.28.self_attn.o_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.28.mlp.gate_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.28.mlp.down_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.28.mlp.up_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.29.self_attn.q_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.29.self_attn.k_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.29.self_attn.v_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.29.self_attn.o_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.29.mlp.gate_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.29.mlp.down_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.29.mlp.up_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.30.self_attn.q_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.30.self_attn.k_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.30.self_attn.v_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.30.self_attn.o_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.30.mlp.gate_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.30.mlp.down_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.30.mlp.up_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.31.self_attn.q_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.31.self_attn.k_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.31.self_attn.v_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.31.self_attn.o_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.31.mlp.gate_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.31.mlp.down_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.31.mlp.up_proj.lora_A.weight\n",
      "Loading LoRA: ../../output/llama-alpaca-zh/lora , tokenizer path: ../../output/llama-alpaca-zh \n",
      "base_model vocab size: 49953\n",
      "tokenizer vocab size: 49954\n",
      "Extended vocabulary size to 49954\n",
      "Loading LoRA weights\n",
      "merging base_model.model.model.embed_tokens.weight\n",
      "merging base_model.model.lm_head.weight\n",
      "merging base_model.model.model.layers.0.self_attn.q_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.0.self_attn.k_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.0.self_attn.v_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.0.self_attn.o_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.0.mlp.gate_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.0.mlp.down_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.0.mlp.up_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.1.self_attn.q_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.1.self_attn.k_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.1.self_attn.v_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.1.self_attn.o_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.1.mlp.gate_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.1.mlp.down_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.1.mlp.up_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.2.self_attn.q_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.2.self_attn.k_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.2.self_attn.v_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.2.self_attn.o_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.2.mlp.gate_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.2.mlp.down_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.2.mlp.up_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.3.self_attn.q_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.3.self_attn.k_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.3.self_attn.v_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.3.self_attn.o_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.3.mlp.gate_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.3.mlp.down_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.3.mlp.up_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.4.self_attn.q_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.4.self_attn.k_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.4.self_attn.v_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.4.self_attn.o_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.4.mlp.gate_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.4.mlp.down_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.4.mlp.up_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.5.self_attn.q_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.5.self_attn.k_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.5.self_attn.v_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.5.self_attn.o_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.5.mlp.gate_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.5.mlp.down_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.5.mlp.up_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.6.self_attn.q_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.6.self_attn.k_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.6.self_attn.v_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.6.self_attn.o_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.6.mlp.gate_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.6.mlp.down_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.6.mlp.up_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.7.self_attn.q_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.7.self_attn.k_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.7.self_attn.v_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.7.self_attn.o_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.7.mlp.gate_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.7.mlp.down_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.7.mlp.up_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.8.self_attn.q_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.8.self_attn.k_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.8.self_attn.v_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.8.self_attn.o_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.8.mlp.gate_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.8.mlp.down_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.8.mlp.up_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.9.self_attn.q_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.9.self_attn.k_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.9.self_attn.v_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.9.self_attn.o_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.9.mlp.gate_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.9.mlp.down_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.9.mlp.up_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.10.self_attn.q_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.10.self_attn.k_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.10.self_attn.v_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.10.self_attn.o_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.10.mlp.gate_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.10.mlp.down_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.10.mlp.up_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.11.self_attn.q_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.11.self_attn.k_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.11.self_attn.v_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.11.self_attn.o_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.11.mlp.gate_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.11.mlp.down_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.11.mlp.up_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.12.self_attn.q_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.12.self_attn.k_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.12.self_attn.v_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.12.self_attn.o_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.12.mlp.gate_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.12.mlp.down_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.12.mlp.up_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.13.self_attn.q_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.13.self_attn.k_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.13.self_attn.v_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.13.self_attn.o_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.13.mlp.gate_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.13.mlp.down_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.13.mlp.up_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.14.self_attn.q_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.14.self_attn.k_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.14.self_attn.v_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.14.self_attn.o_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.14.mlp.gate_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.14.mlp.down_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.14.mlp.up_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.15.self_attn.q_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.15.self_attn.k_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.15.self_attn.v_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.15.self_attn.o_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.15.mlp.gate_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.15.mlp.down_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.15.mlp.up_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.16.self_attn.q_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.16.self_attn.k_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.16.self_attn.v_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.16.self_attn.o_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.16.mlp.gate_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.16.mlp.down_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.16.mlp.up_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.17.self_attn.q_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.17.self_attn.k_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.17.self_attn.v_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.17.self_attn.o_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.17.mlp.gate_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.17.mlp.down_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.17.mlp.up_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.18.self_attn.q_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.18.self_attn.k_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.18.self_attn.v_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.18.self_attn.o_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.18.mlp.gate_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.18.mlp.down_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.18.mlp.up_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.19.self_attn.q_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.19.self_attn.k_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.19.self_attn.v_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.19.self_attn.o_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.19.mlp.gate_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.19.mlp.down_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.19.mlp.up_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.20.self_attn.q_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.20.self_attn.k_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.20.self_attn.v_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.20.self_attn.o_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.20.mlp.gate_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.20.mlp.down_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.20.mlp.up_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.21.self_attn.q_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.21.self_attn.k_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.21.self_attn.v_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.21.self_attn.o_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.21.mlp.gate_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.21.mlp.down_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.21.mlp.up_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.22.self_attn.q_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.22.self_attn.k_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.22.self_attn.v_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.22.self_attn.o_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.22.mlp.gate_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.22.mlp.down_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.22.mlp.up_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.23.self_attn.q_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.23.self_attn.k_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.23.self_attn.v_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.23.self_attn.o_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.23.mlp.gate_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.23.mlp.down_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.23.mlp.up_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.24.self_attn.q_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.24.self_attn.k_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.24.self_attn.v_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.24.self_attn.o_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.24.mlp.gate_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.24.mlp.down_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.24.mlp.up_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.25.self_attn.q_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.25.self_attn.k_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.25.self_attn.v_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.25.self_attn.o_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.25.mlp.gate_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.25.mlp.down_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.25.mlp.up_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.26.self_attn.q_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.26.self_attn.k_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.26.self_attn.v_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.26.self_attn.o_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.26.mlp.gate_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.26.mlp.down_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.26.mlp.up_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.27.self_attn.q_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.27.self_attn.k_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.27.self_attn.v_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.27.self_attn.o_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.27.mlp.gate_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.27.mlp.down_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.27.mlp.up_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.28.self_attn.q_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.28.self_attn.k_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.28.self_attn.v_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.28.self_attn.o_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.28.mlp.gate_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.28.mlp.down_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.28.mlp.up_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.29.self_attn.q_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.29.self_attn.k_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.29.self_attn.v_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.29.self_attn.o_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.29.mlp.gate_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.29.mlp.down_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.29.mlp.up_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.30.self_attn.q_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.30.self_attn.k_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.30.self_attn.v_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.30.self_attn.o_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.30.mlp.gate_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.30.mlp.down_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.30.mlp.up_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.31.self_attn.q_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.31.self_attn.k_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.31.self_attn.v_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.31.self_attn.o_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.31.mlp.gate_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.31.mlp.down_proj.lora_A.weight\n",
      "merging base_model.model.model.layers.31.mlp.up_proj.lora_A.weight\n",
      "Saving to Hugging Face format...\n"
     ]
    }
   ],
   "source": [
    "!cp merge_llama_with_chinese_lora.py {script_dir}\n",
    "!python {script_dir}/merge_llama_with_chinese_lora.py --base_model {llama_tokenizer_dir} --tokenizer_path {output_dir}/llama-zh,{output_dir}/llama-alpaca-zh --lora_model {output_dir}/llama-zh/lora,{output_dir}/llama-alpaca-zh/lora --output_type huggingface --output_dir {output_dir}/merge-alpaca-hf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step6: try inference\n",
    "Now we can try to test our new trained model for some simple questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/libbitsandbytes_cuda114.so\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /opt/conda did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/cuda/compat/lib'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vscode-local'), PosixPath('/c%3A/Users/zzz48/.vscode/extensions/ms-ceintl.vscode-language-pack-zh-hans-1.84.2023111509/translations/extensions/vscode.markdown-language-features.i18n.json')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('[\"/tmp/vscode-ssh-auth-99aaedd1-49f2-4aea-b7fd-66afd70c1944.sock\",\"/root/.gnupg/S.gpg-agent\"]')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('\"zh-cn\",\"osLocale\"'), PosixPath('true}'), PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn/tcf.json\",\"_cacheRoot\"'), PosixPath('\"zh-cn\",\"availableLanguages\"'), PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn/1a5daa3a0231a0fbba4f14db7ec463cf99d7768e\",\"_corruptedFile\"'), PosixPath('{\"*\"'), PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn/corrupted.info\",\"_languagePackSupport\"'), PosixPath('\"/root/.vscode-server/data/clp/b21449bfcc24b92b09a8c487b9b70068.zh-cn\",\"_resolvedLanguagePackCoreLocation\"'), PosixPath('\"zh-cn\"},\"_languagePackId\"'), PosixPath('{\"locale\"'), PosixPath('\"b21449bfcc24b92b09a8c487b9b70068.zh-cn\",\"_translationsConfigFile\"')}\n",
      "  warn(msg)\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 114\n",
      "CUDA SETUP: Loading binary /opt/conda/lib/python3.8/site-packages/bitsandbytes-0.39.0-py3.8.egg/bitsandbytes/libbitsandbytes_cuda114.so...\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:05<00:00,  2.94s/it]\n",
      "Vocab of the base model: 49954\n",
      "Vocab of the tokenizer: 49954\n",
      "Start inference with instruction mode.\n",
      "=====================================================================================\n",
      "+ 该模式下仅支持单轮问答，无多轮对话能力。\n",
      "+ 如要进行多轮对话，请使用llama.cpp或llamachat工具。\n",
      "-------------------------------------------------------------------------------------\n",
      "+ This mode only supports single-turn QA.\n",
      "+ If you want to experience multi-turn dialogue, please use llama.cpp or llamachat.\n",
      "=====================================================================================\n",
      "Input:^C\n",
      "Traceback (most recent call last):\n",
      "  File \"../../Chinese-LLaMA-Alpaca/scripts/inference_hf.py\", line 110, in <module>\n",
      "    raw_input_text = input(\"Input:\")\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python {script_dir}/inference_hf.py --base_model {output_dir}/merge-alpaca-hf --with_prompt --interactive"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
