{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step0: setup\n",
    "Firstly, we should install the required dependency and source code of the project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers==4.28.1 sentencepiece==0.1.97 google protobuf deepspeed==0.9.2 datasets -i https://pypi.tuna.tsinghua.edu.cn/simple  --trusted-host pypi.tuna.tsinghua.edu.cn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Chinese-LLaMA-Alpaca library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ymcui/Chinese-LLaMA-Alpaca.git ../../Chinese-LLaMA-Alpaca\n",
    "!git -C ../../Chinese-LLaMA-Alpaca checkout 7bc1f3d7c426e3685d14eb1e5614066650f94838"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install peft library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/huggingface/peft.git ../../peft\n",
    "!git -C ../../peft checkout 13e53fc\n",
    "!pip install ../../peft -i https://pypi.tuna.tsinghua.edu.cn/simple  --trusted-host pypi.tuna.tsinghua.edu.cn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we should setup the project by adding directories, downloading the model, and preprocessing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ../../cache\n",
    "!mkdir ../../output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"/workspace/llama-7b-hf\"\n",
    "!ln -s {model_dir} ../../"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a textbook of computer system for pretraining. Now let us clean the data simply. The original data has been placed on `../../data/book`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def clean_txt_file(input_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        data = file.readlines()\n",
    "\n",
    "    cleaned_lines = []\n",
    "    for line in data:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            cleaned_lines.append(line)\n",
    "\n",
    "    cleaned_data = '\\n'.join(cleaned_lines)\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8', errors='ignore') as file:\n",
    "        file.write(cleaned_data)\n",
    "\n",
    "def clean_txt_files_in_directory(in_directory, out_directory):\n",
    "    for filename in os.listdir(in_directory):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            input_file = os.path.join(in_directory, filename)\n",
    "            output_file = os.path.join(out_directory, \"cleaned_\" + filename)\n",
    "            clean_txt_file(input_file, output_file)\n",
    "\n",
    "in_directory = '../../data/book'  \n",
    "out_directory = '../../data/clean_book/'  \n",
    "\n",
    "clean_txt_files_in_directory(in_directory, out_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all preparation ready. Let us define some key directories for following usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_tokenizer_dir=\"../../llama-7b-hf\"\n",
    "chinese_sp_model_file=\"../../Chinese-LLaMA-Alpaca/scripts/chinese_sp.model\"\n",
    "output_dir=\"../../output\"\n",
    "script_dir=\"../../Chinese-LLaMA-Alpaca/scripts\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: merge tokens\n",
    "Secondly, let us merge the chinese vocabulary with the original vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python {script_dir}/merge_tokenizers.py --llama_tokenizer_dir {llama_tokenizer_dir} --chinese_sp_model_file {chinese_sp_model_file}\n",
    "!mv merged_tokenizer_hf {output_dir}\n",
    "!mv merged_tokenizer_sp {output_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step2: pretrain\n",
    "Now let us pretrain the model using the prepared data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we adjust the lora saving path in the original pretrain script\n",
    "!cp run_clm_pt_with_peft.py {script_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before start training, we should first learn to manage our training arguments using class inheritance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List, Dict, Any, Mapping\n",
    "from dataclasses import dataclass, field\n",
    "from transformers import TrainingArguments, MODEL_FOR_CAUSAL_LM_MAPPING\n",
    "from transformers.utils.versions import require_version\n",
    "\n",
    "MODEL_CONFIG_CLASSES = list(MODEL_FOR_CAUSAL_LM_MAPPING.keys())\n",
    "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name_or_path: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"The model checkpoint for weights initialization.Don't set if you want to train a model from scratch.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    tokenizer_name_or_path: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"The tokenizer for weights initialization.Don't set if you want to train a model from scratch.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    model_type: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"If training from scratch, pass a model type from the list: \" + \", \".join(MODEL_TYPES)},\n",
    "    )\n",
    "    config_overrides: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Override some existing default config settings when a model is trained from scratch. Example: \"\n",
    "                \"n_embd=10,resid_pdrop=0.2,scale_attn_weights=false,summary_type=cls_index\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
    "    )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
    "    )\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n",
    "    )\n",
    "    use_fast_tokenizer: bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n",
    "    )\n",
    "    model_revision: str = field(\n",
    "        default=\"main\",\n",
    "        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n",
    "    )\n",
    "    use_auth_token: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Will use the token generated when running `huggingface-cli login` (necessary to use this script \"\n",
    "                \"with private models).\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    torch_dtype: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Override the default `torch.dtype` and load the model under this dtype. If `auto` is passed, the \"\n",
    "                \"dtype will be automatically derived from the model's weights.\"\n",
    "            ),\n",
    "            \"choices\": [\"auto\", \"bfloat16\", \"float16\", \"float32\"],\n",
    "        },\n",
    "    )\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.config_overrides is not None and (self.config_name is not None or self.model_name_or_path is not None):\n",
    "            raise ValueError(\n",
    "                \"--config_overrides can't be used in combination with --config_name or --model_name_or_path\"\n",
    "            )\n",
    "\n",
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_dir: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    dataset_config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    train_file: Optional[str] = field(default=None, metadata={\"help\": \"The input training data file (a text file).\"})\n",
    "    validation_file: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"An optional input evaluation data file to evaluate the perplexity on (a text file).\"},\n",
    "    )\n",
    "    max_train_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n",
    "                \"value if set.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    max_eval_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n",
    "                \"value if set.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    streaming: bool = field(default=False, metadata={\"help\": \"Enable streaming mode\"})\n",
    "    block_size: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Optional input sequence length after tokenization. \"\n",
    "                \"The training dataset will be truncated in block of this size for training. \"\n",
    "                \"Default to the model max input length for single sentence inputs (take into account special tokens).\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    overwrite_cache: bool = field(\n",
    "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
    "    )\n",
    "    validation_split_percentage: Optional[float] = field(\n",
    "        default=0.05,\n",
    "        metadata={\n",
    "            \"help\": \"The percentage of the train set used as validation set in case there's no validation split\"\n",
    "        },\n",
    "    )\n",
    "    preprocessing_num_workers: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n",
    "    )\n",
    "    keep_linebreaks: bool = field(\n",
    "        default=True, metadata={\"help\": \"Whether to keep line breaks when using TXT files or not.\"}\n",
    "    )\n",
    "    data_cache_dir: Optional[str] = field(default=\"./\", metadata={\"help\": \"The datasets processed stored\"})\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.streaming:\n",
    "            require_version(\"datasets>=2.0.0\", \"The streaming feature requires `datasets>=2.0.0`\")\n",
    "\n",
    "@dataclass\n",
    "class MyTrainingArguments(TrainingArguments):\n",
    "    trainable : Optional[str] = field(default=\"q_proj,v_proj\")\n",
    "    lora_rank : Optional[int] = field(default=8)\n",
    "    lora_dropout : Optional[float] = field(default=0.1)\n",
    "    lora_alpha : Optional[float] = field(default=32.)\n",
    "    modules_to_save : Optional[str] = field(default=None)\n",
    "    debug_mode : Optional[bool] = field(default=False)\n",
    "    peft_path : Optional[str] = field(default=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We proivde a basic version of training arguments, you can create your own arguments by inheriting it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = ModelArguments(model_name_or_path=\"../../llama-7b-hf/\", \n",
    "                            tokenizer_name_or_path=\"../../output/merged_tokenizer_hf\",\n",
    "                            torch_dtype=\"float16\")\n",
    "\n",
    "data_args = DataTrainingArguments(dataset_dir=\"../../data/clean_book\",\n",
    "                                  data_cache_dir=\"../../cache\",\n",
    "                                  validation_split_percentage=0.001,\n",
    "                                  block_size=512,\n",
    "                                  preprocessing_num_workers=8)\n",
    "\n",
    "deepspeed_config_file = script_dir+\"/\"+\"ds_zero2_no_offload.json\" # we can not include deepspeed config path directly here otherwise we will init deepspeed environment in advance\n",
    "training_args =  MyTrainingArguments(per_device_train_batch_size=1,\n",
    "                                    per_device_eval_batch_size=1,\n",
    "                                    do_train=True,\n",
    "                                    seed=100,\n",
    "                                    fp16=True,\n",
    "                                    max_steps=100,\n",
    "                                    lr_scheduler_type=\"cosine\",\n",
    "                                    learning_rate=2e-4,\n",
    "                                    warmup_ratio=0.05,\n",
    "                                    weight_decay=0.01,\n",
    "                                    logging_strategy=\"steps\",\n",
    "                                    logging_steps=10,\n",
    "                                    logging_first_step=True,\n",
    "                                    save_strategy=\"steps\",\n",
    "                                    save_steps=500,\n",
    "                                    save_total_limit=3,\n",
    "                                    gradient_accumulation_steps=1,\n",
    "                                    gradient_checkpointing=True,\n",
    "                                    ddp_find_unused_parameters=False,\n",
    "                                    ddp_timeout=30000,\n",
    "                                    output_dir=\"../../output/llama-zh\",\n",
    "                                    overwrite_output_dir=True,\n",
    "                                    lora_rank=8,\n",
    "                                    lora_alpha=32,\n",
    "                                    trainable=\"q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj\",\n",
    "                                    modules_to_save=\"embed_tokens,lm_head\",\n",
    "                                    lora_dropout=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us start pretraining using the prepared arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!torchrun --nnodes 1 --nproc_per_node 8 {script_dir}/run_clm_pt_with_peft.py \\\n",
    "    --deepspeed {deepspeed_config_file} \\\n",
    "    --model_name_or_path {model_args.model_name_or_path} \\\n",
    "    --tokenizer_name_or_path {model_args.tokenizer_name_or_path} \\\n",
    "    --dataset_dir {data_args.dataset_dir} \\\n",
    "    --data_cache_dir {data_args.data_cache_dir} \\\n",
    "    --validation_split_percentage {data_args.validation_split_percentage} \\\n",
    "    --per_device_train_batch_size {training_args.per_device_train_batch_size} \\\n",
    "    --per_device_eval_batch_size {training_args.per_device_eval_batch_size} \\\n",
    "    --do_train {training_args.do_train}\\\n",
    "    --seed {training_args.seed} \\\n",
    "    --fp16 {training_args.fp16}\\\n",
    "    --max_steps {training_args.max_steps} \\\n",
    "    --lr_scheduler_type {training_args.lr_scheduler_type} \\\n",
    "    --learning_rate {training_args.learning_rate} \\\n",
    "    --warmup_ratio {training_args.warmup_ratio} \\\n",
    "    --weight_decay {training_args.weight_decay} \\\n",
    "    --logging_strategy {training_args.logging_strategy} \\\n",
    "    --logging_steps {training_args.logging_steps} \\\n",
    "    --save_strategy {training_args.save_strategy} \\\n",
    "    --save_total_limit {training_args.save_total_limit} \\\n",
    "    --save_steps {training_args.save_steps} \\\n",
    "    --gradient_accumulation_steps {training_args.gradient_accumulation_steps} \\\n",
    "    --preprocessing_num_workers {data_args.preprocessing_num_workers} \\\n",
    "    --block_size {data_args.block_size} \\\n",
    "    --output_dir {training_args.output_dir} \\\n",
    "    --overwrite_output_dir {training_args.overwrite_output_dir} \\\n",
    "    --ddp_timeout {training_args.ddp_timeout} \\\n",
    "    --logging_first_step {training_args.logging_first_step} \\\n",
    "    --lora_rank {training_args.lora_rank} \\\n",
    "    --lora_alpha {training_args.lora_alpha} \\\n",
    "    --trainable {training_args.trainable} \\\n",
    "    --modules_to_save {training_args.modules_to_save} \\\n",
    "    --lora_dropout {training_args.lora_dropout} \\\n",
    "    --torch_dtype {model_args.torch_dtype} \\\n",
    "    --gradient_checkpointing {training_args.gradient_checkpointing} \\\n",
    "    --ddp_find_unused_parameters {training_args.ddp_find_unused_parameters}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step3: merge pretrained lora\n",
    "Having trained the lora model, we need to merge it into the original llama model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we expose an interface to change the save path of tokenizers\n",
    "!cp merge_llama_with_chinese_lora.py {script_dir}\n",
    "!python {script_dir}/merge_llama_with_chinese_lora.py --base_model {llama_tokenizer_dir} --tokenizer_path {output_dir}/merged_tokenizer_hf --lora_model {output_dir}/llama-zh/lora --output_type huggingface --output_dir {output_dir}/merge-lora-hf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Superviser fine-tuning\n",
    "Now we can start fine-tuning the model using instructions. You should reprepare the arguments similar to that in the pretrain step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarly we adjust the lora saving path in the original pretrain script\n",
    "!cp run_clm_sft_with_peft.py {script_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SftDataTrainingArguments(DataTrainingArguments):\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    \"\"\"\n",
    "    max_seq_length: Optional[int] = field(default=512)\n",
    "\n",
    "sft_model_args, sft_data_args, sft_training_args = model_args, SftDataTrainingArguments(data_args), training_args\n",
    "# we only need to adjust some parameters\n",
    "sft_model_args.model_name_or_path = \"../../output/merge-lora-hf/\"\n",
    "sft_model_args.tokenizer_name_or_path = \"../../output/merged_tokenizer_hf/\"\n",
    "\n",
    "sft_data_args.dataset_dir = \"../../Chinese-LLaMA-Alpaca/data/\"\n",
    "sft_data_args.validation_file = \"../../data/alpaca_data.json\"\n",
    "sft_data_args.block_size = None\n",
    "sft_data_args.max_seq_length = 512\n",
    "\n",
    "sft_training_args.learning_rate = 1e-4\n",
    "sft_training_args.output_dir = \"../../output/llama-alpaca-zh\"\n",
    "sft_training_args.warmup_ratio = 0.03\n",
    "sft_training_args.weight_decay = 0\n",
    "sft_training_args.eval_steps = 250\n",
    "sft_training_args.evaluation_strategy = \"steps\"\n",
    "sft_training_args.do_eval = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!torchrun --nnodes 1 --nproc_per_node 8 {script_dir}/run_clm_sft_with_peft.py \\\n",
    "    --deepspeed {deepspeed_config_file} \\\n",
    "    --model_name_or_path {sft_model_args.model_name_or_path} \\\n",
    "    --tokenizer_name_or_path {sft_model_args.tokenizer_name_or_path} \\\n",
    "    --dataset_dir {sft_data_args.dataset_dir} \\\n",
    "    --per_device_train_batch_size {sft_training_args.per_device_train_batch_size} \\\n",
    "    --per_device_eval_batch_size {sft_training_args.per_device_eval_batch_size} \\\n",
    "    --do_train {sft_training_args.do_train}\\\n",
    "    --do_eval {sft_training_args.do_eval} \\\n",
    "    --seed {sft_training_args.seed} \\\n",
    "    --fp16 {sft_training_args.fp16}\\\n",
    "    --max_steps {sft_training_args.max_steps} \\\n",
    "    --max_seq_length {sft_data_args.max_seq_length} \\\n",
    "    --lr_scheduler_type {sft_training_args.lr_scheduler_type} \\\n",
    "    --learning_rate {sft_training_args.learning_rate} \\\n",
    "    --warmup_ratio {sft_training_args.warmup_ratio} \\\n",
    "    --weight_decay {sft_training_args.weight_decay} \\\n",
    "    --logging_strategy {sft_training_args.logging_strategy} \\\n",
    "    --logging_steps {sft_training_args.logging_steps} \\\n",
    "    --save_strategy {sft_training_args.save_strategy} \\\n",
    "    --save_total_limit {sft_training_args.save_total_limit} \\\n",
    "    --save_steps {sft_training_args.save_steps} \\\n",
    "    --evaluation_strategy {sft_training_args.evaluation_strategy} \\\n",
    "    --eval_steps {sft_training_args.eval_steps} \\\n",
    "    --gradient_accumulation_steps {sft_training_args.gradient_accumulation_steps} \\\n",
    "    --preprocessing_num_workers {sft_data_args.preprocessing_num_workers} \\\n",
    "    --overwrite_output_dir {sft_training_args.overwrite_output_dir} \\\n",
    "    --ddp_timeout {sft_training_args.ddp_timeout} \\\n",
    "    --logging_first_step {sft_training_args.logging_first_step} \\\n",
    "    --lora_rank {sft_training_args.lora_rank} \\\n",
    "    --lora_alpha {sft_training_args.lora_alpha} \\\n",
    "    --trainable {sft_training_args.trainable} \\\n",
    "    --modules_to_save {sft_training_args.modules_to_save} \\\n",
    "    --lora_dropout {sft_training_args.lora_dropout} \\\n",
    "    --torch_dtype {sft_model_args.torch_dtype} \\\n",
    "    --gradient_checkpointing {sft_training_args.gradient_checkpointing} \\\n",
    "    --ddp_find_unused_parameters {sft_training_args.ddp_find_unused_parameters} \\\n",
    "    --validation_file {sft_data_args.validation_file} \\\n",
    "    --validation_split_percentage {sft_data_args.validation_split_percentage} \\\n",
    "    --output_dir {sft_training_args.output_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: merge fine-tuned lora\n",
    "Having fine-tuned the lora model, we need to merge it into the original llama-alpaca model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp merge_llama_with_chinese_lora.py {script_dir}\n",
    "!python {script_dir}/merge_llama_with_chinese_lora.py --base_model {llama_tokenizer_dir} --tokenizer_path {output_dir}/llama-zh,{output_dir}/llama-alpaca-zh --lora_model {output_dir}/llama-zh/lora,{output_dir}/llama-alpaca-zh/lora --output_type huggingface --output_dir {output_dir}/merge-alpaca-hf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step6: try inference\n",
    "Now we can try to test our new trained model for some simple questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python {script_dir}/inference_hf.py --base_model {output_dir}/merge-alpaca-hf --with_prompt --interactive"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
